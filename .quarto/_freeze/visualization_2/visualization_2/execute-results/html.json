{
  "hash": "147abdcbf8cf9a3ee8d02fb9a48da1a6",
  "result": {
    "markdown": "---\ntitle: Visualization II\nformat:\n  html:\n    toc: true\n    toc-depth: 5\n    toc-location: right\n    code-fold: false\n    theme:\n      - cosmo\n      - cerulean\n    callout-icon: false\n---\n\n::: {.callout-note collapse=\"true\"}\n## Learning Outcomes\n- Use KDE for estimating density curve.\n- Using transformations to analyze the relationship between two variables.\n- Evalauting quality of a visualization based on visualization theory concepts.\n:::\n\n## Kernel Density Functions\n\n\n### KDE Mechanics\n\nIn the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we'll learn how to construct density curves using Kernel Density Estimation (KDE).\n\n#### Smoothing\n\nKernel Density Estimation involves a technique called **smoothing** - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\n\nMany of the visualizations we learned during the last lecture are examples of smoothing. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the larger properties of our distribution.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\nsns.rugplot(titanic['age'],height = 0.5);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-2-output-1.png){width=590 height=422}\n:::\n:::\n\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nsns.histplot(titanic['age']);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-3-output-1.png){width=593 height=422}\n:::\n:::\n\n\n#### Kernel Density Estimation\n\n**Kernel Density Estimation** is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\n1. Place a kernel at each data point\n2. Normalize kernels to have total area of 1 (across all kernels)\n3. Sum kernels together\n\nSuppose we have 5 data points: $[2.2, 2.8, 3.7, 5.3, 5.7]$. We wish to recreate the following Kernel Density Estimate:\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-4-output-1.png){width=606 height=404}\n:::\n:::\n\n\nLet's walk through each step to construct this density curve.\n\n##### Step 1 - Place a Kernel at Each Data Point\n\nTo begin generating a density curve, we need to choose a **kernel** and **bandwidth value ($\\alpha$)**. What are these exactly? A **kernel** is a density curve itself, and the **bandwidth ($\\alpha$)** is a measure of the kernel's width. Recall that a valid density has an area of 1.\n\nAt each of our 5 points (depicted in the rug plot on the left), we've placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We'll explore what these are in the next section.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Rugplot of Data**\n\n<img src=\"images/rugplot_ex.png\" alt='rugplot_ex' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Kernelized Data**\n\n<img src=\"images/kde_step_1.png\" alt='kde_step_1' width='360'>\n:::\n\n::::\n\n##### Step 2 - Normalize Kernels to Have Total Area of 1\n\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by $\\frac{1}{5}$.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Kernelized Data**\n\n<img src=\"images/kde_step_1.png\" alt='kde_step_1' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Normalized Kernels**\n\n<img src=\"images/kde_step_2.png\" alt='kde_step_2' width='345'>\n:::\n\n::::\n\n##### Step 3 - Sum Kernels Together\n\nOur kernel density estimate (KDE) is the sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Normalized Kernels**\n\n<img src=\"images/kde_step_2.png\" alt='kde_step_2' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Kernel Density Estimate**\n\n<img src=\"images/kde_step_3.png\" alt='kde_step_3' width='345'>\n:::\n\n::::\n\n### Kernel Functions and Bandwidth\n\n#### Kernels\n\nA **kernel** (for our purposes) is a valid density function. This means it:\n\n- Must be non-negative for all inputs.\n- Must integrate to 1.\n\n<img src= \"./images/kde_function.png\"></img>\n\nA general “KDE formula” function is given above.\n\n1. $K_{\\alpha}(x, xi)$ is the kernel centered on the observation `i`. \n    - Each kernel individually has area 1.\n    - x represents any number on the number line. It is the input to our function.\n2. $n$ is the number of observed data points that we have.\n    - We multiply by $\\frac{1}{n}$ so that the total area of the KDE is still 1.\n3. Each $x_i \\in \\{x_1, x_2, \\dots, x_n\\}$ represents an observed data point. \n    - These are what we use to create our KDE by summing multiple shifted kernels centered at these points.\n\n\\*$\\alpha$ (alpha) is the bandwidth or smoothing parameter. \n\n\n##### Gaussian Kernel\n\nThe most common kernel is the **Gaussian kernel**. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value $x_i$ with a standard deviation of $\\alpha$ (this is known as the **bandwidth** parameter).\n\n$K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2\\alpha^{2}}}$\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n```\n\n::: {.cell-output .cell-output-display}\n![The Gaussian kernel centered at 0 with bandwidth $\\alpha$ = 1.](visualization_2_files/figure-html/cell-5-output-1.png){width=579 height=404}\n:::\n:::\n\n\nIf you've taken a probability class, you'll recognize that the mean of this Gaussian kernel is $x_i$ and the standard deviation is $\\alpha$.  Increasing $\\alpha$ - equivalently, the bandwidth - smoothens the density curve. Larger values of $\\alpha$ are typically easier to understand; however, we begin to lose important distributional information. \n\nHere is how adjusting $\\alpha$ affects a distribution in some variable from an arbitrary dataset.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 0.1**\n\n<img src=\"images/gaussian_0.1.png\" alt='gaussian_0.1' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 1**\n\n<img src=\"images/gaussian_1.png\" alt='gaussian_1' width='345'>\n:::\n\n::::\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 2**\n\n<img src=\"images/gaussian_2.png\" alt='gaussian_2' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, $\\alpha$ = 10**\n\n<img src=\"images/gaussian_10.png\" alt='gaussian_10' width='345'>\n:::\n\n::::\n\n##### Boxcar Kernel\n\nAnother example of a kernel is the **Boxcar kernel**. The boxcar kernel assigns a uniform density to points within a \"window\" of the observation, and a density of 0 elsewhere. The equation below is a Boxcar kernel with the center at $x_i$ and the bandwidth of $\\alpha$.\n\n$K_a(x, x_i) = \\begin{cases}\n        \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\\n        0, & \\text{else }\n    \\end{cases}$\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n```\n\n::: {.cell-output .cell-output-display}\n![The Boxcar kernel centered at 0 with bandwidth $\\alpha$ = 1.](visualization_2_files/figure-html/cell-6-output-1.png){width=571 height=404}\n:::\n:::\n\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth $\\alpha$ = 1.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n<img src=\"images/gaussian_kernel.png\" alt='kde_step_3' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/boxcar_kernel.png\" alt='boxcar_kernel' width='350'>\n:::\n\n::::\n\n### Relationships Between Quantitative Variables\n\nUp until now, we've discussed how to visualize single-variable distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n\n#### Scatter Plots\n\n**Scatter plots** are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\n\nFor example, let's plot a scatter plot comparing the `Maternal Height` and `Birth Weight` colums, using both `matplotlib` and `seaborn`.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n# Matplotlib Example\nplt.scatter(births['Maternal Height'], births['Birth Weight'])\nplt.xlabel('Maternal Height')\nplt.ylabel('Birth Weight');\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-7-output-1.png){width=597 height=422}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Seaborn Example\nsns.scatterplot(data = births, x = 'Maternal Height', y = 'Birth Weight',\n                hue = 'Maternal Smoker')\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<AxesSubplot:xlabel='Maternal Height', ylabel='Birth Weight'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-8-output-2.png){width=597 height=422}\n:::\n:::\n\n\nThis is an example where color is used to add a third dimension to our plot. This is possible with the `hue` paramater in `seaborn`, which adds a categorical column encoding to an existing visualization. This way, we can look for relationships in `Maternal Height` and `Birth Weight` in both maternal smokers and non-smokers. If we wish to see the relationship's strength more clearly, we can use `sns.lmplot`.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsns.lmplot(data = births, x = 'Maternal Height', y = 'Birth Weight', \n           hue = 'Maternal Smoker', ci = False);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-9-output-1.png){width=584 height=468}\n:::\n:::\n\n\nWe can make out a weak, positive relationship in the mother's height and birth weight for both maternal smokers and non-smokers (the baseline is slightly lower in maternal smokers).\n\n### Overplotting\n\nAs you may have noticed, the scatterplots of `Maternal Height` vs. `Birth Weight` have many densely plotted areas. Many of the points are on top of one other! This makes it difficult to tell exactly how many babies are plotted in each the more densely populated regions of the graph. This can arise when the tools used for measuring data have low granularity, many different values are rounded to the same value, or if the ranges of the two variables differ greatly in scale.  \n\nWe can overcome this by introducing a small amount of uniform random noise to our data. This is called *jittering*. Let's see what happens when we introduce noise to the `Maternal Height`.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nbirths[\"Maternal Height (jittered)\"] = births[\"Maternal Height\"] + np.random.uniform(-0.2, 0.2, len(births))\nsns.lmplot(data = births, x = 'Maternal Height (jittered)', y = 'Birth Weight', \n           hue = 'Maternal Smoker', ci = False);\n```\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-10-output-1.png){width=584 height=468}\n:::\n:::\n\n\nThis plot more clearly shows that most of the data is clustered tightly around the point (62.5,120) and gradually becomes more loose further away from the center. It is much easier for us and others to see how the data is distributed. In conclusion, *jittering* helps us better understand our own data (Goal 1) and communicate results to others (Goal 2).  \n\n\n#### Hex Plots and Contour Plots\n\nUnfortunately, our scatter plots above suffered from overplotting, which made them hard to interpret. And with a large number of points, jittering is unlikely to resolve the issue. Instead, we can look to hex plots and contour plots.\n\n**Hex Plots** can be thought of as a two dimensional histogram that shows the joint distribution between two variables. This is particularly useful working with very dense data. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nsns.jointplot(data = births, x = 'Maternal Pregnancy Weight', \n              y = 'Birth Weight', kind = 'hex')\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n<seaborn.axisgrid.JointGrid at 0x1a11ca03ca0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-11-output-2.png){width=570 height=564}\n:::\n:::\n\n\nThe axes are evidently binned into hexagons, which makes the linear relationship easier to decipher. Darker regions generally indicate a higher density of points.\n\nOn the other hand, **contour plots** are two dimensional versions of density curves with marginal distributions of each variable on the axes. We've used very similar code here to generate our contour plots, with the addition of the `kind = 'kde'` and `fill = True` arguments.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nsns.jointplot(data = births, x = 'Maternal Height', y = 'Birth Weight',\\\n              kind = 'kde', fill = True)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<seaborn.axisgrid.JointGrid at 0x1a11e925190>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](visualization_2_files/figure-html/cell-12-output-2.png){width=570 height=564}\n:::\n:::\n\n\n## Transformations\n\nThese last two lectures have covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\n\nMuch of this was done to uncover insights in data, which will prove necessary for the modeling process. A strong graphical correlation between two variables hinted an underlying relationship that has reason for further study. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\n\n**Transformations** are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that \"transform\" their range of possible values and highlight some previously hidden associations between data.\n\n#### Transforming a Distribution\n\nWhen a distribution has a large dynamic range, it can be useful to take the logarithm of the data. For example, computing the logarithm of the ticket prices on the Titanic reduces skeweness and yields a distribution that is more \"spread\" across the x-axis. While it makes individual observations harder to interpret, the distribution is more favorable for subsequent analysis.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/untransformed_titanic.png\" alt='untransformed_titanic' width='400'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/transformed_titanic.png\" alt='transformed_titanic' width='400'>\n:::\n\n::::\n\n#### Linearizing a Relationship\n\nTransformations are perhaps most useful to **linearize a relationship** between variables. If we find a transformation to make a scatter plot of two variables linear, we can \"backtrack\" to find the exact relationship between the variables. Linear relationships are particularly simple to interpret, and we'll be doing a lot of linear modeling in Data 100 - starting next week!\n\nSay we want to understand the relationship between healthcare and life expectancy. Intuitively there should be a positive correlation, but upon plotting values from a dataset, we find a non-linear relationship that is somewhat hard to understand. However, applying a logarithmic transformation to both variables - healthcare and life expectancy - results in a scatter plot with a linear trend that we can interpret.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/untransformed_healthcare_lifeexp.png\" alt='untransformed_titanic' width='400'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/transformed_healthcare_lifeexp.png\" alt='transformed_titanic' width='400'>\n:::\n\n::::\n\nHow can we find the relationship between the original variables? We know that taking a log of both axes gave us a linear relationship, so we can say (roughly) that \n\n$$\\log y= a\\times\\log x + b$$\n\nSolving for $y$ implies a **power** relationship in the original plot.\n\n$$y= e^{a\\times\\log x + b}$$\n$$y= Ce^{a\\times\\log x}$$\n$$y= Cx^{a}$$\n\nHow did we know that taking the logarithm of both sides would result in a linear relationship? The **Tukey-Mosteller Bulge Diagram** is helpful here. We can use the direction of the buldge in our original data to find the appropriate transformations that will linearize the relationship. These transformations are found on axes that are nearest to the buldge. The buldge in our earlier example lay in Quadrant 2, so the transformations $\\log x$, $\\sqrt x$, $y^{2}$, or $y^{3}$ are possible contenders. It's important to note that this diagram is not perfect, and some transformations will work better than others. In our case, $\\log x$ and $\\log y$ (found in Quadrant 3) were the best.\n\n<img src=\"images/tukey_mosteller.png\" alt='tukey_mosteller' width='400'>\n\n#### Additional Remarks\n\nVisualization requires a lot of thought!\n- There are many tools for visualizing distributions.\n    - Distribution of a single variable:\n        1. rug plot\n        2. histogram\n        3. density plot\n        4. box plot\n        5. violin plot\n    - Joint distribution of two quantitative variables: \n        1. scatter plot\n        2. hex plot\n        3. contour plot.\n\nThis class primarily uses `seaborn` and `matplotlib`, but `Pandas` also has basic built-in plotting methods. Many other visualization libraries exist, and `plotly` is one of them.\n- `plotly` creates very easily creates interactive plots.\n- `plotly` will occasionally appear in lecture code, labs, and assignments! \n\nNext, we’ll go deeper into the theory behind visualization.\n\n\n## Visualization Theory\n\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We'll discuss the abstract nature of visualizations and analyze how they convey information.\n\nRemember, we had two goals for visualizing data. This section is particularly important in:\n\n1. Helping us understand the data and results\n2. Communicating our results and conclusions with others\n\n### Information Channels\n\nVisualizations are able to convey information through various encodings. In the remainder of this lecture, we'll look at the use of color, scale, and depth, to name a few. \n\n#### Encodings in Rugplots\n\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n\n<img src=\"images/rugplot_encoding.png\" alt='rugplot_encoding' width='600'> \n\n#### Multi-Dimensional Encodings\n\nEncodings are also useful for representing multi-dimensional data. Notice how the following visual highlights four distinct \"dimensions\" of data:\n\n- X-axis\n- Y-axis\n- Area\n- Color\n\n<img src=\"images/mutli_dim_encodings.png\" alt='multi_dim_encoding' width='400'> \n\nThe human visual perception sytem is only capable of visualizing data in a three-dimensional plane, but as you've seen, we can encode many more channels of information.\n\n### Harnessing the Axes\n\n#### Consider Scale of the Data\n\nHowever, we should be careful to not misrepresent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n\n<img src=\"images/wrong_scale_viz.png\" alt='wrong_scale_viz' width='350'> \n\nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not be intersecting.\n\nWhen there is a large difference in the magnitude of the data, it's advised to analyze percentages instead of counts. The following diagrams correctly display the trends in cancer screening and abortion rates. \n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/good_viz_scale_1.png\" alt='good_viz_scale_1' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/good_viz_scale_2.png\" alt='good_viz_scale_2' width='345'>\n:::\n\n::::\n\n#### Reveal the Data\n\nGreat visualizations not only consider the scale of the data, but also utilize the axes in a way that best conveys information. For example, data scientists commonly set certain axes limits to highlight parts of the visualization they are most interested in.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/unrevealed_viz.png\" alt='unrevealed_viz' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/revealed_viz.png\" alt='revealed_viz' width='345'>\n:::\n\n::::\n\nThe visualization on the right captures the trend in coronavirus cases during the month March in 2020. From only looking at the visualization on the left, a viewer may incorrectly believe that coronavirus began to skyrocket on March 4^th^, 2020. However, the second illustration tells a different story - cases rose closer to March 21^th^, 2020.\n\n### Harnessing Color\n\nColor is another important feature in visualizations that does more than what meets the eye. \n\nLast lecture, we used color to encode a categorical variable in our scatter plot. In this section, we will discuss uses of color in novel visualizations like colormaps and heatmaps.\n\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors which work together is evidently a challenging task!\n\n#### Colormaps\n\nColormaps are mappings from pixel data to color values, and they're often used to highlight distinct parts of an image. Let's investigate a few properties of colormaps.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n**Jet Colormap**\n<img src=\"images/jet_colormap.png\" alt='jet_colormap' width='250'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Viridis Colormap**\n<img src=\"images/viridis_colormap.png\" alt='viridis_colormap' width='250'>\n:::\n\n::::\n\nThe jet colormap is infamous for being misleading. While it seems more vibrant than viridis, the aggressive colors poorly encode numerical data. To understand why, let's analyze the following images.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/jet_four_by_four.png\" alt='four_by_four_colormap' width='300'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/jet_3_images.png\" alt='jet_3_colormap' width='250'>\n:::\n\n::::\n\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low intensity. These include the jet colormap (row a) and grayscale (row b). Notice how the grayscale images do the best job in smoothly transitioning between pixel data. The jet colormap is the worst at this - the four images in row (a) look like a conglomeration of individual colors. \n\nThe difference is also evident in the images labeled (a) and (b) on the left side. The grayscale image is better at preserving finer detail in the vertical line strokes. Additionally, grayscale is preferred in x-ray scans for being more neutral. The intensity of dark red color in the jet colormap is frightening and indicates something is wrong.\n\nWhy is the jet colormap so much worse? The answer lies in how its color composition is percieved to the human eye.\n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n**Jet Colormap Perception**\n<img src=\"images/jet_perceptually_uniform.png\" alt='jet_perceptually_uniform' width='300'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Viridis Colormap Perception**\n<img src=\"images/viridis_perceptually_uniform.png\" alt='viridis_perceptually_uniform' width='336'>\n:::\n\n::::\n\nThe jet colormap is largely misleading because it is not perceptually uniform. **Perceptually uniform colormaps** have the property that if the pixel data goes from 0.1 to 0.2, the perceptual change is the same as when the data goes from 0.8 to 0.9.\n\nNotice how the said uniformity is present within the linear trend displayed in the viridis colormap. On the other hand, the jet colormap is largely non-linear - this is precisely why it's considered a worse colormap.\n\n### Harnessing Markings\n\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these \"markings\".\n\n<img src=\"images/markings_viz.png\" alt='markings_viz' width='300'>\n\nThere are a few key takeaways from this diagram\n\n- Lengths are easy to discern. Don't use plots with jiggled baselines - keep everything axis-aligned.\n- Avoid pie charts! Angle judgements are inaccurate.\n- Areas and volumes are hard to distinguish (area charts, word clouds, etc)\n\n### Harnessing Conditioning\n\nConditioning is the process of comparing data that belong to seperate groups. We've seen this before in overlayed distributions, side-by-side box-plots, and scatter plots with categorical encodings. Here, we'll introduce terminology that formalizes these examples.\n\nConsider an example where we want to analyze income earnings for male and females with varying levels of education. There are multiple ways to compare this data. \n\n:::: {.columns}\n\n\n::: {.column width=\"30%\"}\n<img src=\"images/male_female_earnings_barplot.png\" alt='jet_perceptually_uniform' width='360'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/male_female_earnings_scatterplot.png\" alt='viridis_perceptually_uniform' width='340'>\n:::\n\n::::\n\nThe barplot is an example of **juxtaposition**: placing multiple plots side by side, with the same scale. The scatter plot is an example of **superposition**: placing multiple density curves, scatter plots on top of each other. \n\nWhich is better depends on the problem at hand. Here, superposition makes the precise wage difference very clear from a quick glance. But many sophisticated plots convey information that favors the use of juxtaposition. Below is one example.\n\n<img src=\"images/small_multiples.png\" alt='small_multiples' width='600'>\n                                                        \n\n### Harnessing Context\n\nThe last component to a great visualization is perhaps the most critical - the use of context. Adding informative titles, axis labels, and descriptive captions are all best practices that we've heard repeatedly in Data 8.\n\nA publication-ready plot (and every Data 100 plot) needs:\n\n- Informative title (takeaway, not description)\n- Axis labels\n- Reference lines, markers, etc\n- Legends, if appropriate\n- Captions that describe data\n\nCaptions should be:\n\n- Comprehensive and self-contained\n- Describe what has been graphed\n- Draw attention to important features\n- Describe conclusions drawn from graphs\n\n",
    "supporting": [
      "visualization_2_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}