{
  "hash": "c550cdf627dea949863a8da237ebecb7",
  "result": {
    "markdown": "---\ntitle: Introduction to Modeling\nexecute:\n  echo: true\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    toc: true\n    toc-title: Introduction to Modeling\n    page-layout: full\n    theme:\n      - cosmo\n      - cerulean\n    callout-icon: false\n---\n\n::: {.callout-note collapse=\"true\"}\n## Learning Outcomes\n* Understand what models are and how to carry out the four-step modeling process\n* Define the concept of loss and gain familiarity with L1 and L2 loss\n* Fit a model using minimization techniques\n:::\n\nUp until this point in the semester, we've focused on analyzing datasets. We've looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.\n\nThis lecture marks a shift in focus. We will move away from examining datasets to actually *using* our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some prediction about the world around us. In this lecture, we'll introduce the conceptual framework for setting up a modeling task. In the next few lectures, we'll put this framework into practice by implementing several kinds of models.\n\n## What is a Model?\n\nA model is an **idealized representation** of a system. A system is a set of principles or procedures according to which something functions. We live in a world full of systems: the procedure of turning on a light happens according to a specific set of rules dictating the flow of electricity. The truth behind how any event occurs are usually complex, and many times the specifics are unknown. The workings of the world can be viewed is its own giant procedure. Models seek to simplify the world and distill them it into workable pieces.  \n\nExample:\nWe model the fall of an object on Earth as subject to a constant acceleration of $9.81 \\frac{m}{s^2}$  due to gravity.\n\n- While this describes the behavior of our system, it is merely an approximation.\n- It doesn’t account for the effects of air resistance, local variations in gravity, etc.\n- In practice, it’s accurate enough to be useful!\n\n### Reasons for building models\n\nOften times, (1) we care about creating models that are simple and interpretable, allowing us to understand what the relationships between our variables are. Other times, (2) we care more about making extremely accurate predictions, at the cost of having an uninterpretable model. These are sometimes called black-box models, and are common in fields like deep learning.\n\n\n1. To understand complex phenomena occurring in the world we live in.\n\n    - What factors play a role in the growth of COVID-19?\n    - How do an object’s velocity and acceleration impact how far it travels? (Physics: $d = d_0 + vt + \\frac{1}{2}at^2$) \n\n2. To make accurate predictions about unseen data.\n\n    - Can we predict if an email is spam or not?\n    - Can we generate a one-sentence summary of this 10-page long article?  \n\n\n### Common Types of Models\n\nIn general, models can be split into two categories:\n\nNote: These specific models are not in the scope of Data 100 and exist to serve as motivation.\n\n1. Deterministic physical (mechanistic) models: Laws that govern how the world works.\n\n    - [Kepler's Third Law of Planetary Motion (1619)](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion#Third_law): The ratio of the square of an object's orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.\n        - $T^2 \\propto R^3$\n\n    - [Newton's Laws: motion and gravitation (1687)](https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion): Newton’s second law of motion models the relationship between the mass of an object and the force required to accelerate it.\n        - $F = ma$\n        - $F_g = G \\frac{m_1 m_2}{r^2}$\n\n2. Probabilistic models: models that attempt to understand how random processes evolve. These are more general and can be used describe many phenomena in the real world. These models commonly make simplifying assumption about the nature of the world.\n\n    - [Poisson Process models](https://en.wikipedia.org/wiki/Poisson_point_process): Used to model random events that can happen with some probability at any point in time and are strictly increasing in count, such as the arrival of customers at a store. \n\n\n## Simple Linear Regression \n\nThe **regression line** is the unique straight line that minimizes the **mean squared error** of estimation among all straight lines. As with any straight line, it can be defined by a slope and a y-intercept:\n\n- slope: $r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}$\n- y-intercept: $\\text{average of y} - \\text{slope}\\cdot\\text{average of x}$\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Set random seed for consistency \nnp.random.seed(43)\nplt.style.use('default') \n\n#Generate random noise for plotting\nx = np.linspace(-3, 3, 100)\ny = x * 0.5 - 1 + np.random.randn(100) * 0.3\n\n#plot regression line\nsns.regplot(x=x,y=y);\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_to_modeling_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n### Definitions\n\nFor a random variable x:\n\n - Mean: $\\bar{x}$ \n - Standard Deviation: $\\sigma_x$\n - Predicted value: $\\hat{x}$\n\n#### Standard Units\nA random variable is represented in standard units if the following are true:\n\n1. 0 in standard units is the mean ($\\bar{x}$) in the original variable's units.\n2. An increase of 1 standard unit is an increase of 1 standard deviation($\\sigma_x$) in the original variable's units\n\n#### Correlation\n\nThe correlation ($r$) is the average of the product of $x$ and $y$, both measured in *standard units*. Correlation measures the strength of a linear association between two variables.\n\n\n1. $r = \\frac{1}{n} \\sum_1^n (\\frac{x_i - \\bar{x}}{\\sigma_x})(\\frac{y_i - \\bar{y}}{\\sigma_y})$\n2. Correlations are between -1 and 1: $|r| < 1$\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=2}\n``` {.python .cell-code}\ndef plot_and_get_corr(ax, x, y, title):\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.scatter(x, y, alpha = 0.73)\n    r = np.corrcoef(x, y)[0, 1]\n    ax.set_title(title + \" (corr: {})\".format(r.round(2)))\n    return r\n\nfig, axs = plt.subplots(2, 2, figsize = (10, 10))\n\n# Just noise\nx1, y1 = np.random.randn(2, 100)\ncorr1 = plot_and_get_corr(axs[0, 0], x1, y1, title = \"noise\")\n\n# Strong linear\nx2 = np.linspace(-3, 3, 100)\ny2 = x2 * 0.5 - 1 + np.random.randn(100) * 0.3\ncorr2 = plot_and_get_corr(axs[0, 1], x2, y2, title = \"strong linear\")\n\n# Unequal spread\nx3 = np.linspace(-3, 3, 100)\ny3 = - x3/3 + np.random.randn(100)*(x3)/2.5\ncorr3 = plot_and_get_corr(axs[1, 0], x3, y3, title = \"strong linear\")\nextent = axs[1, 0].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n\n# Strong non-linear\nx4 = np.linspace(-3, 3, 100)\ny4 = 2*np.sin(x3 - 1.5) + np.random.randn(100) * 0.3\ncorr4 = plot_and_get_corr(axs[1, 1], x4, y4, title = \"strong non-linear\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_to_modeling_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n### Alternate Form\nWhen the variables $y$ and $x$ are measured in *standard units*, the regression line for predicting $y$ based on $x$ has slope $r$ and passes through the origin.\n\n $$\\hat{y}_{su} = r \\cdot x_{su}$$\n\n\n<div style=\"text-align: center;\"><img src = \"./images/reg_line_su.png\"></img></div>\n\n\n- In the original units, this becomes\n\n$$\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}$$\n\n<div style=\"text-align: center;\"><img src = \"./images/reg_line_ou.png\"></img></div>\n\n### Derivation\n\nStarting from the top, we have our claimed form of the regression line and we want to show that its equivalent to the optimal linear regression line: $\\hat{y} = \\hat{a} + \\hat{b}x$\n\nRecall: \n\n- $\\hat{b}$: $r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}$\n- $\\hat{a}$: $\\text{average of y} - \\text{slope}\\cdot\\text{average of x}$\n\n:::{.callout}\n\nProof: \n\n$$\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}$$\n\nMultiply by $\\sigma_y$ and add $\\bar{y}$ on both sides.\n\n$$\\hat{y} = \\sigma_y \\cdot r \\cdot \\frac{x - \\bar{x}}{\\sigma_x} + \\bar{y}$$\n\nDistribute coefficient $\\sigma_{y}\\cdot r$ to the $\\frac{x - \\bar{x}}{\\sigma_x}$ term\n\n$$\\hat{y} = (\\frac{r\\sigma_y}{\\sigma_x} ) \\cdot x + (\\bar{y} - (\\frac{r\\sigma_y}{\\sigma_x} ) \\bar{x})$$\n\nWe now see that we have a line that matches our claim:\n\n- slope: $r\\cdot\\frac{\\text{SD of x}}{\\text{SD of y}} = r\\cdot\\frac{\\sigma_x}{\\sigma_y}$\n- intercept: $\\bar{y} - \\text{slope}\\cdot x$\n\n:::\n\n\n## The Modeling Process\n\nAt a high level, a model is some way of representing a system. In Data 100, we'll treat a model as some mathematical rule we use to describe the relationship between variables. \n\nWhat variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset $\\mathbb{D}$:\n\n$$\\mathbb{D} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$$\n\nEach pair of values $(x_i, y_i)$ represents a datapoint. In a modeling setting, we call these **observations**. $y_i$ is the dependent variable we are trying to model, also called an **output** or **response**. $x_i$ is the independent variable inputted into the model to make predictions, also known as a **feature**. \n\nOur goal in modeling is to use the observed data $\\mathbb{D}$ to predict the output variable $y_i$. We denote each prediction as $\\hat{y}_i$ (read: \"y hat sub i\").\n\nHow do we generate these predictions? Some examples of models we'll encounter in the next few lectures are given below:\n\n$$\\hat{y}_i = \\theta$$\n$$\\hat{y}_i = \\theta_0 + \\theta_1 x_i$$\n\nThe examples above are known as **parametric models**. They relate the collected data, $x_i$, to the prediction we make, $\\hat{y}_i$. A few parameters ($\\theta$, $\\theta_0$, $\\theta_1$) are used to describe the relationship between $x_i$ and $\\hat{y}_i$.\n\nNotice that we don't immediately know the values of these parameters. While the features, $x_i$, are taken from our observed data, we need to decide what values to give $\\theta$, $\\theta_0$, and $\\theta_1$ ourselves. This is the heart of parametric modeling: *what parameter values should we choose so our model makes the best possible predictions?*\n\nTo choose our model parameters, we'll work through the **modeling process**. \n\n1. Choose a model: how should we represent the world?\n2. Choose a loss function: how do we quantify prediction error?\n3. Fit the model: how do we choose the best parameters of our model given our data?\n4. Evaluate model performance: how do we evaluate whether this process gave rise to a good model?\n\n## Choosing a Model\n\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, $x_i$, and predictions $\\hat{y}_i$. \n\nIn [Data 8](https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html), you learned about the **Simple Linear Regression (SLR) model**. You learned that the model takes the form:\n$$\\hat{y}_i = a + bx_i$$\n\nIn Data 100, we'll use slightly different notation: we will replace $a$ with $\\theta_0$ and $b$ with $\\theta_1$. This will allow us to use the same notation when we explore more complex models later on in the course.\n\n$$\\hat{y}_i = \\theta_0 + \\theta_1 x_i$$\n\nThe parameters of the SLR model are $\\theta_0$, also called the intercept term, and $\\theta_1$, also called the slope term. To create an effective model, we want to choose values for $\\theta_0$ and $\\theta_1$ that most accurately predict the output variable. The \"best\" fitting model parameters are given the special names $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ – they are the specific parameter values that allow our model to generate the best possible predictions.\n\nIn Data 8, you learned that the best SLR model parameters are:\n$$\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} \\qquad \\qquad \\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}$$\n\nA quick reminder on notation:\n\n* $\\bar{y}$ and $\\bar{x}$ indicate the mean value of $y$ and $x$, respectively\n* $\\sigma_y$ and $\\sigma_x$ indicate the standard deviations of $y$ and $x$\n* $r$ is the [correlation coefficient](https://inferentialthinking.com/chapters/15/1/Correlation.html#the-correlation-coefficient), defined as the average of the product of $x$ and $y$ measured in standard units: $\\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y})$\n\nIn Data 100, we want to understand *how* to derive these best model coefficients. To do so, we'll introduce the concept of a loss function.\n\n## Choosing a Loss Function\n\nWe've talked about the idea of creating the \"best\" possible predictions. This begs the question: how do we decide how \"good\" or \"bad\" our model's predictions are?\n\nA **loss function** characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, $L(y, \\hat{y})$, quantifies how \"far off\" a single prediction by our model is from a true, observed value in our collected data. \n\nThe choice of loss function for a particular model depends on the modeling task at hand. Regardless of the specific function used, a loss function should follow two basic principles:\n\n* If the prediction $\\hat{y}_i$ is *close* to the actual value $y_i$, loss should be low\n* If the prediction $\\hat{y}_i$ is *far* from the actual value $y_i$, loss should be high\n\nTwo common choices of loss function are squared loss and absolute loss. \n\n**Squared loss**, also known as **L2 loss**, computes loss as the square of the difference between the observed $y_i$ and predicted $\\hat{y}_i$:\n$$L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2$$\n\n**Absolute loss**, also known as **L1 loss**, computes loss as the absolute difference between the observed $y_i$ and predicted $\\hat{y}_i$:\n$$L(y_i, \\hat{y}_i) = |y_i - \\hat{y}_i|$$\n\nL1 and L2 loss give us a tool for quantifying our model's performance on a single datapoint. This is a good start, but ideally we want to understand how our model performs across our *entire* dataset. A natural way to do this is to compute the average loss across all datapoints in the dataset. This is known as the **cost function**, $\\hat{R}(\\theta)$:\n$$\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)$$\n\nThe cost function has many names in statistics literature. You may also encounter the terms:\n\n* Empirical risk (this is why we give the cost function the name $R$)\n* Error function\n* Average loss\n\nWe can substitute our L1 and L2 loss into the cost function definition. The **Mean Squared Error (MSE)** is the average squared loss across a dataset:\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n\nThe **Mean Absolute Error (MAE)** is the average absolute loss across a dataset:\n$$\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n\n## Fitting the Model\n\nNow that we've established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model's cost on our dataset. This process is called fitting the model.\n\nWe know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the **objective function** (our objective is to find its minimum).\n\nTo find the optimal model parameter, we:\n\n1. Take the derivative of the cost function with respect to that parameter\n2. Set the derivative equal to 0\n3. Solve for the parameter\n\nWe repeat this process for each parameter present in the model. For now, we'll disregard the second derivative condition. \n\nTo help us make sense of this process, let's put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!\n\nStep 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, $\\hat{y}_i = \\theta_0+\\theta_1 x_i$, into the definition of MSE above and differentiate with respect to $\\theta_0$ and $\\theta_1$.\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)^2$$\n\n$$\\frac{\\partial}{\\partial \\theta_0} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i$$\n\n$$\\frac{\\partial}{\\partial \\theta_1} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i$$\n\nStep 2: set the derivatives equal to 0. After simplifying terms, this produces two **estimating equations**. The best set of model parameters $(\\theta_0, \\theta_1)$ *must* satisfy these two optimality conditions.\n$$0 = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{y}_i = 0$$\n$$0 = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_i = 0$$\n\nStep 3: solve the estimating equations to compute estimates for $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$.\n\nTaking the first equation gives the estimate of $\\hat{\\theta}_0$:\n$$\n\\frac{1}{n} \\sum_{i=1}^n y_i - \\hat{\\theta}_0 - \\hat{\\theta}_1 x_i = 0 \\\\\n\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{\\theta}_0 - \\hat{\\theta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) = 0 \\\\\n\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1 \\bar{x}\n$$\n\nWith a bit more maneuvering, the second equation gives the estimate of $\\hat{\\theta}_1$. Start by multiplying the first estimating equation by $\\bar{x}$, then subtracting the result from the second estimating equation.\n$$\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\bar{x} = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)(x_i - \\bar{x}) = 0\n$$\n\nNext, plug in $\\hat{y}_i = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_i = \\bar{y} + \\hat{\\theta}_1(x_i - \\bar{x})$:\n$$\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{\\theta}_1(x - \\bar{x}))(x_i - \\bar{x}) = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) = \\hat{\\theta}_1 \\times \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n$$\n\nBy using the definition of correlation $\\left(r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)$ and standard deviation $\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)$, we can conclude:\n$$r \\sigma_x \\sigma_y = \\hat{\\theta}_1 \\times \\sigma_x^2$$\n$$\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}$$\n\nJust as was given in Data 8! \n\nRemember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can *always* follow these three steps to fit the model.\n\n## Evaluating Performance\n\nAt this point, we've:\n\n* Defined our model\n* Defined our loss function\n* Fit the model to identify the best model parameters\n\nNow, what are some ways to determine if our model was a good fit to our data? We will delve into this more in the next chapter, but there are three main ways for evaluating a model.\n\n1. Statistics:\n\n    * Plot original data\n    * Compute column means\n    * Compute standard deviations\n    * If we want to fit a linear model, compute correlation (r)\n\n2. Performance metrics:\n\n    * Root Mean Square Error (RMSE). It is the square root of MSE, which is the average loss that we've been minimizing to determine optimal model parameters. \n    * RMSE is in the same units as $y$.\n    * A lower RMSE indicates more \"accurate\" predictions (lower \"average loss\" across data)\n\n3. Visualization:\n\n    * Look at a residual plot of $e_i = y_i - \\hat{y_i}$ to visualize the difference between actual and predicted $y$ values.\n\n",
    "supporting": [
      "intro_to_modeling_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}