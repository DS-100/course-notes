{"title":"Visualization II","markdown":{"yaml":{"title":"Visualization II","format":{"html":{"toc":true,"toc-depth":5,"toc-location":"right","code-fold":false,"theme":["cosmo","cerulean"],"callout-icon":false}},"jupyter":"python3"},"headingText":"Kernel Density Functions","containsRefs":false,"markdown":"\n\n\n### KDE Mechanics\n\nIn the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we'll learn how to construct density curves using Kernel Density Estimation.\n\n#### Smoothing\n\nKernel Density Estimation involves a technique called **smoothing** - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\n\nMany of the visualizations we learned during the last lecture are examples of this. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the patterns in our distribution.\n\n#### Kernel Density Estimation\n\n**Kernel Density Estimation** is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\n1. Place a kernel at each data point\n2. Normalize kernels to have total area of 1\n3. Sum kernels together\n\nSuppose we have 5 data points: $[2.2, 2.8, 3.7, 5.3, 5.7]$. We wish to recreate the following Kernel Density Estimate:\n\n```{python}\n#| code-fold: true\n\nimport seaborn as sns\n\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n```\n\nLet's walk through each step to construct this density curve.\n\n##### Step 1 - Place a Kernel at Each Data Point\n\nTo begin generating a density curve, we need to choose a **kernel** and **bandwidth value**. What are these exactly? A kernel is a density curve itself, and the bandwidth is a measure of the kernel's width. Recall that a valid density has an area of 1.\n\nAt each of our 5 points (depicted in the rug plot on the left), we've placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We'll explore what these are in the next section.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Rugplot of Data**\n\n<img src=\"images/rugplot_ex.png\" alt='rugplot_ex' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Kernelized Data**\n\n<img src=\"images/kde_step_1.png\" alt='kde_step_1' width='360'>\n:::\n\n::::\n\n##### Step 2 - Normalize Kernels to Have Total Area of 1\n\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by $\\frac{1}{5}$.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Kernelized Data**\n\n<img src=\"images/kde_step_1.png\" alt='kde_step_1' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Normalized Kernels**\n\n<img src=\"images/kde_step_2.png\" alt='kde_step_2' width='345'>\n:::\n\n::::\n\n##### Step 3 - Sum Kernels Together\n\nOur kernel density estimate (KDE) is the vertical sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Normalized Kernels**\n\n<img src=\"images/kde_step_2.png\" alt='kde_step_2' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Kernel Density Estimate**\n\n<img src=\"images/kde_step_3.png\" alt='kde_step_3' width='345'>\n:::\n\n::::\n\n### Kernel Functions and Bandwidth\n\n#### Kernels\n\nA **kernel** (for our purposes) is a valid density function. This means it:\n\n- Must be non-negative for all inputs.\n- Must integrate to 1.\n\n##### Gaussian Kernel\n\nThe most common kernel is the **Gaussian kernel**. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value $x_i$ with a standard deviation of $\\alpha$ (this is known as the **bandwidth** parameter).\n\n$K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2a^{2}}}$$\n\n```{python}\n#| code-fold: true\n#| fig-cap: The Gaussian kernel centered at 0 with bandwidth $\\alpha$ = 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n```\n\nIf you've taken a probability class, you'll recognize that the mean of this Gaussian kernel is $x_i$ and the standard deviation is $\\alpha$.  Increasing $\\alpha$ - equivalently, the bandwidth - smoothens the density curve. Larger values of $\\alpha$ are typically easier to understand; however, we begin to lose important distributional information as $\\alpha$ increases. \n\nHere is how adjusting $\\alpha$ affects a distribution in some variable from an arbitrary dataset.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, Alpha = 0.1**\n\n<img src=\"images/gaussian_0.1.png\" alt='gaussian_0.1' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, Alpha = 1**\n\n<img src=\"images/gaussian_1.png\" alt='gaussian_1' width='345'>\n:::\n\n::::\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, Alpha = 2**\n\n<img src=\"images/gaussian_2.png\" alt='gaussian_2' width='345'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n**Gaussian Kernel, Alpha = 10**\n\n<img src=\"images/gaussian_10.png\" alt='gaussian_10' width='345'>\n:::\n\n::::\n\n##### Boxcar Kernel\n\nAnother example of a kernel is the **Boxcar kernel**. The boxcar kernel assigns a uniform density to points within a \"window\" of the observation, and a density of 0 elsewhere.\n\n$K_a(x, x_i) = \\begin{cases}\n        \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\\n        0, & \\text{else }\n    \\end{cases}$\n\n```{python}\n#| code-fold: true\n#| fig-cap: The Boxcar kernel centered at 0 with bandwidth $\\alpha$ = 1.\n\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n```\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth $\\alpha$ = 1.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n<img src=\"images/gaussian_kernel.png\" alt='kde_step_3' width='350'>\n:::\n\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"30%\"}\n<img src=\"images/boxcar_kernel.png\" alt='boxcar_kernel' width='350'>\n:::\n\n::::\n\n## Visualization Theory\n\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We'll discuss the abstract nature of visualizations, which will help us make informed decisions to construct them.\n\nRemember, we had two goals for visualizing data. This section is particularly important in motivating\n\n1. Help our understanding of the data and results\n2. Communicating our results and conclusions with others\n\n### Information Channels\n\nThere are various channels of information in visualizations - these include encodings, color, and scale, to name a few. In constructing good visuals, we should utilize these channels to convey information that answers our questions.\n\nFor example, we learned a few ways to picture a distribution: rugplots, KDEs, and histograms. Neither is strictle better than any other; they all convey varying levels of detail, and some may be more advantageous depending on the application.\n\n#### Encodings in Rugplots\n\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize encodings in line thickness to convey frequency. Consider the following diagram:\n\n<img src=\"images/rugplot_encoding.png\" alt='rugplot_encoding' width='600'> \n\n#### Multi-Dimensional Data\n\nEncodings are useful in representing complex \n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":5,"css":["../styles.css"],"output-file":"visualization_2.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":["cosmo"],"callout-icon":false,"title":"Visualization II","jupyter":"python3","toc-location":"right"},"extensions":{"book":{"multiFile":true}}}}}