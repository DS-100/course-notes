---
title: Logistic Regression I
format:
  html:
    toc: true
    toc-depth: 5
    toc-location: right
    code-fold: false
    theme:
      - cosmo
      - cerulean
    callout-icon: false
jupyter: python3
---

Up until now, we've focused on modeling regression problems - that is, predicting a *numerical* quantity from a given dataset. We explored the following workflow in the context of linear regression:

1. Choose a model
2. Choose a loss function
3. Fit the model
4. Evaluate model performance

In this lecture, we will look at an entirely new modeling problem in **classification**, which involves predicting a *categorical* variable. We will discuss how to modify the steps in our existing modeling framework to meet the needs of this new machine learning problem.

## Regression vs. Classification

You may have seen this diagram which outlines the taxonomy of machine learning.

![](images/ml_taxonomy.png) 

<br /> Regression and classification are both **supervised learning** problems, meaning they consist of models that learn from data where the $y$ observations are known. You've likely seen examples of classification models before - for example, the k-nearest neighbors algorithm from Data 8. This lecture will focus on another such model known as **Logistic Regression**. 

More generally, a classification problem aims to *categorize* data. Today, we will focus on **binary classification**, a classification problem where data may only belong to one of two groups. One such example is the result of a coin toss. Many of the principles we'll learn today can extended to three or more groups - called **multiclass classification**. In the next few lectures, we will learn about models designed specifically for these problems.

## Intuition: The Coin Flip

To build some intuition for logistic regression, let's look at an introductory example to classification: the coin flip. Suppose we observe some outcomes of a coin flip (1 = Heads, 0 = Tails).

```{python}
flips = [0, 0, 1, 1, 1, 1, 0, 0, 0, 0]
flips
```

For the next flip, would you predict heads or tails?

A reasonable model is to assume all flips are IID (independent and identitically distributed). In other words, each flip has the same probability of returning a 1 (or heads). Let's define a parameter $\theta$, the probability that the next flip is a heads. We will use this parameter to inform our decision for $\hat y$, or our 0 -- 1 prediction of the next flip. If $\theta \ge 0.5, \hat y = 1, \text{else } \hat y = 0$.

You may be inclined to say $0.5$ is the best choice for $\theta$. However, notice that we made no assumption about the coin itself. The coin may be biased, so we should make our decision based only on the data. We know that exactly $\frac{4}{10}$ of the flips were heads, so we should think $\hat \theta = 0.4$. In the next section, we will mathematically prove why this is the best possible estimate.

### Likelihood of Data

Let's call the result of the coin flip a random variable $Y$. This is a Bernoulli random variable with two outcomes. $Y$ has the following distribution: 

$$P(Y = y) = \begin{cases}
        1, \text{with probability }  \theta\\
        0, \text{with probability }  1 - \theta
    \end{cases} $$

$\theta$ is unknown to us. But we can find the $\theta$ that makes the data we observed the most *likely*, by looking towards the **likelihood** of the data. The likelihood is proportional to the probability of observing the data. 

The probability of observing 4 Heads and 6 Tails follows the Binomial Distribution.

$$\binom{10}{4} (\theta)^4 (1-\theta)^6$$ 

The likelihood is proportional to the probability above. To find it, simply multiply the probabilities of obtaining each coin flip.

$$(\theta)^{4} (1-\theta)^6$$ 

The technique known as **maximum likelihood estimation** finds the $\theta$ that maximizes the above likelihood. You can find this maximum by taking the derivative of the likelihood, but we'll provide a more intuitive graphical solution.

```{python}
import numpy as np
import matplotlib.pyplot as plt

theta = np.linspace(0, 1, 100)
plt.plot(theta, theta**4 * (1-theta)**6);
```

You can see this function is maximized at the value $0.4$. Thus, $\hat\theta = 0.4$. We will revisit these methods in our derivation of the logistic regression model.

## Deriving the Logistic Regression Model

This section will focus on deriving the logistic regression model in the context of a breast cancer study. Our goal will be to use the mean radius of a tumor to predict whether a cancer is malignant ($y = 1$) or not ($y = 0$). Data from the study is plotted below, along with the overlayed least squares model that minimizes MSE.

<img src="images/radius_vs_malig.png" alt='radius_vs_malig' width='500'>

The least squares approach is technically a *valid* model, but it is not *good*. The linear model's predictions $\hat y$ can be outside of the label range {$0$, $1$}, which doesn't make sense. It is also  sensitive to outliers -- that is, if we found a data point far from these two clusters, the least squares line would  be significantly worse.

Instead, what if used a non-linear model that was a better fit of our data, and bounded $\hat y$ between [$0$, $1$]? We can look to the Data 8 Graph of Averages to formulate this model.

<!-- This way, we'd be able classify data as such: $\hat y \ge 0.5$ corresponds to class $1$, and $\hat y < 0.5$ corresponds to class $0$. This is similar to our coin flip example.--> 

### The Graph of Averages

The Graph of Averages says

- For each input $x$, compute the average value of $y$ for all nearby $x$, and predict that. 
    - "nearby" is a loose definition; we'll say for example the nearest 5 points

Here, $x$ is `mean radius` and $y$ is `malignant`. By the definition of an average, the prediction for a given $x$ is thus:

$$\hat y = \frac{\text{Sum of Nearby ``Malignant" Values}}{\text{Number of Points in Bin}}$$

You'll notice that  $\hat y$ is the proportion of malignant tumors in some point $x$'s bin. In a similar light, we can think of $\hat y$ as the probability that point $x$ belongs to the malignant class (derived from it's nearest neighbors). That is, $\hat y = P(Y = 1|x)$. This notation reads "given a tumor with a mean radius of $x$, it is malignant with probability $\hat y$".

You may recognize some similarities between $\hat y$ and $\hat \theta$ from our earlier coin toss example. $\hat y$ is a proportion of malignant tumors in a bin, whereas $\hat \theta$ was a proportion of heads in 10 coin tosses. Both also represent probabilities of some new point belonging to class $1$.

Notice how the Graph of Averages is a much better fit of the data. 

<img src="images/average_y.png" alt='average_y' width='500'>

Unfortunately, the Graph of Averages begins to degenerate as we add more features. The exact reason is out of scope, but this model becomes harder to use in higher dimensions. Instead, we use **logistic Regression**: a probabilistic model that tries to model the Graph of Averages.

As a recap of what we've discussed so far:

- We want to fit this "S" shaped curve as best as possible
- This curve models the probability of belonging to class $1$
    - $P(Y = 1|x)$

### Transforming the Graph of Averages

Our goal here is to find a mathematical function that models the Graph of Averages, or $P(Y = 1|x)$. For shorthand, let's call this function $p$. We will

1. Transform $p$ until it looks linear.
2. Then, use algebra to invert all transformations.

**Note**: $P(Y = 1|x)$ and $p$ are used interchangeably throughout the rest of the note.

#### 1. Transform $p$ until it looks linear

To transform this "S" shaped curve, we will define a quantity called the odds ratio. The **odds of an event** is the probability the event happens divided by the probability that it doesn't happen. Remember, $p$ is shorthand for our event $P(Y = 1|x)$.

$$\text{odds}(p) = \frac{p}{1 - p}$$

Here, we've applied the odds ratio to our Graph of Averages. The result is an exponential relationship.

<img src="images/odds.png" alt='odds' width='500'>

Applying a logarithmic transformation to the y-axis will linearize the data. We know this from the Tukey-Mosteller Buldge Diagram.

$$\text{log-odds}(p) = \text{log}(\frac{p}{1-p})$$

<img src="images/log_odds.png" alt='log_odds' width='500'>

We have a roughly linear relationship! Specifically, this is linear in our features $x$ and parameters $\theta$. In our one dimensional feature space, $x$ is the `mean radius` and $\theta$ some constant.

$$\text{log}(\frac{p}{1-p}) = x^T\theta$$

#### 2. Use algebra to invert all transformations

Remember our original goal was to model the Graph of Averages, which we called $p$. Here, we will algebraically solve for $p$.

$$\text{log}(\frac{p}{1-p}) = x^T\theta$$ 
$$\frac{p}{1-p} = e^{x^T\theta}$$ 
$$p = e^{x^T\theta} - pe^{x^T\theta}$$
$$p = \frac{e^{x^T\theta}}{1 + e^{x^T\theta}}$$
$$p = \frac{1}{1 + e^{-x^T\theta}}$$

## The Logistic Regression Model

The last line in this derivation is the **logistic regression model**. It models the Graph of Averages quite well.

$$P(Y = 1|x) = \frac{1}{1 + e^{-x^T\theta}}$$

<img src="images/logistic_vs_emperical.png" alt='logistic_vs_emperical' width='500'>

### The Logistic Function

While it may look daunting, the logistic regression model is just a composition of two simpler functions. One of these we know well from OLS -- the linear model $x^T\theta$. The second is the **logistic function** -- commonly referred to as the **sigmoid function**.

$$\sigma(t) = \frac{1}{1 + e^{-t}}$$

<img src="images/logistic_func.png" alt='logistic_func' width='500'>

This logistic function has various properties, although we won't prove any of them.


1. Definition: $\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^{t}}$
2. Reflection/Symmetry: $1 - \sigma(t) = \frac{e^{-t}}{1 + e^{-t}} = \sigma(-t)$
3. Inverse: $t = \sigma^{-1}(p) = \log(\frac{p}{1-p})$
4. Domain: $-\infty < t < \infty$
5. Range: $0 < \sigma(t) < 1$

You'll notice the logistic regression model is just the logistic function evaluated on the linear model $x^T\theta$. The logistic function squashes the result of $x^T\theta$ to a probability between $0$ and $1$. Larger values of $x^T\theta$ will be mapped to probabilities closer to $1$, and vice versa. Putting it all together, we have:

$$P(Y = 1|x) = \sigma(x^T\theta) = \frac{1}{1 + e^{-x^T\theta}}$$

### Example Calcualtion

Suppose we want to predict the probability that a tumor is malignant, given it's mean radius and mean smoothness. Assume that we fit a logistic regression model (with no intercept), and somehow estimate the optimal parameters: $\hat \theta = [0.1, -0.5]$.

Say we encounter a new breast tumor with data: $x = [15, 1]$. We want to answer two questions:

1. What is the probability that this tumor is malignant?
2. Should we predict the tumor is malignant or benign?

$$P(Y = 1|x) = \sigma(x^T\hat\theta)$$ 
$$= \sigma(0.1 * 15 + (-0.5) * 1) = \sigma(1)$$
$$= \frac{1}{1 + e^{-1}}$$
$$\approx 0.7311$$

Because the tumor has a $73\%$ chance of being malignant, we should predict $\hat y = 1$.

### Parameter Interpretation

In our derivation of logistic regression, we showed that $\frac{p}{1-p} = e^{x^T\theta}$. Equivalently, 
$$\frac{P(Y=1|x)}{P(Y=0|x)} = e^{x^T\theta}$$

Imagine our linear component has just a single feature, along with an intercept term.

$$\frac{P(Y=1|x)}{P(Y=0|x)} = e^{\theta_0 + \theta_1 x}$$

What happens when you increase $x$ by one unit?

- Odds is multiplied by $e^{\theta_1}$
- If $\theta_1 > 0$, then odds increase.
- If $\theta_1 < 0$, then odds decrease.

Rememer, the odds ratio can be interpreted as the “number of successes for each failure.” When odds increase, the proportion of successes likewise increases.

### Comparison to Linear Regression

#### Linear Regression

<img src="images/linear_regression.png" alt='linear_regression' width='800'>

#### Logistic Regression

<img src="images/logistic_regression.png" alt='logistic_regression' width='800'>

## Parameter Estimation

Having derived the logistic regression model, the next step is to estimate the optimal parameters $\hat\theta$.

### Pitfalls of Squared Error Loss
In linear regression, we found these optimal parameters by minimizing a cost function. The same applies to logistic regression. Let's begin by evaluating the MSE loss function on the logistic regression model.

$$L(\theta) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \sigma(x_i^T\theta))^2$$

However, there are 3 flaws with this approach.

#### 1. Non-convex 

The MSE loss surface for logistic regression is non-convex. In the following example, you can see the function rises above the secant line, a clear violation of convexity.

<img src="images/mse_secant_line.png" alt='mse_secant_line' width='500'>

Depending on the initialization points, gradient descent may find multiple non-optimal solutions.

#### 2. Bounded

A good loss function should penalize incorrect predictions; the MSE doesn't do this. Consider the following scenario.

- The predicted probability that some tumor is malignant is $0.99$ ($\hat y = 0.99$). However, this tumor is truly benign ($y = 0$). The loss incurred by this misprediction is only $(0 - 0.99)^2 = 0.98$. 

In fact, loss is always bounded $< 1$.

#### 3. Conceptually Questionable

The MSE loss function itself is conceptually questionable. 

- $y$ is a class label
- $\hat y$ is a probability

Why are we squaring the difference of a class label and probability?

### Cross-Entropy Loss

This section will introduce a more intuitive loss function - cross-entropy loss. The **cross-entropy loss** is defined as 

$$L(\theta) = -(y\text{ log}(p) + (1-y)\text{log}(1-p))$$

$$\text{where } p = \sigma(x^T\theta)$$

The average risk over an entire dataset is

$$R(\theta) = -\frac{1}{n} \sum_{i=1}^{n}(y_i\text{ log}(p) + (1-y_i)\text{log}(1-p))$$

Cross-entropy loss addresses the 3 pitfalls of squared loss.

1. Convex. No local minima for logistic regression
2. A good measure of model error. Strongly penalizes bad predictions.
3. Conceptually sound.

#### 1. Convex

To prove that cross-entropy loss is *always* convex, we'll need to take its derivative, which becomes difficult and out of scope. Instead, in Data 100, we will show a proof by picture.

Plotted below is the cross-entropy loss function applied to the same toy dataset as before. As you'll notice, this is a convex function - any line connecting two points lies entirely *above* the function.

<img src="images/cle-convex.png" alt='cle-convex' width='500'>

#### 2. Strong Error Penalization

To understand why cross-entropy loss has strong penalization, we need to take a step back. Let's decompose the loss function into two components.

$$L(\theta) = -(y\text{ log}(p) + (1-y)\text{log}(1-p))$$

1. $-(1-y)\text{ log}(1 - p)$
    - For $y = 0$, only this term stays in cross-entropy loss
    - See the plot on the bottom left. 
        - $p \rightarrow 0$: zero loss
        - $p \rightarrow 1$: infinite loss

2. $- y\text{ log}(p)$
    - For $y = 1$, only this term stays in cross-entropy loss
    - See the plot on the bottom right. Notice how error increases exponentially as $p \rightarrow 0$.
        - $p \rightarrow 0$: infinite loss
        - $p \rightarrow 1$: zero loss

:::: {.columns}

::: {.column width="30%"}
<img src="images/cle-loss-y_is_0.png" alt='cle-loss-y_is_0' width='400'>
:::

::: {.column width="20%"}
:::

::: {.column width="30%"}
<img src="images/cle-loss-y_is_1.png" alt='cle-loss-y_is_1' width='400'>
:::

::::

#### 3. Conceptually Sound

To understand why cross-entropy loss makes a great intuitive loss function, we will look towards maximum likelihood estimation in the next section.

### Maximum Likelihood Estimation

**Preface**: This section will not be directly tested, but it is immensely helpful in your understanding of cross-entropy loss.

In our earlier coin toss example, we had data on 10 coin flips, and wanted to estimate $\hat \theta$, the probability of a heads.

```{python}
flips
```

$\hat \theta = 0.4$ is the most intuitive two reasons:

1. It is the frequency of heads in our data
2. It maximizes the **likelihood** of our data 

$$\hat \theta = \text{argmax}_\theta (\theta^4(1-\theta)^6)$$

More generally, we can apply this notion of likelihood to any random binary sample. For example, we can find the likelihood of the data observed in our breast cancer study. We will show how the likelihood is intrinsically related to cross-entropy loss.

As a quick refresher on likelihood:

For some Bernoulli($p$) random variable $Y$ the probability distribution, or likelihood is:

$$P(Y = y) = \begin{cases}
        1, \text{with probability }  p\\
        0, \text{with probability }  1 - p
    \end{cases} $$
    
Equivalently, this can be written in a compact way:

$$P(Y=y) = p^y(1-p)^{1-y}$$

- When $y = 1$, this reads $P(Y=y) = p$
- When $y = 0$, this reads $P(Y=y) = (1-p)$

In our example, a Bernoulli random variable is analagous to a single data point, or tumor. All together, our breast cancer study consist of multiple IID Bernoulli($p$) random variables. To find the likelihood of independent events in succession, simply multiply their likelihoods.

$$\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}$$

As with the coin example, we want to find the parameter $p$ that maximizes this likelihood - this technique is known as **maximum likelihood estimation**. Earlier, we gave an intuitive graphical solution, but let's take the derivative of the likelihood to find this maximum.

From a first glance, this derivative will be complicated! We will have to use the product rule, followed by the chain rule. Instead, we can make an observation that simplifies the problem. 

Finding the $p$ that maximizes $$\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}$$ is equivalent to the $p$ that maximizes $$\text{log}(\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i})$$

This is because $\text{log}$ is a strictly *increasing* function. It won't change the maximum or minimum of the function it was applied to. From $\text{log}$ properties, $\text{log}(a*b)$ = $\text{log}(a) + \text{log}(b)$. We can apply this to our equation above to get:

$$\text{argmax}_p \sum_{i=1}^{n} \text{log}(p^{y_i} (1-p)^{1-y_i})$$

$$= \text{argmax}_p \sum_{i=1}^{n} \text{log}(p^{y_i}) + \text{log}((1-p)^{1-y_i})$$

$$= \text{argmax}_p \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)$$

We can add a constant factor of $\frac{1}{n}$ out front. It won't affect the $p$ that maximizes our likelihood.

$$=\text{argmax}_p  \frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)$$

One last "trick" we can do is change this to a minimization problem by negating the result. This works because we are dealing with a *concave* function, which can be made *convex*.

$$= \text{argmin}_p -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)$$

This is exactly our average cross-entropy loss minimization problem from before! 

Why did we do all this complicated math? We have shown that *minimizing* cross-entropy loss is equivalent to maximizing the likelihood of the training data.

- By minimizing cross-entropy loss, we are choosing the model parameters that are "most likely" for the data we observed.

## Parting Note

You will study MLE further in probability and ML classes. But now you know it exists. It turns out that many of the model + loss combinations we’ve seen can be motivated using MLE (OLS, Ridge Regression, etc.)

The two important takeways from this lecture were 

1. Formulating the Logistic Regression Model
2. Motivating the Cross-Entropy Loss

In the next lecture, we will learn how to evaluate the strength of logistic regression models.


