% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Principles and Techniques of Data Science},
  pdfauthor={Bella Crouch; Yash Dave; Kanu Grover; Ishani Gupta; Sakshi Kolli; Minh Phan; Nikhil Reddy; Milad Shafaie; Matthew Shen; Lillian Weng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Principles and Techniques of Data Science}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Data 100}
\author{Bella Crouch \and Yash Dave \and Kanu Grover \and Ishani
Gupta \and Sakshi Kolli \and Minh Phan \and Nikhil Reddy \and Milad
Shafaie \and Matthew Shen \and Lillian Weng}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[borderline west={3pt}{0pt}{shadecolor}, breakable, interior hidden, enhanced, frame hidden, sharp corners, boxrule=0pt]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

\hypertarget{about-the-course-notes}{%
\section*{About the Course Notes}\label{about-the-course-notes}}
\addcontentsline{toc}{section}{About the Course Notes}

\markright{About the Course Notes}

This text offers supplementary resources to accompany lectures presented
in the Spring 2024 Edition of the UC Berkeley course Data 100:
Principles and Techniques of Data Science.

New notes will be added each week to accompany live lectures. See the
full calendar of lectures on the \href{https://ds100.org/sp24/}{course
website}.

If you spot any typos or would like to suggest any changes, please fill
out the \href{https://forms.gle/UL4xMNZVmjTbBLea9}{Data 100 Content
Feedback Form (Spring 2024)}. Note that this link will only work if you
have an @berkeley.edu email address. If you're not a student at Berkeley
and would like to provide feedback, please email us at
\textbf{data100.instructors@berkeley.edu}.

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Acquaint yourself with the overarching goals of Data 100
\item
  Understand the stages of the data science lifecycle
\end{itemize}

\end{tcolorbox}

Data science is an interdisciplinary field with a variety of
applications and offers great potential to address challenging societal
issues. By building data science skills, you can empower yourself to
participate in and drive conversations that shape your life and society
as a whole, whether that be fighting against climate change, launching
diversity initiatives, or more.

The field of data science is rapidly evolving; many of the key technical
underpinnings in modern-day data science have been popularized during
the early 21\textsuperscript{st} century, and you will learn them
throughout the course. It has a wide range of applications from science
and medicine to sports.

While data science has immense potential to address challenging problems
facing society by enhancing our critical thinking, it can also be used
obscure complex decisions and reinforce historical trends and biases.
This course will implore you to consider the ethics of data science
within its applications.

Data science is fundamentally human-centered and facilitates
decision-making by quantitatively balancing tradeoffs. To quantify
things reliably, we must use and analyze data appropriately, apply
critical thinking and skepticism at every step of the way, and consider
how our decisions affect others.

Ultimately, data science is the application of data-centric,
computational, and inferential thinking to:

\begin{itemize}
\tightlist
\item
  Understand the world (science).
\item
  Solve problems (engineering).
\end{itemize}

A true mastery of data science requires a deep theoretical understanding
and strong grasp of domain expertise. This course will help you build on
the former -- specifically, the foundation of your technical knowledge,
allowing you to take data and produce useful insights on the world's
most challenging and ambiguous problems.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Course Goals}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Prepare you for advanced Berkeley courses in \textbf{data management,
  machine learning, and statistics.}
\item
  Enable you to launch a career as a data scientist by providing
  experience working with \textbf{real-world data, tools, and
  techniques}.
\item
  Empower you to apply computational and inferential thinking to address
  \textbf{real-world problems}.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Some Topics We'll Cover}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  \texttt{pandas} and \texttt{NumPy}
\item
  Exploratory Data Analysis
\item
  Regular Expressions
\item
  Visualization
\item
  Sampling
\item
  Model Design and Loss Formulation
\item
  Linear Regression
\item
  Gradient Descent
\item
  Logistic Regression
\item
  Clustering
\item
  PCA
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Prerequisites}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

To ensure that you can get the most out of the course content, please
make sure that you are familiar with:

\begin{itemize}
\tightlist
\item
  Using Python.
\item
  Using Jupyter notebooks.
\item
  Inference from Data 8.
\item
  Linear algebra
\end{itemize}

\end{tcolorbox}

To set you up for success, we've organized concepts in Data 100 around
the \textbf{data science lifecycle}: an \emph{iterative} process that
encompasses the various statistical and computational building blocks of
data science.

\hypertarget{data-science-lifecycle}{%
\section{Data Science Lifecycle}\label{data-science-lifecycle}}

The data science lifecycle is a \emph{high-level overview} of the data
science workflow. It's a cycle of stages that a data scientist should
explore as they conduct a thorough analysis of a data-driven problem.

There are many variations of the key ideas present in the data science
lifecycle. In Data 100, we visualize the stages of the lifecycle using a
flow diagram. Notice how there are two entry points.

\hypertarget{ask-a-question}{%
\subsection{Ask a Question}\label{ask-a-question}}

Whether by curiosity or necessity, data scientists constantly ask
questions. For example, in the business world, data scientists may be
interested in predicting the profit generated by a certain investment.
In the field of medicine, they may ask whether some patients are more
likely than others to benefit from a treatment.

Posing questions is one of the primary ways the data science lifecycle
begins. It helps to fully define the question. Here are some things you
should ask yourself before framing a question.

\begin{itemize}
\tightlist
\item
  What do we want to know?

  \begin{itemize}
  \tightlist
  \item
    A question that is too ambiguous may lead to confusion.
  \end{itemize}
\item
  What problems are we trying to solve?

  \begin{itemize}
  \tightlist
  \item
    The goal of asking a question should be clear in order to justify
    your efforts to stakeholders.
  \end{itemize}
\item
  What are the hypotheses we want to test?

  \begin{itemize}
  \tightlist
  \item
    This gives a clear perspective from which to analyze final results.
  \end{itemize}
\item
  What are the metrics for our success?

  \begin{itemize}
  \tightlist
  \item
    This establishes a clear point to know when to conclude the project.
  \end{itemize}
\end{itemize}

\hypertarget{obtain-data}{%
\subsection{Obtain Data}\label{obtain-data}}

The second entry point to the lifecycle is by obtaining data. A careful
analysis of any problem requires the use of data. Data may be readily
available to us, or we may have to embark on a process to collect it.
When doing so, it is crucial to ask the following:

\begin{itemize}
\tightlist
\item
  What data do we have, and what data do we need?

  \begin{itemize}
  \tightlist
  \item
    Define the units of the data (people, cities, points in time, etc.)
    and what features to measure.
  \end{itemize}
\item
  How will we sample more data?

  \begin{itemize}
  \tightlist
  \item
    Scrape the web, collect manually, run experiments, etc.
  \end{itemize}
\item
  Is our data representative of the population we want to study?

  \begin{itemize}
  \tightlist
  \item
    If our data is not representative of our population of interest,
    then we can come to incorrect conclusions.
  \end{itemize}
\end{itemize}

Key procedures: \emph{data acquisition}, \emph{data cleaning}

\hypertarget{understand-the-data}{%
\subsection{Understand the Data}\label{understand-the-data}}

Raw data itself is not inherently useful. It's impossible to discern all
the patterns and relationships between variables without carefully
investigating them. Therefore, translating pure data into actionable
insights is a key job of a data scientist. For example, we may choose to
ask:

\begin{itemize}
\tightlist
\item
  How is our data organized, and what does it contain?

  \begin{itemize}
  \tightlist
  \item
    Knowing what the data says about the world helps us better
    understand the world.
  \end{itemize}
\item
  Do we have relevant data?

  \begin{itemize}
  \tightlist
  \item
    If the data we have collected is not useful to the question at hand,
    then we must collect more data.
  \end{itemize}
\item
  What are the biases, anomalies, or other issues with the data?

  \begin{itemize}
  \tightlist
  \item
    These can lead to many false conclusions if ignored, so data
    scientists must always be aware of these issues.
  \end{itemize}
\item
  How do we transform the data to enable effective analysis?

  \begin{itemize}
  \tightlist
  \item
    Data is not always easy to interpret at first glance, so a data
    scientist should strive to reveal the hidden insights.
  \end{itemize}
\end{itemize}

Key procedures: \emph{exploratory data analysis}, \emph{data
visualization}.

\hypertarget{understand-the-world}{%
\subsection{Understand the World}\label{understand-the-world}}

After observing the patterns in our data, we can begin answering our
questions. This may require that we predict a quantity (machine
learning) or measure the effect of some treatment (inference).

From here, we may choose to report our results, or possibly conduct more
analysis. We may not be satisfied with our findings, or our initial
exploration may have brought up new questions that require new data.

\begin{itemize}
\tightlist
\item
  What does the data say about the world?

  \begin{itemize}
  \tightlist
  \item
    Given our models, the data will lead us to certain conclusions about
    the real world.\\
  \end{itemize}
\item
  Does it answer our questions or accurately solve the problem?

  \begin{itemize}
  \tightlist
  \item
    If our model and data can not accomplish our goals, then we must
    reform our question, model, or both.\\
  \end{itemize}
\item
  How robust are our conclusions and can we trust the predictions?

  \begin{itemize}
  \tightlist
  \item
    Inaccurate models can lead to false conclusions.
  \end{itemize}
\end{itemize}

Key procedures: \emph{model creation}, \emph{prediction},
\emph{inference}.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The data science lifecycle is meant to be a set of general guidelines
rather than a hard set of requirements. In our journey exploring the
lifecycle, we'll cover both the underlying theory and technologies used
in data science. By the end of the course, we hope that you start to see
yourself as a data scientist.

With that, we'll begin by introducing one of the most important tools in
exploratory data analysis: \texttt{pandas}.

\bookmarksetup{startatroot}

\hypertarget{pandas-i}{%
\chapter{Pandas I}\label{pandas-i}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Build familiarity with \texttt{pandas} and \texttt{pandas} syntax.
\item
  Learn key data structures: \texttt{DataFrame}, \texttt{Series}, and
  \texttt{Index}.
\item
  Understand methods for extracting data: \texttt{.loc}, \texttt{.iloc},
  and \texttt{{[}{]}}.
\end{itemize}

\end{tcolorbox}

In this sequence of lectures, we will dive right into things by having
you explore and manipulate real-world data. We'll first introduce
\texttt{pandas}, a popular Python library for interacting with
\textbf{tabular data}.

\hypertarget{tabular-data}{%
\section{Tabular Data}\label{tabular-data}}

Data scientists work with data stored in a variety of formats. This
class focuses primarily on \emph{tabular data} --- data that is stored
in a table.

Tabular data is one of the most common systems that data scientists use
to organize data. This is in large part due to the simplicity and
flexibility of tables. Tables allow us to represent each
\textbf{observation}, or instance of collecting data from an individual,
as its own \emph{row}. We can record each observation's distinct
characteristics, or \textbf{features}, in separate \emph{columns}.

To see this in action, we'll explore the \texttt{elections} dataset,
which stores information about political candidates who ran for
president of the United States in previous years.

In the \texttt{elections} dataset, each row (blue box) represents one
instance of a candidate running for president in a particular year. For
example, the first row represents Andrew Jackson running for president
in the year 1824. Each column (yellow box) represents one characteristic
piece of information about each presidential candidate. For example, the
column named ``Result'' stores whether or not the candidate won the
election.

Your work in Data 8 helped you grow very familiar with using and
interpreting data stored in a tabular format. Back then, you used the
\texttt{Table} class of the \texttt{datascience} library, a special
programming library created specifically for Data 8 students.

In Data 100, we will be working with the programming library
\texttt{pandas}, which is generally accepted in the data science
community as the industry- and academia-standard tool for manipulating
tabular data (as well as the inspiration for Petey, our panda bear
mascot).

Using \texttt{pandas}, we can

\begin{itemize}
\tightlist
\item
  Arrange data in a tabular format.
\item
  Extract useful information filtered by specific conditions.
\item
  Operate on data to gain new insights.
\item
  Apply \texttt{NumPy} functions to our data (our friends from Data 8).
\item
  Perform vectorized computations to speed up our analysis (Lab 1).
\end{itemize}

\hypertarget{series-dataframes-and-indices}{%
\section{\texorpdfstring{\texttt{Series}, \texttt{DataFrame}s, and
Indices}{Series, DataFrames, and Indices}}\label{series-dataframes-and-indices}}

To begin our work in \texttt{pandas}, we must first import the library
into our Python environment. This will allow us to use \texttt{pandas}
data structures and methods in our code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \textasciigrave{}pd\textasciigrave{} is the conventional alias for Pandas, as \textasciigrave{}np\textasciigrave{} is for NumPy}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

There are three fundamental data structures in \texttt{pandas}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\texttt{Series}}: 1D labeled array data; best thought of as
  columnar data.
\item
  \textbf{\texttt{DataFrame}}: 2D tabular data with rows and columns.
\item
  \textbf{\texttt{Index}}: A sequence of row/column labels.
\end{enumerate}

\texttt{DataFrame}s, \texttt{Series}, and Indices can be represented
visually in the following diagram, which considers the first few rows of
the \texttt{elections} dataset.

Notice how the \textbf{DataFrame} is a two-dimensional object --- it
contains both rows and columns. The \textbf{Series} above is a singular
column of this \texttt{DataFrame}, namely the \texttt{Result} column.
Both contain an \textbf{Index}, or a shared list of row labels (the
integers from 0 to 4, inclusive).

\hypertarget{series}{%
\subsection{Series}\label{series}}

A \texttt{Series} represents a column of a \texttt{DataFrame}; more
generally, it can be any 1-dimensional array-like object. It contains
both:

\begin{itemize}
\tightlist
\item
  A sequence of \textbf{values} of the same type.
\item
  A sequence of data labels called the \textbf{index}.
\end{itemize}

In the cell below, we create a \texttt{Series} named \texttt{s}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OperatorTok{=}\NormalTok{ pd.Series([}\StringTok{"welcome"}\NormalTok{, }\StringTok{"to"}\NormalTok{, }\StringTok{"data 100"}\NormalTok{])}
\NormalTok{s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &         0 \\
\midrule
0 &   welcome \\
1 &        to \\
2 &  data 100 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# Accessing data values within the Series}
\NormalTok{ s.values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array(['welcome', 'to', 'data 100'], dtype=object)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# Accessing the Index of the Series}
\NormalTok{ s.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RangeIndex(start=0, stop=3, step=1)
\end{verbatim}

By default, the \texttt{index} of a \texttt{Series} is a sequential list
of integers beginning from 0. Optionally, a manually specified list of
desired indices can be passed to the \texttt{index} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OperatorTok{=}\NormalTok{ pd.Series([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{])}
\NormalTok{s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &   0 \\
\midrule
a &  -1 \\
b &  10 \\
c &   2 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['a', 'b', 'c'], dtype='object')
\end{verbatim}

Indices can also be changed after initialization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index }\OperatorTok{=}\NormalTok{ [}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{, }\StringTok{"third"}\NormalTok{]}
\NormalTok{s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &   0 \\
\midrule
first  &  -1 \\
second &  10 \\
third  &   2 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['first', 'second', 'third'], dtype='object')
\end{verbatim}

\hypertarget{selection-in-series}{%
\subsubsection{\texorpdfstring{Selection in
\texttt{Series}}{Selection in Series}}\label{selection-in-series}}

Much like when working with \texttt{NumPy} arrays, we can select a
single value or a set of values from a \texttt{Series}. To do so, there
are three primary methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A single label.
\item
  A list of labels.
\item
  A filtering condition.
\end{enumerate}

To demonstrate this, let's define the Series \texttt{ser}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser }\OperatorTok{=}\NormalTok{ pd.Series([}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{])}
\NormalTok{ser}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  0 \\
\midrule
a &  4 \\
b & -2 \\
c &  0 \\
d &  6 \\
\bottomrule
\end{tabular}

\hypertarget{a-single-label}{%
\paragraph{A Single Label}\label{a-single-label}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We return the value stored at the index label "a"}
\NormalTok{ser[}\StringTok{"a"}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4
\end{verbatim}

\hypertarget{a-list-of-labels}{%
\paragraph{A List of Labels}\label{a-list-of-labels}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We return a Series of the values stored at the index labels "a" and "c"}
\NormalTok{ser[[}\StringTok{"a"}\NormalTok{, }\StringTok{"c"}\NormalTok{]] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  0 \\
\midrule
a &  4 \\
c &  0 \\
\bottomrule
\end{tabular}

\hypertarget{a-filtering-condition}{%
\paragraph{A Filtering Condition}\label{a-filtering-condition}}

Perhaps the most interesting (and useful) method of selecting data from
a \texttt{Series} is by using a filtering condition.

First, we apply a boolean operation to the \texttt{Series}. This creates
\textbf{a new \texttt{Series} of boolean values}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter condition: select all elements greater than 0}
\NormalTok{ser }\OperatorTok{\textgreater{}} \DecValTok{0} 
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &      0 \\
\midrule
a &   True \\
b &  False \\
c &  False \\
d &   True \\
\bottomrule
\end{tabular}

We then use this boolean condition to index into our original
\texttt{Series}. \texttt{pandas} will select only the entries in the
original \texttt{Series} that satisfy the condition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser[ser }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  0 \\
\midrule
a &  4 \\
d &  6 \\
\bottomrule
\end{tabular}

\hypertarget{dataframes}{%
\subsection{\texorpdfstring{\texttt{DataFrames}}{DataFrames}}\label{dataframes}}

Typically, we will work with \texttt{Series} using the perspective that
they are columns in a \texttt{DataFrame}. We can think of a
\textbf{\texttt{DataFrame}} as a collection of \textbf{\texttt{Series}}
that all share the same \textbf{\texttt{Index}}.

In Data 8, you encountered the \texttt{Table} class of the
\texttt{datascience} library, which represented tabular data. In Data
100, we'll be using the \texttt{DataFrame} class of the \texttt{pandas}
library.

\hypertarget{creating-a-dataframe}{%
\subsubsection{\texorpdfstring{Creating a
\texttt{DataFrame}}{Creating a DataFrame}}\label{creating-a-dataframe}}

There are many ways to create a \texttt{DataFrame}. Here, we will cover
the most popular approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From a CSV file.
\item
  Using a list and column name(s).
\item
  From a dictionary.
\item
  From a \texttt{Series}.
\end{enumerate}

More generally, the syntax for creating a \texttt{DataFrame} is:

\begin{verbatim}
 pandas.DataFrame(data, index, columns)
\end{verbatim}

\hypertarget{from-a-csv-file}{%
\paragraph{From a CSV file}\label{from-a-csv-file}}

In Data 100, our data are typically stored in a CSV (comma-separated
values) file format. We can import a CSV file into a \texttt{DataFrame}
by passing the data path as an argument to the following \texttt{pandas}
function.  \texttt{pd.read\_csv("filename.csv")}

With our new understanding of \texttt{pandas} in hand, let's return to
the \texttt{elections} dataset from before. Now, we can recognize that
it is represented as a \texttt{pandas} \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{)}
\NormalTok{elections}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &               Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0   &  1824 &          Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1   &  1824 &       John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2   &  1828 &          Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3   &  1828 &       John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4   &  1832 &          Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
5   &  1832 &              Henry Clay &    National Republican &        484205 &   loss &  37.603628 \\
6   &  1832 &            William Wirt &           Anti-Masonic &        100715 &   loss &   7.821583 \\
7   &  1836 &       Hugh Lawson White &                   Whig &        146109 &   loss &  10.005985 \\
8   &  1836 &        Martin Van Buren &             Democratic &        763291 &    win &  52.272472 \\
9   &  1836 &  William Henry Harrison &                   Whig &        550816 &   loss &  37.721543 \\
10  &  1840 &        Martin Van Buren &             Democratic &       1128854 &   loss &  46.948787 \\
11  &  1840 &  William Henry Harrison &                   Whig &       1275583 &    win &  53.051213 \\
12  &  1844 &              Henry Clay &                   Whig &       1300004 &   loss &  49.250523 \\
13  &  1844 &              James Polk &             Democratic &       1339570 &    win &  50.749477 \\
14  &  1848 &              Lewis Cass &             Democratic &       1223460 &   loss &  42.552229 \\
15  &  1848 &        Martin Van Buren &              Free Soil &        291501 &   loss &  10.138474 \\
16  &  1848 &          Zachary Taylor &                   Whig &       1360235 &    win &  47.309296 \\
17  &  1852 &         Franklin Pierce &             Democratic &       1605943 &    win &  51.013168 \\
18  &  1852 &            John P. Hale &              Free Soil &        155210 &   loss &   4.930283 \\
19  &  1852 &          Winfield Scott &                   Whig &       1386942 &   loss &  44.056548 \\
20  &  1856 &          James Buchanan &             Democratic &       1835140 &    win &  45.306080 \\
21  &  1856 &         John C. Frémont &             Republican &       1342345 &   loss &  33.139919 \\
22  &  1856 &        Millard Fillmore &               American &        873053 &   loss &  21.554001 \\
23  &  1860 &         Abraham Lincoln &             Republican &       1855993 &    win &  39.699408 \\
24  &  1860 &               John Bell &   Constitutional Union &        590901 &   loss &  12.639283 \\
25  &  1860 &    John C. Breckinridge &    Southern Democratic &        848019 &   loss &  18.138998 \\
26  &  1860 &      Stephen A. Douglas &    Northern Democratic &       1380202 &   loss &  29.522311 \\
27  &  1864 &         Abraham Lincoln &         National Union &       2211317 &    win &  54.951512 \\
28  &  1864 &     George B. McClellan &             Democratic &       1812807 &   loss &  45.048488 \\
29  &  1868 &         Horatio Seymour &             Democratic &       2708744 &   loss &  47.334695 \\
30  &  1868 &           Ulysses Grant &             Republican &       3013790 &    win &  52.665305 \\
31  &  1872 &          Horace Greeley &     Liberal Republican &       2834761 &   loss &  44.071406 \\
32  &  1872 &           Ulysses Grant &             Republican &       3597439 &    win &  55.928594 \\
33  &  1876 &        Rutherford Hayes &             Republican &       4034142 &    win &  48.471624 \\
34  &  1876 &        Samuel J. Tilden &             Democratic &       4288546 &   loss &  51.528376 \\
35  &  1880 &         James B. Weaver &              Greenback &        308649 &   loss &   3.352344 \\
36  &  1880 &          James Garfield &             Republican &       4453337 &    win &  48.369234 \\
37  &  1880 &  Winfield Scott Hancock &             Democratic &       4444976 &   loss &  48.278422 \\
38  &  1884 &         Benjamin Butler &          Anti-Monopoly &        134294 &   loss &   1.335838 \\
39  &  1884 &        Grover Cleveland &             Democratic &       4914482 &    win &  48.884933 \\
40  &  1884 &         James G. Blaine &             Republican &       4856905 &   loss &  48.312208 \\
41  &  1884 &           John St. John &            Prohibition &        147482 &   loss &   1.467021 \\
42  &  1888 &          Alson Streeter &            Union Labor &        146602 &   loss &   1.288861 \\
43  &  1888 &       Benjamin Harrison &             Republican &       5443633 &    win &  47.858041 \\
44  &  1888 &         Clinton B. Fisk &            Prohibition &        249819 &   loss &   2.196299 \\
45  &  1888 &        Grover Cleveland &             Democratic &       5534488 &   loss &  48.656799 \\
46  &  1892 &       Benjamin Harrison &             Republican &       5176108 &   loss &  42.984101 \\
47  &  1892 &        Grover Cleveland &             Democratic &       5553898 &    win &  46.121393 \\
48  &  1892 &         James B. Weaver &               Populist &       1041028 &   loss &   8.645038 \\
49  &  1892 &            John Bidwell &            Prohibition &        270879 &   loss &   2.249468 \\
50  &  1896 &          John M. Palmer &    National Democratic &        134645 &   loss &   0.969566 \\
51  &  1896 &         Joshua Levering &            Prohibition &        131312 &   loss &   0.945565 \\
52  &  1896 &  William Jennings Bryan &             Democratic &       6509052 &   loss &  46.871053 \\
53  &  1896 &        William McKinley &             Republican &       7112138 &    win &  51.213817 \\
54  &  1900 &         John G. Woolley &            Prohibition &        210864 &   loss &   1.526821 \\
55  &  1900 &  William Jennings Bryan &             Democratic &       6370932 &   loss &  46.130540 \\
56  &  1900 &        William McKinley &             Republican &       7228864 &    win &  52.342640 \\
57  &  1904 &         Alton B. Parker &             Democratic &       5083880 &   loss &  37.685116 \\
58  &  1904 &          Eugene V. Debs &              Socialist &        402810 &   loss &   2.985897 \\
59  &  1904 &        Silas C. Swallow &            Prohibition &        259102 &   loss &   1.920637 \\
60  &  1904 &      Theodore Roosevelt &             Republican &       7630557 &    win &  56.562787 \\
61  &  1904 &        Thomas E. Watson &               Populist &        114070 &   loss &   0.845563 \\
62  &  1908 &          Eugene V. Debs &              Socialist &        420852 &   loss &   2.850866 \\
63  &  1908 &        Eugene W. Chafin &            Prohibition &        254087 &   loss &   1.721194 \\
64  &  1908 &  William Jennings Bryan &             Democratic &       6408979 &   loss &  43.414640 \\
65  &  1908 &            William Taft &             Republican &       7678335 &    win &  52.013300 \\
66  &  1912 &          Eugene V. Debs &              Socialist &        901551 &   loss &   6.004354 \\
67  &  1912 &        Eugene W. Chafin &            Prohibition &        208156 &   loss &   1.386325 \\
68  &  1912 &      Theodore Roosevelt &            Progressive &       4122721 &   loss &  27.457433 \\
69  &  1912 &            William Taft &             Republican &       3486242 &   loss &  23.218466 \\
70  &  1912 &          Woodrow Wilson &             Democratic &       6296284 &    win &  41.933422 \\
71  &  1916 &         Allan L. Benson &              Socialist &        590524 &   loss &   3.194193 \\
72  &  1916 &    Charles Evans Hughes &             Republican &       8548728 &   loss &  46.240779 \\
73  &  1916 &             Frank Hanly &            Prohibition &        221302 &   loss &   1.197041 \\
74  &  1916 &          Woodrow Wilson &             Democratic &       9126868 &    win &  49.367987 \\
75  &  1920 &        Aaron S. Watkins &            Prohibition &        188787 &   loss &   0.708351 \\
76  &  1920 &          Eugene V. Debs &              Socialist &        913693 &   loss &   3.428282 \\
77  &  1920 &            James M. Cox &             Democratic &       9139661 &   loss &  34.293063 \\
78  &  1920 &   Parley P. Christensen &           Farmer–Labor &        265398 &   loss &   0.995804 \\
79  &  1920 &          Warren Harding &             Republican &      16144093 &    win &  60.574501 \\
80  &  1924 &         Calvin Coolidge &             Republican &      15723789 &    win &  54.329113 \\
81  &  1924 &           John W. Davis &             Democratic &       8386242 &   loss &  28.976291 \\
82  &  1924 &      Robert La Follette &            Progressive &       4831706 &   loss &  16.694596 \\
83  &  1928 &                Al Smith &             Democratic &      15015464 &   loss &  40.902853 \\
84  &  1928 &          Herbert Hoover &             Republican &      21427123 &    win &  58.368524 \\
85  &  1928 &           Norman Thomas &              Socialist &        267478 &   loss &   0.728623 \\
86  &  1932 &      Franklin Roosevelt &             Democratic &      22821277 &    win &  57.672125 \\
87  &  1932 &          Herbert Hoover &             Republican &      15761254 &   loss &  39.830594 \\
88  &  1932 &           Norman Thomas &              Socialist &        884885 &   loss &   2.236211 \\
89  &  1932 &       William Z. Foster &              Communist &        103307 &   loss &   0.261069 \\
90  &  1936 &              Alf Landon &             Republican &      16679543 &   loss &  36.648285 \\
91  &  1936 &      Franklin Roosevelt &             Democratic &      27752648 &    win &  60.978107 \\
92  &  1936 &           Norman Thomas &              Socialist &        187910 &   loss &   0.412876 \\
93  &  1936 &           William Lemke &                  Union &        892378 &   loss &   1.960733 \\
94  &  1940 &      Franklin Roosevelt &             Democratic &      27313945 &    win &  54.871202 \\
95  &  1940 &           Norman Thomas &              Socialist &        116599 &   loss &   0.234237 \\
96  &  1940 &         Wendell Willkie &             Republican &      22347744 &   loss &  44.894561 \\
97  &  1944 &      Franklin Roosevelt &             Democratic &      25612916 &    win &  53.773801 \\
98  &  1944 &         Thomas E. Dewey &             Republican &      22017929 &   loss &  46.226199 \\
99  &  1948 &        Claude A. Watson &            Prohibition &        103708 &   loss &   0.212747 \\
100 &  1948 &            Harry Truman &             Democratic &      24179347 &    win &  49.601536 \\
101 &  1948 &        Henry A. Wallace &            Progressive &       1157328 &   loss &   2.374144 \\
102 &  1948 &           Norman Thomas &              Socialist &        139569 &   loss &   0.286312 \\
103 &  1948 &          Strom Thurmond &              Dixiecrat &       1175930 &   loss &   2.412304 \\
104 &  1948 &         Thomas E. Dewey &             Republican &      21991292 &   loss &  45.112958 \\
105 &  1952 &         Adlai Stevenson &             Democratic &      27375090 &   loss &  44.446312 \\
106 &  1952 &       Dwight Eisenhower &             Republican &      34075529 &    win &  55.325173 \\
107 &  1952 &        Vincent Hallinan &            Progressive &        140746 &   loss &   0.228516 \\
108 &  1956 &         Adlai Stevenson &             Democratic &      26028028 &   loss &  42.174464 \\
109 &  1956 &       Dwight Eisenhower &             Republican &      35579180 &    win &  57.650654 \\
110 &  1956 &      T. Coleman Andrews &         States' Rights &        107929 &   loss &   0.174883 \\
111 &  1960 &            John Kennedy &             Democratic &      34220984 &    win &  50.082561 \\
112 &  1960 &           Richard Nixon &             Republican &      34108157 &   loss &  49.917439 \\
113 &  1964 &         Barry Goldwater &             Republican &      27175754 &   loss &  38.655297 \\
114 &  1964 &          Lyndon Johnson &             Democratic &      43127041 &    win &  61.344703 \\
115 &  1968 &          George Wallace &   American Independent &       9901118 &   loss &  13.571218 \\
116 &  1968 &         Hubert Humphrey &             Democratic &      31271839 &   loss &  42.863537 \\
117 &  1968 &           Richard Nixon &             Republican &      31783783 &    win &  43.565246 \\
118 &  1972 &         George McGovern &             Democratic &      29173222 &   loss &  37.670670 \\
119 &  1972 &         John G. Schmitz &   American Independent &       1100868 &   loss &   1.421524 \\
120 &  1972 &           Richard Nixon &             Republican &      47168710 &    win &  60.907806 \\
121 &  1976 &         Eugene McCarthy &            Independent &        740460 &   loss &   0.911649 \\
122 &  1976 &             Gerald Ford &             Republican &      39148634 &   loss &  48.199499 \\
123 &  1976 &            Jimmy Carter &             Democratic &      40831881 &    win &  50.271900 \\
124 &  1976 &           Lester Maddox &   American Independent &        170274 &   loss &   0.209640 \\
125 &  1976 &          Roger MacBride &            Libertarian &        172557 &   loss &   0.212451 \\
126 &  1976 &      Thomas J. Anderson &               American &        158271 &   loss &   0.194862 \\
127 &  1980 &          Barry Commoner &               Citizens &        233052 &   loss &   0.270182 \\
128 &  1980 &                Ed Clark &            Libertarian &        921128 &   loss &   1.067883 \\
129 &  1980 &            Jimmy Carter &             Democratic &      35480115 &   loss &  41.132848 \\
130 &  1980 &        John B. Anderson &            Independent &       5719850 &   loss &   6.631143 \\
131 &  1980 &           Ronald Reagan &             Republican &      43903230 &    win &  50.897944 \\
132 &  1984 &          David Bergland &            Libertarian &        228111 &   loss &   0.247245 \\
133 &  1984 &           Ronald Reagan &             Republican &      54455472 &    win &  59.023326 \\
134 &  1984 &          Walter Mondale &             Democratic &      37577352 &   loss &  40.729429 \\
135 &  1988 &       George H. W. Bush &             Republican &      48886597 &    win &  53.518845 \\
136 &  1988 &           Lenora Fulani &           New Alliance &        217221 &   loss &   0.237804 \\
137 &  1988 &         Michael Dukakis &             Democratic &      41809074 &   loss &  45.770691 \\
138 &  1988 &                Ron Paul &            Libertarian &        431750 &   loss &   0.472660 \\
139 &  1992 &            Andre Marrou &            Libertarian &        290087 &   loss &   0.278516 \\
140 &  1992 &            Bill Clinton &             Democratic &      44909806 &    win &  43.118485 \\
141 &  1992 &                Bo Gritz &               Populist &        106152 &   loss &   0.101918 \\
142 &  1992 &       George H. W. Bush &             Republican &      39104550 &   loss &  37.544784 \\
143 &  1992 &              Ross Perot &            Independent &      19743821 &   loss &  18.956298 \\
144 &  1996 &            Bill Clinton &             Democratic &      47400125 &    win &  49.296938 \\
145 &  1996 &                Bob Dole &             Republican &      39197469 &   loss &  40.766036 \\
146 &  1996 &            Harry Browne &            Libertarian &        485759 &   loss &   0.505198 \\
147 &  1996 &         Howard Phillips &              Taxpayers &        184656 &   loss &   0.192045 \\
148 &  1996 &            John Hagelin &            Natural Law &        113670 &   loss &   0.118219 \\
149 &  1996 &             Ralph Nader &                  Green &        685297 &   loss &   0.712721 \\
150 &  1996 &              Ross Perot &                 Reform &       8085294 &   loss &   8.408844 \\
151 &  2000 &                 Al Gore &             Democratic &      50999897 &   loss &  48.491813 \\
152 &  2000 &          George W. Bush &             Republican &      50456002 &    win &  47.974666 \\
153 &  2000 &            Harry Browne &            Libertarian &        384431 &   loss &   0.365525 \\
154 &  2000 &            Pat Buchanan &                 Reform &        448895 &   loss &   0.426819 \\
155 &  2000 &             Ralph Nader &                  Green &       2882955 &   loss &   2.741176 \\
156 &  2004 &              David Cobb &                  Green &        119859 &   loss &   0.098088 \\
157 &  2004 &          George W. Bush &             Republican &      62040610 &    win &  50.771824 \\
158 &  2004 &              John Kerry &             Democratic &      59028444 &   loss &  48.306775 \\
159 &  2004 &        Michael Badnarik &            Libertarian &        397265 &   loss &   0.325108 \\
160 &  2004 &        Michael Peroutka &           Constitution &        143630 &   loss &   0.117542 \\
161 &  2004 &             Ralph Nader &            Independent &        465151 &   loss &   0.380663 \\
162 &  2008 &            Barack Obama &             Democratic &      69498516 &    win &  53.023510 \\
163 &  2008 &                Bob Barr &            Libertarian &        523715 &   loss &   0.399565 \\
164 &  2008 &           Chuck Baldwin &           Constitution &        199750 &   loss &   0.152398 \\
165 &  2008 &        Cynthia McKinney &                  Green &        161797 &   loss &   0.123442 \\
166 &  2008 &             John McCain &             Republican &      59948323 &   loss &  45.737243 \\
167 &  2008 &             Ralph Nader &            Independent &        739034 &   loss &   0.563842 \\
168 &  2012 &            Barack Obama &             Democratic &      65915795 &    win &  51.258484 \\
169 &  2012 &            Gary Johnson &            Libertarian &       1275971 &   loss &   0.992241 \\
170 &  2012 &              Jill Stein &                  Green &        469627 &   loss &   0.365199 \\
171 &  2012 &             Mitt Romney &             Republican &      60933504 &   loss &  47.384076 \\
172 &  2016 &          Darrell Castle &           Constitution &        203091 &   loss &   0.149640 \\
173 &  2016 &            Donald Trump &             Republican &      62984828 &    win &  46.407862 \\
174 &  2016 &           Evan McMullin &            Independent &        732273 &   loss &   0.539546 \\
175 &  2016 &            Gary Johnson &            Libertarian &       4489235 &   loss &   3.307714 \\
176 &  2016 &         Hillary Clinton &             Democratic &      65853514 &   loss &  48.521539 \\
177 &  2016 &              Jill Stein &                  Green &       1457226 &   loss &   1.073699 \\
178 &  2020 &            Joseph Biden &             Democratic &      81268924 &    win &  51.311515 \\
179 &  2020 &            Donald Trump &             Republican &      74216154 &   loss &  46.858542 \\
180 &  2020 &            Jo Jorgensen &            Libertarian &       1865724 &   loss &   1.177979 \\
181 &  2020 &          Howard Hawkins &                  Green &        405035 &   loss &   0.255731 \\
\bottomrule
\end{tabular}

This code stores our \texttt{DataFrame} object in the \texttt{elections}
variable. Upon inspection, our \texttt{elections} \texttt{DataFrame} has
182 rows and 6 columns (\texttt{Year}, \texttt{Candidate},
\texttt{Party}, \texttt{Popular\ Vote}, \texttt{Result}, \texttt{\%}).
Each row represents a single record --- in our example, a presidential
candidate from some particular year. Each column represents a single
attribute or feature of the record.

\hypertarget{using-a-list-and-column-names}{%
\paragraph{Using a List and Column
Name(s)}\label{using-a-list-and-column-names}}

We'll now explore creating a \texttt{DataFrame} with data of our own.

Consider the following examples. The first code cell creates a
\texttt{DataFrame} with a single column \texttt{Numbers}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_list }\OperatorTok{=}\NormalTok{ pd.DataFrame([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], columns}\OperatorTok{=}\NormalTok{[}\StringTok{"Numbers"}\NormalTok{])}
\NormalTok{df\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Numbers \\
\midrule
0 &        1 \\
1 &        2 \\
2 &        3 \\
\bottomrule
\end{tabular}

The second creates a \texttt{DataFrame} with the columns
\texttt{Numbers} and \texttt{Description}. Notice how a 2D list of
values is required to initialize the second \texttt{DataFrame} --- each
nested list represents a single row of data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_list }\OperatorTok{=}\NormalTok{ pd.DataFrame([[}\DecValTok{1}\NormalTok{, }\StringTok{"one"}\NormalTok{], [}\DecValTok{2}\NormalTok{, }\StringTok{"two"}\NormalTok{]], columns }\OperatorTok{=}\NormalTok{ [}\StringTok{"Number"}\NormalTok{, }\StringTok{"Description"}\NormalTok{])}
\NormalTok{df\_list}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrl}
\toprule
{} &  Number & Description \\
\midrule
0 &       1 &         one \\
1 &       2 &         two \\
\bottomrule
\end{tabular}

\hypertarget{from-a-dictionary}{%
\paragraph{From a Dictionary}\label{from-a-dictionary}}

A third (and more common) way to create a \texttt{DataFrame} is with a
dictionary. The dictionary keys represent the column names, and the
dictionary values represent the column values.

Below are two ways of implementing this approach. The first is based on
specifying the columns of the \texttt{DataFrame}, whereas the second is
based on specifying the rows of the \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_dict }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
    \StringTok{"Fruit"}\NormalTok{: [}\StringTok{"Strawberry"}\NormalTok{, }\StringTok{"Orange"}\NormalTok{], }
    \StringTok{"Price"}\NormalTok{: [}\FloatTok{5.49}\NormalTok{, }\FloatTok{3.99}\NormalTok{]}
\NormalTok{\})}
\NormalTok{df\_dict}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llr}
\toprule
{} &       Fruit &  Price \\
\midrule
0 &  Strawberry &   5.49 \\
1 &      Orange &   3.99 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_dict }\OperatorTok{=}\NormalTok{ pd.DataFrame(}
\NormalTok{    [}
\NormalTok{        \{}\StringTok{"Fruit"}\NormalTok{:}\StringTok{"Strawberry"}\NormalTok{, }\StringTok{"Price"}\NormalTok{:}\FloatTok{5.49}\NormalTok{\}, }
\NormalTok{        \{}\StringTok{"Fruit"}\NormalTok{: }\StringTok{"Orange"}\NormalTok{, }\StringTok{"Price"}\NormalTok{:}\FloatTok{3.99}\NormalTok{\}}
\NormalTok{    ]}
\NormalTok{)}
\NormalTok{df\_dict}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llr}
\toprule
{} &       Fruit &  Price \\
\midrule
0 &  Strawberry &   5.49 \\
1 &      Orange &   3.99 \\
\bottomrule
\end{tabular}

\hypertarget{from-a-series}{%
\paragraph{\texorpdfstring{From a
\texttt{Series}}{From a Series}}\label{from-a-series}}

Earlier, we explained how a \texttt{Series} was synonymous to a column
in a \texttt{DataFrame}. It follows, then, that a \texttt{DataFrame} is
equivalent to a collection of \texttt{Series}, which all share the same
\texttt{Index}.

In fact, we can initialize a \texttt{DataFrame} by merging two or more
\texttt{Series}. Consider the \texttt{Series} \texttt{s\_a} and
\texttt{s\_b}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Notice how our indices, or row labels, are the same}

\NormalTok{s\_a }\OperatorTok{=}\NormalTok{ pd.Series([}\StringTok{"a1"}\NormalTok{, }\StringTok{"a2"}\NormalTok{, }\StringTok{"a3"}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"r1"}\NormalTok{, }\StringTok{"r2"}\NormalTok{, }\StringTok{"r3"}\NormalTok{])}
\NormalTok{s\_b }\OperatorTok{=}\NormalTok{ pd.Series([}\StringTok{"b1"}\NormalTok{, }\StringTok{"b2"}\NormalTok{, }\StringTok{"b3"}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"r1"}\NormalTok{, }\StringTok{"r2"}\NormalTok{, }\StringTok{"r3"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

We can turn individual \texttt{Series} into a \texttt{DataFrame} using
two common methods (shown below):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.DataFrame(s\_a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &   0 \\
\midrule
r1 &  a1 \\
r2 &  a2 \\
r3 &  a3 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\_b.to\_frame()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &   0 \\
\midrule
r1 &  b1 \\
r2 &  b2 \\
r3 &  b3 \\
\bottomrule
\end{tabular}

To merge the two \texttt{Series} and specify their column names, we use
the following syntax:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.DataFrame(\{}
    \StringTok{"A{-}column"}\NormalTok{: s\_a, }
    \StringTok{"B{-}column"}\NormalTok{: s\_b}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lll}
\toprule
{} & A-column & B-column \\
\midrule
r1 &       a1 &       b1 \\
r2 &       a2 &       b2 \\
r3 &       a3 &       b3 \\
\bottomrule
\end{tabular}

\hypertarget{indices}{%
\subsection{Indices}\label{indices}}

On a more technical note, an index doesn't have to be an integer, nor
does it have to be unique. For example, we can set the index of the
\texttt{elections} \texttt{DataFrame} to be the name of presidential
candidates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating a DataFrame from a CSV file and specifying the index column}
\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{, index\_col }\OperatorTok{=} \StringTok{"Candidate"}\NormalTok{)}
\NormalTok{elections}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlrlr}
\toprule
{} &  Year &                  Party &  Popular vote & Result &          \% \\
Candidate              &       &                        &               &        &            \\
\midrule
Andrew Jackson         &  1824 &  Democratic-Republican &        151271 &   loss &  57.210122 \\
John Quincy Adams      &  1824 &  Democratic-Republican &        113142 &    win &  42.789878 \\
Andrew Jackson         &  1828 &             Democratic &        642806 &    win &  56.203927 \\
John Quincy Adams      &  1828 &    National Republican &        500897 &   loss &  43.796073 \\
Andrew Jackson         &  1832 &             Democratic &        702735 &    win &  54.574789 \\
Henry Clay             &  1832 &    National Republican &        484205 &   loss &  37.603628 \\
William Wirt           &  1832 &           Anti-Masonic &        100715 &   loss &   7.821583 \\
Hugh Lawson White      &  1836 &                   Whig &        146109 &   loss &  10.005985 \\
Martin Van Buren       &  1836 &             Democratic &        763291 &    win &  52.272472 \\
William Henry Harrison &  1836 &                   Whig &        550816 &   loss &  37.721543 \\
Martin Van Buren       &  1840 &             Democratic &       1128854 &   loss &  46.948787 \\
William Henry Harrison &  1840 &                   Whig &       1275583 &    win &  53.051213 \\
Henry Clay             &  1844 &                   Whig &       1300004 &   loss &  49.250523 \\
James Polk             &  1844 &             Democratic &       1339570 &    win &  50.749477 \\
Lewis Cass             &  1848 &             Democratic &       1223460 &   loss &  42.552229 \\
Martin Van Buren       &  1848 &              Free Soil &        291501 &   loss &  10.138474 \\
Zachary Taylor         &  1848 &                   Whig &       1360235 &    win &  47.309296 \\
Franklin Pierce        &  1852 &             Democratic &       1605943 &    win &  51.013168 \\
John P. Hale           &  1852 &              Free Soil &        155210 &   loss &   4.930283 \\
Winfield Scott         &  1852 &                   Whig &       1386942 &   loss &  44.056548 \\
James Buchanan         &  1856 &             Democratic &       1835140 &    win &  45.306080 \\
John C. Frémont        &  1856 &             Republican &       1342345 &   loss &  33.139919 \\
Millard Fillmore       &  1856 &               American &        873053 &   loss &  21.554001 \\
Abraham Lincoln        &  1860 &             Republican &       1855993 &    win &  39.699408 \\
John Bell              &  1860 &   Constitutional Union &        590901 &   loss &  12.639283 \\
John C. Breckinridge   &  1860 &    Southern Democratic &        848019 &   loss &  18.138998 \\
Stephen A. Douglas     &  1860 &    Northern Democratic &       1380202 &   loss &  29.522311 \\
Abraham Lincoln        &  1864 &         National Union &       2211317 &    win &  54.951512 \\
George B. McClellan    &  1864 &             Democratic &       1812807 &   loss &  45.048488 \\
Horatio Seymour        &  1868 &             Democratic &       2708744 &   loss &  47.334695 \\
Ulysses Grant          &  1868 &             Republican &       3013790 &    win &  52.665305 \\
Horace Greeley         &  1872 &     Liberal Republican &       2834761 &   loss &  44.071406 \\
Ulysses Grant          &  1872 &             Republican &       3597439 &    win &  55.928594 \\
Rutherford Hayes       &  1876 &             Republican &       4034142 &    win &  48.471624 \\
Samuel J. Tilden       &  1876 &             Democratic &       4288546 &   loss &  51.528376 \\
James B. Weaver        &  1880 &              Greenback &        308649 &   loss &   3.352344 \\
James Garfield         &  1880 &             Republican &       4453337 &    win &  48.369234 \\
Winfield Scott Hancock &  1880 &             Democratic &       4444976 &   loss &  48.278422 \\
Benjamin Butler        &  1884 &          Anti-Monopoly &        134294 &   loss &   1.335838 \\
Grover Cleveland       &  1884 &             Democratic &       4914482 &    win &  48.884933 \\
James G. Blaine        &  1884 &             Republican &       4856905 &   loss &  48.312208 \\
John St. John          &  1884 &            Prohibition &        147482 &   loss &   1.467021 \\
Alson Streeter         &  1888 &            Union Labor &        146602 &   loss &   1.288861 \\
Benjamin Harrison      &  1888 &             Republican &       5443633 &    win &  47.858041 \\
Clinton B. Fisk        &  1888 &            Prohibition &        249819 &   loss &   2.196299 \\
Grover Cleveland       &  1888 &             Democratic &       5534488 &   loss &  48.656799 \\
Benjamin Harrison      &  1892 &             Republican &       5176108 &   loss &  42.984101 \\
Grover Cleveland       &  1892 &             Democratic &       5553898 &    win &  46.121393 \\
James B. Weaver        &  1892 &               Populist &       1041028 &   loss &   8.645038 \\
John Bidwell           &  1892 &            Prohibition &        270879 &   loss &   2.249468 \\
John M. Palmer         &  1896 &    National Democratic &        134645 &   loss &   0.969566 \\
Joshua Levering        &  1896 &            Prohibition &        131312 &   loss &   0.945565 \\
William Jennings Bryan &  1896 &             Democratic &       6509052 &   loss &  46.871053 \\
William McKinley       &  1896 &             Republican &       7112138 &    win &  51.213817 \\
John G. Woolley        &  1900 &            Prohibition &        210864 &   loss &   1.526821 \\
William Jennings Bryan &  1900 &             Democratic &       6370932 &   loss &  46.130540 \\
William McKinley       &  1900 &             Republican &       7228864 &    win &  52.342640 \\
Alton B. Parker        &  1904 &             Democratic &       5083880 &   loss &  37.685116 \\
Eugene V. Debs         &  1904 &              Socialist &        402810 &   loss &   2.985897 \\
Silas C. Swallow       &  1904 &            Prohibition &        259102 &   loss &   1.920637 \\
Theodore Roosevelt     &  1904 &             Republican &       7630557 &    win &  56.562787 \\
Thomas E. Watson       &  1904 &               Populist &        114070 &   loss &   0.845563 \\
Eugene V. Debs         &  1908 &              Socialist &        420852 &   loss &   2.850866 \\
Eugene W. Chafin       &  1908 &            Prohibition &        254087 &   loss &   1.721194 \\
William Jennings Bryan &  1908 &             Democratic &       6408979 &   loss &  43.414640 \\
William Taft           &  1908 &             Republican &       7678335 &    win &  52.013300 \\
Eugene V. Debs         &  1912 &              Socialist &        901551 &   loss &   6.004354 \\
Eugene W. Chafin       &  1912 &            Prohibition &        208156 &   loss &   1.386325 \\
Theodore Roosevelt     &  1912 &            Progressive &       4122721 &   loss &  27.457433 \\
William Taft           &  1912 &             Republican &       3486242 &   loss &  23.218466 \\
Woodrow Wilson         &  1912 &             Democratic &       6296284 &    win &  41.933422 \\
Allan L. Benson        &  1916 &              Socialist &        590524 &   loss &   3.194193 \\
Charles Evans Hughes   &  1916 &             Republican &       8548728 &   loss &  46.240779 \\
Frank Hanly            &  1916 &            Prohibition &        221302 &   loss &   1.197041 \\
Woodrow Wilson         &  1916 &             Democratic &       9126868 &    win &  49.367987 \\
Aaron S. Watkins       &  1920 &            Prohibition &        188787 &   loss &   0.708351 \\
Eugene V. Debs         &  1920 &              Socialist &        913693 &   loss &   3.428282 \\
James M. Cox           &  1920 &             Democratic &       9139661 &   loss &  34.293063 \\
Parley P. Christensen  &  1920 &           Farmer–Labor &        265398 &   loss &   0.995804 \\
Warren Harding         &  1920 &             Republican &      16144093 &    win &  60.574501 \\
Calvin Coolidge        &  1924 &             Republican &      15723789 &    win &  54.329113 \\
John W. Davis          &  1924 &             Democratic &       8386242 &   loss &  28.976291 \\
Robert La Follette     &  1924 &            Progressive &       4831706 &   loss &  16.694596 \\
Al Smith               &  1928 &             Democratic &      15015464 &   loss &  40.902853 \\
Herbert Hoover         &  1928 &             Republican &      21427123 &    win &  58.368524 \\
Norman Thomas          &  1928 &              Socialist &        267478 &   loss &   0.728623 \\
Franklin Roosevelt     &  1932 &             Democratic &      22821277 &    win &  57.672125 \\
Herbert Hoover         &  1932 &             Republican &      15761254 &   loss &  39.830594 \\
Norman Thomas          &  1932 &              Socialist &        884885 &   loss &   2.236211 \\
William Z. Foster      &  1932 &              Communist &        103307 &   loss &   0.261069 \\
Alf Landon             &  1936 &             Republican &      16679543 &   loss &  36.648285 \\
Franklin Roosevelt     &  1936 &             Democratic &      27752648 &    win &  60.978107 \\
Norman Thomas          &  1936 &              Socialist &        187910 &   loss &   0.412876 \\
William Lemke          &  1936 &                  Union &        892378 &   loss &   1.960733 \\
Franklin Roosevelt     &  1940 &             Democratic &      27313945 &    win &  54.871202 \\
Norman Thomas          &  1940 &              Socialist &        116599 &   loss &   0.234237 \\
Wendell Willkie        &  1940 &             Republican &      22347744 &   loss &  44.894561 \\
Franklin Roosevelt     &  1944 &             Democratic &      25612916 &    win &  53.773801 \\
Thomas E. Dewey        &  1944 &             Republican &      22017929 &   loss &  46.226199 \\
Claude A. Watson       &  1948 &            Prohibition &        103708 &   loss &   0.212747 \\
Harry Truman           &  1948 &             Democratic &      24179347 &    win &  49.601536 \\
Henry A. Wallace       &  1948 &            Progressive &       1157328 &   loss &   2.374144 \\
Norman Thomas          &  1948 &              Socialist &        139569 &   loss &   0.286312 \\
Strom Thurmond         &  1948 &              Dixiecrat &       1175930 &   loss &   2.412304 \\
Thomas E. Dewey        &  1948 &             Republican &      21991292 &   loss &  45.112958 \\
Adlai Stevenson        &  1952 &             Democratic &      27375090 &   loss &  44.446312 \\
Dwight Eisenhower      &  1952 &             Republican &      34075529 &    win &  55.325173 \\
Vincent Hallinan       &  1952 &            Progressive &        140746 &   loss &   0.228516 \\
Adlai Stevenson        &  1956 &             Democratic &      26028028 &   loss &  42.174464 \\
Dwight Eisenhower      &  1956 &             Republican &      35579180 &    win &  57.650654 \\
T. Coleman Andrews     &  1956 &         States' Rights &        107929 &   loss &   0.174883 \\
John Kennedy           &  1960 &             Democratic &      34220984 &    win &  50.082561 \\
Richard Nixon          &  1960 &             Republican &      34108157 &   loss &  49.917439 \\
Barry Goldwater        &  1964 &             Republican &      27175754 &   loss &  38.655297 \\
Lyndon Johnson         &  1964 &             Democratic &      43127041 &    win &  61.344703 \\
George Wallace         &  1968 &   American Independent &       9901118 &   loss &  13.571218 \\
Hubert Humphrey        &  1968 &             Democratic &      31271839 &   loss &  42.863537 \\
Richard Nixon          &  1968 &             Republican &      31783783 &    win &  43.565246 \\
George McGovern        &  1972 &             Democratic &      29173222 &   loss &  37.670670 \\
John G. Schmitz        &  1972 &   American Independent &       1100868 &   loss &   1.421524 \\
Richard Nixon          &  1972 &             Republican &      47168710 &    win &  60.907806 \\
Eugene McCarthy        &  1976 &            Independent &        740460 &   loss &   0.911649 \\
Gerald Ford            &  1976 &             Republican &      39148634 &   loss &  48.199499 \\
Jimmy Carter           &  1976 &             Democratic &      40831881 &    win &  50.271900 \\
Lester Maddox          &  1976 &   American Independent &        170274 &   loss &   0.209640 \\
Roger MacBride         &  1976 &            Libertarian &        172557 &   loss &   0.212451 \\
Thomas J. Anderson     &  1976 &               American &        158271 &   loss &   0.194862 \\
Barry Commoner         &  1980 &               Citizens &        233052 &   loss &   0.270182 \\
Ed Clark               &  1980 &            Libertarian &        921128 &   loss &   1.067883 \\
Jimmy Carter           &  1980 &             Democratic &      35480115 &   loss &  41.132848 \\
John B. Anderson       &  1980 &            Independent &       5719850 &   loss &   6.631143 \\
Ronald Reagan          &  1980 &             Republican &      43903230 &    win &  50.897944 \\
David Bergland         &  1984 &            Libertarian &        228111 &   loss &   0.247245 \\
Ronald Reagan          &  1984 &             Republican &      54455472 &    win &  59.023326 \\
Walter Mondale         &  1984 &             Democratic &      37577352 &   loss &  40.729429 \\
George H. W. Bush      &  1988 &             Republican &      48886597 &    win &  53.518845 \\
Lenora Fulani          &  1988 &           New Alliance &        217221 &   loss &   0.237804 \\
Michael Dukakis        &  1988 &             Democratic &      41809074 &   loss &  45.770691 \\
Ron Paul               &  1988 &            Libertarian &        431750 &   loss &   0.472660 \\
Andre Marrou           &  1992 &            Libertarian &        290087 &   loss &   0.278516 \\
Bill Clinton           &  1992 &             Democratic &      44909806 &    win &  43.118485 \\
Bo Gritz               &  1992 &               Populist &        106152 &   loss &   0.101918 \\
George H. W. Bush      &  1992 &             Republican &      39104550 &   loss &  37.544784 \\
Ross Perot             &  1992 &            Independent &      19743821 &   loss &  18.956298 \\
Bill Clinton           &  1996 &             Democratic &      47400125 &    win &  49.296938 \\
Bob Dole               &  1996 &             Republican &      39197469 &   loss &  40.766036 \\
Harry Browne           &  1996 &            Libertarian &        485759 &   loss &   0.505198 \\
Howard Phillips        &  1996 &              Taxpayers &        184656 &   loss &   0.192045 \\
John Hagelin           &  1996 &            Natural Law &        113670 &   loss &   0.118219 \\
Ralph Nader            &  1996 &                  Green &        685297 &   loss &   0.712721 \\
Ross Perot             &  1996 &                 Reform &       8085294 &   loss &   8.408844 \\
Al Gore                &  2000 &             Democratic &      50999897 &   loss &  48.491813 \\
George W. Bush         &  2000 &             Republican &      50456002 &    win &  47.974666 \\
Harry Browne           &  2000 &            Libertarian &        384431 &   loss &   0.365525 \\
Pat Buchanan           &  2000 &                 Reform &        448895 &   loss &   0.426819 \\
Ralph Nader            &  2000 &                  Green &       2882955 &   loss &   2.741176 \\
David Cobb             &  2004 &                  Green &        119859 &   loss &   0.098088 \\
George W. Bush         &  2004 &             Republican &      62040610 &    win &  50.771824 \\
John Kerry             &  2004 &             Democratic &      59028444 &   loss &  48.306775 \\
Michael Badnarik       &  2004 &            Libertarian &        397265 &   loss &   0.325108 \\
Michael Peroutka       &  2004 &           Constitution &        143630 &   loss &   0.117542 \\
Ralph Nader            &  2004 &            Independent &        465151 &   loss &   0.380663 \\
Barack Obama           &  2008 &             Democratic &      69498516 &    win &  53.023510 \\
Bob Barr               &  2008 &            Libertarian &        523715 &   loss &   0.399565 \\
Chuck Baldwin          &  2008 &           Constitution &        199750 &   loss &   0.152398 \\
Cynthia McKinney       &  2008 &                  Green &        161797 &   loss &   0.123442 \\
John McCain            &  2008 &             Republican &      59948323 &   loss &  45.737243 \\
Ralph Nader            &  2008 &            Independent &        739034 &   loss &   0.563842 \\
Barack Obama           &  2012 &             Democratic &      65915795 &    win &  51.258484 \\
Gary Johnson           &  2012 &            Libertarian &       1275971 &   loss &   0.992241 \\
Jill Stein             &  2012 &                  Green &        469627 &   loss &   0.365199 \\
Mitt Romney            &  2012 &             Republican &      60933504 &   loss &  47.384076 \\
Darrell Castle         &  2016 &           Constitution &        203091 &   loss &   0.149640 \\
Donald Trump           &  2016 &             Republican &      62984828 &    win &  46.407862 \\
Evan McMullin          &  2016 &            Independent &        732273 &   loss &   0.539546 \\
Gary Johnson           &  2016 &            Libertarian &       4489235 &   loss &   3.307714 \\
Hillary Clinton        &  2016 &             Democratic &      65853514 &   loss &  48.521539 \\
Jill Stein             &  2016 &                  Green &       1457226 &   loss &   1.073699 \\
Joseph Biden           &  2020 &             Democratic &      81268924 &    win &  51.311515 \\
Donald Trump           &  2020 &             Republican &      74216154 &   loss &  46.858542 \\
Jo Jorgensen           &  2020 &            Libertarian &       1865724 &   loss &   1.177979 \\
Howard Hawkins         &  2020 &                  Green &        405035 &   loss &   0.255731 \\
\bottomrule
\end{tabular}

We can also select a new column and set it as the index of the
\texttt{DataFrame}. For example, we can set the index of the
\texttt{elections} \texttt{DataFrame} to represent the candidate's
party.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.reset\_index(inplace }\OperatorTok{=} \VariableTok{True}\NormalTok{) }\CommentTok{\# Resetting the index so we can set it again}
\CommentTok{\# This sets the index to the "Party" column}
\NormalTok{elections.set\_index(}\StringTok{"Party"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llrrlr}
\toprule
{} &               Candidate &  Year &  Popular vote & Result &          \% \\
Party                 &                         &       &               &        &            \\
\midrule
Democratic-Republican &          Andrew Jackson &  1824 &        151271 &   loss &  57.210122 \\
Democratic-Republican &       John Quincy Adams &  1824 &        113142 &    win &  42.789878 \\
Democratic            &          Andrew Jackson &  1828 &        642806 &    win &  56.203927 \\
National Republican   &       John Quincy Adams &  1828 &        500897 &   loss &  43.796073 \\
Democratic            &          Andrew Jackson &  1832 &        702735 &    win &  54.574789 \\
National Republican   &              Henry Clay &  1832 &        484205 &   loss &  37.603628 \\
Anti-Masonic          &            William Wirt &  1832 &        100715 &   loss &   7.821583 \\
Whig                  &       Hugh Lawson White &  1836 &        146109 &   loss &  10.005985 \\
Democratic            &        Martin Van Buren &  1836 &        763291 &    win &  52.272472 \\
Whig                  &  William Henry Harrison &  1836 &        550816 &   loss &  37.721543 \\
Democratic            &        Martin Van Buren &  1840 &       1128854 &   loss &  46.948787 \\
Whig                  &  William Henry Harrison &  1840 &       1275583 &    win &  53.051213 \\
Whig                  &              Henry Clay &  1844 &       1300004 &   loss &  49.250523 \\
Democratic            &              James Polk &  1844 &       1339570 &    win &  50.749477 \\
Democratic            &              Lewis Cass &  1848 &       1223460 &   loss &  42.552229 \\
Free Soil             &        Martin Van Buren &  1848 &        291501 &   loss &  10.138474 \\
Whig                  &          Zachary Taylor &  1848 &       1360235 &    win &  47.309296 \\
Democratic            &         Franklin Pierce &  1852 &       1605943 &    win &  51.013168 \\
Free Soil             &            John P. Hale &  1852 &        155210 &   loss &   4.930283 \\
Whig                  &          Winfield Scott &  1852 &       1386942 &   loss &  44.056548 \\
Democratic            &          James Buchanan &  1856 &       1835140 &    win &  45.306080 \\
Republican            &         John C. Frémont &  1856 &       1342345 &   loss &  33.139919 \\
American              &        Millard Fillmore &  1856 &        873053 &   loss &  21.554001 \\
Republican            &         Abraham Lincoln &  1860 &       1855993 &    win &  39.699408 \\
Constitutional Union  &               John Bell &  1860 &        590901 &   loss &  12.639283 \\
Southern Democratic   &    John C. Breckinridge &  1860 &        848019 &   loss &  18.138998 \\
Northern Democratic   &      Stephen A. Douglas &  1860 &       1380202 &   loss &  29.522311 \\
National Union        &         Abraham Lincoln &  1864 &       2211317 &    win &  54.951512 \\
Democratic            &     George B. McClellan &  1864 &       1812807 &   loss &  45.048488 \\
Democratic            &         Horatio Seymour &  1868 &       2708744 &   loss &  47.334695 \\
Republican            &           Ulysses Grant &  1868 &       3013790 &    win &  52.665305 \\
Liberal Republican    &          Horace Greeley &  1872 &       2834761 &   loss &  44.071406 \\
Republican            &           Ulysses Grant &  1872 &       3597439 &    win &  55.928594 \\
Republican            &        Rutherford Hayes &  1876 &       4034142 &    win &  48.471624 \\
Democratic            &        Samuel J. Tilden &  1876 &       4288546 &   loss &  51.528376 \\
Greenback             &         James B. Weaver &  1880 &        308649 &   loss &   3.352344 \\
Republican            &          James Garfield &  1880 &       4453337 &    win &  48.369234 \\
Democratic            &  Winfield Scott Hancock &  1880 &       4444976 &   loss &  48.278422 \\
Anti-Monopoly         &         Benjamin Butler &  1884 &        134294 &   loss &   1.335838 \\
Democratic            &        Grover Cleveland &  1884 &       4914482 &    win &  48.884933 \\
Republican            &         James G. Blaine &  1884 &       4856905 &   loss &  48.312208 \\
Prohibition           &           John St. John &  1884 &        147482 &   loss &   1.467021 \\
Union Labor           &          Alson Streeter &  1888 &        146602 &   loss &   1.288861 \\
Republican            &       Benjamin Harrison &  1888 &       5443633 &    win &  47.858041 \\
Prohibition           &         Clinton B. Fisk &  1888 &        249819 &   loss &   2.196299 \\
Democratic            &        Grover Cleveland &  1888 &       5534488 &   loss &  48.656799 \\
Republican            &       Benjamin Harrison &  1892 &       5176108 &   loss &  42.984101 \\
Democratic            &        Grover Cleveland &  1892 &       5553898 &    win &  46.121393 \\
Populist              &         James B. Weaver &  1892 &       1041028 &   loss &   8.645038 \\
Prohibition           &            John Bidwell &  1892 &        270879 &   loss &   2.249468 \\
National Democratic   &          John M. Palmer &  1896 &        134645 &   loss &   0.969566 \\
Prohibition           &         Joshua Levering &  1896 &        131312 &   loss &   0.945565 \\
Democratic            &  William Jennings Bryan &  1896 &       6509052 &   loss &  46.871053 \\
Republican            &        William McKinley &  1896 &       7112138 &    win &  51.213817 \\
Prohibition           &         John G. Woolley &  1900 &        210864 &   loss &   1.526821 \\
Democratic            &  William Jennings Bryan &  1900 &       6370932 &   loss &  46.130540 \\
Republican            &        William McKinley &  1900 &       7228864 &    win &  52.342640 \\
Democratic            &         Alton B. Parker &  1904 &       5083880 &   loss &  37.685116 \\
Socialist             &          Eugene V. Debs &  1904 &        402810 &   loss &   2.985897 \\
Prohibition           &        Silas C. Swallow &  1904 &        259102 &   loss &   1.920637 \\
Republican            &      Theodore Roosevelt &  1904 &       7630557 &    win &  56.562787 \\
Populist              &        Thomas E. Watson &  1904 &        114070 &   loss &   0.845563 \\
Socialist             &          Eugene V. Debs &  1908 &        420852 &   loss &   2.850866 \\
Prohibition           &        Eugene W. Chafin &  1908 &        254087 &   loss &   1.721194 \\
Democratic            &  William Jennings Bryan &  1908 &       6408979 &   loss &  43.414640 \\
Republican            &            William Taft &  1908 &       7678335 &    win &  52.013300 \\
Socialist             &          Eugene V. Debs &  1912 &        901551 &   loss &   6.004354 \\
Prohibition           &        Eugene W. Chafin &  1912 &        208156 &   loss &   1.386325 \\
Progressive           &      Theodore Roosevelt &  1912 &       4122721 &   loss &  27.457433 \\
Republican            &            William Taft &  1912 &       3486242 &   loss &  23.218466 \\
Democratic            &          Woodrow Wilson &  1912 &       6296284 &    win &  41.933422 \\
Socialist             &         Allan L. Benson &  1916 &        590524 &   loss &   3.194193 \\
Republican            &    Charles Evans Hughes &  1916 &       8548728 &   loss &  46.240779 \\
Prohibition           &             Frank Hanly &  1916 &        221302 &   loss &   1.197041 \\
Democratic            &          Woodrow Wilson &  1916 &       9126868 &    win &  49.367987 \\
Prohibition           &        Aaron S. Watkins &  1920 &        188787 &   loss &   0.708351 \\
Socialist             &          Eugene V. Debs &  1920 &        913693 &   loss &   3.428282 \\
Democratic            &            James M. Cox &  1920 &       9139661 &   loss &  34.293063 \\
Farmer–Labor          &   Parley P. Christensen &  1920 &        265398 &   loss &   0.995804 \\
Republican            &          Warren Harding &  1920 &      16144093 &    win &  60.574501 \\
Republican            &         Calvin Coolidge &  1924 &      15723789 &    win &  54.329113 \\
Democratic            &           John W. Davis &  1924 &       8386242 &   loss &  28.976291 \\
Progressive           &      Robert La Follette &  1924 &       4831706 &   loss &  16.694596 \\
Democratic            &                Al Smith &  1928 &      15015464 &   loss &  40.902853 \\
Republican            &          Herbert Hoover &  1928 &      21427123 &    win &  58.368524 \\
Socialist             &           Norman Thomas &  1928 &        267478 &   loss &   0.728623 \\
Democratic            &      Franklin Roosevelt &  1932 &      22821277 &    win &  57.672125 \\
Republican            &          Herbert Hoover &  1932 &      15761254 &   loss &  39.830594 \\
Socialist             &           Norman Thomas &  1932 &        884885 &   loss &   2.236211 \\
Communist             &       William Z. Foster &  1932 &        103307 &   loss &   0.261069 \\
Republican            &              Alf Landon &  1936 &      16679543 &   loss &  36.648285 \\
Democratic            &      Franklin Roosevelt &  1936 &      27752648 &    win &  60.978107 \\
Socialist             &           Norman Thomas &  1936 &        187910 &   loss &   0.412876 \\
Union                 &           William Lemke &  1936 &        892378 &   loss &   1.960733 \\
Democratic            &      Franklin Roosevelt &  1940 &      27313945 &    win &  54.871202 \\
Socialist             &           Norman Thomas &  1940 &        116599 &   loss &   0.234237 \\
Republican            &         Wendell Willkie &  1940 &      22347744 &   loss &  44.894561 \\
Democratic            &      Franklin Roosevelt &  1944 &      25612916 &    win &  53.773801 \\
Republican            &         Thomas E. Dewey &  1944 &      22017929 &   loss &  46.226199 \\
Prohibition           &        Claude A. Watson &  1948 &        103708 &   loss &   0.212747 \\
Democratic            &            Harry Truman &  1948 &      24179347 &    win &  49.601536 \\
Progressive           &        Henry A. Wallace &  1948 &       1157328 &   loss &   2.374144 \\
Socialist             &           Norman Thomas &  1948 &        139569 &   loss &   0.286312 \\
Dixiecrat             &          Strom Thurmond &  1948 &       1175930 &   loss &   2.412304 \\
Republican            &         Thomas E. Dewey &  1948 &      21991292 &   loss &  45.112958 \\
Democratic            &         Adlai Stevenson &  1952 &      27375090 &   loss &  44.446312 \\
Republican            &       Dwight Eisenhower &  1952 &      34075529 &    win &  55.325173 \\
Progressive           &        Vincent Hallinan &  1952 &        140746 &   loss &   0.228516 \\
Democratic            &         Adlai Stevenson &  1956 &      26028028 &   loss &  42.174464 \\
Republican            &       Dwight Eisenhower &  1956 &      35579180 &    win &  57.650654 \\
States' Rights        &      T. Coleman Andrews &  1956 &        107929 &   loss &   0.174883 \\
Democratic            &            John Kennedy &  1960 &      34220984 &    win &  50.082561 \\
Republican            &           Richard Nixon &  1960 &      34108157 &   loss &  49.917439 \\
Republican            &         Barry Goldwater &  1964 &      27175754 &   loss &  38.655297 \\
Democratic            &          Lyndon Johnson &  1964 &      43127041 &    win &  61.344703 \\
American Independent  &          George Wallace &  1968 &       9901118 &   loss &  13.571218 \\
Democratic            &         Hubert Humphrey &  1968 &      31271839 &   loss &  42.863537 \\
Republican            &           Richard Nixon &  1968 &      31783783 &    win &  43.565246 \\
Democratic            &         George McGovern &  1972 &      29173222 &   loss &  37.670670 \\
American Independent  &         John G. Schmitz &  1972 &       1100868 &   loss &   1.421524 \\
Republican            &           Richard Nixon &  1972 &      47168710 &    win &  60.907806 \\
Independent           &         Eugene McCarthy &  1976 &        740460 &   loss &   0.911649 \\
Republican            &             Gerald Ford &  1976 &      39148634 &   loss &  48.199499 \\
Democratic            &            Jimmy Carter &  1976 &      40831881 &    win &  50.271900 \\
American Independent  &           Lester Maddox &  1976 &        170274 &   loss &   0.209640 \\
Libertarian           &          Roger MacBride &  1976 &        172557 &   loss &   0.212451 \\
American              &      Thomas J. Anderson &  1976 &        158271 &   loss &   0.194862 \\
Citizens              &          Barry Commoner &  1980 &        233052 &   loss &   0.270182 \\
Libertarian           &                Ed Clark &  1980 &        921128 &   loss &   1.067883 \\
Democratic            &            Jimmy Carter &  1980 &      35480115 &   loss &  41.132848 \\
Independent           &        John B. Anderson &  1980 &       5719850 &   loss &   6.631143 \\
Republican            &           Ronald Reagan &  1980 &      43903230 &    win &  50.897944 \\
Libertarian           &          David Bergland &  1984 &        228111 &   loss &   0.247245 \\
Republican            &           Ronald Reagan &  1984 &      54455472 &    win &  59.023326 \\
Democratic            &          Walter Mondale &  1984 &      37577352 &   loss &  40.729429 \\
Republican            &       George H. W. Bush &  1988 &      48886597 &    win &  53.518845 \\
New Alliance          &           Lenora Fulani &  1988 &        217221 &   loss &   0.237804 \\
Democratic            &         Michael Dukakis &  1988 &      41809074 &   loss &  45.770691 \\
Libertarian           &                Ron Paul &  1988 &        431750 &   loss &   0.472660 \\
Libertarian           &            Andre Marrou &  1992 &        290087 &   loss &   0.278516 \\
Democratic            &            Bill Clinton &  1992 &      44909806 &    win &  43.118485 \\
Populist              &                Bo Gritz &  1992 &        106152 &   loss &   0.101918 \\
Republican            &       George H. W. Bush &  1992 &      39104550 &   loss &  37.544784 \\
Independent           &              Ross Perot &  1992 &      19743821 &   loss &  18.956298 \\
Democratic            &            Bill Clinton &  1996 &      47400125 &    win &  49.296938 \\
Republican            &                Bob Dole &  1996 &      39197469 &   loss &  40.766036 \\
Libertarian           &            Harry Browne &  1996 &        485759 &   loss &   0.505198 \\
Taxpayers             &         Howard Phillips &  1996 &        184656 &   loss &   0.192045 \\
Natural Law           &            John Hagelin &  1996 &        113670 &   loss &   0.118219 \\
Green                 &             Ralph Nader &  1996 &        685297 &   loss &   0.712721 \\
Reform                &              Ross Perot &  1996 &       8085294 &   loss &   8.408844 \\
Democratic            &                 Al Gore &  2000 &      50999897 &   loss &  48.491813 \\
Republican            &          George W. Bush &  2000 &      50456002 &    win &  47.974666 \\
Libertarian           &            Harry Browne &  2000 &        384431 &   loss &   0.365525 \\
Reform                &            Pat Buchanan &  2000 &        448895 &   loss &   0.426819 \\
Green                 &             Ralph Nader &  2000 &       2882955 &   loss &   2.741176 \\
Green                 &              David Cobb &  2004 &        119859 &   loss &   0.098088 \\
Republican            &          George W. Bush &  2004 &      62040610 &    win &  50.771824 \\
Democratic            &              John Kerry &  2004 &      59028444 &   loss &  48.306775 \\
Libertarian           &        Michael Badnarik &  2004 &        397265 &   loss &   0.325108 \\
Constitution          &        Michael Peroutka &  2004 &        143630 &   loss &   0.117542 \\
Independent           &             Ralph Nader &  2004 &        465151 &   loss &   0.380663 \\
Democratic            &            Barack Obama &  2008 &      69498516 &    win &  53.023510 \\
Libertarian           &                Bob Barr &  2008 &        523715 &   loss &   0.399565 \\
Constitution          &           Chuck Baldwin &  2008 &        199750 &   loss &   0.152398 \\
Green                 &        Cynthia McKinney &  2008 &        161797 &   loss &   0.123442 \\
Republican            &             John McCain &  2008 &      59948323 &   loss &  45.737243 \\
Independent           &             Ralph Nader &  2008 &        739034 &   loss &   0.563842 \\
Democratic            &            Barack Obama &  2012 &      65915795 &    win &  51.258484 \\
Libertarian           &            Gary Johnson &  2012 &       1275971 &   loss &   0.992241 \\
Green                 &              Jill Stein &  2012 &        469627 &   loss &   0.365199 \\
Republican            &             Mitt Romney &  2012 &      60933504 &   loss &  47.384076 \\
Constitution          &          Darrell Castle &  2016 &        203091 &   loss &   0.149640 \\
Republican            &            Donald Trump &  2016 &      62984828 &    win &  46.407862 \\
Independent           &           Evan McMullin &  2016 &        732273 &   loss &   0.539546 \\
Libertarian           &            Gary Johnson &  2016 &       4489235 &   loss &   3.307714 \\
Democratic            &         Hillary Clinton &  2016 &      65853514 &   loss &  48.521539 \\
Green                 &              Jill Stein &  2016 &       1457226 &   loss &   1.073699 \\
Democratic            &            Joseph Biden &  2020 &      81268924 &    win &  51.311515 \\
Republican            &            Donald Trump &  2020 &      74216154 &   loss &  46.858542 \\
Libertarian           &            Jo Jorgensen &  2020 &       1865724 &   loss &   1.177979 \\
Green                 &          Howard Hawkins &  2020 &        405035 &   loss &   0.255731 \\
\bottomrule
\end{tabular}

And, if we'd like, we can revert the index back to the default list of
integers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This resets the index to be the default list of integer}
\NormalTok{elections.reset\_index(inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{) }
\NormalTok{elections.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RangeIndex(start=0, stop=182, step=1)
\end{verbatim}

It is also important to note that the row labels that constitute an
index don't have to be unique. While index values can be unique and
numeric, acting as a row number, they can also be named and non-unique.

Here we see unique and numeric index values.

However, here the index values are not unique.

\hypertarget{dataframe-attributes-index-columns-and-shape}{%
\section{\texorpdfstring{\texttt{DataFrame} Attributes: Index, Columns,
and
Shape}{DataFrame Attributes: Index, Columns, and Shape}}\label{dataframe-attributes-index-columns-and-shape}}

On the other hand, column names in a \texttt{DataFrame} are almost
always unique. Looking back to the \texttt{elections} dataset, it
wouldn't make sense to have two columns named \texttt{"Candidate"}.
Sometimes, you'll want to extract these different values, in particular,
the list of row and column labels.

For index/row labels, use \texttt{DataFrame.index}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.set\_index(}\StringTok{"Party"}\NormalTok{, inplace }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\NormalTok{elections.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['Democratic-Republican', 'Democratic-Republican', 'Democratic',
       'National Republican', 'Democratic', 'National Republican',
       'Anti-Masonic', 'Whig', 'Democratic', 'Whig',
       ...
       'Constitution', 'Republican', 'Independent', 'Libertarian',
       'Democratic', 'Green', 'Democratic', 'Republican', 'Libertarian',
       'Green'],
      dtype='object', name='Party', length=182)
\end{verbatim}

For column labels, use \texttt{DataFrame.columns}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['index', 'Candidate', 'Year', 'Popular vote', 'Result', '%'], dtype='object')
\end{verbatim}

And for the shape of the \texttt{DataFrame}, we can use
\texttt{DataFrame.shape} to get the number of rows followed by the
number of columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(182, 6)
\end{verbatim}

\hypertarget{slicing-in-dataframes}{%
\section{\texorpdfstring{Slicing in
\texttt{DataFrame}s}{Slicing in DataFrames}}\label{slicing-in-dataframes}}

Now that we've learned more about \texttt{DataFrame}s, let's dive deeper
into their capabilities.

The API (Application Programming Interface) for the \texttt{DataFrame}
class is enormous. In this section, we'll discuss several methods of the
\texttt{DataFrame} API that allow us to extract subsets of data.

The simplest way to manipulate a \texttt{DataFrame} is to extract a
subset of rows and columns, known as \textbf{slicing}.

Common ways we may want to extract data are grabbing:

\begin{itemize}
\tightlist
\item
  The first or last \texttt{n} rows in the \texttt{DataFrame}.
\item
  Data with a certain label.
\item
  Data at a certain position.
\end{itemize}

We will do so with four primary methods of the \texttt{DataFrame} class:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.head} and \texttt{.tail}
\item
  \texttt{.loc}
\item
  \texttt{.iloc}
\item
  \texttt{{[}{]}}
\end{enumerate}

\hypertarget{extracting-data-with-.head-and-.tail}{%
\subsection{\texorpdfstring{Extracting data with \texttt{.head} and
\texttt{.tail}}{Extracting data with .head and .tail}}\label{extracting-data-with-.head-and-.tail}}

The simplest scenario in which we want to extract data is when we simply
want to select the first or last few rows of the \texttt{DataFrame}.

To extract the first \texttt{n} rows of a \texttt{DataFrame}
\texttt{df}, we use the syntax \texttt{df.head(n)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract the first 5 rows of the DataFrame}
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

Similarly, calling \texttt{df.tail(n)} allows us to extract the last
\texttt{n} rows of the \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract the last 5 rows of the DataFrame}
\NormalTok{elections.tail(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &       Candidate &        Party &  Popular vote & Result &          \% \\
\midrule
177 &  2016 &      Jill Stein &        Green &       1457226 &   loss &   1.073699 \\
178 &  2020 &    Joseph Biden &   Democratic &      81268924 &    win &  51.311515 \\
179 &  2020 &    Donald Trump &   Republican &      74216154 &   loss &  46.858542 \\
180 &  2020 &    Jo Jorgensen &  Libertarian &       1865724 &   loss &   1.177979 \\
181 &  2020 &  Howard Hawkins &        Green &        405035 &   loss &   0.255731 \\
\bottomrule
\end{tabular}

\hypertarget{label-based-extraction-indexing-with-.loc}{%
\subsection{\texorpdfstring{Label-based Extraction: Indexing with
\texttt{.loc}}{Label-based Extraction: Indexing with .loc}}\label{label-based-extraction-indexing-with-.loc}}

For the more complex task of extracting data with specific column or
index labels, we can use \texttt{.loc}. The \texttt{.loc} accessor
allows us to specify the \textbf{\emph{labels}} of rows and columns we
wish to extract. The \textbf{labels} (commonly referred to as the
\textbf{indices}) are the bold text on the far \emph{left} of a
\texttt{DataFrame}, while the \textbf{column labels} are the column
names found at the \emph{top} of a \texttt{DataFrame}.

To grab data with \texttt{.loc}, we must specify the row and column
label(s) where the data exists. The row labels are the first argument to
the \texttt{.loc} function; the column labels are the second.

Arguments to \texttt{.loc} can be:

\begin{itemize}
\tightlist
\item
  A single value.
\item
  A slice.
\item
  A list.
\end{itemize}

For example, to select a single value, we can select the row labeled
\texttt{0} and the column labeled \texttt{Candidate} from the
\texttt{elections} \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}Candidate\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Andrew Jackson'
\end{verbatim}

Keep in mind that passing in just one argument as a single value will
produce a \texttt{Series}. Below, we've extracted a subset of the
\texttt{"Popular\ vote"} column as a \texttt{Series}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[[}\DecValTok{87}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{179}\NormalTok{], }\StringTok{"Popular vote"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Popular vote \\
\midrule
87  &      15761254 \\
25  &        848019 \\
179 &      74216154 \\
\bottomrule
\end{tabular}

To select \emph{multiple} rows and columns, we can use Python slice
notation. Here, we select the rows from labels \texttt{0} to \texttt{3}
and the columns from labels \texttt{"Year"} to \texttt{"Popular\ vote"}.
Notice that unlike Python slicing, \texttt{.loc} is \emph{inclusive} of
the right upper bound.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Popular vote\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Suppose that instead, we want to extract \emph{all} column values for
the first four rows in the \texttt{elections} \texttt{DataFrame}. The
shorthand \texttt{:} is useful for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{, :]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

We can use the same shorthand to extract all rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[:, [}\StringTok{"Year"}\NormalTok{, }\StringTok{"Candidate"}\NormalTok{, }\StringTok{"Result"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrll}
\toprule
{} &  Year &               Candidate & Result \\
\midrule
0   &  1824 &          Andrew Jackson &   loss \\
1   &  1824 &       John Quincy Adams &    win \\
2   &  1828 &          Andrew Jackson &    win \\
3   &  1828 &       John Quincy Adams &   loss \\
4   &  1832 &          Andrew Jackson &    win \\
5   &  1832 &              Henry Clay &   loss \\
6   &  1832 &            William Wirt &   loss \\
7   &  1836 &       Hugh Lawson White &   loss \\
8   &  1836 &        Martin Van Buren &    win \\
9   &  1836 &  William Henry Harrison &   loss \\
10  &  1840 &        Martin Van Buren &   loss \\
11  &  1840 &  William Henry Harrison &    win \\
12  &  1844 &              Henry Clay &   loss \\
13  &  1844 &              James Polk &    win \\
14  &  1848 &              Lewis Cass &   loss \\
15  &  1848 &        Martin Van Buren &   loss \\
16  &  1848 &          Zachary Taylor &    win \\
17  &  1852 &         Franklin Pierce &    win \\
18  &  1852 &            John P. Hale &   loss \\
19  &  1852 &          Winfield Scott &   loss \\
20  &  1856 &          James Buchanan &    win \\
21  &  1856 &         John C. Frémont &   loss \\
22  &  1856 &        Millard Fillmore &   loss \\
23  &  1860 &         Abraham Lincoln &    win \\
24  &  1860 &               John Bell &   loss \\
25  &  1860 &    John C. Breckinridge &   loss \\
26  &  1860 &      Stephen A. Douglas &   loss \\
27  &  1864 &         Abraham Lincoln &    win \\
28  &  1864 &     George B. McClellan &   loss \\
29  &  1868 &         Horatio Seymour &   loss \\
30  &  1868 &           Ulysses Grant &    win \\
31  &  1872 &          Horace Greeley &   loss \\
32  &  1872 &           Ulysses Grant &    win \\
33  &  1876 &        Rutherford Hayes &    win \\
34  &  1876 &        Samuel J. Tilden &   loss \\
35  &  1880 &         James B. Weaver &   loss \\
36  &  1880 &          James Garfield &    win \\
37  &  1880 &  Winfield Scott Hancock &   loss \\
38  &  1884 &         Benjamin Butler &   loss \\
39  &  1884 &        Grover Cleveland &    win \\
40  &  1884 &         James G. Blaine &   loss \\
41  &  1884 &           John St. John &   loss \\
42  &  1888 &          Alson Streeter &   loss \\
43  &  1888 &       Benjamin Harrison &    win \\
44  &  1888 &         Clinton B. Fisk &   loss \\
45  &  1888 &        Grover Cleveland &   loss \\
46  &  1892 &       Benjamin Harrison &   loss \\
47  &  1892 &        Grover Cleveland &    win \\
48  &  1892 &         James B. Weaver &   loss \\
49  &  1892 &            John Bidwell &   loss \\
50  &  1896 &          John M. Palmer &   loss \\
51  &  1896 &         Joshua Levering &   loss \\
52  &  1896 &  William Jennings Bryan &   loss \\
53  &  1896 &        William McKinley &    win \\
54  &  1900 &         John G. Woolley &   loss \\
55  &  1900 &  William Jennings Bryan &   loss \\
56  &  1900 &        William McKinley &    win \\
57  &  1904 &         Alton B. Parker &   loss \\
58  &  1904 &          Eugene V. Debs &   loss \\
59  &  1904 &        Silas C. Swallow &   loss \\
60  &  1904 &      Theodore Roosevelt &    win \\
61  &  1904 &        Thomas E. Watson &   loss \\
62  &  1908 &          Eugene V. Debs &   loss \\
63  &  1908 &        Eugene W. Chafin &   loss \\
64  &  1908 &  William Jennings Bryan &   loss \\
65  &  1908 &            William Taft &    win \\
66  &  1912 &          Eugene V. Debs &   loss \\
67  &  1912 &        Eugene W. Chafin &   loss \\
68  &  1912 &      Theodore Roosevelt &   loss \\
69  &  1912 &            William Taft &   loss \\
70  &  1912 &          Woodrow Wilson &    win \\
71  &  1916 &         Allan L. Benson &   loss \\
72  &  1916 &    Charles Evans Hughes &   loss \\
73  &  1916 &             Frank Hanly &   loss \\
74  &  1916 &          Woodrow Wilson &    win \\
75  &  1920 &        Aaron S. Watkins &   loss \\
76  &  1920 &          Eugene V. Debs &   loss \\
77  &  1920 &            James M. Cox &   loss \\
78  &  1920 &   Parley P. Christensen &   loss \\
79  &  1920 &          Warren Harding &    win \\
80  &  1924 &         Calvin Coolidge &    win \\
81  &  1924 &           John W. Davis &   loss \\
82  &  1924 &      Robert La Follette &   loss \\
83  &  1928 &                Al Smith &   loss \\
84  &  1928 &          Herbert Hoover &    win \\
85  &  1928 &           Norman Thomas &   loss \\
86  &  1932 &      Franklin Roosevelt &    win \\
87  &  1932 &          Herbert Hoover &   loss \\
88  &  1932 &           Norman Thomas &   loss \\
89  &  1932 &       William Z. Foster &   loss \\
90  &  1936 &              Alf Landon &   loss \\
91  &  1936 &      Franklin Roosevelt &    win \\
92  &  1936 &           Norman Thomas &   loss \\
93  &  1936 &           William Lemke &   loss \\
94  &  1940 &      Franklin Roosevelt &    win \\
95  &  1940 &           Norman Thomas &   loss \\
96  &  1940 &         Wendell Willkie &   loss \\
97  &  1944 &      Franklin Roosevelt &    win \\
98  &  1944 &         Thomas E. Dewey &   loss \\
99  &  1948 &        Claude A. Watson &   loss \\
100 &  1948 &            Harry Truman &    win \\
101 &  1948 &        Henry A. Wallace &   loss \\
102 &  1948 &           Norman Thomas &   loss \\
103 &  1948 &          Strom Thurmond &   loss \\
104 &  1948 &         Thomas E. Dewey &   loss \\
105 &  1952 &         Adlai Stevenson &   loss \\
106 &  1952 &       Dwight Eisenhower &    win \\
107 &  1952 &        Vincent Hallinan &   loss \\
108 &  1956 &         Adlai Stevenson &   loss \\
109 &  1956 &       Dwight Eisenhower &    win \\
110 &  1956 &      T. Coleman Andrews &   loss \\
111 &  1960 &            John Kennedy &    win \\
112 &  1960 &           Richard Nixon &   loss \\
113 &  1964 &         Barry Goldwater &   loss \\
114 &  1964 &          Lyndon Johnson &    win \\
115 &  1968 &          George Wallace &   loss \\
116 &  1968 &         Hubert Humphrey &   loss \\
117 &  1968 &           Richard Nixon &    win \\
118 &  1972 &         George McGovern &   loss \\
119 &  1972 &         John G. Schmitz &   loss \\
120 &  1972 &           Richard Nixon &    win \\
121 &  1976 &         Eugene McCarthy &   loss \\
122 &  1976 &             Gerald Ford &   loss \\
123 &  1976 &            Jimmy Carter &    win \\
124 &  1976 &           Lester Maddox &   loss \\
125 &  1976 &          Roger MacBride &   loss \\
126 &  1976 &      Thomas J. Anderson &   loss \\
127 &  1980 &          Barry Commoner &   loss \\
128 &  1980 &                Ed Clark &   loss \\
129 &  1980 &            Jimmy Carter &   loss \\
130 &  1980 &        John B. Anderson &   loss \\
131 &  1980 &           Ronald Reagan &    win \\
132 &  1984 &          David Bergland &   loss \\
133 &  1984 &           Ronald Reagan &    win \\
134 &  1984 &          Walter Mondale &   loss \\
135 &  1988 &       George H. W. Bush &    win \\
136 &  1988 &           Lenora Fulani &   loss \\
137 &  1988 &         Michael Dukakis &   loss \\
138 &  1988 &                Ron Paul &   loss \\
139 &  1992 &            Andre Marrou &   loss \\
140 &  1992 &            Bill Clinton &    win \\
141 &  1992 &                Bo Gritz &   loss \\
142 &  1992 &       George H. W. Bush &   loss \\
143 &  1992 &              Ross Perot &   loss \\
144 &  1996 &            Bill Clinton &    win \\
145 &  1996 &                Bob Dole &   loss \\
146 &  1996 &            Harry Browne &   loss \\
147 &  1996 &         Howard Phillips &   loss \\
148 &  1996 &            John Hagelin &   loss \\
149 &  1996 &             Ralph Nader &   loss \\
150 &  1996 &              Ross Perot &   loss \\
151 &  2000 &                 Al Gore &   loss \\
152 &  2000 &          George W. Bush &    win \\
153 &  2000 &            Harry Browne &   loss \\
154 &  2000 &            Pat Buchanan &   loss \\
155 &  2000 &             Ralph Nader &   loss \\
156 &  2004 &              David Cobb &   loss \\
157 &  2004 &          George W. Bush &    win \\
158 &  2004 &              John Kerry &   loss \\
159 &  2004 &        Michael Badnarik &   loss \\
160 &  2004 &        Michael Peroutka &   loss \\
161 &  2004 &             Ralph Nader &   loss \\
162 &  2008 &            Barack Obama &    win \\
163 &  2008 &                Bob Barr &   loss \\
164 &  2008 &           Chuck Baldwin &   loss \\
165 &  2008 &        Cynthia McKinney &   loss \\
166 &  2008 &             John McCain &   loss \\
167 &  2008 &             Ralph Nader &   loss \\
168 &  2012 &            Barack Obama &    win \\
169 &  2012 &            Gary Johnson &   loss \\
170 &  2012 &              Jill Stein &   loss \\
171 &  2012 &             Mitt Romney &   loss \\
172 &  2016 &          Darrell Castle &   loss \\
173 &  2016 &            Donald Trump &    win \\
174 &  2016 &           Evan McMullin &   loss \\
175 &  2016 &            Gary Johnson &   loss \\
176 &  2016 &         Hillary Clinton &   loss \\
177 &  2016 &              Jill Stein &   loss \\
178 &  2020 &            Joseph Biden &    win \\
179 &  2020 &            Donald Trump &   loss \\
180 &  2020 &            Jo Jorgensen &   loss \\
181 &  2020 &          Howard Hawkins &   loss \\
\bottomrule
\end{tabular}

There are a couple of things we should note. Firstly, unlike
conventional Python, \texttt{pandas} allows us to slice string values
(in our example, the column labels). Secondly, slicing with
\texttt{.loc} is \emph{inclusive}. Notice how our resulting
\texttt{DataFrame} includes every row and column between and including
the slice labels we specified.

Equivalently, we can use a list to obtain multiple rows and columns in
our \texttt{elections} \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Candidate\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Popular vote\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Lastly, we can interchange list and slicing notation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], :]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

\hypertarget{integer-based-extraction-indexing-with-.iloc}{%
\subsection{\texorpdfstring{Integer-based Extraction: Indexing with
\texttt{.iloc}}{Integer-based Extraction: Indexing with .iloc}}\label{integer-based-extraction-indexing-with-.iloc}}

Slicing with \texttt{.iloc} works similarly to \texttt{.loc}. However,
\texttt{.iloc} uses the \emph{index positions} of rows and columns
rather than the labels (think to yourself: \textbf{l}oc uses
\textbf{l}ables; \textbf{i}loc uses \textbf{i}ndices). The arguments to
the \texttt{.iloc} function also behave similarly --- single values,
lists, indices, and any combination of these are permitted.

Let's begin reproducing our results from above. We'll begin by selecting
the first presidential candidate in our \texttt{elections}
\texttt{DataFrame}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# elections.loc[0, "Candidate"] {-} Previous approach}
\NormalTok{elections.iloc[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Andrew Jackson'
\end{verbatim}

Notice how the first argument to both \texttt{.loc} and \texttt{.iloc}
are the same. This is because the row with a label of \texttt{0} is
conveniently in the \(0^{\text{th}}\) (equivalently, the first position)
of the \texttt{elections} \texttt{DataFrame}. Generally, this is true of
any \texttt{DataFrame} where the row labels are incremented in ascending
order from 0.

And, as before, if we were to pass in only one single value argument,
our result would be a \texttt{Series}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.iloc[[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{],}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &          Candidate \\
\midrule
1 &  John Quincy Adams \\
2 &     Andrew Jackson \\
3 &  John Quincy Adams \\
\bottomrule
\end{tabular}

However, when we select the first four rows and columns using
\texttt{.iloc}, we notice something.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# elections.loc[0:3, \textquotesingle{}Year\textquotesingle{}:\textquotesingle{}Popular vote\textquotesingle{}] {-} Previous approach}
\NormalTok{elections.iloc[}\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Slicing is no longer inclusive in \texttt{.iloc} --- it's
\emph{exclusive}. In other words, the right end of a slice is not
included when using \texttt{.iloc}. This is one of the subtleties of
\texttt{pandas} syntax; you will get used to it with practice.

List behavior works just as expected.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#elections.loc[[0, 1, 2, 3], [\textquotesingle{}Year\textquotesingle{}, \textquotesingle{}Candidate\textquotesingle{}, \textquotesingle{}Party\textquotesingle{}, \textquotesingle{}Popular vote\textquotesingle{}]] {-} Previous Approach}
\NormalTok{elections.iloc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 \\
\bottomrule
\end{tabular}

And just like with \texttt{.loc}, we can use a colon with \texttt{.iloc}
to extract all rows or columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.iloc[:, }\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrll}
\toprule
{} &  Year &               Candidate &                  Party \\
\midrule
0   &  1824 &          Andrew Jackson &  Democratic-Republican \\
1   &  1824 &       John Quincy Adams &  Democratic-Republican \\
2   &  1828 &          Andrew Jackson &             Democratic \\
3   &  1828 &       John Quincy Adams &    National Republican \\
4   &  1832 &          Andrew Jackson &             Democratic \\
5   &  1832 &              Henry Clay &    National Republican \\
6   &  1832 &            William Wirt &           Anti-Masonic \\
7   &  1836 &       Hugh Lawson White &                   Whig \\
8   &  1836 &        Martin Van Buren &             Democratic \\
9   &  1836 &  William Henry Harrison &                   Whig \\
10  &  1840 &        Martin Van Buren &             Democratic \\
11  &  1840 &  William Henry Harrison &                   Whig \\
12  &  1844 &              Henry Clay &                   Whig \\
13  &  1844 &              James Polk &             Democratic \\
14  &  1848 &              Lewis Cass &             Democratic \\
15  &  1848 &        Martin Van Buren &              Free Soil \\
16  &  1848 &          Zachary Taylor &                   Whig \\
17  &  1852 &         Franklin Pierce &             Democratic \\
18  &  1852 &            John P. Hale &              Free Soil \\
19  &  1852 &          Winfield Scott &                   Whig \\
20  &  1856 &          James Buchanan &             Democratic \\
21  &  1856 &         John C. Frémont &             Republican \\
22  &  1856 &        Millard Fillmore &               American \\
23  &  1860 &         Abraham Lincoln &             Republican \\
24  &  1860 &               John Bell &   Constitutional Union \\
25  &  1860 &    John C. Breckinridge &    Southern Democratic \\
26  &  1860 &      Stephen A. Douglas &    Northern Democratic \\
27  &  1864 &         Abraham Lincoln &         National Union \\
28  &  1864 &     George B. McClellan &             Democratic \\
29  &  1868 &         Horatio Seymour &             Democratic \\
30  &  1868 &           Ulysses Grant &             Republican \\
31  &  1872 &          Horace Greeley &     Liberal Republican \\
32  &  1872 &           Ulysses Grant &             Republican \\
33  &  1876 &        Rutherford Hayes &             Republican \\
34  &  1876 &        Samuel J. Tilden &             Democratic \\
35  &  1880 &         James B. Weaver &              Greenback \\
36  &  1880 &          James Garfield &             Republican \\
37  &  1880 &  Winfield Scott Hancock &             Democratic \\
38  &  1884 &         Benjamin Butler &          Anti-Monopoly \\
39  &  1884 &        Grover Cleveland &             Democratic \\
40  &  1884 &         James G. Blaine &             Republican \\
41  &  1884 &           John St. John &            Prohibition \\
42  &  1888 &          Alson Streeter &            Union Labor \\
43  &  1888 &       Benjamin Harrison &             Republican \\
44  &  1888 &         Clinton B. Fisk &            Prohibition \\
45  &  1888 &        Grover Cleveland &             Democratic \\
46  &  1892 &       Benjamin Harrison &             Republican \\
47  &  1892 &        Grover Cleveland &             Democratic \\
48  &  1892 &         James B. Weaver &               Populist \\
49  &  1892 &            John Bidwell &            Prohibition \\
50  &  1896 &          John M. Palmer &    National Democratic \\
51  &  1896 &         Joshua Levering &            Prohibition \\
52  &  1896 &  William Jennings Bryan &             Democratic \\
53  &  1896 &        William McKinley &             Republican \\
54  &  1900 &         John G. Woolley &            Prohibition \\
55  &  1900 &  William Jennings Bryan &             Democratic \\
56  &  1900 &        William McKinley &             Republican \\
57  &  1904 &         Alton B. Parker &             Democratic \\
58  &  1904 &          Eugene V. Debs &              Socialist \\
59  &  1904 &        Silas C. Swallow &            Prohibition \\
60  &  1904 &      Theodore Roosevelt &             Republican \\
61  &  1904 &        Thomas E. Watson &               Populist \\
62  &  1908 &          Eugene V. Debs &              Socialist \\
63  &  1908 &        Eugene W. Chafin &            Prohibition \\
64  &  1908 &  William Jennings Bryan &             Democratic \\
65  &  1908 &            William Taft &             Republican \\
66  &  1912 &          Eugene V. Debs &              Socialist \\
67  &  1912 &        Eugene W. Chafin &            Prohibition \\
68  &  1912 &      Theodore Roosevelt &            Progressive \\
69  &  1912 &            William Taft &             Republican \\
70  &  1912 &          Woodrow Wilson &             Democratic \\
71  &  1916 &         Allan L. Benson &              Socialist \\
72  &  1916 &    Charles Evans Hughes &             Republican \\
73  &  1916 &             Frank Hanly &            Prohibition \\
74  &  1916 &          Woodrow Wilson &             Democratic \\
75  &  1920 &        Aaron S. Watkins &            Prohibition \\
76  &  1920 &          Eugene V. Debs &              Socialist \\
77  &  1920 &            James M. Cox &             Democratic \\
78  &  1920 &   Parley P. Christensen &           Farmer–Labor \\
79  &  1920 &          Warren Harding &             Republican \\
80  &  1924 &         Calvin Coolidge &             Republican \\
81  &  1924 &           John W. Davis &             Democratic \\
82  &  1924 &      Robert La Follette &            Progressive \\
83  &  1928 &                Al Smith &             Democratic \\
84  &  1928 &          Herbert Hoover &             Republican \\
85  &  1928 &           Norman Thomas &              Socialist \\
86  &  1932 &      Franklin Roosevelt &             Democratic \\
87  &  1932 &          Herbert Hoover &             Republican \\
88  &  1932 &           Norman Thomas &              Socialist \\
89  &  1932 &       William Z. Foster &              Communist \\
90  &  1936 &              Alf Landon &             Republican \\
91  &  1936 &      Franklin Roosevelt &             Democratic \\
92  &  1936 &           Norman Thomas &              Socialist \\
93  &  1936 &           William Lemke &                  Union \\
94  &  1940 &      Franklin Roosevelt &             Democratic \\
95  &  1940 &           Norman Thomas &              Socialist \\
96  &  1940 &         Wendell Willkie &             Republican \\
97  &  1944 &      Franklin Roosevelt &             Democratic \\
98  &  1944 &         Thomas E. Dewey &             Republican \\
99  &  1948 &        Claude A. Watson &            Prohibition \\
100 &  1948 &            Harry Truman &             Democratic \\
101 &  1948 &        Henry A. Wallace &            Progressive \\
102 &  1948 &           Norman Thomas &              Socialist \\
103 &  1948 &          Strom Thurmond &              Dixiecrat \\
104 &  1948 &         Thomas E. Dewey &             Republican \\
105 &  1952 &         Adlai Stevenson &             Democratic \\
106 &  1952 &       Dwight Eisenhower &             Republican \\
107 &  1952 &        Vincent Hallinan &            Progressive \\
108 &  1956 &         Adlai Stevenson &             Democratic \\
109 &  1956 &       Dwight Eisenhower &             Republican \\
110 &  1956 &      T. Coleman Andrews &         States' Rights \\
111 &  1960 &            John Kennedy &             Democratic \\
112 &  1960 &           Richard Nixon &             Republican \\
113 &  1964 &         Barry Goldwater &             Republican \\
114 &  1964 &          Lyndon Johnson &             Democratic \\
115 &  1968 &          George Wallace &   American Independent \\
116 &  1968 &         Hubert Humphrey &             Democratic \\
117 &  1968 &           Richard Nixon &             Republican \\
118 &  1972 &         George McGovern &             Democratic \\
119 &  1972 &         John G. Schmitz &   American Independent \\
120 &  1972 &           Richard Nixon &             Republican \\
121 &  1976 &         Eugene McCarthy &            Independent \\
122 &  1976 &             Gerald Ford &             Republican \\
123 &  1976 &            Jimmy Carter &             Democratic \\
124 &  1976 &           Lester Maddox &   American Independent \\
125 &  1976 &          Roger MacBride &            Libertarian \\
126 &  1976 &      Thomas J. Anderson &               American \\
127 &  1980 &          Barry Commoner &               Citizens \\
128 &  1980 &                Ed Clark &            Libertarian \\
129 &  1980 &            Jimmy Carter &             Democratic \\
130 &  1980 &        John B. Anderson &            Independent \\
131 &  1980 &           Ronald Reagan &             Republican \\
132 &  1984 &          David Bergland &            Libertarian \\
133 &  1984 &           Ronald Reagan &             Republican \\
134 &  1984 &          Walter Mondale &             Democratic \\
135 &  1988 &       George H. W. Bush &             Republican \\
136 &  1988 &           Lenora Fulani &           New Alliance \\
137 &  1988 &         Michael Dukakis &             Democratic \\
138 &  1988 &                Ron Paul &            Libertarian \\
139 &  1992 &            Andre Marrou &            Libertarian \\
140 &  1992 &            Bill Clinton &             Democratic \\
141 &  1992 &                Bo Gritz &               Populist \\
142 &  1992 &       George H. W. Bush &             Republican \\
143 &  1992 &              Ross Perot &            Independent \\
144 &  1996 &            Bill Clinton &             Democratic \\
145 &  1996 &                Bob Dole &             Republican \\
146 &  1996 &            Harry Browne &            Libertarian \\
147 &  1996 &         Howard Phillips &              Taxpayers \\
148 &  1996 &            John Hagelin &            Natural Law \\
149 &  1996 &             Ralph Nader &                  Green \\
150 &  1996 &              Ross Perot &                 Reform \\
151 &  2000 &                 Al Gore &             Democratic \\
152 &  2000 &          George W. Bush &             Republican \\
153 &  2000 &            Harry Browne &            Libertarian \\
154 &  2000 &            Pat Buchanan &                 Reform \\
155 &  2000 &             Ralph Nader &                  Green \\
156 &  2004 &              David Cobb &                  Green \\
157 &  2004 &          George W. Bush &             Republican \\
158 &  2004 &              John Kerry &             Democratic \\
159 &  2004 &        Michael Badnarik &            Libertarian \\
160 &  2004 &        Michael Peroutka &           Constitution \\
161 &  2004 &             Ralph Nader &            Independent \\
162 &  2008 &            Barack Obama &             Democratic \\
163 &  2008 &                Bob Barr &            Libertarian \\
164 &  2008 &           Chuck Baldwin &           Constitution \\
165 &  2008 &        Cynthia McKinney &                  Green \\
166 &  2008 &             John McCain &             Republican \\
167 &  2008 &             Ralph Nader &            Independent \\
168 &  2012 &            Barack Obama &             Democratic \\
169 &  2012 &            Gary Johnson &            Libertarian \\
170 &  2012 &              Jill Stein &                  Green \\
171 &  2012 &             Mitt Romney &             Republican \\
172 &  2016 &          Darrell Castle &           Constitution \\
173 &  2016 &            Donald Trump &             Republican \\
174 &  2016 &           Evan McMullin &            Independent \\
175 &  2016 &            Gary Johnson &            Libertarian \\
176 &  2016 &         Hillary Clinton &             Democratic \\
177 &  2016 &              Jill Stein &                  Green \\
178 &  2020 &            Joseph Biden &             Democratic \\
179 &  2020 &            Donald Trump &             Republican \\
180 &  2020 &            Jo Jorgensen &            Libertarian \\
181 &  2020 &          Howard Hawkins &                  Green \\
\bottomrule
\end{tabular}

This discussion begs the question: when should we use \texttt{.loc}
vs.~\texttt{.iloc}? In most cases, \texttt{.loc} is generally safer to
use. You can imagine \texttt{.iloc} may return incorrect values when
applied to a dataset where the ordering of data can change. However,
\texttt{.iloc} can still be useful --- for example, if you are looking
at a \texttt{DataFrame} of sorted movie earnings and want to get the
median earnings for a given year, you can use \texttt{.iloc} to index
into the middle.

Overall, it is important to remember that:

\begin{itemize}
\tightlist
\item
  \texttt{.loc} performances \textbf{l}abel-based extraction.
\item
  \texttt{.iloc} performs \textbf{i}nteger-based extraction.
\end{itemize}

\hypertarget{context-dependent-extraction-indexing-with}{%
\subsection{\texorpdfstring{Context-dependent Extraction: Indexing with
\texttt{{[}{]}}}{Context-dependent Extraction: Indexing with {[}{]}}}\label{context-dependent-extraction-indexing-with}}

The \texttt{{[}{]}} selection operator is the most baffling of all, yet
the most commonly used. It only takes a single argument, which may be
one of the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A slice of row numbers.
\item
  A list of column labels.
\item
  A single-column label.
\end{enumerate}

That is, \texttt{{[}{]}} is \emph{context-dependent}. Let's see some
examples.

\hypertarget{a-slice-of-row-numbers}{%
\subsubsection{A slice of row numbers}\label{a-slice-of-row-numbers}}

Say we wanted the first four rows of our \texttt{elections}
\texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[}\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

\hypertarget{a-list-of-column-labels}{%
\subsubsection{A list of column labels}\label{a-list-of-column-labels}}

Suppose we now want the first four columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Candidate"}\NormalTok{, }\StringTok{"Party"}\NormalTok{, }\StringTok{"Popular vote"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  Year &               Candidate &                  Party &  Popular vote \\
\midrule
0   &  1824 &          Andrew Jackson &  Democratic-Republican &        151271 \\
1   &  1824 &       John Quincy Adams &  Democratic-Republican &        113142 \\
2   &  1828 &          Andrew Jackson &             Democratic &        642806 \\
3   &  1828 &       John Quincy Adams &    National Republican &        500897 \\
4   &  1832 &          Andrew Jackson &             Democratic &        702735 \\
5   &  1832 &              Henry Clay &    National Republican &        484205 \\
6   &  1832 &            William Wirt &           Anti-Masonic &        100715 \\
7   &  1836 &       Hugh Lawson White &                   Whig &        146109 \\
8   &  1836 &        Martin Van Buren &             Democratic &        763291 \\
9   &  1836 &  William Henry Harrison &                   Whig &        550816 \\
10  &  1840 &        Martin Van Buren &             Democratic &       1128854 \\
11  &  1840 &  William Henry Harrison &                   Whig &       1275583 \\
12  &  1844 &              Henry Clay &                   Whig &       1300004 \\
13  &  1844 &              James Polk &             Democratic &       1339570 \\
14  &  1848 &              Lewis Cass &             Democratic &       1223460 \\
15  &  1848 &        Martin Van Buren &              Free Soil &        291501 \\
16  &  1848 &          Zachary Taylor &                   Whig &       1360235 \\
17  &  1852 &         Franklin Pierce &             Democratic &       1605943 \\
18  &  1852 &            John P. Hale &              Free Soil &        155210 \\
19  &  1852 &          Winfield Scott &                   Whig &       1386942 \\
20  &  1856 &          James Buchanan &             Democratic &       1835140 \\
21  &  1856 &         John C. Frémont &             Republican &       1342345 \\
22  &  1856 &        Millard Fillmore &               American &        873053 \\
23  &  1860 &         Abraham Lincoln &             Republican &       1855993 \\
24  &  1860 &               John Bell &   Constitutional Union &        590901 \\
25  &  1860 &    John C. Breckinridge &    Southern Democratic &        848019 \\
26  &  1860 &      Stephen A. Douglas &    Northern Democratic &       1380202 \\
27  &  1864 &         Abraham Lincoln &         National Union &       2211317 \\
28  &  1864 &     George B. McClellan &             Democratic &       1812807 \\
29  &  1868 &         Horatio Seymour &             Democratic &       2708744 \\
30  &  1868 &           Ulysses Grant &             Republican &       3013790 \\
31  &  1872 &          Horace Greeley &     Liberal Republican &       2834761 \\
32  &  1872 &           Ulysses Grant &             Republican &       3597439 \\
33  &  1876 &        Rutherford Hayes &             Republican &       4034142 \\
34  &  1876 &        Samuel J. Tilden &             Democratic &       4288546 \\
35  &  1880 &         James B. Weaver &              Greenback &        308649 \\
36  &  1880 &          James Garfield &             Republican &       4453337 \\
37  &  1880 &  Winfield Scott Hancock &             Democratic &       4444976 \\
38  &  1884 &         Benjamin Butler &          Anti-Monopoly &        134294 \\
39  &  1884 &        Grover Cleveland &             Democratic &       4914482 \\
40  &  1884 &         James G. Blaine &             Republican &       4856905 \\
41  &  1884 &           John St. John &            Prohibition &        147482 \\
42  &  1888 &          Alson Streeter &            Union Labor &        146602 \\
43  &  1888 &       Benjamin Harrison &             Republican &       5443633 \\
44  &  1888 &         Clinton B. Fisk &            Prohibition &        249819 \\
45  &  1888 &        Grover Cleveland &             Democratic &       5534488 \\
46  &  1892 &       Benjamin Harrison &             Republican &       5176108 \\
47  &  1892 &        Grover Cleveland &             Democratic &       5553898 \\
48  &  1892 &         James B. Weaver &               Populist &       1041028 \\
49  &  1892 &            John Bidwell &            Prohibition &        270879 \\
50  &  1896 &          John M. Palmer &    National Democratic &        134645 \\
51  &  1896 &         Joshua Levering &            Prohibition &        131312 \\
52  &  1896 &  William Jennings Bryan &             Democratic &       6509052 \\
53  &  1896 &        William McKinley &             Republican &       7112138 \\
54  &  1900 &         John G. Woolley &            Prohibition &        210864 \\
55  &  1900 &  William Jennings Bryan &             Democratic &       6370932 \\
56  &  1900 &        William McKinley &             Republican &       7228864 \\
57  &  1904 &         Alton B. Parker &             Democratic &       5083880 \\
58  &  1904 &          Eugene V. Debs &              Socialist &        402810 \\
59  &  1904 &        Silas C. Swallow &            Prohibition &        259102 \\
60  &  1904 &      Theodore Roosevelt &             Republican &       7630557 \\
61  &  1904 &        Thomas E. Watson &               Populist &        114070 \\
62  &  1908 &          Eugene V. Debs &              Socialist &        420852 \\
63  &  1908 &        Eugene W. Chafin &            Prohibition &        254087 \\
64  &  1908 &  William Jennings Bryan &             Democratic &       6408979 \\
65  &  1908 &            William Taft &             Republican &       7678335 \\
66  &  1912 &          Eugene V. Debs &              Socialist &        901551 \\
67  &  1912 &        Eugene W. Chafin &            Prohibition &        208156 \\
68  &  1912 &      Theodore Roosevelt &            Progressive &       4122721 \\
69  &  1912 &            William Taft &             Republican &       3486242 \\
70  &  1912 &          Woodrow Wilson &             Democratic &       6296284 \\
71  &  1916 &         Allan L. Benson &              Socialist &        590524 \\
72  &  1916 &    Charles Evans Hughes &             Republican &       8548728 \\
73  &  1916 &             Frank Hanly &            Prohibition &        221302 \\
74  &  1916 &          Woodrow Wilson &             Democratic &       9126868 \\
75  &  1920 &        Aaron S. Watkins &            Prohibition &        188787 \\
76  &  1920 &          Eugene V. Debs &              Socialist &        913693 \\
77  &  1920 &            James M. Cox &             Democratic &       9139661 \\
78  &  1920 &   Parley P. Christensen &           Farmer–Labor &        265398 \\
79  &  1920 &          Warren Harding &             Republican &      16144093 \\
80  &  1924 &         Calvin Coolidge &             Republican &      15723789 \\
81  &  1924 &           John W. Davis &             Democratic &       8386242 \\
82  &  1924 &      Robert La Follette &            Progressive &       4831706 \\
83  &  1928 &                Al Smith &             Democratic &      15015464 \\
84  &  1928 &          Herbert Hoover &             Republican &      21427123 \\
85  &  1928 &           Norman Thomas &              Socialist &        267478 \\
86  &  1932 &      Franklin Roosevelt &             Democratic &      22821277 \\
87  &  1932 &          Herbert Hoover &             Republican &      15761254 \\
88  &  1932 &           Norman Thomas &              Socialist &        884885 \\
89  &  1932 &       William Z. Foster &              Communist &        103307 \\
90  &  1936 &              Alf Landon &             Republican &      16679543 \\
91  &  1936 &      Franklin Roosevelt &             Democratic &      27752648 \\
92  &  1936 &           Norman Thomas &              Socialist &        187910 \\
93  &  1936 &           William Lemke &                  Union &        892378 \\
94  &  1940 &      Franklin Roosevelt &             Democratic &      27313945 \\
95  &  1940 &           Norman Thomas &              Socialist &        116599 \\
96  &  1940 &         Wendell Willkie &             Republican &      22347744 \\
97  &  1944 &      Franklin Roosevelt &             Democratic &      25612916 \\
98  &  1944 &         Thomas E. Dewey &             Republican &      22017929 \\
99  &  1948 &        Claude A. Watson &            Prohibition &        103708 \\
100 &  1948 &            Harry Truman &             Democratic &      24179347 \\
101 &  1948 &        Henry A. Wallace &            Progressive &       1157328 \\
102 &  1948 &           Norman Thomas &              Socialist &        139569 \\
103 &  1948 &          Strom Thurmond &              Dixiecrat &       1175930 \\
104 &  1948 &         Thomas E. Dewey &             Republican &      21991292 \\
105 &  1952 &         Adlai Stevenson &             Democratic &      27375090 \\
106 &  1952 &       Dwight Eisenhower &             Republican &      34075529 \\
107 &  1952 &        Vincent Hallinan &            Progressive &        140746 \\
108 &  1956 &         Adlai Stevenson &             Democratic &      26028028 \\
109 &  1956 &       Dwight Eisenhower &             Republican &      35579180 \\
110 &  1956 &      T. Coleman Andrews &         States' Rights &        107929 \\
111 &  1960 &            John Kennedy &             Democratic &      34220984 \\
112 &  1960 &           Richard Nixon &             Republican &      34108157 \\
113 &  1964 &         Barry Goldwater &             Republican &      27175754 \\
114 &  1964 &          Lyndon Johnson &             Democratic &      43127041 \\
115 &  1968 &          George Wallace &   American Independent &       9901118 \\
116 &  1968 &         Hubert Humphrey &             Democratic &      31271839 \\
117 &  1968 &           Richard Nixon &             Republican &      31783783 \\
118 &  1972 &         George McGovern &             Democratic &      29173222 \\
119 &  1972 &         John G. Schmitz &   American Independent &       1100868 \\
120 &  1972 &           Richard Nixon &             Republican &      47168710 \\
121 &  1976 &         Eugene McCarthy &            Independent &        740460 \\
122 &  1976 &             Gerald Ford &             Republican &      39148634 \\
123 &  1976 &            Jimmy Carter &             Democratic &      40831881 \\
124 &  1976 &           Lester Maddox &   American Independent &        170274 \\
125 &  1976 &          Roger MacBride &            Libertarian &        172557 \\
126 &  1976 &      Thomas J. Anderson &               American &        158271 \\
127 &  1980 &          Barry Commoner &               Citizens &        233052 \\
128 &  1980 &                Ed Clark &            Libertarian &        921128 \\
129 &  1980 &            Jimmy Carter &             Democratic &      35480115 \\
130 &  1980 &        John B. Anderson &            Independent &       5719850 \\
131 &  1980 &           Ronald Reagan &             Republican &      43903230 \\
132 &  1984 &          David Bergland &            Libertarian &        228111 \\
133 &  1984 &           Ronald Reagan &             Republican &      54455472 \\
134 &  1984 &          Walter Mondale &             Democratic &      37577352 \\
135 &  1988 &       George H. W. Bush &             Republican &      48886597 \\
136 &  1988 &           Lenora Fulani &           New Alliance &        217221 \\
137 &  1988 &         Michael Dukakis &             Democratic &      41809074 \\
138 &  1988 &                Ron Paul &            Libertarian &        431750 \\
139 &  1992 &            Andre Marrou &            Libertarian &        290087 \\
140 &  1992 &            Bill Clinton &             Democratic &      44909806 \\
141 &  1992 &                Bo Gritz &               Populist &        106152 \\
142 &  1992 &       George H. W. Bush &             Republican &      39104550 \\
143 &  1992 &              Ross Perot &            Independent &      19743821 \\
144 &  1996 &            Bill Clinton &             Democratic &      47400125 \\
145 &  1996 &                Bob Dole &             Republican &      39197469 \\
146 &  1996 &            Harry Browne &            Libertarian &        485759 \\
147 &  1996 &         Howard Phillips &              Taxpayers &        184656 \\
148 &  1996 &            John Hagelin &            Natural Law &        113670 \\
149 &  1996 &             Ralph Nader &                  Green &        685297 \\
150 &  1996 &              Ross Perot &                 Reform &       8085294 \\
151 &  2000 &                 Al Gore &             Democratic &      50999897 \\
152 &  2000 &          George W. Bush &             Republican &      50456002 \\
153 &  2000 &            Harry Browne &            Libertarian &        384431 \\
154 &  2000 &            Pat Buchanan &                 Reform &        448895 \\
155 &  2000 &             Ralph Nader &                  Green &       2882955 \\
156 &  2004 &              David Cobb &                  Green &        119859 \\
157 &  2004 &          George W. Bush &             Republican &      62040610 \\
158 &  2004 &              John Kerry &             Democratic &      59028444 \\
159 &  2004 &        Michael Badnarik &            Libertarian &        397265 \\
160 &  2004 &        Michael Peroutka &           Constitution &        143630 \\
161 &  2004 &             Ralph Nader &            Independent &        465151 \\
162 &  2008 &            Barack Obama &             Democratic &      69498516 \\
163 &  2008 &                Bob Barr &            Libertarian &        523715 \\
164 &  2008 &           Chuck Baldwin &           Constitution &        199750 \\
165 &  2008 &        Cynthia McKinney &                  Green &        161797 \\
166 &  2008 &             John McCain &             Republican &      59948323 \\
167 &  2008 &             Ralph Nader &            Independent &        739034 \\
168 &  2012 &            Barack Obama &             Democratic &      65915795 \\
169 &  2012 &            Gary Johnson &            Libertarian &       1275971 \\
170 &  2012 &              Jill Stein &                  Green &        469627 \\
171 &  2012 &             Mitt Romney &             Republican &      60933504 \\
172 &  2016 &          Darrell Castle &           Constitution &        203091 \\
173 &  2016 &            Donald Trump &             Republican &      62984828 \\
174 &  2016 &           Evan McMullin &            Independent &        732273 \\
175 &  2016 &            Gary Johnson &            Libertarian &       4489235 \\
176 &  2016 &         Hillary Clinton &             Democratic &      65853514 \\
177 &  2016 &              Jill Stein &                  Green &       1457226 \\
178 &  2020 &            Joseph Biden &             Democratic &      81268924 \\
179 &  2020 &            Donald Trump &             Republican &      74216154 \\
180 &  2020 &            Jo Jorgensen &            Libertarian &       1865724 \\
181 &  2020 &          Howard Hawkins &                  Green &        405035 \\
\bottomrule
\end{tabular}

\hypertarget{a-single-column-label}{%
\subsubsection{A single-column label}\label{a-single-column-label}}

Lastly, \texttt{{[}{]}} allows us to extract only the
\texttt{"Candidate"} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[}\StringTok{"Candidate"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &               Candidate \\
\midrule
0   &          Andrew Jackson \\
1   &       John Quincy Adams \\
2   &          Andrew Jackson \\
3   &       John Quincy Adams \\
4   &          Andrew Jackson \\
5   &              Henry Clay \\
6   &            William Wirt \\
7   &       Hugh Lawson White \\
8   &        Martin Van Buren \\
9   &  William Henry Harrison \\
10  &        Martin Van Buren \\
11  &  William Henry Harrison \\
12  &              Henry Clay \\
13  &              James Polk \\
14  &              Lewis Cass \\
15  &        Martin Van Buren \\
16  &          Zachary Taylor \\
17  &         Franklin Pierce \\
18  &            John P. Hale \\
19  &          Winfield Scott \\
20  &          James Buchanan \\
21  &         John C. Frémont \\
22  &        Millard Fillmore \\
23  &         Abraham Lincoln \\
24  &               John Bell \\
25  &    John C. Breckinridge \\
26  &      Stephen A. Douglas \\
27  &         Abraham Lincoln \\
28  &     George B. McClellan \\
29  &         Horatio Seymour \\
30  &           Ulysses Grant \\
31  &          Horace Greeley \\
32  &           Ulysses Grant \\
33  &        Rutherford Hayes \\
34  &        Samuel J. Tilden \\
35  &         James B. Weaver \\
36  &          James Garfield \\
37  &  Winfield Scott Hancock \\
38  &         Benjamin Butler \\
39  &        Grover Cleveland \\
40  &         James G. Blaine \\
41  &           John St. John \\
42  &          Alson Streeter \\
43  &       Benjamin Harrison \\
44  &         Clinton B. Fisk \\
45  &        Grover Cleveland \\
46  &       Benjamin Harrison \\
47  &        Grover Cleveland \\
48  &         James B. Weaver \\
49  &            John Bidwell \\
50  &          John M. Palmer \\
51  &         Joshua Levering \\
52  &  William Jennings Bryan \\
53  &        William McKinley \\
54  &         John G. Woolley \\
55  &  William Jennings Bryan \\
56  &        William McKinley \\
57  &         Alton B. Parker \\
58  &          Eugene V. Debs \\
59  &        Silas C. Swallow \\
60  &      Theodore Roosevelt \\
61  &        Thomas E. Watson \\
62  &          Eugene V. Debs \\
63  &        Eugene W. Chafin \\
64  &  William Jennings Bryan \\
65  &            William Taft \\
66  &          Eugene V. Debs \\
67  &        Eugene W. Chafin \\
68  &      Theodore Roosevelt \\
69  &            William Taft \\
70  &          Woodrow Wilson \\
71  &         Allan L. Benson \\
72  &    Charles Evans Hughes \\
73  &             Frank Hanly \\
74  &          Woodrow Wilson \\
75  &        Aaron S. Watkins \\
76  &          Eugene V. Debs \\
77  &            James M. Cox \\
78  &   Parley P. Christensen \\
79  &          Warren Harding \\
80  &         Calvin Coolidge \\
81  &           John W. Davis \\
82  &      Robert La Follette \\
83  &                Al Smith \\
84  &          Herbert Hoover \\
85  &           Norman Thomas \\
86  &      Franklin Roosevelt \\
87  &          Herbert Hoover \\
88  &           Norman Thomas \\
89  &       William Z. Foster \\
90  &              Alf Landon \\
91  &      Franklin Roosevelt \\
92  &           Norman Thomas \\
93  &           William Lemke \\
94  &      Franklin Roosevelt \\
95  &           Norman Thomas \\
96  &         Wendell Willkie \\
97  &      Franklin Roosevelt \\
98  &         Thomas E. Dewey \\
99  &        Claude A. Watson \\
100 &            Harry Truman \\
101 &        Henry A. Wallace \\
102 &           Norman Thomas \\
103 &          Strom Thurmond \\
104 &         Thomas E. Dewey \\
105 &         Adlai Stevenson \\
106 &       Dwight Eisenhower \\
107 &        Vincent Hallinan \\
108 &         Adlai Stevenson \\
109 &       Dwight Eisenhower \\
110 &      T. Coleman Andrews \\
111 &            John Kennedy \\
112 &           Richard Nixon \\
113 &         Barry Goldwater \\
114 &          Lyndon Johnson \\
115 &          George Wallace \\
116 &         Hubert Humphrey \\
117 &           Richard Nixon \\
118 &         George McGovern \\
119 &         John G. Schmitz \\
120 &           Richard Nixon \\
121 &         Eugene McCarthy \\
122 &             Gerald Ford \\
123 &            Jimmy Carter \\
124 &           Lester Maddox \\
125 &          Roger MacBride \\
126 &      Thomas J. Anderson \\
127 &          Barry Commoner \\
128 &                Ed Clark \\
129 &            Jimmy Carter \\
130 &        John B. Anderson \\
131 &           Ronald Reagan \\
132 &          David Bergland \\
133 &           Ronald Reagan \\
134 &          Walter Mondale \\
135 &       George H. W. Bush \\
136 &           Lenora Fulani \\
137 &         Michael Dukakis \\
138 &                Ron Paul \\
139 &            Andre Marrou \\
140 &            Bill Clinton \\
141 &                Bo Gritz \\
142 &       George H. W. Bush \\
143 &              Ross Perot \\
144 &            Bill Clinton \\
145 &                Bob Dole \\
146 &            Harry Browne \\
147 &         Howard Phillips \\
148 &            John Hagelin \\
149 &             Ralph Nader \\
150 &              Ross Perot \\
151 &                 Al Gore \\
152 &          George W. Bush \\
153 &            Harry Browne \\
154 &            Pat Buchanan \\
155 &             Ralph Nader \\
156 &              David Cobb \\
157 &          George W. Bush \\
158 &              John Kerry \\
159 &        Michael Badnarik \\
160 &        Michael Peroutka \\
161 &             Ralph Nader \\
162 &            Barack Obama \\
163 &                Bob Barr \\
164 &           Chuck Baldwin \\
165 &        Cynthia McKinney \\
166 &             John McCain \\
167 &             Ralph Nader \\
168 &            Barack Obama \\
169 &            Gary Johnson \\
170 &              Jill Stein \\
171 &             Mitt Romney \\
172 &          Darrell Castle \\
173 &            Donald Trump \\
174 &           Evan McMullin \\
175 &            Gary Johnson \\
176 &         Hillary Clinton \\
177 &              Jill Stein \\
178 &            Joseph Biden \\
179 &            Donald Trump \\
180 &            Jo Jorgensen \\
181 &          Howard Hawkins \\
\bottomrule
\end{tabular}

The output is a \texttt{Series}! In this course, we'll become very
comfortable with \texttt{{[}{]}}, especially for selecting columns. In
practice, \texttt{{[}{]}} is much more common than \texttt{.loc},
especially since it is far more concise.

\hypertarget{parting-note}{%
\section{Parting Note}\label{parting-note}}

The \texttt{pandas} library is enormous and contains many useful
functions. Here is a link to its
\href{https://pandas.pydata.org/docs/}{documentation}. We certainly
don't expect you to memorize each and every method of the library, and
we will give you a reference sheet for exams.

The introductory Data 100 \texttt{pandas} lectures will provide a
high-level view of the key data structures and methods that will form
the foundation of your \texttt{pandas} knowledge. A goal of this course
is to help you build your familiarity with the real-world programming
practice of \ldots{} Googling! Answers to your questions can be found in
documentation, Stack Overflow, etc. Being able to search for, read, and
implement documentation is an important life skill for any data
scientist.

With that, we will move on to Pandas II!

\bookmarksetup{startatroot}

\hypertarget{pandas-ii}{%
\chapter{Pandas II}\label{pandas-ii}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Continue building familiarity with \texttt{pandas} syntax.
\item
  Extract data from a \texttt{DataFrame} using conditional selection.
\item
  Recognize situations where aggregation is useful and identify the
  correct technique for performing an aggregation.
\end{itemize}

\end{tcolorbox}

Last time, we introduced the \texttt{pandas} library as a toolkit for
processing data. We learned the \texttt{DataFrame} and \texttt{Series}
data structures, familiarized ourselves with the basic syntax for
manipulating tabular data, and began writing our first lines of
\texttt{pandas} code.

In this lecture, we'll start to dive into some advanced \texttt{pandas}
syntax. You may find it helpful to follow along with a notebook of your
own as we walk through these new pieces of code.

We'll start by loading the \texttt{babynames} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code pulls census data and loads it into a DataFrame}
\CommentTok{\# We won\textquotesingle{}t cover it explicitly in this class, but you are welcome to explore it on your own}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{import}\NormalTok{ os.path}
\ImportTok{import}\NormalTok{ zipfile}

\NormalTok{data\_url }\OperatorTok{=} \StringTok{"https://www.ssa.gov/oact/babynames/state/namesbystate.zip"}
\NormalTok{local\_filename }\OperatorTok{=} \StringTok{"data/babynamesbystate.zip"}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ os.path.exists(local\_filename): }\CommentTok{\# If the data exists don\textquotesingle{}t download again}
    \ControlFlowTok{with}\NormalTok{ urllib.request.urlopen(data\_url) }\ImportTok{as}\NormalTok{ resp, }\BuiltInTok{open}\NormalTok{(local\_filename, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(resp.read())}

\NormalTok{zf }\OperatorTok{=}\NormalTok{ zipfile.ZipFile(local\_filename, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{)}

\NormalTok{ca\_name }\OperatorTok{=} \StringTok{\textquotesingle{}STATE.CA.TXT\textquotesingle{}}
\NormalTok{field\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Name\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{]}
\ControlFlowTok{with}\NormalTok{ zf.}\BuiltInTok{open}\NormalTok{(ca\_name) }\ImportTok{as}\NormalTok{ fh:}
\NormalTok{    babynames }\OperatorTok{=}\NormalTok{ pd.read\_csv(fh, header}\OperatorTok{=}\VariableTok{None}\NormalTok{, names}\OperatorTok{=}\NormalTok{field\_names)}

\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

\hypertarget{conditional-selection}{%
\section{Conditional Selection}\label{conditional-selection}}

Conditional selection allows us to select a subset of rows in a
\texttt{DataFrame} that satisfy some specified condition.

To understand how to use conditional selection, we must look at another
possible input of the \texttt{.loc} and \texttt{{[}{]}} methods -- a
boolean array, which is simply an array or \texttt{Series} where each
element is either \texttt{True} or \texttt{False}. This boolean array
must have a length equal to the number of rows in the
\texttt{DataFrame}. It will return all rows that correspond to a value
of \texttt{True} in the array. We used a very similar technique when
performing conditional extraction from a \texttt{Series} in the last
lecture.

To see this in action, let's select all even-indexed rows in the first
10 rows of our \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ask yourself: why is :9 is the correct slice to select the first 10 rows?}
\NormalTok{babynames\_first\_10\_rows }\OperatorTok{=}\NormalTok{ babynames.loc[:}\DecValTok{9}\NormalTok{, :]}

\CommentTok{\# Notice how we have exactly 10 elements in our boolean array argument}
\NormalTok{babynames\_first\_10\_rows[[}\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
6 &    CA &   F &  1910 &    Evelyn &    126 \\
8 &    CA &   F &  1910 &  Virginia &    101 \\
\bottomrule
\end{tabular}

We can perform a similar operation using \texttt{.loc}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames\_first\_10\_rows.loc[[}\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{], :]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
6 &    CA &   F &  1910 &    Evelyn &    126 \\
8 &    CA &   F &  1910 &  Virginia &    101 \\
\bottomrule
\end{tabular}

These techniques worked well in this example, but you can imagine how
tedious it might be to list out \texttt{True} and \texttt{False}for
every row in a larger \texttt{DataFrame}. To make things easier, we can
instead provide a logical condition as an input to \texttt{.loc} or
\texttt{{[}{]}} that returns a boolean array with the necessary length.

For example, to return all names associated with \texttt{F} sex:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, use a logical condition to generate a boolean array}
\NormalTok{logical\_operator }\OperatorTok{=}\NormalTok{ (babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{)}

\CommentTok{\# Then, use this boolean array to filter the DataFrame}
\NormalTok{babynames[logical\_operator].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Recall from the previous lecture that \texttt{.head()} will return only
the first few rows in the \texttt{DataFrame}. In reality,
\texttt{babynames{[}logical\ operator{]}} contains as many rows as there
are entries in the original \texttt{babynames} \texttt{DataFrame} with
sex \texttt{"F"}.

Here, \texttt{logical\_operator} evaluates to a \texttt{Series} of
boolean values with length 407428.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"There are a total of }\SpecialCharTok{\{\}}\StringTok{ values in \textquotesingle{}logical\_operator\textquotesingle{}"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(}\BuiltInTok{len}\NormalTok{(logical\_operator)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
There are a total of 407428 values in 'logical_operator'
\end{verbatim}

Rows starting at row 0 and ending at row 239536 evaluate to
\texttt{True} and are thus returned in the \texttt{DataFrame}. Rows from
239537 onwards evaluate to \texttt{False} and are omitted from the
output.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 0th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{0}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 239536th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{239536}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 239537th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{239537}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The 0th item in this 'logical_operator' is: True
The 239536th item in this 'logical_operator' is: True
The 239537th item in this 'logical_operator' is: False
\end{verbatim}

Passing a \texttt{Series} as an argument to \texttt{babynames{[}{]}} has
the same effect as using a boolean array. In fact, the \texttt{{[}{]}}
selection operator can take a boolean \texttt{Series}, array, and list
as arguments. These three are used interchangeably throughout the
course.

We can also use \texttt{.loc} to achieve similar results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.loc[babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Boolean conditions can be combined using various bitwise operators,
allowing us to filter results by multiple conditions. In the table
below, p and q are boolean arrays or \texttt{Series}.

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Symbol & Usage & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textasciitilde{} & \textasciitilde p & Returns negation of p \\
\textbar{} & p \textbar{} q & p OR q \\
\& & p \& q & p AND q \\
\^{} & p \^{} q & p XOR q (exclusive or) \\
\end{longtable}

When combining multiple conditions with logical operators, we surround
each individual condition with a set of parenthesis \texttt{()}. This
imposes an order of operations on \texttt{pandas} evaluating your logic
and can avoid code erroring.

For example, if we want to return data on all names with sex
\texttt{"F"} born before the year 2000, we can write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[(babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{2000}\NormalTok{)].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Note that we're working with \texttt{Series}, so using \texttt{and} in
place of \texttt{\&}, or \texttt{or} in place \texttt{\textbar{}} will
error.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This line of code will raise a ValueError}
\CommentTok{\# babynames[(babynames["Sex"] == "F") and (babynames["Year"] \textless{} 2000)].head()}
\end{Highlighting}
\end{Shaded}

If we want to return data on all names with sex \texttt{"F"} \emph{or}
all born before the year 2000, we can write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[(babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{) }\OperatorTok{|}\NormalTok{ (babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{2000}\NormalTok{)].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Boolean array selection is a useful tool, but can lead to overly verbose
code for complex conditions. In the example below, our boolean condition
is long enough to extend for several lines of code.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note: The parentheses surrounding the code make it possible to break the code on to multiple lines for readability}
\NormalTok{(}
\NormalTok{    babynames[(babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Bella"}\NormalTok{) }\OperatorTok{|} 
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Alex"}\NormalTok{) }\OperatorTok{|}
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Ani"}\NormalTok{) }\OperatorTok{|}
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Lisa"}\NormalTok{)]}
\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &   Name &  Count \\
\midrule
6289  &    CA &   F &  1923 &  Bella &      5 \\
7512  &    CA &   F &  1925 &  Bella &      8 \\
12368 &    CA &   F &  1932 &   Lisa &      5 \\
14741 &    CA &   F &  1936 &   Lisa &      8 \\
17084 &    CA &   F &  1939 &   Lisa &      5 \\
\bottomrule
\end{tabular}

Fortunately, \texttt{pandas} provides many alternative methods for
constructing boolean filters.

The \texttt{.isin} function is one such example. This method evaluates
if the values in a \texttt{Series} are contained in a different sequence
(list, array, or \texttt{Series}) of values. In the cell below, we
achieve equivalent results to the \texttt{DataFrame} above with far more
concise code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OperatorTok{=}\NormalTok{ [}\StringTok{"Bella"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{, }\StringTok{"Narges"}\NormalTok{, }\StringTok{"Lisa"}\NormalTok{]}
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].isin(names).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &   Name \\
\midrule
0 &  False \\
1 &  False \\
2 &  False \\
3 &  False \\
4 &  False \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[babynames[}\StringTok{"Name"}\NormalTok{].isin(names)].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &   Name &  Count \\
\midrule
6289  &    CA &   F &  1923 &  Bella &      5 \\
7512  &    CA &   F &  1925 &  Bella &      8 \\
12368 &    CA &   F &  1932 &   Lisa &      5 \\
14741 &    CA &   F &  1936 &   Lisa &      8 \\
17084 &    CA &   F &  1939 &   Lisa &      5 \\
\bottomrule
\end{tabular}

The function \texttt{str.startswith} can be used to define a filter
based on string values in a \texttt{Series} object. It checks to see if
string values in a \texttt{Series} start with a particular character.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Identify whether names begin with the letter "N"}
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.startswith(}\StringTok{"N"}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &   Name \\
\midrule
0 &  False \\
1 &  False \\
2 &  False \\
3 &  False \\
4 &  False \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extracting names that begin with the letter "N"}
\NormalTok{babynames[babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.startswith(}\StringTok{"N"}\NormalTok{)].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &    Name &  Count \\
\midrule
76  &    CA &   F &  1910 &   Norma &     23 \\
83  &    CA &   F &  1910 &  Nellie &     20 \\
127 &    CA &   F &  1910 &    Nina &     11 \\
198 &    CA &   F &  1910 &    Nora &      6 \\
310 &    CA &   F &  1911 &  Nellie &     23 \\
\bottomrule
\end{tabular}

\hypertarget{adding-removing-and-modifying-columns}{%
\section{Adding, Removing, and Modifying
Columns}\label{adding-removing-and-modifying-columns}}

In many data science tasks, we may need to change the columns contained
in our \texttt{DataFrame} in some way. Fortunately, the syntax to do so
is fairly straightforward.

To add a new column to a \texttt{DataFrame}, we use a syntax similar to
that used when accessing an existing column. Specify the name of the new
column by writing \texttt{df{[}"column"{]}}, then assign this to a
\texttt{Series} or array containing the values that will populate this
column.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a Series of the length of each name. }
\NormalTok{babyname\_lengths }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{()}

\CommentTok{\# Add a column named "name\_lengths" that includes the length of each name}
\NormalTok{babynames[}\StringTok{"name\_lengths"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babyname\_lengths}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  name\_lengths \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 &             4 \\
1 &    CA &   F &  1910 &     Helen &    239 &             5 \\
2 &    CA &   F &  1910 &   Dorothy &    220 &             7 \\
3 &    CA &   F &  1910 &  Margaret &    163 &             8 \\
4 &    CA &   F &  1910 &   Frances &    134 &             7 \\
\bottomrule
\end{tabular}

If we need to later modify an existing column, we can do so by
referencing this column again with the syntax \texttt{df{[}"column"{]}},
then re-assigning it to a new \texttt{Series} or array of the
appropriate length.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Modify the “name\_lengths” column to be one less than its original value}
\NormalTok{babynames[}\StringTok{"name\_lengths"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"name\_lengths"}\NormalTok{] }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  name\_lengths \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 &             3 \\
1 &    CA &   F &  1910 &     Helen &    239 &             4 \\
2 &    CA &   F &  1910 &   Dorothy &    220 &             6 \\
3 &    CA &   F &  1910 &  Margaret &    163 &             7 \\
4 &    CA &   F &  1910 &   Frances &    134 &             6 \\
\bottomrule
\end{tabular}

We can rename a column using the \texttt{.rename()} method. It takes in
a dictionary that maps old column names to their new ones.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename “name\_lengths” to “Length”}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"name\_lengths"}\NormalTok{:}\StringTok{"Length"}\NormalTok{\})}
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  Length \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 &       3 \\
1 &    CA &   F &  1910 &     Helen &    239 &       4 \\
2 &    CA &   F &  1910 &   Dorothy &    220 &       6 \\
3 &    CA &   F &  1910 &  Margaret &    163 &       7 \\
4 &    CA &   F &  1910 &   Frances &    134 &       6 \\
\bottomrule
\end{tabular}

If we want to remove a column or row of a \texttt{DataFrame}, we can
call the \texttt{.drop}
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html}{(documentation)}
method. Use the \texttt{axis} parameter to specify whether a column or
row should be dropped. Unless otherwise specified, \texttt{pandas} will
assume that we are dropping a row by default.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop our new "Length" column from the DataFrame}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.drop(}\StringTok{"Length"}\NormalTok{, axis}\OperatorTok{=}\StringTok{"columns"}\NormalTok{)}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Notice that we \emph{re-assigned} \texttt{babynames} to the result of
\texttt{babynames.drop(...)}. This is a subtle but important point:
\texttt{pandas} table operations \textbf{do not occur in-place}. Calling
\texttt{df.drop(...)} will output a \emph{copy} of \texttt{df} with the
row/column of interest removed without modifying the original
\texttt{df} table.

In other words, if we simply call:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This creates a copy of \textasciigrave{}babynames\textasciigrave{} and removes the column "Name"...}
\NormalTok{babynames.drop(}\StringTok{"Name"}\NormalTok{, axis}\OperatorTok{=}\StringTok{"columns"}\NormalTok{)}

\CommentTok{\# ...but the original \textasciigrave{}babynames\textasciigrave{} is unchanged! }
\CommentTok{\# Notice that the "Name" column is still present}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

\hypertarget{useful-utility-functions}{%
\section{Useful Utility Functions}\label{useful-utility-functions}}

\texttt{pandas} contains an extensive library of functions that can help
shorten the process of setting and getting information from its data
structures. In the following section, we will give overviews of each of
the main utility functions that will help us in Data 100.

Discussing all functionality offered by \texttt{pandas} could take an
entire semester! We will walk you through the most commonly-used
functions and encourage you to explore and experiment on your own.

\begin{itemize}
\tightlist
\item
  \texttt{NumPy} and built-in function support
\item
  \texttt{.shape}
\item
  \texttt{.size}
\item
  \texttt{.describe()}
\item
  \texttt{.sample()}
\item
  \texttt{.value\_counts()}
\item
  \texttt{.unique()}
\item
  \texttt{.sort\_values()}
\end{itemize}

The \texttt{pandas}
\href{https://pandas.pydata.org/docs/reference/index.html}{documentation}
will be a valuable resource in Data 100 and beyond.

\hypertarget{numpy}{%
\subsection{\texorpdfstring{\texttt{NumPy}}{NumPy}}\label{numpy}}

\texttt{pandas} is designed to work well with \texttt{NumPy}, the
framework for array computations you encountered in
\href{https://www.data8.org/su23/reference/\#array-functions-and-methods}{Data
8}. Just about any \texttt{NumPy} function can be applied to
\texttt{pandas} \texttt{DataFrame}s and \texttt{Series}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pull out the number of babies named Yash each year}
\NormalTok{yash\_count }\OperatorTok{=}\NormalTok{ babynames[babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Yash"}\NormalTok{][}\StringTok{"Count"}\NormalTok{]}
\NormalTok{yash\_count.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Count \\
\midrule
331824 &      8 \\
334114 &      9 \\
336390 &     11 \\
338773 &     12 \\
341387 &     10 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Average number of babies named Yash each year}
\NormalTok{np.mean(yash\_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
17.142857142857142
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Max number of babies named Yash born in any one year}
\NormalTok{np.}\BuiltInTok{max}\NormalTok{(yash\_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
29
\end{verbatim}

\hypertarget{shape-and-.size}{%
\subsection{\texorpdfstring{\texttt{.shape} and
\texttt{.size}}{.shape and .size}}\label{shape-and-.size}}

\texttt{.shape} and \texttt{.size} are attributes of \texttt{Series} and
\texttt{DataFrame}s that measure the ``amount'' of data stored in the
structure. Calling \texttt{.shape} returns a tuple containing the number
of rows and columns present in the \texttt{DataFrame} or
\texttt{Series}. \texttt{.size} is used to find the total number of
elements in a structure, equivalent to the number of rows times the
number of columns.

Many functions strictly require the dimensions of the arguments along
certain axes to match. Calling these dimension-finding functions is much
faster than counting all of the items by hand.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Return the shape of the DataFrame, in the format (num\_rows, num\_columns)}
\NormalTok{babynames.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(407428, 5)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Return the size of the DataFrame, equal to num\_rows * num\_columns}
\NormalTok{babynames.size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2037140
\end{verbatim}

\hypertarget{describe}{%
\subsection{\texorpdfstring{\texttt{.describe()}}{.describe()}}\label{describe}}

If many statistics are required from a \texttt{DataFrame} (minimum
value, maximum value, mean value, etc.), then \texttt{.describe()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html}{(documentation)}
can be used to compute all of them at once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.describe()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &           Year &          Count \\
\midrule
count &  407428.000000 &  407428.000000 \\
mean  &    1985.733609 &      79.543456 \\
std   &      27.007660 &     293.698654 \\
min   &    1910.000000 &       5.000000 \\
25\%   &    1969.000000 &       7.000000 \\
50\%   &    1992.000000 &      13.000000 \\
75\%   &    2008.000000 &      38.000000 \\
max   &    2022.000000 &    8260.000000 \\
\bottomrule
\end{tabular}

A different set of statistics will be reported if \texttt{.describe()}
is called on a \texttt{Series}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Sex"}\NormalTok{].describe()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &     Sex \\
\midrule
count  &  407428 \\
unique &       2 \\
top    &       F \\
freq   &  239537 \\
\bottomrule
\end{tabular}

\hypertarget{sample}{%
\subsection{\texorpdfstring{\texttt{.sample()}}{.sample()}}\label{sample}}

As we will see later in the semester, random processes are at the heart
of many data science techniques (for example, train-test splits,
bootstrapping, and cross-validation). \texttt{.sample()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html}{(documentation)}
lets us quickly select random entries (a row if called from a
\texttt{DataFrame}, or a value if called from a \texttt{Series}).

By default, \texttt{.sample()} selects entries \emph{without}
replacement. Pass in the argument \texttt{replace=True} to sample with
replacement.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample a single row}
\NormalTok{babynames.sample()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &   Name &  Count \\
\midrule
101835 &    CA &   F &  1986 &  Leona &      7 \\
\bottomrule
\end{tabular}

Naturally, this can be chained with other methods and operators
(\texttt{iloc}, etc.).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sample 5 random rows, and select all columns after column 2}
\NormalTok{babynames.sample(}\DecValTok{5}\NormalTok{).iloc[:, }\DecValTok{2}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlr}
\toprule
{} &  Year &     Name &  Count \\
\midrule
194685 &  2011 &  Marcela &     27 \\
81635  &  1979 &  Kristyn &     19 \\
195249 &  2011 &     Jean &     15 \\
226146 &  2019 &   Sariah &     22 \\
59507  &  1969 &   Deidre &     27 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Randomly sample 4 names from the year 2000, with replacement, and select all columns after column 2}
\NormalTok{babynames[babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{==} \DecValTok{2000}\NormalTok{].sample(}\DecValTok{4}\NormalTok{, replace }\OperatorTok{=} \VariableTok{True}\NormalTok{).iloc[:, }\DecValTok{2}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlr}
\toprule
{} &  Year &     Name &  Count \\
\midrule
152778 &  2000 &    Torri &      5 \\
151153 &  2000 &  Karisma &     10 \\
344142 &  2000 &   Ulyses &      8 \\
149422 &  2000 &   Payton &    115 \\
\bottomrule
\end{tabular}

\hypertarget{value_counts}{%
\subsection{\texorpdfstring{\texttt{.value\_counts()}}{.value\_counts()}}\label{value_counts}}

The \texttt{Series.value\_counts()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html}{(documentation)}
method counts the number of occurrence of each unique value in a
\texttt{Series}. In other words, it \emph{counts} the number of times
each unique \emph{value} appears. This is often useful for determining
the most or least common entries in a \texttt{Series}.

In the example below, we can determine the name with the most years in
which at least one person has taken that name by counting the number of
times each name appears in the \texttt{"Name"} column of
\texttt{babynames}. Note that the return value is also a
\texttt{Series}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].value\_counts().head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Name \\
\midrule
Jean      &   223 \\
Francis   &   221 \\
Guadalupe &   218 \\
Jessie    &   217 \\
Marion    &   214 \\
\bottomrule
\end{tabular}

\hypertarget{unique}{%
\subsection{\texorpdfstring{\texttt{.unique()}}{.unique()}}\label{unique}}

If we have a \texttt{Series} with many repeated values, then
\texttt{.unique()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.unique.html}{(documentation)}
can be used to identify only the \emph{unique} values. Here we return an
array of all the names in \texttt{babynames}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].unique()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array(['Mary', 'Helen', 'Dorothy', ..., 'Zae', 'Zai', 'Zayvier'],
      dtype=object)
\end{verbatim}

\hypertarget{sort_values}{%
\subsection{\texorpdfstring{\texttt{.sort\_values()}}{.sort\_values()}}\label{sort_values}}

Ordering a \texttt{DataFrame} can be useful for isolating extreme
values. For example, the first 5 entries of a row sorted in descending
order (that is, from highest to lowest) are the largest 5 values.
\texttt{.sort\_values}
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html}{(documentation)}
allows us to order a \texttt{DataFrame} or \texttt{Series} by a
specified column. We can choose to either receive the rows in
\texttt{ascending} order (default) or \texttt{descending} order.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort the "Count" column from highest to lowest}
\NormalTok{babynames.sort\_values(by}\OperatorTok{=}\StringTok{"Count"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &     Name &  Count \\
\midrule
268041 &    CA &   M &  1957 &  Michael &   8260 \\
267017 &    CA &   M &  1956 &  Michael &   8258 \\
317387 &    CA &   M &  1990 &  Michael &   8246 \\
281850 &    CA &   M &  1969 &  Michael &   8245 \\
283146 &    CA &   M &  1970 &  Michael &   8196 \\
\bottomrule
\end{tabular}

Unlike when calling \texttt{.value\_counts()} on a \texttt{DataFrame},
we do not need to explicitly specify the column used for sorting when
calling \texttt{.value\_counts()} on a \texttt{Series}. We can still
specify the ordering paradigm -- that is, whether values are sorted in
ascending or descending order.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort the "Name" Series alphabetically}
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].sort\_values(ascending}\OperatorTok{=}\VariableTok{True}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &     Name \\
\midrule
366001 &    Aadan \\
384005 &    Aadan \\
369120 &    Aadan \\
398211 &  Aadarsh \\
370306 &    Aaden \\
\bottomrule
\end{tabular}

\hypertarget{parting-note-1}{%
\section{Parting Note}\label{parting-note-1}}

Manipulating \texttt{DataFrames} is not a skill that is mastered in just
one day. Due to the flexibility of \texttt{pandas}, there are many
different ways to get from point A to point B. We recommend trying
multiple different ways to solve the same problem to gain even more
practice and reach that point of mastery sooner.

Next, we will start digging deeper into the mechanics behind grouping
data.

\bookmarksetup{startatroot}

\hypertarget{pandas-iii}{%
\chapter{Pandas III}\label{pandas-iii}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Perform advanced aggregation using \texttt{.groupby()}
\item
  Use the \texttt{pd.pivot\_table} method to construct a pivot table
\item
  Perform simple merges between DataFrames using \texttt{pd.merge()}
\end{itemize}

\end{tcolorbox}

We will introduce the concept of aggregating data -- we will familiarize
ourselves with \texttt{GroupBy} objects and used them as tools to
consolidate and summarize a\texttt{DataFrame}. In this lecture, we will
explore working with the different aggregation functions and dive into
some advanced \texttt{.groupby} methods to show just how powerful of a
resource they can be for understanding our data. We will also introduce
other techniques for data aggregation to provide flexibility in how we
manipulate our tables.

\hypertarget{custom-sorts}{%
\section{Custom Sorts}\label{custom-sorts}}

First, let's finish our discussion about sorting. Let's try to solve a
sorting problem using different approaches. Assume we want to find the
longest baby names and sort our data accordingly.

We'll start by loading the \texttt{babynames} dataset. Note that this
dataset is filtered to only contain data from California.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code pulls census data and loads it into a DataFrame}
\CommentTok{\# We won\textquotesingle{}t cover it explicitly in this class, but you are welcome to explore it on your own}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{import}\NormalTok{ os.path}
\ImportTok{import}\NormalTok{ zipfile}

\NormalTok{data\_url }\OperatorTok{=} \StringTok{"https://www.ssa.gov/oact/babynames/state/namesbystate.zip"}
\NormalTok{local\_filename }\OperatorTok{=} \StringTok{"data/babynamesbystate.zip"}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ os.path.exists(local\_filename): }\CommentTok{\# If the data exists don\textquotesingle{}t download again}
    \ControlFlowTok{with}\NormalTok{ urllib.request.urlopen(data\_url) }\ImportTok{as}\NormalTok{ resp, }\BuiltInTok{open}\NormalTok{(local\_filename, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(resp.read())}

\NormalTok{zf }\OperatorTok{=}\NormalTok{ zipfile.ZipFile(local\_filename, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{)}

\NormalTok{ca\_name }\OperatorTok{=} \StringTok{\textquotesingle{}STATE.CA.TXT\textquotesingle{}}
\NormalTok{field\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Name\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{]}
\ControlFlowTok{with}\NormalTok{ zf.}\BuiltInTok{open}\NormalTok{(ca\_name) }\ImportTok{as}\NormalTok{ fh:}
\NormalTok{    babynames }\OperatorTok{=}\NormalTok{ pd.read\_csv(fh, header}\OperatorTok{=}\VariableTok{None}\NormalTok{, names}\OperatorTok{=}\NormalTok{field\_names)}

\NormalTok{babynames.tail(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &     Name &  Count \\
\midrule
407418 &    CA &   M &  2022 &     Zach &      5 \\
407419 &    CA &   M &  2022 &  Zadkiel &      5 \\
407420 &    CA &   M &  2022 &      Zae &      5 \\
407421 &    CA &   M &  2022 &      Zai &      5 \\
407422 &    CA &   M &  2022 &      Zay &      5 \\
407423 &    CA &   M &  2022 &  Zayvier &      5 \\
407424 &    CA &   M &  2022 &      Zia &      5 \\
407425 &    CA &   M &  2022 &     Zora &      5 \\
407426 &    CA &   M &  2022 &   Zuriel &      5 \\
407427 &    CA &   M &  2022 &     Zylo &      5 \\
\bottomrule
\end{tabular}

\hypertarget{approach-1-create-a-temporary-column}{%
\subsection{Approach 1: Create a Temporary
Column}\label{approach-1-create-a-temporary-column}}

One method to do this is to first start by creating a column that
contains the lengths of the names.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a Series of the length of each name}
\NormalTok{babyname\_lengths }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{()}

\CommentTok{\# Add a column named "name\_lengths" that includes the length of each name}
\NormalTok{babynames[}\StringTok{"name\_lengths"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babyname\_lengths}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  name\_lengths \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 &             4 \\
1 &    CA &   F &  1910 &     Helen &    239 &             5 \\
2 &    CA &   F &  1910 &   Dorothy &    220 &             7 \\
3 &    CA &   F &  1910 &  Margaret &    163 &             8 \\
4 &    CA &   F &  1910 &   Frances &    134 &             7 \\
\bottomrule
\end{tabular}

We can then sort the \texttt{DataFrame} by that column using
\texttt{.sort\_values()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort by the temporary column}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.sort\_values(by}\OperatorTok{=}\StringTok{"name\_lengths"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &             Name &  Count &  name\_lengths \\
\midrule
334166 &    CA &   M &  1996 &  Franciscojavier &      8 &            15 \\
337301 &    CA &   M &  1997 &  Franciscojavier &      5 &            15 \\
339472 &    CA &   M &  1998 &  Franciscojavier &      6 &            15 \\
321792 &    CA &   M &  1991 &  Ryanchristopher &      7 &            15 \\
327358 &    CA &   M &  1993 &  Johnchristopher &      5 &            15 \\
\bottomrule
\end{tabular}

Finally, we can drop the \texttt{name\_length} column from
\texttt{babynames} to prevent our table from getting cluttered.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop the \textquotesingle{}name\_length\textquotesingle{} column}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.drop(}\StringTok{"name\_lengths"}\NormalTok{, axis}\OperatorTok{=}\StringTok{\textquotesingle{}columns\textquotesingle{}}\NormalTok{)}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &             Name &  Count \\
\midrule
334166 &    CA &   M &  1996 &  Franciscojavier &      8 \\
337301 &    CA &   M &  1997 &  Franciscojavier &      5 \\
339472 &    CA &   M &  1998 &  Franciscojavier &      6 \\
321792 &    CA &   M &  1991 &  Ryanchristopher &      7 \\
327358 &    CA &   M &  1993 &  Johnchristopher &      5 \\
\bottomrule
\end{tabular}

\hypertarget{approach-2-sorting-using-the-key-argument}{%
\subsection{\texorpdfstring{Approach 2: Sorting using the \texttt{key}
Argument}{Approach 2: Sorting using the key Argument}}\label{approach-2-sorting-using-the-key-argument}}

Another way to approach this is to use the \texttt{key} argument of
\texttt{.sort\_values()}. Here we can specify that we want to sort
\texttt{"Name"} values by their length.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.sort\_values(}\StringTok{"Name"}\NormalTok{, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x.}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{(), ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &             Name &  Count \\
\midrule
334166 &    CA &   M &  1996 &  Franciscojavier &      8 \\
327472 &    CA &   M &  1993 &  Ryanchristopher &      5 \\
337301 &    CA &   M &  1997 &  Franciscojavier &      5 \\
337477 &    CA &   M &  1997 &  Ryanchristopher &      5 \\
312543 &    CA &   M &  1987 &  Franciscojavier &      5 \\
\bottomrule
\end{tabular}

\hypertarget{approach-3-sorting-using-the-map-function}{%
\subsection{\texorpdfstring{Approach 3: Sorting using the \texttt{map}
Function}{Approach 3: Sorting using the map Function}}\label{approach-3-sorting-using-the-map-function}}

We can also use the \texttt{map} function on a \texttt{Series} to solve
this. Say we want to sort the \texttt{babynames} table by the number of
\texttt{"dr"}'s and \texttt{"ea"}'s in each \texttt{"Name"}. We'll
define the function \texttt{dr\_ea\_count} to help us out.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, define a function to count the number of times "dr" or "ea" appear in each name}
\KeywordTok{def}\NormalTok{ dr\_ea\_count(string):}
    \ControlFlowTok{return}\NormalTok{ string.count(}\StringTok{\textquotesingle{}dr\textquotesingle{}}\NormalTok{) }\OperatorTok{+}\NormalTok{ string.count(}\StringTok{\textquotesingle{}ea\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Then, use \textasciigrave{}map\textasciigrave{} to apply \textasciigrave{}dr\_ea\_count\textasciigrave{} to each name in the "Name" column}
\NormalTok{babynames[}\StringTok{"dr\_ea\_count"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{map}\NormalTok{(dr\_ea\_count)}

\CommentTok{\# Sort the DataFrame by the new "dr\_ea\_count" column so we can see our handiwork}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.sort\_values(by}\OperatorTok{=}\StringTok{"dr\_ea\_count"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  dr\_ea\_count \\
\midrule
115957 &    CA &   F &  1990 &  Deandrea &      5 &            3 \\
101976 &    CA &   F &  1986 &  Deandrea &      6 &            3 \\
131029 &    CA &   F &  1994 &  Leandrea &      5 &            3 \\
108731 &    CA &   F &  1988 &  Deandrea &      5 &            3 \\
308131 &    CA &   M &  1985 &  Deandrea &      6 &            3 \\
\bottomrule
\end{tabular}

We can drop the \texttt{dr\_ea\_count} once we're done using it to
maintain a neat table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop the \textasciigrave{}dr\_ea\_count\textasciigrave{} column}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.drop(}\StringTok{"dr\_ea\_count"}\NormalTok{, axis }\OperatorTok{=} \StringTok{\textquotesingle{}columns\textquotesingle{}}\NormalTok{)}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
115957 &    CA &   F &  1990 &  Deandrea &      5 \\
101976 &    CA &   F &  1986 &  Deandrea &      6 \\
131029 &    CA &   F &  1994 &  Leandrea &      5 \\
108731 &    CA &   F &  1988 &  Deandrea &      5 \\
308131 &    CA &   M &  1985 &  Deandrea &      6 \\
\bottomrule
\end{tabular}

\hypertarget{aggregating-data-with-.groupby}{%
\section{\texorpdfstring{Aggregating Data with
\texttt{.groupby}}{Aggregating Data with .groupby}}\label{aggregating-data-with-.groupby}}

Up until this point, we have been working with individual rows of
\texttt{DataFrame}s. As data scientists, we often wish to investigate
trends across a larger \emph{subset} of our data. For example, we may
want to compute some summary statistic (the mean, median, sum, etc.) for
a group of rows in our \texttt{DataFrame}. To do this, we'll use
\texttt{pandas} \texttt{GroupBy} objects. Our goal is to group together
rows that fall under the same category and perform an operation that
aggregates across all rows in the category.

Let's say we wanted to aggregate all rows in \texttt{babynames} for a
given year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x12c6f87c0>
\end{verbatim}

What does this strange output mean? Calling \texttt{.groupby}
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html}{(documentation)}
has generated a \texttt{GroupBy} object. You can imagine this as a set
of ``mini'' sub-\texttt{DataFrame}s, where each subframe contains all of
the rows from \texttt{babynames} that correspond to a particular year.

The diagram below shows a simplified view of \texttt{babynames} to help
illustrate this idea.

We can't work with a \texttt{GroupBy} object directly -- that is why you
saw that strange output earlier rather than a standard view of a
\texttt{DataFrame}. To actually manipulate values within these ``mini''
\texttt{DataFrame}s, we'll need to call an \emph{aggregation method}.
This is a method that tells \texttt{pandas} how to aggregate the values
within the \texttt{GroupBy} object. Once the aggregation is applied,
\texttt{pandas} will return a normal (now grouped) \texttt{DataFrame}.

The first aggregation method we'll consider is \texttt{.agg}. The
\texttt{.agg} method takes in a function as its argument; this function
is then applied to each column of a ``mini'' grouped DataFrame. We end
up with a new \texttt{DataFrame} with one aggregated row per subframe.
Let's see this in action by finding the \texttt{sum} of all counts for
each year in \texttt{babynames} -- this is equivalent to finding the
number of babies born in each year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Count"}\NormalTok{]].groupby(}\StringTok{"Year"}\NormalTok{).agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &   9163 \\
1911 &   9983 \\
1912 &  17946 \\
1913 &  22094 \\
1914 &  26926 \\
\bottomrule
\end{tabular}

We can relate this back to the diagram we used above. Remember that the
diagram uses a simplified version of \texttt{babynames}, which is why we
see smaller values for the summed counts.

\begin{figure}

{\centering \includegraphics{pandas_3/images/agg.png}

}

\caption{Performing an aggregation}

\end{figure}

Calling \texttt{.agg} has condensed each subframe back into a single
row. This gives us our final output: a \texttt{DataFrame} that is now
indexed by \texttt{"Year"}, with a single row for each unique year in
the original \texttt{babynames} DataFrame.

There are many different aggregation functions we can use, all of which
are useful in different applications.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Count"}\NormalTok{]].groupby(}\StringTok{"Year"}\NormalTok{).agg(}\BuiltInTok{min}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &      5 \\
1911 &      5 \\
1912 &      5 \\
1913 &      5 \\
1914 &      5 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Count"}\NormalTok{]].groupby(}\StringTok{"Year"}\NormalTok{).agg(}\BuiltInTok{max}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &    295 \\
1911 &    390 \\
1912 &    534 \\
1913 &    614 \\
1914 &    773 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Same result, but now we explicitly tell pandas to only consider the "Count" column when summing}
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &   9163 \\
1911 &   9983 \\
1912 &  17946 \\
1913 &  22094 \\
1914 &  26926 \\
\bottomrule
\end{tabular}

There are many different aggregations that can be applied to the grouped
data. The primary requirement is that an aggregation function must:

\begin{itemize}
\tightlist
\item
  Take in a \texttt{Series} of data (a single column of the grouped
  subframe).
\item
  Return a single value that aggregates this \texttt{Series}.
\end{itemize}

\hypertarget{aggregation-functions}{%
\subsection{Aggregation Functions}\label{aggregation-functions}}

Because of this fairly broad requirement, \texttt{pandas} offers many
ways of computing an aggregation.

\textbf{In-built} Python operations -- such as \texttt{sum},
\texttt{max}, and \texttt{min} -- are automatically recognized by
\texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# What is the minimum count for each name in any year?}
\NormalTok{babynames.groupby(}\StringTok{"Name"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{min}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Name    &        \\
\midrule
Aadan   &      5 \\
Aadarsh &      6 \\
Aaden   &     10 \\
Aadhav  &      6 \\
Aadhini &      6 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# What is the largest single{-}year count of each name?}
\NormalTok{babynames.groupby(}\StringTok{"Name"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{max}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Name    &        \\
\midrule
Aadan   &      7 \\
Aadarsh &      6 \\
Aaden   &    158 \\
Aadhav  &      8 \\
Aadhini &      6 \\
\bottomrule
\end{tabular}

As mentioned previously, functions from the \texttt{NumPy} library, such
as \texttt{np.mean}, \texttt{np.max}, \texttt{np.min}, and
\texttt{np.sum}, are also fair game in \texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# What is the average count for each name across all years?}
\NormalTok{babynames.groupby(}\StringTok{"Name"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(np.mean).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &      Count \\
Name    &            \\
\midrule
Aadan   &   6.000000 \\
Aadarsh &   6.000000 \\
Aaden   &  46.214286 \\
Aadhav  &   6.750000 \\
Aadhini &   6.000000 \\
\bottomrule
\end{tabular}

\texttt{pandas} also offers a number of in-built functions. Functions
that are native to \texttt{pandas} can be referenced using their string
name within a call to \texttt{.agg}. Some examples include:

\begin{itemize}
\tightlist
\item
  \texttt{.agg("sum")}
\item
  \texttt{.agg("max")}
\item
  \texttt{.agg("min")}
\item
  \texttt{.agg("mean")}
\item
  \texttt{.agg("first")}
\item
  \texttt{.agg("last")}
\end{itemize}

The latter two entries in this list -- \texttt{"first"} and
\texttt{"last"} -- are unique to \texttt{pandas}. They return the first
or last entry in a subframe column. Why might this be useful? Consider a
case where \emph{multiple} columns in a group share identical
information. To represent this information in the grouped output, we can
simply grab the first or last entry, which we know will be identical to
all other entries.

Let's illustrate this with an example. Say we add a new column to
\texttt{babynames} that contains the first letter of each name.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Imagine we had an additional column, "First Letter". We\textquotesingle{}ll explain this code next week}
\NormalTok{babynames[}\StringTok{"First Letter"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{[}\DecValTok{0}\NormalTok{]}

\CommentTok{\# We construct a simplified DataFrame containing just a subset of columns}
\NormalTok{babynames\_new }\OperatorTok{=}\NormalTok{ babynames[[}\StringTok{"Name"}\NormalTok{, }\StringTok{"First Letter"}\NormalTok{, }\StringTok{"Year"}\NormalTok{]]}
\NormalTok{babynames\_new.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllr}
\toprule
{} &      Name & First Letter &  Year \\
\midrule
115957 &  Deandrea &            D &  1990 \\
101976 &  Deandrea &            D &  1986 \\
131029 &  Leandrea &            L &  1994 \\
108731 &  Deandrea &            D &  1988 \\
308131 &  Deandrea &            D &  1985 \\
\bottomrule
\end{tabular}

If we form groups for each name in the dataset, \texttt{"First\ Letter"}
will be the same for all members of the group. This means that if we
simply select the first entry for \texttt{"First\ Letter"} in the group,
we'll represent all data in that group.

We can use a dictionary to apply different aggregation functions to each
column during grouping.

\begin{figure}

{\centering \includegraphics{pandas_3/images/first.png}

}

\caption{Aggregating using ``first''}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames\_new.groupby(}\StringTok{"Name"}\NormalTok{).agg(\{}\StringTok{"First Letter"}\NormalTok{:}\StringTok{"first"}\NormalTok{, }\StringTok{"Year"}\NormalTok{:}\StringTok{"max"}\NormalTok{\}).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llr}
\toprule
{} & First Letter &  Year \\
Name    &              &       \\
\midrule
Aadan   &            A &  2014 \\
Aadarsh &            A &  2019 \\
Aaden   &            A &  2020 \\
Aadhav  &            A &  2019 \\
Aadhini &            A &  2022 \\
\bottomrule
\end{tabular}

\hypertarget{plotting-birth-counts}{%
\subsection{Plotting Birth Counts}\label{plotting-birth-counts}}

Let's use \texttt{.agg} to find the total number of babies born in each
year. Recall that using \texttt{.agg} with \texttt{.groupby()} follows
the format:
\texttt{df.groupby(column\_name).agg(aggregation\_function)}. The line
of code below gives us the total number of babies born in each year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\CommentTok{\# Alternative 1}
\CommentTok{\# babynames.groupby("Year")[["Count"]].sum()}
\CommentTok{\# Alternative 2}
\CommentTok{\# babynames.groupby("Year").sum(numeric\_only=True)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &   9163 \\
1911 &   9983 \\
1912 &  17946 \\
1913 &  22094 \\
1914 &  26926 \\
\bottomrule
\end{tabular}

Here's an illustration of the process:

Plotting the \texttt{Dataframe} we obtain tells an interesting story.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.express }\ImportTok{as}\NormalTok{ px}
\NormalTok{puzzle2 }\OperatorTok{=}\NormalTok{ babynames.groupby(}\StringTok{"Year"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{)}
\NormalTok{px.line(puzzle2, y }\OperatorTok{=} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\textbf{A word of warning}: we made an enormous assumption when we
decided to use this dataset to estimate birth rate. According to
\href{https://lao.ca.gov/LAOEconTax/Article/Detail/691}{this article
from the Legistlative Analyst Office}, the true number of babies born in
California in 2020 was 421,275. However, our plot shows 362,882 babies
------ what happened?

\hypertarget{summary-of-the-.groupby-function}{%
\subsection{\texorpdfstring{Summary of the \texttt{.groupby()}
Function}{Summary of the .groupby() Function}}\label{summary-of-the-.groupby-function}}

A \texttt{groupby} operation involves some combination of
\textbf{splitting a \texttt{DataFrame} into grouped subframes},
\textbf{applying a function}, and \textbf{combining the results}.

For some arbitrary \texttt{DataFrame} \texttt{df} below, the code
\texttt{df.groupby("year").agg(sum)} does the following:

\begin{itemize}
\tightlist
\item
  \textbf{Splits} the \texttt{DataFrame} into sub-\texttt{DataFrame}s
  with rows belonging to the same year.
\item
  \textbf{Applies} the \texttt{sum} function to each column of each
  sub-\texttt{DataFrame}.
\item
  \textbf{Combines} the results of \texttt{sum} into a single
  \texttt{DataFrame}, indexed by \texttt{year}.
\end{itemize}

\hypertarget{revisiting-the-.agg-function}{%
\subsection{\texorpdfstring{Revisiting the \texttt{.agg()}
Function}{Revisiting the .agg() Function}}\label{revisiting-the-.agg-function}}

\texttt{.agg()} can take in any function that aggregates several values
into one summary value. Some commonly-used aggregation functions can
even be called directly, without explicit use of \texttt{.agg()}. For
example, we can call \texttt{.mean()} on \texttt{.groupby()}:

\begin{verbatim}
babynames.groupby("Year").mean().head()
\end{verbatim}

We can now put this all into practice. Say we want to find the baby name
with sex ``F'' that has fallen in popularity the most in California. To
calculate this, we can first create a metric: ``Ratio to Peak'' (RTP).
The RTP is the ratio of babies born with a given name in 2022 to the
\emph{maximum} number of babies born with the name in \emph{any} year.

Let's start with calculating this for one baby, ``Jennifer''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We filter by babies with sex "F" and sort by "Year"}
\NormalTok{f\_babynames }\OperatorTok{=}\NormalTok{ babynames[babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{]}
\NormalTok{f\_babynames }\OperatorTok{=}\NormalTok{ f\_babynames.sort\_values([}\StringTok{"Year"}\NormalTok{])}

\CommentTok{\# Determine how many Jennifers were born in CA per year}
\NormalTok{jenn\_counts\_series }\OperatorTok{=}\NormalTok{ f\_babynames[f\_babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Jennifer"}\NormalTok{][}\StringTok{"Count"}\NormalTok{]}

\CommentTok{\# Determine the max number of Jennifers born in a year and the number born in 2022 }
\CommentTok{\# to calculate RTP}
\NormalTok{max\_jenn }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(f\_babynames[f\_babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Jennifer"}\NormalTok{][}\StringTok{"Count"}\NormalTok{])}
\NormalTok{curr\_jenn }\OperatorTok{=}\NormalTok{ f\_babynames[f\_babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Jennifer"}\NormalTok{][}\StringTok{"Count"}\NormalTok{].iloc[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{rtp }\OperatorTok{=}\NormalTok{ curr\_jenn }\OperatorTok{/}\NormalTok{ max\_jenn}
\NormalTok{rtp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.018796372629843364
\end{verbatim}

By creating a function to calculate RTP and applying it to our
\texttt{DataFrame} by using \texttt{.groupby()}, we can easily compute
the RTP for all names at once!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ ratio\_to\_peak(series):}
    \ControlFlowTok{return}\NormalTok{ series.iloc[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{/} \BuiltInTok{max}\NormalTok{(series)}

\CommentTok{\#Using .groupby() to apply the function}
\NormalTok{rtp\_table }\OperatorTok{=}\NormalTok{ f\_babynames.groupby(}\StringTok{"Name"}\NormalTok{)[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Count"}\NormalTok{]].agg(ratio\_to\_peak)}
\NormalTok{rtp\_table.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &  Year &     Count \\
Name    &       &           \\
\midrule
Aadhini &   1.0 &  1.000000 \\
Aadhira &   1.0 &  0.500000 \\
Aadhya  &   1.0 &  0.660000 \\
Aadya   &   1.0 &  0.586207 \\
Aahana  &   1.0 &  0.269231 \\
\bottomrule
\end{tabular}

In the rows shown above, we can see that every row shown has a
\texttt{Year} value of \texttt{1.0}.

This is the ``\textbf{\texttt{pandas}}-ification'' of logic you saw in
Data 8. Much of the logic you've learned in Data 8 will serve you well
in Data 100.

\hypertarget{nuisance-columns}{%
\subsection{Nuisance Columns}\label{nuisance-columns}}

Note that you must be careful with which columns you apply the
\texttt{.agg()} function to. If we were to apply our function to the
table as a whole by doing
\texttt{f\_babynames.groupby("Name").agg(ratio\_to\_peak)}, executing
our \texttt{.agg()} call would result in a \texttt{TypeError}.

We can avoid this issue (and prevent unintentional loss of data) by
explicitly selecting column(s) we want to apply our aggregation function
to \textbf{BEFORE} calling \texttt{.agg()},

\hypertarget{renaming-columns-after-grouping}{%
\subsection{Renaming Columns After
Grouping}\label{renaming-columns-after-grouping}}

By default, \texttt{.groupby} will not rename any aggregated columns. As
we can see in the table above, the aggregated column is still named
\texttt{Count} even though it now represents the RTP. For better
readability, we can rename \texttt{Count} to \texttt{Count\ RTP}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rtp\_table }\OperatorTok{=}\NormalTok{ rtp\_table.rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Count"}\NormalTok{: }\StringTok{"Count RTP"}\NormalTok{\})}
\NormalTok{rtp\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &  Year &  Count RTP \\
Name            &       &            \\
\midrule
Aadhini         &   1.0 &   1.000000 \\
Aadhira         &   1.0 &   0.500000 \\
Aadhya          &   1.0 &   0.660000 \\
Aadya           &   1.0 &   0.586207 \\
Aahana          &   1.0 &   0.269231 \\
Aahna           &   1.0 &   1.000000 \\
Aaira           &   1.0 &   1.000000 \\
Aairah          &   1.0 &   1.000000 \\
Aalani          &   1.0 &   1.000000 \\
Aalaya          &   1.0 &   1.000000 \\
Aalayah         &   1.0 &   0.538462 \\
Aaleah          &   1.0 &   1.000000 \\
Aaleyah         &   1.0 &   0.526316 \\
Aalia           &   1.0 &   0.625000 \\
Aaliah          &   1.0 &   0.333333 \\
Aaliya          &   1.0 &   0.428571 \\
Aaliyah         &   1.0 &   0.817568 \\
Aaliyha         &   1.0 &   1.000000 \\
Aalyah          &   1.0 &   0.500000 \\
Aalyiah         &   1.0 &   1.000000 \\
Aamina          &   1.0 &   1.000000 \\
Aaminah         &   1.0 &   1.000000 \\
Aamira          &   1.0 &   1.000000 \\
Aamiyah         &   1.0 &   0.833333 \\
Aanchal         &   1.0 &   1.000000 \\
Aanika          &   1.0 &   0.555556 \\
Aaniya          &   1.0 &   1.000000 \\
Aaniyah         &   1.0 &   0.600000 \\
Aanshi          &   1.0 &   1.000000 \\
Aanvi           &   1.0 &   0.727273 \\
Aanya           &   1.0 &   0.673077 \\
Aaradhya        &   1.0 &   0.411765 \\
Aaralyn         &   1.0 &   0.304348 \\
Aaria           &   1.0 &   0.583333 \\
Aariah          &   1.0 &   1.000000 \\
Aariana         &   1.0 &   1.000000 \\
Aarika          &   1.0 &   0.555556 \\
Aarin           &   1.0 &   1.000000 \\
Aarini          &   1.0 &   1.000000 \\
Aariyah         &   1.0 &   0.666667 \\
Aarna           &   1.0 &   0.723404 \\
Aarohi          &   1.0 &   0.300000 \\
Aaron           &   1.0 &   0.192308 \\
Aarti           &   1.0 &   1.000000 \\
Aarushi         &   1.0 &   0.500000 \\
Aarvi           &   1.0 &   1.000000 \\
Aarya           &   1.0 &   0.820000 \\
Aaryn           &   1.0 &   0.700000 \\
Aasha           &   1.0 &   1.000000 \\
Aashi           &   1.0 &   0.750000 \\
Aashna          &   1.0 &   0.700000 \\
Aashritha       &   1.0 &   1.000000 \\
Aashvi          &   1.0 &   0.894737 \\
Aastha          &   1.0 &   0.714286 \\
Aavya           &   1.0 &   1.000000 \\
Aayat           &   1.0 &   1.000000 \\
Aayla           &   1.0 &   1.000000 \\
Aayra           &   1.0 &   0.500000 \\
Aayushi         &   1.0 &   1.000000 \\
Abagail         &   1.0 &   0.238095 \\
Abbagail        &   1.0 &   1.000000 \\
Abbe            &   1.0 &   1.000000 \\
Abbey           &   1.0 &   0.133333 \\
Abbie           &   1.0 &   0.259259 \\
Abbigail        &   1.0 &   0.102041 \\
Abbigale        &   1.0 &   0.454545 \\
Abbigayle       &   1.0 &   1.000000 \\
Abby            &   1.0 &   0.500000 \\
Abbygail        &   1.0 &   0.162791 \\
Abbygale        &   1.0 &   1.000000 \\
Abbygayle       &   1.0 &   1.000000 \\
Abeer           &   1.0 &   1.000000 \\
Abegail         &   1.0 &   0.833333 \\
Abelina         &   1.0 &   1.000000 \\
Abella          &   1.0 &   0.529412 \\
Aberdeen        &   1.0 &   1.000000 \\
Abi             &   1.0 &   1.000000 \\
Abigael         &   1.0 &   0.888889 \\
Abigail         &   1.0 &   0.473356 \\
Abigal          &   1.0 &   1.000000 \\
Abigale         &   1.0 &   0.333333 \\
Abigayl         &   1.0 &   1.000000 \\
Abigayle        &   1.0 &   0.300000 \\
Abilene         &   1.0 &   0.400000 \\
Abisai          &   1.0 &   1.000000 \\
Abra            &   1.0 &   0.555556 \\
Abraham         &   1.0 &   1.000000 \\
Abrar           &   1.0 &   1.000000 \\
Abreanna        &   1.0 &   1.000000 \\
Abree           &   1.0 &   0.714286 \\
Abriana         &   1.0 &   0.277778 \\
Abrianna        &   1.0 &   0.172414 \\
Abriella        &   1.0 &   0.250000 \\
Abrielle        &   1.0 &   0.375000 \\
Abril           &   1.0 &   0.298913 \\
Abrina          &   1.0 &   1.000000 \\
Aby             &   1.0 &   1.000000 \\
Abygail         &   1.0 &   0.218750 \\
Acacia          &   1.0 &   0.280000 \\
Acadia          &   1.0 &   1.000000 \\
Ace             &   1.0 &   1.000000 \\
Acelyn          &   1.0 &   1.000000 \\
Acelynn         &   1.0 &   0.500000 \\
Ada             &   1.0 &   0.850000 \\
Adabella        &   1.0 &   1.000000 \\
Adabelle        &   1.0 &   1.000000 \\
Adah            &   1.0 &   1.000000 \\
Adahli          &   1.0 &   0.500000 \\
Adahlia         &   1.0 &   1.000000 \\
Adaia           &   1.0 &   1.000000 \\
Adaiah          &   1.0 &   1.000000 \\
Adair           &   1.0 &   1.000000 \\
Adalay          &   1.0 &   1.000000 \\
Adaleah         &   1.0 &   1.000000 \\
Adalee          &   1.0 &   1.000000 \\
Adaleen         &   1.0 &   1.000000 \\
Adaleigh        &   1.0 &   0.625000 \\
Adalena         &   1.0 &   0.625000 \\
Adalene         &   1.0 &   0.357143 \\
Adalhi          &   1.0 &   1.000000 \\
Adali           &   1.0 &   0.800000 \\
Adalia          &   1.0 &   0.777778 \\
Adaliah         &   1.0 &   1.000000 \\
Adalie          &   1.0 &   0.848485 \\
Adalin          &   1.0 &   1.000000 \\
Adalina         &   1.0 &   0.666667 \\
Adalind         &   1.0 &   0.500000 \\
Adaline         &   1.0 &   0.549784 \\
Adaliz          &   1.0 &   1.000000 \\
Adaly           &   1.0 &   1.000000 \\
Adalyn          &   1.0 &   0.855000 \\
Adalyna         &   1.0 &   1.000000 \\
Adalyne         &   1.0 &   1.000000 \\
Adalynn         &   1.0 &   0.742857 \\
Adalynne        &   1.0 &   0.500000 \\
Adam            &   1.0 &   0.428571 \\
Adamari         &   1.0 &   0.111111 \\
Adamaris        &   1.0 &   0.112676 \\
Adamariz        &   1.0 &   0.545455 \\
Adamary         &   1.0 &   0.315789 \\
Adamarys        &   1.0 &   1.000000 \\
Adana           &   1.0 &   1.000000 \\
Adanely         &   1.0 &   1.000000 \\
Adanna          &   1.0 &   0.833333 \\
Adanya          &   1.0 &   1.000000 \\
Adara           &   1.0 &   0.941176 \\
Adaya           &   1.0 &   0.583333 \\
Adayah          &   1.0 &   1.000000 \\
Addalyn         &   1.0 &   1.000000 \\
Addalynn        &   1.0 &   0.777778 \\
Addeline        &   1.0 &   1.000000 \\
Addelyn         &   1.0 &   0.608696 \\
Addelynn        &   1.0 &   1.000000 \\
Addi            &   1.0 &   1.000000 \\
Addie           &   1.0 &   0.750000 \\
Addilyn         &   1.0 &   0.457627 \\
Addilynn        &   1.0 &   0.473684 \\
Addis           &   1.0 &   1.000000 \\
Addisen         &   1.0 &   0.357143 \\
Addison         &   1.0 &   0.421138 \\
Addisyn         &   1.0 &   0.222222 \\
Addy            &   1.0 &   0.714286 \\
Addysen         &   1.0 &   0.600000 \\
Addyson         &   1.0 &   0.175676 \\
Adeena          &   1.0 &   1.000000 \\
Adel            &   1.0 &   1.000000 \\
Adela           &   1.0 &   0.673077 \\
Adelaida        &   1.0 &   1.000000 \\
Adelaide        &   1.0 &   0.606061 \\
Adelaine        &   1.0 &   1.000000 \\
Adelayda        &   1.0 &   1.000000 \\
Adele           &   1.0 &   0.494505 \\
Adeleine        &   1.0 &   1.000000 \\
Adelena         &   1.0 &   1.000000 \\
Adelene         &   1.0 &   0.666667 \\
Adelia          &   1.0 &   0.384615 \\
Adelie          &   1.0 &   1.000000 \\
Adelin          &   1.0 &   0.500000 \\
Adelina         &   1.0 &   1.000000 \\
Adeline         &   1.0 &   0.656371 \\
Adelise         &   1.0 &   1.000000 \\
Adelita         &   1.0 &   0.416667 \\
Adell           &   1.0 &   1.000000 \\
Adella          &   1.0 &   0.529412 \\
Adelle          &   1.0 &   0.450000 \\
Adely           &   1.0 &   1.000000 \\
Adelyn          &   1.0 &   0.911243 \\
Adelyne         &   1.0 &   0.736842 \\
Adelynn         &   1.0 &   0.748031 \\
Adelynne        &   1.0 &   0.500000 \\
Aden            &   1.0 &   1.000000 \\
Adena           &   1.0 &   0.454545 \\
Adeya           &   1.0 &   0.750000 \\
Adhara          &   1.0 &   1.000000 \\
Adhira          &   1.0 &   0.500000 \\
Adhya           &   1.0 &   0.700000 \\
Adi             &   1.0 &   0.545455 \\
Adia            &   1.0 &   0.437500 \\
Adiana          &   1.0 &   1.000000 \\
Adianna         &   1.0 &   1.000000 \\
Adilene         &   1.0 &   0.417476 \\
Adileni         &   1.0 &   1.000000 \\
Adilenne        &   1.0 &   1.000000 \\
Adileny         &   1.0 &   1.000000 \\
Adilyn          &   1.0 &   0.384615 \\
Adilynn         &   1.0 &   0.842105 \\
Adina           &   1.0 &   0.583333 \\
Adira           &   1.0 &   0.821429 \\
Adison          &   1.0 &   0.222222 \\
Adisyn          &   1.0 &   0.666667 \\
Adithi          &   1.0 &   0.714286 \\
Aditi           &   1.0 &   0.468085 \\
Aditri          &   1.0 &   0.833333 \\
Adlee           &   1.0 &   1.000000 \\
Adleigh         &   1.0 &   1.000000 \\
Adley           &   1.0 &   0.541667 \\
Adora           &   1.0 &   0.666667 \\
Adore           &   1.0 &   0.777778 \\
Adrea           &   1.0 &   0.714286 \\
Adreana         &   1.0 &   0.500000 \\
Adreanna        &   1.0 &   0.454545 \\
Adreena         &   1.0 &   1.000000 \\
Adrena          &   1.0 &   1.000000 \\
Adria           &   1.0 &   0.521739 \\
Adrian          &   1.0 &   0.145833 \\
Adriana         &   1.0 &   0.164130 \\
Adriane         &   1.0 &   0.296296 \\
Adriann         &   1.0 &   0.625000 \\
Adrianna        &   1.0 &   0.121339 \\
Adrianne        &   1.0 &   0.095238 \\
Adrieanna       &   1.0 &   1.000000 \\
Adriel          &   1.0 &   0.555556 \\
Adrielle        &   1.0 &   0.700000 \\
Adrien          &   1.0 &   0.833333 \\
Adriene         &   1.0 &   0.238095 \\
Adrienna        &   1.0 &   1.000000 \\
Adrienne        &   1.0 &   0.127358 \\
Adrina          &   1.0 &   0.225806 \\
Adryana         &   1.0 &   1.000000 \\
Adryanna        &   1.0 &   1.000000 \\
Advika          &   1.0 &   0.384615 \\
Adya            &   1.0 &   0.636364 \\
Adylene         &   1.0 &   0.545455 \\
Adyline         &   1.0 &   1.000000 \\
Adysen          &   1.0 &   1.000000 \\
Adyson          &   1.0 &   0.380952 \\
Aela            &   1.0 &   1.000000 \\
Aeliana         &   1.0 &   1.000000 \\
Aella           &   1.0 &   1.000000 \\
Aemilia         &   1.0 &   1.000000 \\
Aerabella       &   1.0 &   1.000000 \\
Aerial          &   1.0 &   0.714286 \\
Aeriel          &   1.0 &   1.000000 \\
Aerilyn         &   1.0 &   1.000000 \\
Aerin           &   1.0 &   0.555556 \\
Aeris           &   1.0 &   0.466667 \\
Aerith          &   1.0 &   1.000000 \\
Aeryn           &   1.0 &   0.625000 \\
Aesha           &   1.0 &   1.000000 \\
Aeva            &   1.0 &   0.777778 \\
Africa          &   1.0 &   0.714286 \\
Afton           &   1.0 &   0.294118 \\
Agam            &   1.0 &   1.000000 \\
Agatha          &   1.0 &   1.000000 \\
Agena           &   1.0 &   1.000000 \\
Agnes           &   1.0 &   0.247312 \\
Agripina        &   1.0 &   0.833333 \\
Agueda          &   1.0 &   0.833333 \\
Agustina        &   1.0 &   0.750000 \\
Ahaana          &   1.0 &   0.666667 \\
Ahana           &   1.0 &   0.851852 \\
Ahava           &   1.0 &   1.000000 \\
Ahlam           &   1.0 &   1.000000 \\
Ahlana          &   1.0 &   1.000000 \\
Ahlani          &   1.0 &   0.615385 \\
Ahmari          &   1.0 &   1.000000 \\
Ahna            &   1.0 &   1.000000 \\
Ahniah          &   1.0 &   1.000000 \\
Ahri            &   1.0 &   0.800000 \\
Ahriana         &   1.0 &   1.000000 \\
Ahsley          &   1.0 &   0.384615 \\
Ahsoka          &   1.0 &   1.000000 \\
Ahtziri         &   1.0 &   0.171429 \\
Ahtziry         &   1.0 &   0.500000 \\
Ai              &   1.0 &   0.714286 \\
Aibhlinn        &   1.0 &   1.000000 \\
Aida            &   1.0 &   0.468085 \\
Aidan           &   1.0 &   0.192308 \\
Aidana          &   1.0 &   1.000000 \\
Aide            &   1.0 &   0.193548 \\
Aidee           &   1.0 &   0.315789 \\
Aideliz         &   1.0 &   1.000000 \\
Aiden           &   1.0 &   0.388889 \\
Aidsa           &   1.0 &   1.000000 \\
Aidyn           &   1.0 &   0.750000 \\
Aiesha          &   1.0 &   0.857143 \\
Aika            &   1.0 &   1.000000 \\
Aiko            &   1.0 &   0.363636 \\
Aila            &   1.0 &   1.000000 \\
Ailah           &   1.0 &   1.000000 \\
Ailana          &   1.0 &   1.000000 \\
Ailani          &   1.0 &   1.000000 \\
Ailanie         &   1.0 &   1.000000 \\
Ailany          &   1.0 &   1.000000 \\
Ailed           &   1.0 &   1.000000 \\
Ailee           &   1.0 &   0.750000 \\
Aileen          &   1.0 &   0.390625 \\
Aileene         &   1.0 &   1.000000 \\
Ailen           &   1.0 &   1.000000 \\
Ailene          &   1.0 &   0.437500 \\
Aileth          &   1.0 &   1.000000 \\
Ailey           &   1.0 &   0.384615 \\
Aili            &   1.0 &   0.857143 \\
Ailin           &   1.0 &   0.185185 \\
Ailis           &   1.0 &   1.000000 \\
Ailish          &   1.0 &   0.625000 \\
Aily            &   1.0 &   1.000000 \\
Ailyn           &   1.0 &   0.525424 \\
Ailynn          &   1.0 &   0.666667 \\
Aimar           &   1.0 &   1.000000 \\
Aime            &   1.0 &   0.166667 \\
Aimee           &   1.0 &   0.156379 \\
Aimie           &   1.0 &   1.000000 \\
Aimme           &   1.0 &   1.000000 \\
Aina            &   1.0 &   0.888889 \\
Ainara          &   1.0 &   1.000000 \\
Aine            &   1.0 &   0.727273 \\
Ainhara         &   1.0 &   1.000000 \\
Ainhoa          &   1.0 &   0.767442 \\
Ainslee         &   1.0 &   0.833333 \\
Ainsley         &   1.0 &   0.444444 \\
Aira            &   1.0 &   0.888889 \\
Airam           &   1.0 &   0.937500 \\
Aireana         &   1.0 &   1.000000 \\
Aireanna        &   1.0 &   1.000000 \\
Airi            &   1.0 &   0.555556 \\
Airiana         &   1.0 &   1.000000 \\
Airianna        &   1.0 &   1.000000 \\
Aisha           &   1.0 &   0.360248 \\
Aishah          &   1.0 &   0.777778 \\
Aishani         &   1.0 &   0.583333 \\
Aishwarya       &   1.0 &   0.461538 \\
Aisla           &   1.0 &   1.000000 \\
Aisley          &   1.0 &   0.875000 \\
Aislin          &   1.0 &   0.461538 \\
Aisling         &   1.0 &   0.538462 \\
Aislinn         &   1.0 &   0.611940 \\
Aislyn          &   1.0 &   0.333333 \\
Aislynn         &   1.0 &   0.545455 \\
Aissa           &   1.0 &   0.625000 \\
Aitana          &   1.0 &   1.000000 \\
Aitanna         &   1.0 &   0.555556 \\
Aithana         &   1.0 &   1.000000 \\
Aiva            &   1.0 &   0.750000 \\
Aivah           &   1.0 &   1.000000 \\
Aivy            &   1.0 &   0.750000 \\
Aixa            &   1.0 &   0.192308 \\
Aiya            &   1.0 &   1.000000 \\
Aiyana          &   1.0 &   0.437500 \\
Aiyanah         &   1.0 &   0.625000 \\
Aiyanna         &   1.0 &   0.421053 \\
Aiyla           &   1.0 &   0.800000 \\
Aiza            &   1.0 &   0.954545 \\
Aizah           &   1.0 &   1.000000 \\
Aizza           &   1.0 &   1.000000 \\
Aja             &   1.0 &   0.061224 \\
Ajah            &   1.0 &   0.545455 \\
Ajanae          &   1.0 &   0.454545 \\
Ajane           &   1.0 &   1.000000 \\
Ajee            &   1.0 &   0.227273 \\
Ajia            &   1.0 &   0.833333 \\
Ajooni          &   1.0 &   1.000000 \\
Akane           &   1.0 &   1.000000 \\
Akanksha        &   1.0 &   1.000000 \\
Akansha         &   1.0 &   1.000000 \\
Akari           &   1.0 &   0.545455 \\
Akasha          &   1.0 &   0.600000 \\
Akayla          &   1.0 &   0.357143 \\
Akaylah         &   1.0 &   1.000000 \\
Akeelah         &   1.0 &   0.500000 \\
Akela           &   1.0 &   0.714286 \\
Akemi           &   1.0 &   0.352941 \\
Akhila          &   1.0 &   1.000000 \\
Akia            &   1.0 &   0.555556 \\
Akiba           &   1.0 &   1.000000 \\
Akiko           &   1.0 &   0.384615 \\
Akila           &   1.0 &   0.833333 \\
Akilah          &   1.0 &   0.368421 \\
Akina           &   1.0 &   0.833333 \\
Akira           &   1.0 &   0.717391 \\
Akirah          &   1.0 &   1.000000 \\
Akisha          &   1.0 &   0.625000 \\
Akshara         &   1.0 &   0.437500 \\
Akshaya         &   1.0 &   0.750000 \\
Akshita         &   1.0 &   0.750000 \\
Akyra           &   1.0 &   1.000000 \\
Alaa            &   1.0 &   0.500000 \\
Alabama         &   1.0 &   0.857143 \\
Alahna          &   1.0 &   1.000000 \\
Alahni          &   1.0 &   0.882353 \\
Alaia           &   1.0 &   0.958678 \\
Alaiah          &   1.0 &   0.375000 \\
Alaiia          &   1.0 &   0.500000 \\
Alaina          &   1.0 &   0.756303 \\
Alaine          &   1.0 &   1.000000 \\
Alaiya          &   1.0 &   0.465116 \\
Alaiyah         &   1.0 &   0.900000 \\
Alaiza          &   1.0 &   0.833333 \\
Alajah          &   1.0 &   1.000000 \\
Alan            &   1.0 &   0.555556 \\
Alana           &   1.0 &   0.622951 \\
Alanah          &   1.0 &   0.281250 \\
Alandra         &   1.0 &   0.714286 \\
Alane           &   1.0 &   0.454545 \\
Alani           &   1.0 &   0.982833 \\
Alania          &   1.0 &   1.000000 \\
Alanie          &   1.0 &   0.538462 \\
Alanii          &   1.0 &   1.000000 \\
Alanis          &   1.0 &   0.289474 \\
Alanna          &   1.0 &   0.913580 \\
Alannah         &   1.0 &   0.317073 \\
Alanni          &   1.0 &   1.000000 \\
Alanny          &   1.0 &   1.000000 \\
Alany           &   1.0 &   0.636364 \\
Alara           &   1.0 &   0.923077 \\
Alasia          &   1.0 &   1.000000 \\
Alaska          &   1.0 &   0.454545 \\
Alauna          &   1.0 &   1.000000 \\
Alaura          &   1.0 &   0.294118 \\
Alaya           &   1.0 &   0.819048 \\
Alayah          &   1.0 &   0.676056 \\
Alayiah         &   1.0 &   1.000000 \\
Alayjah         &   1.0 &   0.833333 \\
Alayla          &   1.0 &   0.461538 \\
Alaylah         &   1.0 &   1.000000 \\
Alayna          &   1.0 &   0.940171 \\
Alaynah         &   1.0 &   0.833333 \\
Alayne          &   1.0 &   1.000000 \\
Alaynna         &   1.0 &   1.000000 \\
Alaysha         &   1.0 &   0.714286 \\
Alaysia         &   1.0 &   0.227273 \\
Alayza          &   1.0 &   1.000000 \\
Alazay          &   1.0 &   1.000000 \\
Alazne          &   1.0 &   1.000000 \\
Alba            &   1.0 &   0.678571 \\
Albany          &   1.0 &   1.000000 \\
Albert          &   1.0 &   0.625000 \\
Alberta         &   1.0 &   0.093750 \\
Albertina       &   1.0 &   0.555556 \\
Alberto         &   1.0 &   0.500000 \\
Albina          &   1.0 &   0.714286 \\
Alda            &   1.0 &   0.625000 \\
Alden           &   1.0 &   1.000000 \\
Alea            &   1.0 &   0.611111 \\
Aleah           &   1.0 &   0.612245 \\
Aleana          &   1.0 &   0.888889 \\
Aleasha         &   1.0 &   0.555556 \\
Alec            &   1.0 &   1.000000 \\
Alecia          &   1.0 &   0.250000 \\
Aleda           &   1.0 &   1.000000 \\
Aleea           &   1.0 &   1.000000 \\
Aleeah          &   1.0 &   0.250000 \\
Aleece          &   1.0 &   1.000000 \\
Aleecia         &   1.0 &   1.000000 \\
Aleen           &   1.0 &   0.750000 \\
Aleena          &   1.0 &   0.654321 \\
Aleenah         &   1.0 &   0.571429 \\
Aleesa          &   1.0 &   0.875000 \\
Aleesha         &   1.0 &   0.545455 \\
Aleeya          &   1.0 &   0.800000 \\
Aleeyah         &   1.0 &   0.294118 \\
Aleeza          &   1.0 &   0.750000 \\
Alegandra       &   1.0 &   1.000000 \\
Alegra          &   1.0 &   0.714286 \\
Alegria         &   1.0 &   1.000000 \\
Aleia           &   1.0 &   0.882353 \\
Aleiah          &   1.0 &   0.461538 \\
Aleida          &   1.0 &   0.789474 \\
Aleigha         &   1.0 &   0.727273 \\
Aleina          &   1.0 &   0.600000 \\
Aleisha         &   1.0 &   0.714286 \\
Aleiya          &   1.0 &   1.000000 \\
Aleiyah         &   1.0 &   1.000000 \\
Aleja           &   1.0 &   1.000000 \\
Alejah          &   1.0 &   1.000000 \\
Alejandra       &   1.0 &   0.083983 \\
Alejandrina     &   1.0 &   0.250000 \\
Alejandro       &   1.0 &   0.250000 \\
Alekhya         &   1.0 &   1.000000 \\
Aleksa          &   1.0 &   1.000000 \\
Aleksandra      &   1.0 &   0.277778 \\
Aleksia         &   1.0 &   1.000000 \\
Aleli           &   1.0 &   0.833333 \\
Alena           &   1.0 &   0.714286 \\
Alene           &   1.0 &   0.416667 \\
Alenna          &   1.0 &   1.000000 \\
Alesa           &   1.0 &   1.000000 \\
Alesana         &   1.0 &   1.000000 \\
Alesandra       &   1.0 &   0.555556 \\
Alese           &   1.0 &   1.000000 \\
Alesha          &   1.0 &   0.208333 \\
Aleshia         &   1.0 &   0.714286 \\
Alesia          &   1.0 &   0.128205 \\
Alessa          &   1.0 &   1.000000 \\
Alessandra      &   1.0 &   0.704981 \\
Alessandria     &   1.0 &   1.000000 \\
Alessi          &   1.0 &   0.916667 \\
Alessia         &   1.0 &   1.000000 \\
Aleta           &   1.0 &   0.150000 \\
Aletha          &   1.0 &   0.636364 \\
Alethea         &   1.0 &   0.229167 \\
Aletheia        &   1.0 &   0.888889 \\
Alethia         &   1.0 &   1.000000 \\
Alex            &   1.0 &   0.490566 \\
Alexa           &   1.0 &   0.084086 \\
Alexah          &   1.0 &   0.600000 \\
Alexander       &   1.0 &   0.078125 \\
Alexandera      &   1.0 &   1.000000 \\
Alexanderia     &   1.0 &   1.000000 \\
Alexandra       &   1.0 &   0.146789 \\
Alexandrea      &   1.0 &   0.067568 \\
Alexandria      &   1.0 &   0.132184 \\
Alexandrina     &   1.0 &   1.000000 \\
Alexandrya      &   1.0 &   1.000000 \\
Alexas          &   1.0 &   0.833333 \\
Alexcia         &   1.0 &   0.500000 \\
Alexes          &   1.0 &   0.555556 \\
Alexi           &   1.0 &   0.266667 \\
Alexia          &   1.0 &   0.148052 \\
Alexiah         &   1.0 &   0.875000 \\
Alexiana        &   1.0 &   1.000000 \\
Alexie          &   1.0 &   0.352941 \\
Alexis          &   1.0 &   0.042430 \\
Alexiss         &   1.0 &   0.555556 \\
Alexius         &   1.0 &   0.833333 \\
Alexiz          &   1.0 &   0.857143 \\
Alexsa          &   1.0 &   0.666667 \\
Alexsandra      &   1.0 &   0.111111 \\
Alexsia         &   1.0 &   1.000000 \\
Alexsis         &   1.0 &   0.333333 \\
Alexssa         &   1.0 &   1.000000 \\
Alexsys         &   1.0 &   1.000000 \\
Alexus          &   1.0 &   0.048951 \\
Alexx           &   1.0 &   1.000000 \\
Alexxa          &   1.0 &   0.666667 \\
Alexxia         &   1.0 &   1.000000 \\
Alexxis         &   1.0 &   0.250000 \\
Alexxus         &   1.0 &   1.000000 \\
Alexy           &   1.0 &   0.625000 \\
Alexya          &   1.0 &   1.000000 \\
Alexys          &   1.0 &   0.151515 \\
Alexyss         &   1.0 &   0.384615 \\
Alexza          &   1.0 &   0.833333 \\
Alexzandra      &   1.0 &   0.263158 \\
Alexzandria     &   1.0 &   0.666667 \\
Aleya           &   1.0 &   0.666667 \\
Aleyah          &   1.0 &   1.000000 \\
Aleyda          &   1.0 &   0.208333 \\
Aleyna          &   1.0 &   0.903226 \\
Aleysa          &   1.0 &   1.000000 \\
Aleyssa         &   1.0 &   1.000000 \\
Aleyza          &   1.0 &   0.769231 \\
Alfa            &   1.0 &   1.000000 \\
Alfreda         &   1.0 &   0.312500 \\
Alfredo         &   1.0 &   1.000000 \\
Alheli          &   1.0 &   1.000000 \\
Ali             &   1.0 &   0.307692 \\
Alia            &   1.0 &   1.000000 \\
Aliah           &   1.0 &   0.626866 \\
Aliana          &   1.0 &   0.916667 \\
Alianah         &   1.0 &   1.000000 \\
Alianna         &   1.0 &   1.000000 \\
Aliannah        &   1.0 &   0.636364 \\
Alica           &   1.0 &   0.714286 \\
Alice           &   1.0 &   0.692443 \\
Alicea          &   1.0 &   0.833333 \\
Alicen          &   1.0 &   1.000000 \\
Alicia          &   1.0 &   0.174009 \\
Aliciana        &   1.0 &   0.833333 \\
Alida           &   1.0 &   0.500000 \\
Aliea           &   1.0 &   1.000000 \\
Aliena          &   1.0 &   1.000000 \\
Aliesha         &   1.0 &   1.000000 \\
Alijah          &   1.0 &   0.533333 \\
Alik            &   1.0 &   1.000000 \\
Alika           &   1.0 &   1.000000 \\
Alilah          &   1.0 &   1.000000 \\
Alilet          &   1.0 &   1.000000 \\
Alin            &   1.0 &   0.428571 \\
Alina           &   1.0 &   0.967495 \\
Alinah          &   1.0 &   0.526316 \\
Aline           &   1.0 &   0.222222 \\
Alinea          &   1.0 &   1.000000 \\
Alinna          &   1.0 &   0.823529 \\
Alique          &   1.0 &   1.000000 \\
Alis            &   1.0 &   1.000000 \\
Alisa           &   1.0 &   0.233333 \\
Alisandra       &   1.0 &   1.000000 \\
Alise           &   1.0 &   0.312500 \\
Alisha          &   1.0 &   0.082353 \\
Alishba         &   1.0 &   0.714286 \\
Alishia         &   1.0 &   0.357143 \\
Alisi           &   1.0 &   1.000000 \\
Alisia          &   1.0 &   0.315789 \\
Alison          &   1.0 &   0.439103 \\
Alissa          &   1.0 &   0.081081 \\
Alissandra      &   1.0 &   1.000000 \\
Alissia         &   1.0 &   1.000000 \\
Alisson         &   1.0 &   0.561404 \\
Alisyn          &   1.0 &   1.000000 \\
Alita           &   1.0 &   0.472222 \\
Alitza          &   1.0 &   0.500000 \\
Alitzel         &   1.0 &   1.000000 \\
Alivia          &   1.0 &   0.825581 \\
Aliviah         &   1.0 &   0.625000 \\
Alix            &   1.0 &   0.151515 \\
Alixandra       &   1.0 &   0.571429 \\
Alixandria      &   1.0 &   0.750000 \\
Aliya           &   1.0 &   0.644068 \\
Aliyaah         &   1.0 &   1.000000 \\
Aliyah          &   1.0 &   0.559659 \\
Aliyana         &   1.0 &   0.470588 \\
Aliyanah        &   1.0 &   1.000000 \\
Aliyanna        &   1.0 &   0.625000 \\
Aliz            &   1.0 &   1.000000 \\
Aliza           &   1.0 &   0.773585 \\
Alizabeth       &   1.0 &   0.555556 \\
Alizae          &   1.0 &   0.375000 \\
Alizah          &   1.0 &   1.000000 \\
Alizay          &   1.0 &   0.529412 \\
Alize           &   1.0 &   0.163043 \\
Alizea          &   1.0 &   0.625000 \\
Alizee          &   1.0 &   0.357143 \\
Alizey          &   1.0 &   1.000000 \\
Alizon          &   1.0 &   0.384615 \\
Alla            &   1.0 &   1.000000 \\
Allana          &   1.0 &   0.416667 \\
Allanah         &   1.0 &   0.714286 \\
Allegra         &   1.0 &   0.250000 \\
Allen           &   1.0 &   0.833333 \\
Allena          &   1.0 &   0.555556 \\
Allene          &   1.0 &   0.846154 \\
Allexa          &   1.0 &   1.000000 \\
Alley           &   1.0 &   0.833333 \\
Alli            &   1.0 &   0.666667 \\
Allia           &   1.0 &   1.000000 \\
Alliah          &   1.0 &   1.000000 \\
Alliana         &   1.0 &   0.875000 \\
Allie           &   1.0 &   0.466667 \\
Allina          &   1.0 &   0.833333 \\
Allisa          &   1.0 &   0.555556 \\
Allisen         &   1.0 &   1.000000 \\
Allisha         &   1.0 &   1.000000 \\
Allison         &   1.0 &   0.419322 \\
Allissa         &   1.0 &   0.250000 \\
Allisson        &   1.0 &   0.028986 \\
Allisyn         &   1.0 &   1.000000 \\
Alliyah         &   1.0 &   0.250000 \\
Allizon         &   1.0 &   1.000000 \\
Allura          &   1.0 &   0.833333 \\
Allure          &   1.0 &   1.000000 \\
Ally            &   1.0 &   0.311475 \\
Allyana         &   1.0 &   0.625000 \\
Allyce          &   1.0 &   0.714286 \\
Allycia         &   1.0 &   1.000000 \\
Allyn           &   1.0 &   0.600000 \\
Allyna          &   1.0 &   1.000000 \\
Allysa          &   1.0 &   0.363636 \\
Allyse          &   1.0 &   1.000000 \\
Allysen         &   1.0 &   0.833333 \\
Allysha         &   1.0 &   0.833333 \\
Allysia         &   1.0 &   1.000000 \\
Allyson         &   1.0 &   0.227642 \\
Allyssa         &   1.0 &   0.111111 \\
Allysson        &   1.0 &   0.285714 \\
Alma            &   1.0 &   0.249315 \\
Almadelia       &   1.0 &   0.714286 \\
Almarosa        &   1.0 &   0.416667 \\
Almendra        &   1.0 &   1.000000 \\
Almira          &   1.0 &   1.000000 \\
Aloha           &   1.0 &   0.750000 \\
Aloma           &   1.0 &   1.000000 \\
Alona           &   1.0 &   0.333333 \\
Alonda          &   1.0 &   0.857143 \\
Alondra         &   1.0 &   0.127729 \\
Aloni           &   1.0 &   1.000000 \\
Alonna          &   1.0 &   0.900000 \\
Alora           &   1.0 &   1.000000 \\
Alpha           &   1.0 &   0.625000 \\
Alta            &   1.0 &   0.161290 \\
Altagracia      &   1.0 &   0.666667 \\
Althea          &   1.0 &   0.500000 \\
Aluna           &   1.0 &   0.714286 \\
Alva            &   1.0 &   0.444444 \\
Alvera          &   1.0 &   0.555556 \\
Alvina          &   1.0 &   0.545455 \\
Aly             &   1.0 &   0.727273 \\
Alya            &   1.0 &   0.807692 \\
Alyah           &   1.0 &   0.578947 \\
Alyana          &   1.0 &   1.000000 \\
Alyanna         &   1.0 &   0.391304 \\
Alyannah        &   1.0 &   1.000000 \\
Alyce           &   1.0 &   0.272727 \\
Alycia          &   1.0 &   0.118644 \\
Alydia          &   1.0 &   1.000000 \\
Alyiah          &   1.0 &   0.555556 \\
Alyissa         &   1.0 &   0.857143 \\
Alyn            &   1.0 &   0.625000 \\
Alyna           &   1.0 &   0.697674 \\
Alynah          &   1.0 &   0.625000 \\
Alyne           &   1.0 &   1.000000 \\
Alynna          &   1.0 &   0.666667 \\
Alys            &   1.0 &   1.000000 \\
Alysa           &   1.0 &   0.125000 \\
Alysah          &   1.0 &   1.000000 \\
Alyse           &   1.0 &   0.208333 \\
Alysen          &   1.0 &   1.000000 \\
Alysha          &   1.0 &   0.075949 \\
Alyshia         &   1.0 &   0.416667 \\
Alysia          &   1.0 &   0.137255 \\
Alyson          &   1.0 &   0.371429 \\
Alyss           &   1.0 &   1.000000 \\
Alyssa          &   1.0 &   0.099617 \\
Alyssah         &   1.0 &   0.500000 \\
Alyssamae       &   1.0 &   1.000000 \\
Alyssamarie     &   1.0 &   0.454545 \\
Alyssandra      &   1.0 &   0.416667 \\
Alysse          &   1.0 &   0.294118 \\
Alyssia         &   1.0 &   0.233333 \\
Alysson         &   1.0 &   0.307692 \\
Alyvia          &   1.0 &   0.421053 \\
Alyx            &   1.0 &   0.700000 \\
Alyxandra       &   1.0 &   0.416667 \\
Alyxandria      &   1.0 &   1.000000 \\
Alyza           &   1.0 &   0.409091 \\
Alyzah          &   1.0 &   1.000000 \\
Alyzza          &   1.0 &   1.000000 \\
Amada           &   1.0 &   0.631579 \\
Amaia           &   1.0 &   0.720588 \\
Amaiah          &   1.0 &   0.600000 \\
Amaira          &   1.0 &   0.840000 \\
Amairani        &   1.0 &   0.192308 \\
Amairany        &   1.0 &   0.409091 \\
Amaiya          &   1.0 &   0.444444 \\
Amaiyah         &   1.0 &   0.833333 \\
Amal            &   1.0 &   0.483871 \\
Amala           &   1.0 &   1.000000 \\
Amali           &   1.0 &   1.000000 \\
Amalia          &   1.0 &   0.651163 \\
Amalie          &   1.0 &   0.333333 \\
Amalya          &   1.0 &   1.000000 \\
Aman            &   1.0 &   1.000000 \\
Amana           &   1.0 &   1.000000 \\
Amanada         &   1.0 &   0.750000 \\
Amanat          &   1.0 &   1.000000 \\
Amanda          &   1.0 &   0.021419 \\
Amandeep        &   1.0 &   0.666667 \\
Amando          &   1.0 &   1.000000 \\
Amani           &   1.0 &   0.844828 \\
Amanpreet       &   1.0 &   0.625000 \\
Amar            &   1.0 &   1.000000 \\
Amara           &   1.0 &   1.000000 \\
Amarachi        &   1.0 &   1.000000 \\
Amarah          &   1.0 &   0.695652 \\
Amaranta        &   1.0 &   1.000000 \\
Amaree          &   1.0 &   1.000000 \\
Amari           &   1.0 &   1.000000 \\
Amaria          &   1.0 &   0.714286 \\
Amariah         &   1.0 &   0.300000 \\
Amarie          &   1.0 &   0.695652 \\
Amarii          &   1.0 &   1.000000 \\
Amarilis        &   1.0 &   0.714286 \\
Amaris          &   1.0 &   0.896552 \\
Amarissa        &   1.0 &   0.555556 \\
Amariyah        &   1.0 &   0.833333 \\
Amariz          &   1.0 &   1.000000 \\
Amarra          &   1.0 &   1.000000 \\
Amaryllis       &   1.0 &   0.571429 \\
Amauri          &   1.0 &   1.000000 \\
Amaya           &   1.0 &   0.788945 \\
Amayah          &   1.0 &   0.969231 \\
Amayra          &   1.0 &   0.454545 \\
Amayrani        &   1.0 &   0.500000 \\
Amayrany        &   1.0 &   0.625000 \\
Ambar           &   1.0 &   0.158730 \\
Amber           &   1.0 &   0.059675 \\
Amberle         &   1.0 &   1.000000 \\
Amberlee        &   1.0 &   0.666667 \\
Amberley        &   1.0 &   1.000000 \\
Amberly         &   1.0 &   0.640000 \\
Amberlyn        &   1.0 &   0.428571 \\
Amberlynn       &   1.0 &   0.600000 \\
Amberrose       &   1.0 &   1.000000 \\
Ambika          &   1.0 &   1.000000 \\
Ambra           &   1.0 &   0.666667 \\
Ambre           &   1.0 &   1.000000 \\
Ambria          &   1.0 &   0.600000 \\
Ambrielle       &   1.0 &   0.714286 \\
Ambrosia        &   1.0 &   0.277778 \\
Ambur           &   1.0 &   1.000000 \\
Amee            &   1.0 &   0.416667 \\
Ameena          &   1.0 &   0.600000 \\
Ameenah         &   1.0 &   1.000000 \\
Ameera          &   1.0 &   1.000000 \\
Ameerah         &   1.0 &   0.700000 \\
Ameli           &   1.0 &   1.000000 \\
Amelia          &   1.0 &   0.939504 \\
Ameliah         &   1.0 &   1.000000 \\
Ameliana        &   1.0 &   1.000000 \\
Ameliarose      &   1.0 &   1.000000 \\
Amelie          &   1.0 &   0.553719 \\
Ameliya         &   1.0 &   1.000000 \\
Amellia         &   1.0 &   1.000000 \\
Amely           &   1.0 &   1.000000 \\
Amen            &   1.0 &   0.750000 \\
Amena           &   1.0 &   0.857143 \\
Amera           &   1.0 &   0.833333 \\
America         &   1.0 &   0.066667 \\
Amerie          &   1.0 &   0.714286 \\
Amerika         &   1.0 &   1.000000 \\
Amery           &   1.0 &   0.500000 \\
Amethyst        &   1.0 &   1.000000 \\
Amey            &   1.0 &   1.000000 \\
Ameya           &   1.0 &   0.727273 \\
Ameyalli        &   1.0 &   1.000000 \\
Ami             &   1.0 &   0.238095 \\
Amia            &   1.0 &   0.415094 \\
Amiah           &   1.0 &   0.888889 \\
Amie            &   1.0 &   0.123596 \\
Amiee           &   1.0 &   0.416667 \\
Amila           &   1.0 &   0.937500 \\
Amilah          &   1.0 &   0.600000 \\
Amilia          &   1.0 &   0.526316 \\
Amiliana        &   1.0 &   0.714286 \\
Amillia         &   1.0 &   1.000000 \\
Amilliana       &   1.0 &   1.000000 \\
Amilya          &   1.0 &   1.000000 \\
Amina           &   1.0 &   1.000000 \\
Aminah          &   1.0 &   0.542857 \\
Aminata         &   1.0 &   1.000000 \\
Amira           &   1.0 &   1.000000 \\
Amirah          &   1.0 &   0.796875 \\
Amiri           &   1.0 &   1.000000 \\
Amirrah         &   1.0 &   1.000000 \\
Amisadai        &   1.0 &   1.000000 \\
Amisha          &   1.0 &   0.583333 \\
Amit            &   1.0 &   1.000000 \\
Amita           &   1.0 &   1.000000 \\
Amity           &   1.0 &   0.500000 \\
Amiya           &   1.0 &   0.454545 \\
Amiyah          &   1.0 &   0.615385 \\
Ammi            &   1.0 &   1.000000 \\
Ammie           &   1.0 &   0.714286 \\
Ammy            &   1.0 &   0.294118 \\
Amna            &   1.0 &   0.666667 \\
Amoni           &   1.0 &   1.000000 \\
Amor            &   1.0 &   0.842105 \\
Amora           &   1.0 &   0.883117 \\
Amorah          &   1.0 &   0.833333 \\
Amore           &   1.0 &   1.000000 \\
Amoreena        &   1.0 &   0.666667 \\
Amorette        &   1.0 &   0.800000 \\
Amorie          &   1.0 &   1.000000 \\
Amory           &   1.0 &   1.000000 \\
Amour           &   1.0 &   0.857143 \\
Amoura          &   1.0 &   1.000000 \\
Amparo          &   1.0 &   0.230769 \\
Amreen          &   1.0 &   0.714286 \\
Amri            &   1.0 &   0.625000 \\
Amrit           &   1.0 &   0.500000 \\
Amrita          &   1.0 &   0.384615 \\
Amrutha         &   1.0 &   1.000000 \\
Amulya          &   1.0 &   0.714286 \\
Amunique        &   1.0 &   0.833333 \\
Amy             &   1.0 &   0.096542 \\
Amya            &   1.0 &   0.166667 \\
Amyah           &   1.0 &   0.294118 \\
Amyiah          &   1.0 &   1.000000 \\
Amyla           &   1.0 &   1.000000 \\
Amyra           &   1.0 &   1.000000 \\
Amyrah          &   1.0 &   0.800000 \\
An              &   1.0 &   0.470588 \\
Ana             &   1.0 &   0.185436 \\
Anaalicia       &   1.0 &   0.833333 \\
Anaaya          &   1.0 &   0.666667 \\
Anabel          &   1.0 &   0.063830 \\
Anabela         &   1.0 &   1.000000 \\
Anabelen        &   1.0 &   1.000000 \\
Anabell         &   1.0 &   0.230769 \\
Anabella        &   1.0 &   0.160714 \\
Anabelle        &   1.0 &   0.116438 \\
Anabeth         &   1.0 &   1.000000 \\
Anabia          &   1.0 &   1.000000 \\
Anacamila       &   1.0 &   1.000000 \\
Anacani         &   1.0 &   0.666667 \\
Anacaren        &   1.0 &   1.000000 \\
Anacecilia      &   1.0 &   1.000000 \\
Anacristina     &   1.0 &   0.625000 \\
Anadalay        &   1.0 &   1.000000 \\
Anaeli          &   1.0 &   1.000000 \\
Anaelle         &   1.0 &   1.000000 \\
Anagabriela     &   1.0 &   0.714286 \\
Anagha          &   1.0 &   0.888889 \\
Anah            &   1.0 &   1.000000 \\
Anahat          &   1.0 &   0.833333 \\
Anahi           &   1.0 &   0.441667 \\
Anahid          &   1.0 &   0.714286 \\
Anahis          &   1.0 &   1.000000 \\
Anahit          &   1.0 &   0.578947 \\
Anahita         &   1.0 &   0.714286 \\
Anahy           &   1.0 &   0.451613 \\
Anai            &   1.0 &   0.159091 \\
Anaia           &   1.0 &   0.625000 \\
Anaiah          &   1.0 &   0.923077 \\
Anaid           &   1.0 &   0.384615 \\
Anaika          &   1.0 &   1.000000 \\
Anaira          &   1.0 &   0.500000 \\
Anairis         &   1.0 &   1.000000 \\
Anais           &   1.0 &   0.617978 \\
Anaisa          &   1.0 &   0.263158 \\
Anaisabel       &   1.0 &   1.000000 \\
Anaisha         &   1.0 &   0.526316 \\
Anaissa         &   1.0 &   1.000000 \\
Anaiya          &   1.0 &   0.350000 \\
Anaiyah         &   1.0 &   0.437500 \\
Anaiz           &   1.0 &   0.384615 \\
Anaiza          &   1.0 &   0.461538 \\
Anakaren        &   1.0 &   0.171429 \\
Analaura        &   1.0 &   0.454545 \\
Analaya         &   1.0 &   1.000000 \\
Analayah        &   1.0 &   1.000000 \\
Analea          &   1.0 &   1.000000 \\
Analeah         &   1.0 &   0.678571 \\
Analee          &   1.0 &   0.363636 \\
Analeia         &   1.0 &   0.500000 \\
Analeigh        &   1.0 &   0.400000 \\
Analeigha       &   1.0 &   1.000000 \\
Analena         &   1.0 &   1.000000 \\
Analeya         &   1.0 &   1.000000 \\
Analeyah        &   1.0 &   1.000000 \\
Anali           &   1.0 &   0.918919 \\
Analia          &   1.0 &   0.440000 \\
Analiah         &   1.0 &   0.789474 \\
Analicia        &   1.0 &   0.400000 \\
Analie          &   1.0 &   0.500000 \\
Analiese        &   1.0 &   0.294118 \\
Analilia        &   1.0 &   0.600000 \\
Analina         &   1.0 &   1.000000 \\
Analis          &   1.0 &   1.000000 \\
Analisa         &   1.0 &   0.150000 \\
Analise         &   1.0 &   0.285714 \\
Analissa        &   1.0 &   0.714286 \\
Analiya         &   1.0 &   1.000000 \\
Analiyah        &   1.0 &   1.000000 \\
Analiz          &   1.0 &   0.545455 \\
Analiza         &   1.0 &   1.000000 \\
Analleli        &   1.0 &   0.625000 \\
Anallely        &   1.0 &   1.000000 \\
Analucia        &   1.0 &   0.700000 \\
Analuisa        &   1.0 &   0.875000 \\
Analy           &   1.0 &   0.295455 \\
Analya          &   1.0 &   1.000000 \\
Analyah         &   1.0 &   1.000000 \\
Analyn          &   1.0 &   1.000000 \\
Analynn         &   1.0 &   1.000000 \\
Analysa         &   1.0 &   1.000000 \\
Analyse         &   1.0 &   0.555556 \\
Analyssa        &   1.0 &   0.416667 \\
Anam            &   1.0 &   0.714286 \\
Anamaria        &   1.0 &   0.178571 \\
Anamarie        &   1.0 &   0.333333 \\
Ananda          &   1.0 &   0.416667 \\
Ananya          &   1.0 &   0.416667 \\
Anapaola        &   1.0 &   1.000000 \\
Anapatricia     &   1.0 &   1.000000 \\
Anapaula        &   1.0 &   0.666667 \\
Anara           &   1.0 &   1.000000 \\
Anareli         &   1.0 &   1.000000 \\
Anari           &   1.0 &   1.000000 \\
Anarosa         &   1.0 &   0.538462 \\
Anasofia        &   1.0 &   0.315789 \\
Anasophia       &   1.0 &   0.833333 \\
Anastacia       &   1.0 &   0.363636 \\
Anastasia       &   1.0 &   0.932692 \\
Anastasiya      &   1.0 &   0.833333 \\
Anastassia      &   1.0 &   1.000000 \\
Anastazia       &   1.0 &   1.000000 \\
Anaveah         &   1.0 &   1.000000 \\
Anavi           &   1.0 &   1.000000 \\
Anavictoria     &   1.0 &   0.888889 \\
Anay            &   1.0 &   0.733333 \\
Anaya           &   1.0 &   0.835821 \\
Anayah          &   1.0 &   0.444444 \\
Anayat          &   1.0 &   1.000000 \\
Anayeli         &   1.0 &   0.349206 \\
Anayely         &   1.0 &   1.000000 \\
Anayra          &   1.0 &   0.428571 \\
Andee           &   1.0 &   0.545455 \\
Andera          &   1.0 &   0.714286 \\
Andi            &   1.0 &   0.866667 \\
Andie           &   1.0 &   0.642857 \\
Andra           &   1.0 &   0.421053 \\
Andraya         &   1.0 &   0.714286 \\
Andre           &   1.0 &   0.384615 \\
Andrea          &   1.0 &   0.203411 \\
Andreah         &   1.0 &   1.000000 \\
Andreana        &   1.0 &   0.636364 \\
Andreanna       &   1.0 &   0.714286 \\
Andree          &   1.0 &   0.500000 \\
Andreina        &   1.0 &   0.058140 \\
Andrena         &   1.0 &   1.000000 \\
Andres          &   1.0 &   0.333333 \\
Andrew          &   1.0 &   0.200000 \\
Andreya         &   1.0 &   0.700000 \\
Andria          &   1.0 &   0.170732 \\
Andriana        &   1.0 &   0.352941 \\
Andrianna       &   1.0 &   0.555556 \\
Andrina         &   1.0 &   0.555556 \\
Andromeda       &   1.0 &   1.000000 \\
Andy            &   1.0 &   0.555556 \\
Anecia          &   1.0 &   1.000000 \\
Aneesa          &   1.0 &   0.200000 \\
Aneesah         &   1.0 &   1.000000 \\
Aneesha         &   1.0 &   1.000000 \\
Anel            &   1.0 &   0.157895 \\
Anela           &   1.0 &   0.666667 \\
Aneli           &   1.0 &   0.833333 \\
Anelise         &   1.0 &   0.833333 \\
Aneliz          &   1.0 &   0.428571 \\
Anella          &   1.0 &   0.857143 \\
Anelly          &   1.0 &   1.000000 \\
Anely           &   1.0 &   0.750000 \\
Anesia          &   1.0 &   1.000000 \\
Anessa          &   1.0 &   0.178571 \\
Aneth           &   1.0 &   0.555556 \\
Anetra          &   1.0 &   0.833333 \\
Anett           &   1.0 &   0.833333 \\
Anette          &   1.0 &   0.142857 \\
Anevay          &   1.0 &   0.636364 \\
Aneya           &   1.0 &   1.000000 \\
Angel           &   1.0 &   0.147692 \\
Angela          &   1.0 &   0.176509 \\
Angele          &   1.0 &   0.833333 \\
Angelea         &   1.0 &   1.000000 \\
Angeleah        &   1.0 &   0.833333 \\
Angelee         &   1.0 &   0.555556 \\
Angeleen        &   1.0 &   0.625000 \\
Angeleena       &   1.0 &   1.000000 \\
Angelena        &   1.0 &   0.285714 \\
Angelene        &   1.0 &   0.454545 \\
Angeles         &   1.0 &   0.085714 \\
Angeli          &   1.0 &   0.333333 \\
Angelia         &   1.0 &   0.122807 \\
Angeliah        &   1.0 &   1.000000 \\
Angelic         &   1.0 &   0.407407 \\
Angelica        &   1.0 &   0.067698 \\
Angelicamaria   &   1.0 &   1.000000 \\
Angelicamarie   &   1.0 &   1.000000 \\
Angelicia       &   1.0 &   1.000000 \\
Angelie         &   1.0 &   0.250000 \\
Angelik         &   1.0 &   1.000000 \\
Angelika        &   1.0 &   0.444444 \\
Angelin         &   1.0 &   0.625000 \\
Angelina        &   1.0 &   0.148398 \\
Angelinah       &   1.0 &   1.000000 \\
Angeline        &   1.0 &   0.310924 \\
Angelinne       &   1.0 &   1.000000 \\
Angelique       &   1.0 &   0.432432 \\
Angelisa        &   1.0 &   0.857143 \\
Angelise        &   1.0 &   0.625000 \\
Angelita        &   1.0 &   0.142857 \\
Angella         &   1.0 &   0.454545 \\
Angelle         &   1.0 &   0.875000 \\
Angelli         &   1.0 &   0.545455 \\
Angellica       &   1.0 &   1.000000 \\
Angellina       &   1.0 &   1.000000 \\
Angelly         &   1.0 &   0.384615 \\
Angelmarie      &   1.0 &   0.833333 \\
Angelo          &   1.0 &   1.000000 \\
Angely          &   1.0 &   0.416667 \\
Angelyn         &   1.0 &   0.388889 \\
Angelyna        &   1.0 &   0.466667 \\
Angelyne        &   1.0 &   1.000000 \\
Angelynn        &   1.0 &   0.625000 \\
Angeni          &   1.0 &   1.000000 \\
Angi            &   1.0 &   1.000000 \\
Angie           &   1.0 &   0.237500 \\
Angle           &   1.0 &   0.833333 \\
Anglea          &   1.0 &   1.000000 \\
Angy            &   1.0 &   0.714286 \\
Anh             &   1.0 &   0.200000 \\
Anhelica        &   1.0 &   1.000000 \\
Anhthu          &   1.0 &   1.000000 \\
Ani             &   1.0 &   0.478261 \\
Ania            &   1.0 &   0.550000 \\
Aniah           &   1.0 &   0.407407 \\
Anibal          &   1.0 &   1.000000 \\
Anica           &   1.0 &   0.538462 \\
Anicia          &   1.0 &   0.666667 \\
Aniela          &   1.0 &   0.454545 \\
Anifer          &   1.0 &   1.000000 \\
Anijah          &   1.0 &   1.000000 \\
Anika           &   1.0 &   0.564516 \\
Aniko           &   1.0 &   1.000000 \\
Anilah          &   1.0 &   1.000000 \\
Anina           &   1.0 &   1.000000 \\
Anique          &   1.0 &   1.000000 \\
Anisa           &   1.0 &   0.243902 \\
Anisah          &   1.0 &   0.454545 \\
Anise           &   1.0 &   0.857143 \\
Anisha          &   1.0 &   0.178571 \\
Anishka         &   1.0 &   0.555556 \\
Anissa          &   1.0 &   0.086667 \\
Aniston         &   1.0 &   0.750000 \\
Anistyn         &   1.0 &   1.000000 \\
Anita           &   1.0 &   0.037037 \\
Anitra          &   1.0 &   0.135135 \\
Aniya           &   1.0 &   0.225000 \\
Aniyah          &   1.0 &   0.586466 \\
Aniyha          &   1.0 &   1.000000 \\
Aniyla          &   1.0 &   1.000000 \\
Aniylah         &   1.0 &   1.000000 \\
Anja            &   1.0 &   0.312500 \\
Anjalee         &   1.0 &   1.000000 \\
Anjali          &   1.0 &   0.340000 \\
Anjana          &   1.0 &   0.833333 \\
Anjanae         &   1.0 &   1.000000 \\
Anjanette       &   1.0 &   0.096154 \\
Anjani          &   1.0 &   1.000000 \\
Anjannette      &   1.0 &   1.000000 \\
Anjeanette      &   1.0 &   1.000000 \\
Anjela          &   1.0 &   0.625000 \\
Anjelica        &   1.0 &   0.098039 \\
Anjelika        &   1.0 &   1.000000 \\
Anjelina        &   1.0 &   0.727273 \\
Anjenette       &   1.0 &   1.000000 \\
Anjolie         &   1.0 &   0.357143 \\
Anjolina        &   1.0 &   1.000000 \\
Anjuli          &   1.0 &   0.454545 \\
Ankita          &   1.0 &   0.583333 \\
Ankitha         &   1.0 &   1.000000 \\
Anmarie         &   1.0 &   1.000000 \\
Anmol           &   1.0 &   0.500000 \\
Ann             &   1.0 &   0.022989 \\
Anna            &   1.0 &   0.320580 \\
Annabel         &   1.0 &   0.153061 \\
Annabell        &   1.0 &   0.406250 \\
Annabella       &   1.0 &   0.132184 \\
Annabelle       &   1.0 &   0.166333 \\
Annabeth        &   1.0 &   0.437500 \\
Annah           &   1.0 &   0.833333 \\
Annahi          &   1.0 &   0.857143 \\
Annai           &   1.0 &   0.833333 \\
Annais          &   1.0 &   0.625000 \\
Annaka          &   1.0 &   0.714286 \\
Annalea         &   1.0 &   0.625000 \\
Annaleah        &   1.0 &   0.545455 \\
Annalee         &   1.0 &   0.709677 \\
Annaleia        &   1.0 &   1.000000 \\
Annaleigh       &   1.0 &   0.500000 \\
Annali          &   1.0 &   0.625000 \\
Annalia         &   1.0 &   0.916667 \\
Annaliah        &   1.0 &   1.000000 \\
Annalicia       &   1.0 &   0.800000 \\
Annalie         &   1.0 &   0.296296 \\
Annaliese       &   1.0 &   0.322581 \\
Annalina        &   1.0 &   1.000000 \\
Annalisa        &   1.0 &   0.375000 \\
Annalise        &   1.0 &   0.829545 \\
Annaliyah       &   1.0 &   0.777778 \\
Annaliz         &   1.0 &   1.000000 \\
Annaliza        &   1.0 &   1.000000 \\
Annaly          &   1.0 &   0.277778 \\
Annalyn         &   1.0 &   0.875000 \\
Annalynn        &   1.0 &   0.625000 \\
Annalyse        &   1.0 &   0.400000 \\
Annamae         &   1.0 &   1.000000 \\
Annamaria       &   1.0 &   0.444444 \\
Annamarie       &   1.0 &   0.146341 \\
Annarose        &   1.0 &   1.000000 \\
Annasofia       &   1.0 &   1.000000 \\
Annasophia      &   1.0 &   0.454545 \\
Annastacia      &   1.0 &   1.000000 \\
Annastasia      &   1.0 &   0.545455 \\
Annay           &   1.0 &   1.000000 \\
Annaya          &   1.0 &   1.000000 \\
Anndrea         &   1.0 &   1.000000 \\
Anne            &   1.0 &   0.071130 \\
Anneka          &   1.0 &   0.857143 \\
Anneke          &   1.0 &   0.454545 \\
Annel           &   1.0 &   0.357143 \\
Anneli          &   1.0 &   1.000000 \\
Annelie         &   1.0 &   1.000000 \\
Anneliese       &   1.0 &   0.204545 \\
Annelise        &   1.0 &   0.379310 \\
Annelyse        &   1.0 &   1.000000 \\
Annemarie       &   1.0 &   0.162162 \\
Annessa         &   1.0 &   0.857143 \\
Annet           &   1.0 &   1.000000 \\
Annete          &   1.0 &   0.833333 \\
Anneth          &   1.0 &   0.714286 \\
Annett          &   1.0 &   0.625000 \\
Annetta         &   1.0 &   0.357143 \\
Annette         &   1.0 &   0.033724 \\
Anni            &   1.0 &   0.833333 \\
Annia           &   1.0 &   1.000000 \\
Anniah          &   1.0 &   1.000000 \\
Annica          &   1.0 &   1.000000 \\
Annie           &   1.0 &   0.581006 \\
Annika          &   1.0 &   0.208556 \\
Annisa          &   1.0 &   0.833333 \\
Annissa         &   1.0 &   0.466667 \\
Anniston        &   1.0 &   0.692308 \\
Annita          &   1.0 &   1.000000 \\
Anniyah         &   1.0 &   0.625000 \\
Annmarie        &   1.0 &   0.079365 \\
Annora          &   1.0 &   1.000000 \\
Anny            &   1.0 &   0.538462 \\
Annya           &   1.0 &   1.000000 \\
Anora           &   1.0 &   0.538462 \\
Anouk           &   1.0 &   0.625000 \\
Anoushka        &   1.0 &   0.400000 \\
Anshika         &   1.0 &   0.714286 \\
Ansley          &   1.0 &   0.416667 \\
Antara          &   1.0 &   1.000000 \\
Anthea          &   1.0 &   1.000000 \\
Anthony         &   1.0 &   0.166667 \\
Antionette      &   1.0 &   0.240000 \\
Antoinette      &   1.0 &   0.042553 \\
Antonela        &   1.0 &   1.000000 \\
Antonella       &   1.0 &   1.000000 \\
Antonette       &   1.0 &   0.291667 \\
Antonia         &   1.0 &   0.250000 \\
Antonieta       &   1.0 &   0.714286 \\
Antonietta      &   1.0 &   1.000000 \\
Antoniette      &   1.0 &   0.833333 \\
Antonina        &   1.0 &   0.625000 \\
Antonio         &   1.0 &   0.277778 \\
Antonisha       &   1.0 &   0.857143 \\
Antwanette      &   1.0 &   1.000000 \\
Anuhea          &   1.0 &   0.312500 \\
Anum            &   1.0 &   0.500000 \\
Anureet         &   1.0 &   1.000000 \\
Anusha          &   1.0 &   0.263158 \\
Anushka         &   1.0 &   0.166667 \\
Anushree        &   1.0 &   1.000000 \\
Anvi            &   1.0 &   0.500000 \\
Anvika          &   1.0 &   0.526316 \\
Anvita          &   1.0 &   0.461538 \\
Anvitha         &   1.0 &   1.000000 \\
Anwita          &   1.0 &   1.000000 \\
Any             &   1.0 &   0.833333 \\
Anya            &   1.0 &   0.972222 \\
Anyah           &   1.0 &   0.500000 \\
Anyeli          &   1.0 &   0.833333 \\
Anyelin         &   1.0 &   1.000000 \\
Anyia           &   1.0 &   0.857143 \\
Anyiah          &   1.0 &   1.000000 \\
Anyla           &   1.0 &   0.857143 \\
Anylah          &   1.0 &   1.000000 \\
Anysa           &   1.0 &   0.714286 \\
Anysia          &   1.0 &   1.000000 \\
Anyssa          &   1.0 &   0.125000 \\
Aoi             &   1.0 &   1.000000 \\
Aoife           &   1.0 &   0.875000 \\
Aolani          &   1.0 &   0.333333 \\
Aolanis         &   1.0 &   0.687500 \\
Aparna          &   1.0 &   0.857143 \\
Apollonia       &   1.0 &   0.714286 \\
Apolonia        &   1.0 &   0.416667 \\
Apoorva         &   1.0 &   1.000000 \\
Apple           &   1.0 &   0.500000 \\
April           &   1.0 &   0.118881 \\
Aprille         &   1.0 &   1.000000 \\
Apryl           &   1.0 &   0.428571 \\
Aqsa            &   1.0 &   0.666667 \\
Aqueelah        &   1.0 &   1.000000 \\
Aquilla         &   1.0 &   1.000000 \\
Ara             &   1.0 &   0.714286 \\
Arabella        &   1.0 &   0.680162 \\
Arabelle        &   1.0 &   0.809524 \\
Araceli         &   1.0 &   0.224806 \\
Aracelia        &   1.0 &   0.833333 \\
Aracely         &   1.0 &   0.278261 \\
Aradhya         &   1.0 &   0.857143 \\
Araiya          &   1.0 &   0.625000 \\
Araiyah         &   1.0 &   1.000000 \\
Aralyn          &   1.0 &   0.500000 \\
Aralynn         &   1.0 &   0.777778 \\
Arami           &   1.0 &   1.000000 \\
Aramis          &   1.0 &   1.000000 \\
Arantxa         &   1.0 &   0.500000 \\
Arantza         &   1.0 &   0.270833 \\
Aranza          &   1.0 &   0.347826 \\
Araseli         &   1.0 &   0.200000 \\
Arasely         &   1.0 &   1.000000 \\
Araya           &   1.0 &   1.000000 \\
Arayah          &   1.0 &   0.500000 \\
Arcelia         &   1.0 &   0.178571 \\
Archer          &   1.0 &   1.000000 \\
Archita         &   1.0 &   1.000000 \\
Ardath          &   1.0 &   1.000000 \\
Ardell          &   1.0 &   1.000000 \\
Ardelle         &   1.0 &   1.000000 \\
Arden           &   1.0 &   0.705882 \\
Ardeth          &   1.0 &   0.500000 \\
Ardis           &   1.0 &   0.545455 \\
Ardith          &   1.0 &   0.333333 \\
Ardyce          &   1.0 &   1.000000 \\
Ardys           &   1.0 &   1.000000 \\
Areana          &   1.0 &   1.000000 \\
Areanna         &   1.0 &   0.625000 \\
Areej           &   1.0 &   1.000000 \\
Areen           &   1.0 &   1.000000 \\
Areesha         &   1.0 &   1.000000 \\
Areil           &   1.0 &   1.000000 \\
Areisy          &   1.0 &   1.000000 \\
Areli           &   1.0 &   0.284404 \\
Arelie          &   1.0 &   0.857143 \\
Arelis          &   1.0 &   0.714286 \\
Arella          &   1.0 &   0.625000 \\
Arelly          &   1.0 &   0.750000 \\
Arely           &   1.0 &   0.369697 \\
Arelys          &   1.0 &   1.000000 \\
Areni           &   1.0 &   1.000000 \\
Ares            &   1.0 &   0.833333 \\
Aretha          &   1.0 &   0.217391 \\
Areya           &   1.0 &   1.000000 \\
Arfa            &   1.0 &   1.000000 \\
Argelia         &   1.0 &   0.454545 \\
Arha            &   1.0 &   0.833333 \\
Ari             &   1.0 &   1.000000 \\
Aria            &   1.0 &   0.923148 \\
Ariabella       &   1.0 &   1.000000 \\
Ariadna         &   1.0 &   0.714286 \\
Ariadne         &   1.0 &   0.520000 \\
Ariah           &   1.0 &   0.520661 \\
Ariahna         &   1.0 &   1.000000 \\
Arial           &   1.0 &   0.555556 \\
Ariam           &   1.0 &   1.000000 \\
Arian           &   1.0 &   0.875000 \\
Ariana          &   1.0 &   0.572376 \\
Arianah         &   1.0 &   0.416667 \\
Ariane          &   1.0 &   0.240000 \\
Arianelly       &   1.0 &   1.000000 \\
Ariani          &   1.0 &   0.700000 \\
Arianie         &   1.0 &   1.000000 \\
Arianna         &   1.0 &   0.376552 \\
Ariannah        &   1.0 &   0.384615 \\
Arianne         &   1.0 &   0.260870 \\
Arianni         &   1.0 &   0.500000 \\
Ariannie        &   1.0 &   1.000000 \\
Arianny         &   1.0 &   0.355556 \\
Ariany          &   1.0 &   0.461538 \\
Ariarose        &   1.0 &   1.000000 \\
Ariatna         &   1.0 &   0.454545 \\
Ariauna         &   1.0 &   1.000000 \\
Ariba           &   1.0 &   1.000000 \\
Aribella        &   1.0 &   0.875000 \\
Aribelle        &   1.0 &   1.000000 \\
Arica           &   1.0 &   0.416667 \\
Ariday          &   1.0 &   1.000000 \\
Arie            &   1.0 &   0.545455 \\
Arieana         &   1.0 &   1.000000 \\
Arieanna        &   1.0 &   0.461538 \\
Ariel           &   1.0 &   0.325737 \\
Ariela          &   1.0 &   0.853659 \\
Ariele          &   1.0 &   0.714286 \\
Ariell          &   1.0 &   0.416667 \\
Ariella         &   1.0 &   1.000000 \\
Arielle         &   1.0 &   0.569444 \\
Ariely          &   1.0 &   1.000000 \\
Aries           &   1.0 &   0.937500 \\
Arihana         &   1.0 &   1.000000 \\
Arihanna        &   1.0 &   0.625000 \\
Arika           &   1.0 &   0.400000 \\
Arilene         &   1.0 &   1.000000 \\
Arilynn         &   1.0 &   1.000000 \\
Arin            &   1.0 &   0.357143 \\
Arina           &   1.0 &   0.428571 \\
Arion           &   1.0 &   1.000000 \\
Ariona          &   1.0 &   1.000000 \\
Arionna         &   1.0 &   0.416667 \\
Aris            &   1.0 &   0.470588 \\
Arisa           &   1.0 &   0.900000 \\
Arisbeth        &   1.0 &   1.000000 \\
Arisha          &   1.0 &   0.857143 \\
Arissa          &   1.0 &   0.181818 \\
Arista          &   1.0 &   1.000000 \\
Ariya           &   1.0 &   0.972222 \\
Ariyah          &   1.0 &   0.819048 \\
Ariyan          &   1.0 &   1.000000 \\
Ariyana         &   1.0 &   0.208333 \\
Ariyanna        &   1.0 &   0.454545 \\
Arizbeth        &   1.0 &   1.000000 \\
Arizona         &   1.0 &   1.000000 \\
Arla            &   1.0 &   0.875000 \\
Arleen          &   1.0 &   0.363636 \\
Arleene         &   1.0 &   1.000000 \\
Arlen           &   1.0 &   0.500000 \\
Arlena          &   1.0 &   0.500000 \\
Arlene          &   1.0 &   0.088235 \\
Arlenne         &   1.0 &   0.833333 \\
Arlet           &   1.0 &   1.000000 \\
Arleta          &   1.0 &   1.000000 \\
Arleth          &   1.0 &   1.000000 \\
Arlett          &   1.0 &   1.000000 \\
Arletta         &   1.0 &   1.000000 \\
Arlette         &   1.0 &   1.000000 \\
Arley           &   1.0 &   1.000000 \\
Arlin           &   1.0 &   0.333333 \\
Arlinda         &   1.0 &   0.625000 \\
Arline          &   1.0 &   0.156863 \\
Arliz           &   1.0 &   1.000000 \\
Arlo            &   1.0 &   0.529412 \\
Arly            &   1.0 &   0.084746 \\
Arlyn           &   1.0 &   0.333333 \\
Arlyne          &   1.0 &   0.714286 \\
Arlynn          &   1.0 &   1.000000 \\
Armanda         &   1.0 &   0.833333 \\
Armandina       &   1.0 &   1.000000 \\
Armando         &   1.0 &   0.636364 \\
Armani          &   1.0 &   1.000000 \\
Armida          &   1.0 &   0.142857 \\
Arminda         &   1.0 &   1.000000 \\
Armine          &   1.0 &   0.500000 \\
Armoni          &   1.0 &   0.750000 \\
Arna            &   1.0 &   1.000000 \\
Arnelle         &   1.0 &   1.000000 \\
Arnetia         &   1.0 &   1.000000 \\
Arnetta         &   1.0 &   0.857143 \\
Arnika          &   1.0 &   1.000000 \\
Arnita          &   1.0 &   0.714286 \\
Arohi           &   1.0 &   0.714286 \\
Arora           &   1.0 &   1.000000 \\
Arpi            &   1.0 &   0.818182 \\
Arpine          &   1.0 &   1.000000 \\
Arpita          &   1.0 &   1.000000 \\
Arriana         &   1.0 &   0.240000 \\
Arrianna        &   1.0 &   0.416667 \\
Arrielle        &   1.0 &   1.000000 \\
Arrow           &   1.0 &   0.391304 \\
Arsema          &   1.0 &   1.000000 \\
Arshdeep        &   1.0 &   1.000000 \\
Arshia          &   1.0 &   0.750000 \\
Artemis         &   1.0 &   0.750000 \\
Artemisa        &   1.0 &   1.000000 \\
Arthur          &   1.0 &   1.000000 \\
Arti            &   1.0 &   1.000000 \\
Artie           &   1.0 &   1.000000 \\
Artisha         &   1.0 &   1.000000 \\
Arturo          &   1.0 &   0.666667 \\
Aruna           &   1.0 &   0.391304 \\
Arushi          &   1.0 &   0.333333 \\
Arwa            &   1.0 &   1.000000 \\
Arwen           &   1.0 &   0.869565 \\
Arwyn           &   1.0 &   0.714286 \\
Ary             &   1.0 &   1.000000 \\
Arya            &   1.0 &   0.793023 \\
Aryah           &   1.0 &   0.894737 \\
Aryam           &   1.0 &   1.000000 \\
Aryan           &   1.0 &   0.714286 \\
Aryana          &   1.0 &   0.196721 \\
Aryanah         &   1.0 &   1.000000 \\
Aryanna         &   1.0 &   0.237500 \\
Aryannah        &   1.0 &   0.833333 \\
Aryel           &   1.0 &   0.714286 \\
Aryella         &   1.0 &   1.000000 \\
Aryia           &   1.0 &   1.000000 \\
Aryiah          &   1.0 &   1.000000 \\
Aryn            &   1.0 &   0.357143 \\
Arynn           &   1.0 &   1.000000 \\
Aryona          &   1.0 &   1.000000 \\
Aryonna         &   1.0 &   1.000000 \\
Aryssa          &   1.0 &   0.555556 \\
Arzoey          &   1.0 &   1.000000 \\
Arzoi           &   1.0 &   1.000000 \\
Arzoyi          &   1.0 &   0.750000 \\
Arzu            &   1.0 &   1.000000 \\
Asa             &   1.0 &   0.818182 \\
Asako           &   1.0 &   0.714286 \\
Asal            &   1.0 &   1.000000 \\
Asante          &   1.0 &   1.000000 \\
Ascencion       &   1.0 &   1.000000 \\
Ascension       &   1.0 &   1.000000 \\
Aseel           &   1.0 &   0.636364 \\
Asees           &   1.0 &   0.714286 \\
Asenat          &   1.0 &   1.000000 \\
Asenath         &   1.0 &   1.000000 \\
Asenet          &   1.0 &   1.000000 \\
Aseneth         &   1.0 &   1.000000 \\
Asha            &   1.0 &   0.578947 \\
Ashante         &   1.0 &   1.000000 \\
Ashanti         &   1.0 &   0.062500 \\
Ashanty         &   1.0 &   0.416667 \\
Ashby           &   1.0 &   0.833333 \\
Ashe            &   1.0 &   1.000000 \\
Asheley         &   1.0 &   1.000000 \\
Ashely          &   1.0 &   0.090909 \\
Asher           &   1.0 &   1.000000 \\
Ashia           &   1.0 &   0.625000 \\
Ashika          &   1.0 &   1.000000 \\
Ashira          &   1.0 &   1.000000 \\
Ashlan          &   1.0 &   1.000000 \\
Ashland         &   1.0 &   1.000000 \\
Ashle           &   1.0 &   0.750000 \\
Ashlea          &   1.0 &   0.357143 \\
Ashlee          &   1.0 &   0.020690 \\
Ashleen         &   1.0 &   0.333333 \\
Ashlei          &   1.0 &   0.388889 \\
Ashleigh        &   1.0 &   0.030120 \\
Ashlen          &   1.0 &   0.833333 \\
Ashley          &   1.0 &   0.045190 \\
Ashleyann       &   1.0 &   0.714286 \\
Ashleymarie     &   1.0 &   0.636364 \\
Ashleynicole    &   1.0 &   0.555556 \\
Ashli           &   1.0 &   0.151515 \\
Ashlie          &   1.0 &   0.072464 \\
Ashliegh        &   1.0 &   0.500000 \\
Ashlin          &   1.0 &   0.312500 \\
Ashly           &   1.0 &   0.126316 \\
Ashlye          &   1.0 &   1.000000 \\
Ashlyn          &   1.0 &   0.173709 \\
Ashlynn         &   1.0 &   0.180952 \\
Ashlynne        &   1.0 &   0.500000 \\
Ashmita         &   1.0 &   1.000000 \\
Ashna           &   1.0 &   0.416667 \\
Ashten          &   1.0 &   0.857143 \\
Ashtin          &   1.0 &   1.000000 \\
Ashton          &   1.0 &   0.133333 \\
Ashtyn          &   1.0 &   0.631579 \\
Ashvi           &   1.0 &   1.000000 \\
Ashvika         &   1.0 &   1.000000 \\
Ashwika         &   1.0 &   1.000000 \\
Asia            &   1.0 &   0.045455 \\
Asiah           &   1.0 &   0.800000 \\
Asiana          &   1.0 &   1.000000 \\
Asiya           &   1.0 &   0.812500 \\
Asiyah          &   1.0 &   0.600000 \\
Asley           &   1.0 &   0.875000 \\
Aslyn           &   1.0 &   1.000000 \\
Asma            &   1.0 &   0.461538 \\
Asmaa           &   1.0 &   0.833333 \\
Asmara          &   1.0 &   1.000000 \\
Asmi            &   1.0 &   1.000000 \\
Aspen           &   1.0 &   0.943182 \\
Aspyn           &   1.0 &   1.000000 \\
Asra            &   1.0 &   1.000000 \\
Assata          &   1.0 &   1.000000 \\
Astella         &   1.0 &   1.000000 \\
Aster           &   1.0 &   1.000000 \\
Asteria         &   1.0 &   0.857143 \\
Astha           &   1.0 &   1.000000 \\
Astoria         &   1.0 &   1.000000 \\
Astra           &   1.0 &   1.000000 \\
Astraea         &   1.0 &   1.000000 \\
Astrid          &   1.0 &   1.000000 \\
Asucena         &   1.0 &   0.454545 \\
Asuka           &   1.0 &   1.000000 \\
Asuna           &   1.0 &   0.833333 \\
Asusena         &   1.0 &   0.192308 \\
Asuzena         &   1.0 &   1.000000 \\
Asya            &   1.0 &   0.625000 \\
Atalia          &   1.0 &   0.466667 \\
Atalie          &   1.0 &   1.000000 \\
Atara           &   1.0 &   1.000000 \\
Atarah          &   1.0 &   1.000000 \\
Atenas          &   1.0 &   0.833333 \\
Atenea          &   1.0 &   1.000000 \\
Athalia         &   1.0 &   1.000000 \\
Athaliah        &   1.0 &   1.000000 \\
Atheena         &   1.0 &   1.000000 \\
Athena          &   1.0 &   1.000000 \\
Athina          &   1.0 &   0.500000 \\
Athira          &   1.0 &   0.714286 \\
Athyna          &   1.0 &   1.000000 \\
Athziri         &   1.0 &   0.545455 \\
Athziry         &   1.0 &   0.777778 \\
Atiana          &   1.0 &   0.217391 \\
Atianna         &   1.0 &   0.500000 \\
Atiya           &   1.0 &   1.000000 \\
Atlas           &   1.0 &   0.774194 \\
Atzi            &   1.0 &   0.894737 \\
Atziri          &   1.0 &   0.538462 \\
Atziry          &   1.0 &   0.500000 \\
Aubreanna       &   1.0 &   1.000000 \\
Aubree          &   1.0 &   0.287565 \\
Aubreigh        &   1.0 &   0.714286 \\
Aubrey          &   1.0 &   0.384709 \\
Aubreyana       &   1.0 &   0.833333 \\
Aubri           &   1.0 &   0.333333 \\
Aubriana        &   1.0 &   0.241379 \\
Aubrianna       &   1.0 &   0.235294 \\
Aubrie          &   1.0 &   0.220930 \\
Aubriee         &   1.0 &   1.000000 \\
Aubriella       &   1.0 &   0.833333 \\
Aubrielle       &   1.0 &   0.775000 \\
Aubry           &   1.0 &   0.230769 \\
Aubryana        &   1.0 &   1.000000 \\
Aubryanna       &   1.0 &   1.000000 \\
Auburn          &   1.0 &   1.000000 \\
Audelia         &   1.0 &   1.000000 \\
Auden           &   1.0 &   0.750000 \\
Audra           &   1.0 &   0.120000 \\
Audre           &   1.0 &   1.000000 \\
Audrea          &   1.0 &   1.000000 \\
Audreanna       &   1.0 &   0.833333 \\
Audree          &   1.0 &   0.321429 \\
Audreena        &   1.0 &   1.000000 \\
Audrena         &   1.0 &   1.000000 \\
Audrey          &   1.0 &   0.481444 \\
Audreyana       &   1.0 &   1.000000 \\
Audri           &   1.0 &   1.000000 \\
Audria          &   1.0 &   1.000000 \\
Audriana        &   1.0 &   0.326531 \\
Audrianna       &   1.0 &   0.266667 \\
Audrie          &   1.0 &   0.240000 \\
Audrielle       &   1.0 &   1.000000 \\
Audrina         &   1.0 &   0.112281 \\
Audrinna        &   1.0 &   0.500000 \\
Audris          &   1.0 &   1.000000 \\
Audry           &   1.0 &   0.357143 \\
Audryna         &   1.0 &   0.777778 \\
August          &   1.0 &   0.903226 \\
Augusta         &   1.0 &   0.454545 \\
Augustina       &   1.0 &   0.454545 \\
Aujanae         &   1.0 &   1.000000 \\
Aulani          &   1.0 &   0.428571 \\
Aundrea         &   1.0 &   0.166667 \\
Aundria         &   1.0 &   0.714286 \\
Aunika          &   1.0 &   0.833333 \\
Aunna           &   1.0 &   1.000000 \\
Aura            &   1.0 &   0.829268 \\
Aurea           &   1.0 &   0.600000 \\
Aurelia         &   1.0 &   0.988636 \\
Aurelie         &   1.0 &   0.833333 \\
Auri            &   1.0 &   1.000000 \\
Auria           &   1.0 &   1.000000 \\
Auriana         &   1.0 &   1.000000 \\
Aurianna        &   1.0 &   0.625000 \\
Auriel          &   1.0 &   1.000000 \\
Auriella        &   1.0 &   1.000000 \\
Aurielle        &   1.0 &   0.600000 \\
Aurora          &   1.0 &   1.000000 \\
Aurorah         &   1.0 &   1.000000 \\
Austen          &   1.0 &   0.500000 \\
Austin          &   1.0 &   0.656250 \\
Austyn          &   1.0 &   0.791667 \\
Austynn         &   1.0 &   1.000000 \\
Autum           &   1.0 &   0.454545 \\
Autumn          &   1.0 &   0.707463 \\
Ava             &   1.0 &   0.631416 \\
Avagrace        &   1.0 &   0.857143 \\
Avah            &   1.0 &   0.574468 \\
Avaiyah         &   1.0 &   1.000000 \\
Avalee          &   1.0 &   0.700000 \\
Avaleen         &   1.0 &   1.000000 \\
Avaleigh        &   1.0 &   1.000000 \\
Avalene         &   1.0 &   1.000000 \\
Avalina         &   1.0 &   1.000000 \\
Avaline         &   1.0 &   0.500000 \\
Avalon          &   1.0 &   0.550000 \\
Avalyn          &   1.0 &   0.424242 \\
Avalynn         &   1.0 &   0.518519 \\
Avamarie        &   1.0 &   0.333333 \\
Avana           &   1.0 &   1.000000 \\
Avangeline      &   1.0 &   0.833333 \\
Avani           &   1.0 &   0.921053 \\
Avanna          &   1.0 &   1.000000 \\
Avanni          &   1.0 &   0.833333 \\
Avantika        &   1.0 &   0.750000 \\
Avaree          &   1.0 &   1.000000 \\
Avari           &   1.0 &   1.000000 \\
Avarie          &   1.0 &   0.357143 \\
Avarose         &   1.0 &   0.818182 \\
Avary           &   1.0 &   0.368421 \\
Avaya           &   1.0 &   0.739130 \\
Avayah          &   1.0 &   0.702128 \\
Avea            &   1.0 &   1.000000 \\
Aveah           &   1.0 &   0.666667 \\
Avelina         &   1.0 &   0.866667 \\
Aveline         &   1.0 &   0.478261 \\
Avelyn          &   1.0 &   0.333333 \\
Avelynn         &   1.0 &   1.000000 \\
Aven            &   1.0 &   0.400000 \\
Averee          &   1.0 &   0.555556 \\
Averey          &   1.0 &   1.000000 \\
Averi           &   1.0 &   0.580645 \\
Averie          &   1.0 &   0.656566 \\
Averly          &   1.0 &   0.500000 \\
Avery           &   1.0 &   0.683938 \\
Averyrose       &   1.0 &   1.000000 \\
Aveya           &   1.0 &   1.000000 \\
Aveyah          &   1.0 &   1.000000 \\
Avi             &   1.0 &   0.666667 \\
Avia            &   1.0 &   0.437500 \\
Aviah           &   1.0 &   1.000000 \\
Aviana          &   1.0 &   0.431034 \\
Avianah         &   1.0 &   0.833333 \\
Avianna         &   1.0 &   0.776000 \\
Aviannah        &   1.0 &   1.000000 \\
Avie            &   1.0 &   1.000000 \\
Aviella         &   1.0 &   1.000000 \\
Avielle         &   1.0 &   1.000000 \\
Avigail         &   1.0 &   0.533333 \\
Avigayil        &   1.0 &   1.000000 \\
Avika           &   1.0 &   0.625000 \\
Avila           &   1.0 &   0.769231 \\
Avilene         &   1.0 &   1.000000 \\
Avina           &   1.0 &   0.666667 \\
Avira           &   1.0 &   1.000000 \\
Avis            &   1.0 &   0.312500 \\
Avital          &   1.0 &   1.000000 \\
Aviv            &   1.0 &   1.000000 \\
Aviva           &   1.0 &   0.500000 \\
Aviyah          &   1.0 &   1.000000 \\
Avleen          &   1.0 &   0.684211 \\
Avneet          &   1.0 &   0.705882 \\
Avni            &   1.0 &   0.913043 \\
Avnoor          &   1.0 &   0.727273 \\
Avonlea         &   1.0 &   0.416667 \\
Avory           &   1.0 &   1.000000 \\
Avree           &   1.0 &   1.000000 \\
Avrie           &   1.0 &   0.857143 \\
Avril           &   1.0 &   0.275862 \\
Avry            &   1.0 &   0.857143 \\
Avy             &   1.0 &   0.714286 \\
Avya            &   1.0 &   1.000000 \\
Avyana          &   1.0 &   1.000000 \\
Avyanna         &   1.0 &   1.000000 \\
Aya             &   1.0 &   0.783333 \\
Ayah            &   1.0 &   0.777778 \\
Ayaka           &   1.0 &   0.833333 \\
Ayako           &   1.0 &   0.357143 \\
Ayala           &   1.0 &   1.000000 \\
Ayame           &   1.0 &   0.833333 \\
Ayana           &   1.0 &   0.652174 \\
Ayanah          &   1.0 &   1.000000 \\
Ayane           &   1.0 &   1.000000 \\
Ayani           &   1.0 &   1.000000 \\
Ayanna          &   1.0 &   0.253012 \\
Ayannah         &   1.0 &   1.000000 \\
Ayano           &   1.0 &   1.000000 \\
Ayari           &   1.0 &   0.461538 \\
Ayat            &   1.0 &   1.000000 \\
Ayda            &   1.0 &   1.000000 \\
Aydan           &   1.0 &   1.000000 \\
Ayde            &   1.0 &   0.857143 \\
Aydee           &   1.0 &   1.000000 \\
Ayden           &   1.0 &   0.380952 \\
Ayelen          &   1.0 &   0.333333 \\
Ayelet          &   1.0 &   0.625000 \\
Ayerim          &   1.0 &   1.000000 \\
Ayesha          &   1.0 &   0.678571 \\
Ayiana          &   1.0 &   0.500000 \\
Ayisha          &   1.0 &   1.000000 \\
Ayla            &   1.0 &   0.945455 \\
Aylah           &   1.0 &   0.659091 \\
Aylani          &   1.0 &   1.000000 \\
Aylanie         &   1.0 &   1.000000 \\
Aylee           &   1.0 &   1.000000 \\
Ayleen          &   1.0 &   0.638655 \\
Aylen           &   1.0 &   1.000000 \\
Aylene          &   1.0 &   0.500000 \\
Aylet           &   1.0 &   1.000000 \\
Ayleth          &   1.0 &   1.000000 \\
Aylin           &   1.0 &   0.722222 \\
Ayline          &   1.0 &   0.500000 \\
Aylinn          &   1.0 &   0.625000 \\
Aylssa          &   1.0 &   1.000000 \\
Aymar           &   1.0 &   0.263158 \\
Ayme            &   1.0 &   1.000000 \\
Aymee           &   1.0 &   1.000000 \\
Ayomi           &   1.0 &   1.000000 \\
Ayra            &   1.0 &   1.000000 \\
Ayram           &   1.0 &   1.000000 \\
Aysel           &   1.0 &   1.000000 \\
Aysha           &   1.0 &   0.333333 \\
Aysia           &   1.0 &   0.437500 \\
Aytana          &   1.0 &   1.000000 \\
Ayumi           &   1.0 &   0.636364 \\
Ayushi          &   1.0 &   0.833333 \\
Ayva            &   1.0 &   0.236842 \\
Ayvah           &   1.0 &   0.454545 \\
Ayven           &   1.0 &   1.000000 \\
Ayzal           &   1.0 &   1.000000 \\
Ayzel           &   1.0 &   1.000000 \\
Azadeh          &   1.0 &   1.000000 \\
Azalea          &   1.0 &   0.949153 \\
Azaleah         &   1.0 &   0.454545 \\
Azalia          &   1.0 &   1.000000 \\
Azaliah         &   1.0 &   0.714286 \\
Azalie          &   1.0 &   1.000000 \\
Azara           &   1.0 &   1.000000 \\
Azari           &   1.0 &   1.000000 \\
Azaria          &   1.0 &   0.869565 \\
Azariah         &   1.0 &   1.000000 \\
Azariyah        &   1.0 &   1.000000 \\
Azayla          &   1.0 &   0.833333 \\
Azaylah         &   1.0 &   1.000000 \\
Azelea          &   1.0 &   1.000000 \\
Azelia          &   1.0 &   1.000000 \\
Azelie          &   1.0 &   1.000000 \\
Azeneth         &   1.0 &   0.487179 \\
Azhane          &   1.0 &   1.000000 \\
Azia            &   1.0 &   0.625000 \\
Azita           &   1.0 &   1.000000 \\
Aziyah          &   1.0 &   1.000000 \\
Aziza           &   1.0 &   0.416667 \\
Azlyn           &   1.0 &   1.000000 \\
Azlynn          &   1.0 &   1.000000 \\
Azra            &   1.0 &   0.857143 \\
Azrael          &   1.0 &   1.000000 \\
Azriel          &   1.0 &   1.000000 \\
Azrielle        &   1.0 &   1.000000 \\
Azucena         &   1.0 &   0.067568 \\
Azul            &   1.0 &   0.326389 \\
Azula           &   1.0 &   1.000000 \\
Azura           &   1.0 &   0.833333 \\
Azure           &   1.0 &   0.466667 \\
Azusena         &   1.0 &   0.571429 \\
Baani           &   1.0 &   0.923077 \\
Babette         &   1.0 &   0.181818 \\
Baby            &   1.0 &   0.333333 \\
Babygirl        &   1.0 &   0.294118 \\
Bahar           &   1.0 &   1.000000 \\
Bailee          &   1.0 &   0.454545 \\
Baileigh        &   1.0 &   0.625000 \\
Bailey          &   1.0 &   0.430052 \\
Bailie          &   1.0 &   0.333333 \\
Baillie         &   1.0 &   1.000000 \\
Baily           &   1.0 &   0.555556 \\
Baker           &   1.0 &   1.000000 \\
Baleria         &   1.0 &   0.500000 \\
Balinda         &   1.0 &   0.833333 \\
Bambi           &   1.0 &   0.170732 \\
Banesa          &   1.0 &   0.545455 \\
Banessa         &   1.0 &   0.714286 \\
Bani            &   1.0 &   1.000000 \\
Banks           &   1.0 &   1.000000 \\
Bao             &   1.0 &   0.222222 \\
Baran           &   1.0 &   1.000000 \\
Barb            &   1.0 &   0.714286 \\
Barbara         &   1.0 &   0.013333 \\
Barbi           &   1.0 &   1.000000 \\
Barbie          &   1.0 &   0.277778 \\
Barbra          &   1.0 &   0.166667 \\
Bareen          &   1.0 &   1.000000 \\
Bari            &   1.0 &   0.555556 \\
Barrett         &   1.0 &   1.000000 \\
Barri           &   1.0 &   1.000000 \\
Barrie          &   1.0 &   0.238095 \\
Barry           &   1.0 &   1.000000 \\
Basia           &   1.0 &   1.000000 \\
Basil           &   1.0 &   1.000000 \\
Batsheva        &   1.0 &   0.875000 \\
Batya           &   1.0 &   1.000000 \\
Bay             &   1.0 &   0.461538 \\
Baya            &   1.0 &   1.000000 \\
Bayan           &   1.0 &   0.818182 \\
Bayla           &   1.0 &   0.545455 \\
Baylee          &   1.0 &   0.228916 \\
Bayleigh        &   1.0 &   0.384615 \\
Bayley          &   1.0 &   0.538462 \\
Bayli           &   1.0 &   1.000000 \\
Baylie          &   1.0 &   0.272727 \\
Baylor          &   1.0 &   1.000000 \\
Bea             &   1.0 &   0.875000 \\
Beatrice        &   1.0 &   0.335484 \\
Beatris         &   1.0 &   0.217391 \\
Beatrix         &   1.0 &   0.457143 \\
Beatriz         &   1.0 &   0.050000 \\
Beau            &   1.0 &   1.000000 \\
Beautiful       &   1.0 &   0.454545 \\
Beauty          &   1.0 &   1.000000 \\
Beaux           &   1.0 &   1.000000 \\
Bebe            &   1.0 &   1.000000 \\
Becca           &   1.0 &   0.555556 \\
Beckett         &   1.0 &   1.000000 \\
Becki           &   1.0 &   0.333333 \\
Beckie          &   1.0 &   0.315789 \\
Becky           &   1.0 &   0.012920 \\
Bedelia         &   1.0 &   1.000000 \\
Bee             &   1.0 &   1.000000 \\
Bela            &   1.0 &   0.666667 \\
Belanna         &   1.0 &   1.000000 \\
Belem           &   1.0 &   0.500000 \\
Belen           &   1.0 &   0.647541 \\
Belia           &   1.0 &   0.416667 \\
Belicia         &   1.0 &   1.000000 \\
Belina          &   1.0 &   1.000000 \\
Belinda         &   1.0 &   0.131818 \\
Belkis          &   1.0 &   1.000000 \\
Belky           &   1.0 &   0.888889 \\
Bella           &   1.0 &   0.538803 \\
Belladonna      &   1.0 &   0.625000 \\
Bellah          &   1.0 &   0.600000 \\
Bellamarie      &   1.0 &   0.600000 \\
Bellami         &   1.0 &   1.000000 \\
Bellamy         &   1.0 &   0.773585 \\
Bellanie        &   1.0 &   1.000000 \\
Bellarose       &   1.0 &   0.571429 \\
Bellatrix       &   1.0 &   1.000000 \\
Belle           &   1.0 &   0.592593 \\
Bellina         &   1.0 &   1.000000 \\
Belva           &   1.0 &   0.454545 \\
Benicia         &   1.0 &   0.833333 \\
Benita          &   1.0 &   0.131579 \\
Benjamin        &   1.0 &   0.538462 \\
Bennett         &   1.0 &   0.571429 \\
Bennie          &   1.0 &   0.625000 \\
Benny           &   1.0 &   1.000000 \\
Bentlee         &   1.0 &   1.000000 \\
Bentley         &   1.0 &   0.470588 \\
Berania         &   1.0 &   1.000000 \\
Berenice        &   1.0 &   0.045977 \\
Berenis         &   1.0 &   0.545455 \\
Berenise        &   1.0 &   0.208333 \\
Bereniz         &   1.0 &   1.000000 \\
Berenize        &   1.0 &   0.857143 \\
Bergen          &   1.0 &   1.000000 \\
Berkeley        &   1.0 &   0.277778 \\
Berkley         &   1.0 &   1.000000 \\
Berlin          &   1.0 &   0.406250 \\
Berlyn          &   1.0 &   0.562500 \\
Berlynn         &   1.0 &   0.625000 \\
Berna           &   1.0 &   1.000000 \\
Bernadette      &   1.0 &   0.173913 \\
Bernadine       &   1.0 &   0.200000 \\
Bernardette     &   1.0 &   1.000000 \\
Bernardine      &   1.0 &   0.625000 \\
Berneice        &   1.0 &   1.000000 \\
Bernice         &   1.0 &   0.027322 \\
Bernie          &   1.0 &   1.000000 \\
Berniece        &   1.0 &   0.545455 \\
Bernita         &   1.0 &   0.714286 \\
Beronica        &   1.0 &   0.233333 \\
Berta           &   1.0 &   0.263158 \\
Bertha          &   1.0 &   0.071429 \\
Bertie          &   1.0 &   0.750000 \\
Bertina         &   1.0 &   1.000000 \\
Beryl           &   1.0 &   0.250000 \\
Bess            &   1.0 &   1.000000 \\
Bessie          &   1.0 &   0.160714 \\
Bessy           &   1.0 &   0.625000 \\
Betania         &   1.0 &   1.000000 \\
Beth            &   1.0 &   0.018405 \\
Bethani         &   1.0 &   1.000000 \\
Bethanie        &   1.0 &   0.500000 \\
Bethann         &   1.0 &   0.833333 \\
Bethanny        &   1.0 &   0.625000 \\
Bethany         &   1.0 &   0.250000 \\
Bethel          &   1.0 &   0.333333 \\
Bethenny        &   1.0 &   1.000000 \\
Bethlehem       &   1.0 &   1.000000 \\
Bethsaida       &   1.0 &   1.000000 \\
Bethsy          &   1.0 &   1.000000 \\
Bethzaida       &   1.0 &   1.000000 \\
Bethzi          &   1.0 &   1.000000 \\
Bethzy          &   1.0 &   0.096386 \\
Betina          &   1.0 &   1.000000 \\
Betsabe         &   1.0 &   0.370370 \\
Betsaida        &   1.0 &   0.642857 \\
Betsey          &   1.0 &   1.000000 \\
Betsy           &   1.0 &   0.114583 \\
Bette           &   1.0 &   0.064220 \\
Betti           &   1.0 &   1.000000 \\
Bettie          &   1.0 &   0.117647 \\
Bettina         &   1.0 &   0.178571 \\
Betty           &   1.0 &   0.013734 \\
Bettye          &   1.0 &   0.375000 \\
Bettylou        &   1.0 &   0.714286 \\
Betzabe         &   1.0 &   0.714286 \\
Betzabeth       &   1.0 &   0.500000 \\
Betzaida        &   1.0 &   0.092593 \\
Betzaira        &   1.0 &   0.833333 \\
Betzayda        &   1.0 &   1.000000 \\
Betzayra        &   1.0 &   1.000000 \\
Betzi           &   1.0 &   1.000000 \\
Betzy           &   1.0 &   0.078125 \\
Beulah          &   1.0 &   0.222222 \\
Bev             &   1.0 &   0.666667 \\
Beverlee        &   1.0 &   0.357143 \\
Beverley        &   1.0 &   0.094340 \\
Beverly         &   1.0 &   0.043165 \\
Bevin           &   1.0 &   1.000000 \\
Bexley          &   1.0 &   0.588235 \\
Beya            &   1.0 &   0.875000 \\
Beyonce         &   1.0 &   0.102041 \\
Bhavya          &   1.0 &   1.000000 \\
Bianca          &   1.0 &   0.156992 \\
Bianey          &   1.0 &   0.642857 \\
Bianka          &   1.0 &   0.233333 \\
Bibi            &   1.0 &   0.714286 \\
Bibiana         &   1.0 &   0.227273 \\
Bibianna        &   1.0 &   1.000000 \\
Bich            &   1.0 &   0.714286 \\
Bijou           &   1.0 &   0.714286 \\
Billee          &   1.0 &   0.714286 \\
Billi           &   1.0 &   1.000000 \\
Billie          &   1.0 &   0.248062 \\
Billiejo        &   1.0 &   1.000000 \\
Billy           &   1.0 &   0.555556 \\
Billye          &   1.0 &   1.000000 \\
Bilma           &   1.0 &   1.000000 \\
Bindi           &   1.0 &   1.000000 \\
Binh            &   1.0 &   1.000000 \\
Bionca          &   1.0 &   1.000000 \\
Birdie          &   1.0 &   0.961538 \\
Birgit          &   1.0 &   0.833333 \\
Biridiana       &   1.0 &   0.272727 \\
Bita            &   1.0 &   1.000000 \\
Biviana         &   1.0 &   0.555556 \\
Blair           &   1.0 &   1.000000 \\
Blaire          &   1.0 &   0.888889 \\
Blaise          &   1.0 &   1.000000 \\
Blake           &   1.0 &   0.900000 \\
Blakeley        &   1.0 &   0.833333 \\
Blakely         &   1.0 &   0.777778 \\
Blakelyn        &   1.0 &   1.000000 \\
Blanca          &   1.0 &   0.081699 \\
Blanch          &   1.0 &   1.000000 \\
Blanche         &   1.0 &   0.100000 \\
Blayke          &   1.0 &   0.687500 \\
Blen            &   1.0 &   1.000000 \\
Bless           &   1.0 &   1.000000 \\
Blessing        &   1.0 &   0.785714 \\
Blessyn         &   1.0 &   1.000000 \\
Bleu            &   1.0 &   1.000000 \\
Blia            &   1.0 &   0.625000 \\
Bliss           &   1.0 &   0.800000 \\
Bloom           &   1.0 &   1.000000 \\
Blossom         &   1.0 &   1.000000 \\
Blu             &   1.0 &   1.000000 \\
Blue            &   1.0 &   0.636364 \\
Blythe          &   1.0 &   0.217391 \\
Bo              &   1.0 &   1.000000 \\
Bobbe           &   1.0 &   0.833333 \\
Bobbette        &   1.0 &   1.000000 \\
Bobbi           &   1.0 &   0.265306 \\
Bobbie          &   1.0 &   0.090909 \\
Bobby           &   1.0 &   0.428571 \\
Bobbye          &   1.0 &   1.000000 \\
Bobette         &   1.0 &   0.777778 \\
Bodhi           &   1.0 &   0.631579 \\
Boni            &   1.0 &   1.000000 \\
Bonita          &   1.0 &   0.076923 \\
Bonni           &   1.0 &   0.625000 \\
Bonnie          &   1.0 &   0.109034 \\
Bonny           &   1.0 &   0.185185 \\
Boston          &   1.0 &   1.000000 \\
Bostyn          &   1.0 &   1.000000 \\
Bowie           &   1.0 &   0.657143 \\
Bracha          &   1.0 &   0.833333 \\
Bradie          &   1.0 &   1.000000 \\
Bradlee         &   1.0 &   1.000000 \\
Bradley         &   1.0 &   0.714286 \\
Brady           &   1.0 &   0.625000 \\
Braeden         &   1.0 &   1.000000 \\
Braedyn         &   1.0 &   1.000000 \\
Braelyn         &   1.0 &   0.440000 \\
Braelynn        &   1.0 &   0.560976 \\
Brailyn         &   1.0 &   0.833333 \\
Branda          &   1.0 &   0.714286 \\
Brandalyn       &   1.0 &   0.625000 \\
Brande          &   1.0 &   0.214286 \\
Brandee         &   1.0 &   0.153846 \\
Brandi          &   1.0 &   0.016173 \\
Brandice        &   1.0 &   0.454545 \\
Brandie         &   1.0 &   0.071429 \\
Brandilyn       &   1.0 &   0.833333 \\
Brandis         &   1.0 &   0.600000 \\
Brandon         &   1.0 &   0.193548 \\
Brandy          &   1.0 &   0.010593 \\
Brandye         &   1.0 &   1.000000 \\
Brandyn         &   1.0 &   1.000000 \\
Brayden         &   1.0 &   1.000000 \\
Braylee         &   1.0 &   0.692308 \\
Brayleigh       &   1.0 &   0.833333 \\
Braylin         &   1.0 &   0.714286 \\
Braylyn         &   1.0 &   1.000000 \\
Braylynn        &   1.0 &   1.000000 \\
Brazil          &   1.0 &   1.000000 \\
Brea            &   1.0 &   0.200000 \\
Breah           &   1.0 &   1.000000 \\
Breahna         &   1.0 &   0.857143 \\
Breana          &   1.0 &   0.041958 \\
Breanah         &   1.0 &   1.000000 \\
Breanda         &   1.0 &   1.000000 \\
Breann          &   1.0 &   0.089286 \\
Breanna         &   1.0 &   0.027864 \\
Breannah        &   1.0 &   1.000000 \\
Breanne         &   1.0 &   0.067416 \\
Breauna         &   1.0 &   0.400000 \\
Breaunna        &   1.0 &   1.000000 \\
Breckyn         &   1.0 &   1.000000 \\
Bree            &   1.0 &   0.101449 \\
Breeana         &   1.0 &   0.142857 \\
Breeann         &   1.0 &   0.208333 \\
Breeanna        &   1.0 &   0.098039 \\
Breeanne        &   1.0 &   0.500000 \\
Breelyn         &   1.0 &   1.000000 \\
Breena          &   1.0 &   0.666667 \\
Breeze          &   1.0 &   1.000000 \\
Breezy          &   1.0 &   0.555556 \\
Breiana         &   1.0 &   1.000000 \\
Brelynn         &   1.0 &   1.000000 \\
Brena           &   1.0 &   1.000000 \\
Brenae          &   1.0 &   1.000000 \\
Brenda          &   1.0 &   0.020588 \\
Brendan         &   1.0 &   1.000000 \\
Brendy          &   1.0 &   1.000000 \\
Brenna          &   1.0 &   0.042373 \\
Brennah         &   1.0 &   1.000000 \\
Brennan         &   1.0 &   0.714286 \\
Brennen         &   1.0 &   1.000000 \\
Brent           &   1.0 &   1.000000 \\
Breona          &   1.0 &   0.555556 \\
Breonna         &   1.0 &   0.310345 \\
Brett           &   1.0 &   0.312500 \\
Brettany        &   1.0 &   1.000000 \\
Breya           &   1.0 &   1.000000 \\
Breyana         &   1.0 &   0.500000 \\
Breyanna        &   1.0 &   0.800000 \\
Breyona         &   1.0 &   1.000000 \\
Brezhane        &   1.0 &   1.000000 \\
Bria            &   1.0 &   0.252525 \\
Briah           &   1.0 &   1.000000 \\
Briahna         &   1.0 &   0.555556 \\
Brian           &   1.0 &   0.192308 \\
Briana          &   1.0 &   0.105118 \\
Brianah         &   1.0 &   1.000000 \\
Brianda         &   1.0 &   0.033113 \\
Briane          &   1.0 &   1.000000 \\
Briann          &   1.0 &   0.384615 \\
Brianna         &   1.0 &   0.159589 \\
Briannah        &   1.0 &   0.500000 \\
Brianne         &   1.0 &   0.044444 \\
Brianny         &   1.0 &   1.000000 \\
Briar           &   1.0 &   1.000000 \\
Briarrose       &   1.0 &   1.000000 \\
Briauna         &   1.0 &   0.583333 \\
Briceida        &   1.0 &   0.785714 \\
Briceyda        &   1.0 &   0.600000 \\
Bricia          &   1.0 &   1.000000 \\
Bridget         &   1.0 &   0.168539 \\
Bridgett        &   1.0 &   0.142857 \\
Bridgette       &   1.0 &   0.101266 \\
Bridgit         &   1.0 &   0.833333 \\
Bridgitt        &   1.0 &   1.000000 \\
Bridgitte       &   1.0 &   1.000000 \\
Brie            &   1.0 &   0.647059 \\
Brieana         &   1.0 &   0.636364 \\
Brieann         &   1.0 &   0.625000 \\
Brieanna        &   1.0 &   0.315789 \\
Brieanne        &   1.0 &   0.750000 \\
Briella         &   1.0 &   0.833333 \\
Brielle         &   1.0 &   0.840000 \\
Brienna         &   1.0 &   0.777778 \\
Brienne         &   1.0 &   0.312500 \\
Briget          &   1.0 &   1.000000 \\
Brigette        &   1.0 &   0.108696 \\
Brighton        &   1.0 &   0.846154 \\
Brigid          &   1.0 &   0.421053 \\
Brigida         &   1.0 &   1.000000 \\
Brigit          &   1.0 &   1.000000 \\
Brigitta        &   1.0 &   1.000000 \\
Brigitte        &   1.0 &   0.191489 \\
Brihana         &   1.0 &   0.833333 \\
Brihanna        &   1.0 &   0.500000 \\
Brilee          &   1.0 &   1.000000 \\
Briley          &   1.0 &   0.571429 \\
Brilyn          &   1.0 &   1.000000 \\
Brilynn         &   1.0 &   0.714286 \\
Brina           &   1.0 &   0.294118 \\
Brinda          &   1.0 &   0.500000 \\
Brinkley        &   1.0 &   1.000000 \\
Brinlee         &   1.0 &   0.875000 \\
Brinley         &   1.0 &   0.826087 \\
Briona          &   1.0 &   0.625000 \\
Brionna         &   1.0 &   0.304348 \\
Brionne         &   1.0 &   1.000000 \\
Brisa           &   1.0 &   0.204301 \\
Brisamar        &   1.0 &   1.000000 \\
Briseida        &   1.0 &   0.175439 \\
Briseis         &   1.0 &   0.476190 \\
Briselda        &   1.0 &   0.714286 \\
Briseyda        &   1.0 &   0.262295 \\
Brisia          &   1.0 &   0.333333 \\
Brissa          &   1.0 &   0.183333 \\
Brissia         &   1.0 &   0.135593 \\
Bristol         &   1.0 &   0.666667 \\
Brit            &   1.0 &   1.000000 \\
Brita           &   1.0 &   0.833333 \\
Britain         &   1.0 &   1.000000 \\
Britani         &   1.0 &   0.300000 \\
Britania        &   1.0 &   1.000000 \\
Britanie        &   1.0 &   1.000000 \\
Britanny        &   1.0 &   0.888889 \\
Britany         &   1.0 &   0.125000 \\
Brithanny       &   1.0 &   1.000000 \\
Brithany        &   1.0 &   0.615385 \\
Brithney        &   1.0 &   1.000000 \\
Britini         &   1.0 &   1.000000 \\
Britne          &   1.0 &   1.000000 \\
Britnee         &   1.0 &   0.222222 \\
Britney         &   1.0 &   0.033333 \\
Britni          &   1.0 &   0.113636 \\
Britnie         &   1.0 &   0.533333 \\
Britny          &   1.0 &   0.192308 \\
Britt           &   1.0 &   0.238095 \\
Britta          &   1.0 &   0.294118 \\
Brittainy       &   1.0 &   0.625000 \\
Brittan         &   1.0 &   0.857143 \\
Brittanee       &   1.0 &   0.312500 \\
Brittaney       &   1.0 &   0.116279 \\
Brittani        &   1.0 &   0.054054 \\
Brittania       &   1.0 &   0.600000 \\
Brittanie       &   1.0 &   0.101695 \\
Brittanny       &   1.0 &   0.500000 \\
Brittany        &   1.0 &   0.008943 \\
Brittanya       &   1.0 &   1.000000 \\
Britteny        &   1.0 &   0.428571 \\
Brittiany       &   1.0 &   0.857143 \\
Brittiney       &   1.0 &   0.857143 \\
Brittini        &   1.0 &   0.545455 \\
Brittiny        &   1.0 &   0.600000 \\
Brittnay        &   1.0 &   0.352941 \\
Brittnee        &   1.0 &   0.098039 \\
Brittnei        &   1.0 &   1.000000 \\
Brittney        &   1.0 &   0.007143 \\
Brittni         &   1.0 &   0.065789 \\
Brittnie        &   1.0 &   0.147059 \\
Brittny         &   1.0 &   0.162791 \\
Britton         &   1.0 &   1.000000 \\
Brixlee         &   1.0 &   1.000000 \\
Brixley         &   1.0 &   1.000000 \\
Brixton         &   1.0 &   0.454545 \\
Brixtyn         &   1.0 &   1.000000 \\
Briyana         &   1.0 &   1.000000 \\
Briza           &   1.0 &   0.135135 \\
Brizeida        &   1.0 &   0.888889 \\
Brizeyda        &   1.0 &   0.714286 \\
Brizia          &   1.0 &   1.000000 \\
Brizza          &   1.0 &   1.000000 \\
Brodie          &   1.0 &   1.000000 \\
Brogan          &   1.0 &   0.875000 \\
Bronte          &   1.0 &   0.200000 \\
Bronwen         &   1.0 &   1.000000 \\
Bronwyn         &   1.0 &   0.428571 \\
Brook           &   1.0 &   0.128205 \\
Brooke          &   1.0 &   0.207018 \\
Brookelyn       &   1.0 &   0.375000 \\
Brookelynn      &   1.0 &   0.400000 \\
Brooklin        &   1.0 &   0.400000 \\
Brooklyn        &   1.0 &   0.385576 \\
Brooklyne       &   1.0 &   1.000000 \\
Brooklynn       &   1.0 &   0.374408 \\
Brooklynne      &   1.0 &   0.583333 \\
Brooks          &   1.0 &   1.000000 \\
Bruce           &   1.0 &   1.000000 \\
Bruna           &   1.0 &   0.384615 \\
Bryan           &   1.0 &   0.388889 \\
Bryana          &   1.0 &   0.173077 \\
Bryanah         &   1.0 &   1.000000 \\
Bryanda         &   1.0 &   1.000000 \\
Bryanna         &   1.0 &   0.079470 \\
Bryannah        &   1.0 &   1.000000 \\
Bryanne         &   1.0 &   0.777778 \\
Bryce           &   1.0 &   0.291667 \\
Brylee          &   1.0 &   0.363636 \\
Bryleigh        &   1.0 &   0.454545 \\
Brylie          &   1.0 &   0.555556 \\
Bryn            &   1.0 &   0.296296 \\
Bryna           &   1.0 &   0.625000 \\
Brynlee         &   1.0 &   0.521127 \\
Brynleigh       &   1.0 &   0.888889 \\
Brynley         &   1.0 &   0.928571 \\
Brynlie         &   1.0 &   1.000000 \\
Brynn           &   1.0 &   0.586777 \\
Brynna          &   1.0 &   0.400000 \\
Brynne          &   1.0 &   0.357143 \\
Brynnlee        &   1.0 &   0.666667 \\
Brynnley        &   1.0 &   1.000000 \\
Bryonna         &   1.0 &   1.000000 \\
Bryony          &   1.0 &   1.000000 \\
Bryssa          &   1.0 &   1.000000 \\
Brystol         &   1.0 &   1.000000 \\
Buffi           &   1.0 &   1.000000 \\
Buffie          &   1.0 &   0.500000 \\
Buffy           &   1.0 &   0.189189 \\
Bunnie          &   1.0 &   1.000000 \\
Bunny           &   1.0 &   0.555556 \\
Bushra          &   1.0 &   1.000000 \\
Byanca          &   1.0 &   0.357143 \\
Byanka          &   1.0 &   0.250000 \\
Cadance         &   1.0 &   1.000000 \\
Caden           &   1.0 &   0.583333 \\
Cadence         &   1.0 &   0.214286 \\
Cadhla          &   1.0 &   1.000000 \\
Cady            &   1.0 &   0.375000 \\
Caelan          &   1.0 &   0.800000 \\
Caeley          &   1.0 &   1.000000 \\
Caeli           &   1.0 &   1.000000 \\
Caelin          &   1.0 &   1.000000 \\
Caelyn          &   1.0 &   0.227273 \\
Cai             &   1.0 &   1.000000 \\
Caia            &   1.0 &   0.823529 \\
Caiden          &   1.0 &   1.000000 \\
Caidence        &   1.0 &   0.555556 \\
Caila           &   1.0 &   0.714286 \\
Cailee          &   1.0 &   0.555556 \\
Caileigh        &   1.0 &   1.000000 \\
Cailey          &   1.0 &   0.333333 \\
Cailin          &   1.0 &   0.304348 \\
Cailyn          &   1.0 &   0.146341 \\
Cailynn         &   1.0 &   0.750000 \\
Caira           &   1.0 &   0.888889 \\
Cairo           &   1.0 &   1.000000 \\
Caitlan         &   1.0 &   0.500000 \\
Caitlen         &   1.0 &   0.833333 \\
Caitlin         &   1.0 &   0.011158 \\
Caitlyn         &   1.0 &   0.045249 \\
Caitlynn        &   1.0 &   0.272727 \\
Caiya           &   1.0 &   1.000000 \\
Calandra        &   1.0 &   1.000000 \\
Caleb           &   1.0 &   1.000000 \\
Caleigh         &   1.0 &   0.285714 \\
Caley           &   1.0 &   0.545455 \\
Cali            &   1.0 &   0.304659 \\
Calia           &   1.0 &   0.909091 \\
Caliah          &   1.0 &   1.000000 \\
Caliana         &   1.0 &   0.500000 \\
Calianna        &   1.0 &   1.000000 \\
Calie           &   1.0 &   0.454545 \\
California      &   1.0 &   0.875000 \\
Calina          &   1.0 &   0.777778 \\
Calirose        &   1.0 &   1.000000 \\
Calise          &   1.0 &   1.000000 \\
Calisi          &   1.0 &   1.000000 \\
Calissa         &   1.0 &   0.545455 \\
Calista         &   1.0 &   0.500000 \\
Caliyah         &   1.0 &   0.545455 \\
Calla           &   1.0 &   0.500000 \\
Callan          &   1.0 &   0.750000 \\
Calleen         &   1.0 &   1.000000 \\
Calleigh        &   1.0 &   0.250000 \\
Calley          &   1.0 &   1.000000 \\
Calli           &   1.0 &   0.461538 \\
Callia          &   1.0 &   0.500000 \\
Callie          &   1.0 &   0.833333 \\
Calliope        &   1.0 &   0.967742 \\
Callista        &   1.0 &   0.476190 \\
Cally           &   1.0 &   0.857143 \\
Caludia         &   1.0 &   1.000000 \\
Calvin          &   1.0 &   1.000000 \\
Calyn           &   1.0 &   1.000000 \\
Calypso         &   1.0 &   0.857143 \\
Calysta         &   1.0 &   0.555556 \\
Cam             &   1.0 &   1.000000 \\
Cambria         &   1.0 &   0.523077 \\
Cambrie         &   1.0 &   1.000000 \\
Camden          &   1.0 &   0.555556 \\
Camdyn          &   1.0 &   0.428571 \\
Camela          &   1.0 &   1.000000 \\
Camelia         &   1.0 &   0.615385 \\
Camellia        &   1.0 &   0.739130 \\
Cameo           &   1.0 &   0.416667 \\
Cameran         &   1.0 &   0.416667 \\
Camerina        &   1.0 &   1.000000 \\
Cameron         &   1.0 &   0.356481 \\
Cameryn         &   1.0 &   0.185185 \\
Cami            &   1.0 &   0.166667 \\
Camie           &   1.0 &   1.000000 \\
Camila          &   1.0 &   0.935528 \\
Camilah         &   1.0 &   0.727273 \\
Camile          &   1.0 &   1.000000 \\
Camilia         &   1.0 &   1.000000 \\
Camilla         &   1.0 &   0.855670 \\
Camille         &   1.0 &   0.612903 \\
Camillia        &   1.0 &   1.000000 \\
Camily          &   1.0 &   1.000000 \\
Camisha         &   1.0 &   0.333333 \\
Camiyah         &   1.0 &   1.000000 \\
Cammi           &   1.0 &   1.000000 \\
Cammie          &   1.0 &   0.294118 \\
Cammy           &   1.0 &   0.555556 \\
Campbell        &   1.0 &   0.375000 \\
Camren          &   1.0 &   1.000000 \\
Camry           &   1.0 &   0.500000 \\
Camryn          &   1.0 &   0.195652 \\
Camrynn         &   1.0 &   1.000000 \\
Camyla          &   1.0 &   0.714286 \\
Candace         &   1.0 &   0.025926 \\
Candance        &   1.0 &   1.000000 \\
Candee          &   1.0 &   1.000000 \\
Candelaria      &   1.0 &   0.250000 \\
Candi           &   1.0 &   0.166667 \\
Candice         &   1.0 &   0.011211 \\
Candida         &   1.0 &   0.185185 \\
Candie          &   1.0 &   0.416667 \\
Candis          &   1.0 &   0.172414 \\
Candise         &   1.0 &   0.714286 \\
Candiss         &   1.0 &   1.000000 \\
Candra          &   1.0 &   0.454545 \\
Candy           &   1.0 &   0.043103 \\
Candyce         &   1.0 &   0.312500 \\
Caoimhe         &   1.0 &   1.000000 \\
Capri           &   1.0 &   0.700000 \\
Caprice         &   1.0 &   0.291667 \\
Cara            &   1.0 &   0.220472 \\
Caralee         &   1.0 &   1.000000 \\
Caraline        &   1.0 &   1.000000 \\
Caralyn         &   1.0 &   1.000000 \\
Carely          &   1.0 &   1.000000 \\
Caren           &   1.0 &   0.116667 \\
Caressa         &   1.0 &   0.384615 \\
Caresse         &   1.0 &   0.857143 \\
Carey           &   1.0 &   0.090909 \\
Cari            &   1.0 &   0.060241 \\
Caricia         &   1.0 &   1.000000 \\
Caridad         &   1.0 &   1.000000 \\
Carie           &   1.0 &   0.171429 \\
Carin           &   1.0 &   0.161290 \\
Carina          &   1.0 &   0.094406 \\
Carine          &   1.0 &   1.000000 \\
Carinna         &   1.0 &   1.000000 \\
Carinne         &   1.0 &   1.000000 \\
Caris           &   1.0 &   0.555556 \\
Carisa          &   1.0 &   0.263158 \\
Carisma         &   1.0 &   0.454545 \\
Carissa         &   1.0 &   0.066327 \\
Carl            &   1.0 &   0.857143 \\
Carla           &   1.0 &   0.097065 \\
Carlee          &   1.0 &   0.314286 \\
Carleen         &   1.0 &   0.217391 \\
Carleigh        &   1.0 &   0.600000 \\
Carlena         &   1.0 &   0.833333 \\
Carlene         &   1.0 &   0.153846 \\
Carletta        &   1.0 &   0.714286 \\
Carlette        &   1.0 &   1.000000 \\
Carley          &   1.0 &   0.086207 \\
Carli           &   1.0 &   0.184211 \\
Carlie          &   1.0 &   0.179487 \\
Carlin          &   1.0 &   0.625000 \\
Carlina         &   1.0 &   0.750000 \\
Carlisha        &   1.0 &   1.000000 \\
Carlita         &   1.0 &   1.000000 \\
Carlos          &   1.0 &   0.250000 \\
Carlota         &   1.0 &   1.000000 \\
Carlotta        &   1.0 &   0.294118 \\
Carly           &   1.0 &   0.119691 \\
Carlye          &   1.0 &   0.857143 \\
Carlyn          &   1.0 &   0.545455 \\
Carma           &   1.0 &   1.000000 \\
Carman          &   1.0 &   1.000000 \\
Carmel          &   1.0 &   0.263158 \\
Carmela         &   1.0 &   0.423077 \\
Carmelina       &   1.0 &   1.000000 \\
Carmelita       &   1.0 &   0.320000 \\
Carmella        &   1.0 &   0.562500 \\
Carmen          &   1.0 &   0.250000 \\
Carmin          &   1.0 &   0.583333 \\
Carmina         &   1.0 &   0.260870 \\
Carnation       &   1.0 &   1.000000 \\
Carol           &   1.0 &   0.003180 \\
Carola          &   1.0 &   1.000000 \\
Carolann        &   1.0 &   0.545455 \\
Carolanne       &   1.0 &   1.000000 \\
Carole          &   1.0 &   0.010067 \\
Carolee         &   1.0 &   0.291667 \\
Carolena        &   1.0 &   1.000000 \\
Carolin         &   1.0 &   0.833333 \\
Carolina        &   1.0 &   0.266497 \\
Caroline        &   1.0 &   0.387812 \\
Caroll          &   1.0 &   0.666667 \\
Carolyn         &   1.0 &   0.008629 \\
Carolyna        &   1.0 &   1.000000 \\
Carolyne        &   1.0 &   0.333333 \\
Carolynn        &   1.0 &   0.217391 \\
Carolynne       &   1.0 &   1.000000 \\
Caron           &   1.0 &   0.294118 \\
Carri           &   1.0 &   0.259259 \\
Carrie          &   1.0 &   0.007704 \\
Carrigan        &   1.0 &   1.000000 \\
Carrin          &   1.0 &   1.000000 \\
Carrina         &   1.0 &   1.000000 \\
Carrington      &   1.0 &   0.454545 \\
Carrisa         &   1.0 &   0.555556 \\
Carrissa        &   1.0 &   0.416667 \\
Carrol          &   1.0 &   0.227273 \\
Carroll         &   1.0 &   0.291667 \\
Carry           &   1.0 &   1.000000 \\
Carsen          &   1.0 &   1.000000 \\
Carson          &   1.0 &   0.400000 \\
Carsyn          &   1.0 &   0.812500 \\
Carter          &   1.0 &   0.686275 \\
Cary            &   1.0 &   0.185185 \\
Caryl           &   1.0 &   0.368421 \\
Caryn           &   1.0 &   0.105263 \\
Carys           &   1.0 &   0.444444 \\
Casandra        &   1.0 &   0.050000 \\
Casara          &   1.0 &   1.000000 \\
Casarah         &   1.0 &   1.000000 \\
Casaundra       &   1.0 &   1.000000 \\
Casey           &   1.0 &   0.178571 \\
Cashmere        &   1.0 &   1.000000 \\
Casie           &   1.0 &   0.208333 \\
Casondra        &   1.0 &   0.500000 \\
Cassadee        &   1.0 &   1.000000 \\
Cassady         &   1.0 &   1.000000 \\
Cassandra       &   1.0 &   0.103487 \\
Cassandre       &   1.0 &   1.000000 \\
Cassara         &   1.0 &   1.000000 \\
Cassaundra      &   1.0 &   0.240000 \\
Cassey          &   1.0 &   0.500000 \\
Cassi           &   1.0 &   0.500000 \\
Cassia          &   1.0 &   0.722222 \\
Cassidee        &   1.0 &   0.750000 \\
Cassidi         &   1.0 &   1.000000 \\
Cassidie        &   1.0 &   1.000000 \\
Cassidy         &   1.0 &   0.261398 \\
Cassie          &   1.0 &   0.143885 \\
Cassondra       &   1.0 &   0.130435 \\
Cassy           &   1.0 &   0.428571 \\
Catalaya        &   1.0 &   0.888889 \\
Catalea         &   1.0 &   1.000000 \\
Cataleah        &   1.0 &   1.000000 \\
Cataleya        &   1.0 &   0.955357 \\
Cataleyah       &   1.0 &   1.000000 \\
Catalia         &   1.0 &   0.833333 \\
Catalina        &   1.0 &   1.000000 \\
Catalyna        &   1.0 &   1.000000 \\
Catarina        &   1.0 &   0.736842 \\
Cate            &   1.0 &   0.272727 \\
Cateleya        &   1.0 &   1.000000 \\
Catelyn         &   1.0 &   0.400000 \\
Catelynn        &   1.0 &   0.833333 \\
Catera          &   1.0 &   1.000000 \\
Caterin         &   1.0 &   0.714286 \\
Caterina        &   1.0 &   0.416667 \\
Cathaleya       &   1.0 &   1.000000 \\
Catharine       &   1.0 &   0.333333 \\
Cathe           &   1.0 &   1.000000 \\
Catherin        &   1.0 &   0.833333 \\
Catherina       &   1.0 &   0.833333 \\
Catherine       &   1.0 &   0.082447 \\
Catheryn        &   1.0 &   0.714286 \\
Cathey          &   1.0 &   0.888889 \\
Cathi           &   1.0 &   0.208333 \\
Cathie          &   1.0 &   0.116279 \\
Cathleen        &   1.0 &   0.031250 \\
Cathlene        &   1.0 &   0.600000 \\
Cathrine        &   1.0 &   0.206897 \\
Cathryn         &   1.0 &   0.137255 \\
Cathy           &   1.0 &   0.005988 \\
Catie           &   1.0 &   1.000000 \\
Catina          &   1.0 &   0.142857 \\
Catlin          &   1.0 &   0.500000 \\
Catlyn          &   1.0 &   1.000000 \\
Catrice         &   1.0 &   1.000000 \\
Catrina         &   1.0 &   0.090909 \\
Catriona        &   1.0 &   1.000000 \\
Cattaleya       &   1.0 &   0.941176 \\
Cattleya        &   1.0 &   0.515152 \\
Cayce           &   1.0 &   1.000000 \\
Caycee          &   1.0 &   1.000000 \\
Cayden          &   1.0 &   0.400000 \\
Caydence        &   1.0 &   0.363636 \\
Cayenne         &   1.0 &   1.000000 \\
Cayetana        &   1.0 &   1.000000 \\
Cayla           &   1.0 &   0.134615 \\
Caylah          &   1.0 &   1.000000 \\
Caylee          &   1.0 &   0.073684 \\
Cayleigh        &   1.0 &   0.714286 \\
Caylen          &   1.0 &   1.000000 \\
Cayley          &   1.0 &   0.666667 \\
Caylie          &   1.0 &   0.545455 \\
Caylin          &   1.0 &   0.357143 \\
Ceaira          &   1.0 &   1.000000 \\
Ceana           &   1.0 &   1.000000 \\
Ceanna          &   1.0 &   0.833333 \\
Ceara           &   1.0 &   0.500000 \\
Cece            &   1.0 &   1.000000 \\
Ceceilia        &   1.0 &   1.000000 \\
Cecelia         &   1.0 &   0.265060 \\
Cecil           &   1.0 &   1.000000 \\
Cecila          &   1.0 &   1.000000 \\
Cecile          &   1.0 &   0.291667 \\
Cecilia         &   1.0 &   0.433420 \\
Cecilie         &   1.0 &   1.000000 \\
Cecille         &   1.0 &   1.000000 \\
Cecily          &   1.0 &   0.070423 \\
Cecy            &   1.0 &   1.000000 \\
Cedar           &   1.0 &   0.833333 \\
Cedrina         &   1.0 &   1.000000 \\
Celena          &   1.0 &   0.107143 \\
Celene          &   1.0 &   0.140000 \\
Celenia         &   1.0 &   0.312500 \\
Celest          &   1.0 &   0.285714 \\
Celeste         &   1.0 &   0.448276 \\
Celestia        &   1.0 &   1.000000 \\
Celestial       &   1.0 &   1.000000 \\
Celestina       &   1.0 &   0.384615 \\
Celestine       &   1.0 &   0.600000 \\
Celia           &   1.0 &   0.362745 \\
Celica          &   1.0 &   1.000000 \\
Celida          &   1.0 &   1.000000 \\
Celina          &   1.0 &   0.152074 \\
Celinda         &   1.0 &   0.833333 \\
Celine          &   1.0 &   1.000000 \\
Celisse         &   1.0 &   0.333333 \\
Celyna          &   1.0 &   1.000000 \\
Cendy           &   1.0 &   0.700000 \\
Cennet          &   1.0 &   0.666667 \\
Centhia         &   1.0 &   1.000000 \\
Cera            &   1.0 &   0.384615 \\
Cerena          &   1.0 &   1.000000 \\
Cerenity        &   1.0 &   1.000000 \\
Cerina          &   1.0 &   0.500000 \\
Cerise          &   1.0 &   0.833333 \\
Cerissa         &   1.0 &   1.000000 \\
Cesar           &   1.0 &   0.500000 \\
Cesia           &   1.0 &   0.583333 \\
Cesilia         &   1.0 &   0.300000 \\
Chabeli         &   1.0 &   1.000000 \\
Chad            &   1.0 &   1.000000 \\
Chai            &   1.0 &   1.000000 \\
Chaise          &   1.0 &   1.000000 \\
Chaitra         &   1.0 &   1.000000 \\
Chaka           &   1.0 &   0.384615 \\
Chalise         &   1.0 &   1.000000 \\
Chalon          &   1.0 &   1.000000 \\
Champagne       &   1.0 &   0.538462 \\
Chan            &   1.0 &   0.555556 \\
Chana           &   1.0 &   0.375000 \\
Chanae          &   1.0 &   0.545455 \\
Chance          &   1.0 &   0.555556 \\
Chanda          &   1.0 &   0.206897 \\
Chandi          &   1.0 &   1.000000 \\
Chandler        &   1.0 &   0.136842 \\
Chandni         &   1.0 &   1.000000 \\
Chandra         &   1.0 &   0.054945 \\
Chandrika       &   1.0 &   1.000000 \\
Chanel          &   1.0 &   0.342657 \\
Chanell         &   1.0 &   0.272727 \\
Chanelle        &   1.0 &   0.120000 \\
Chanice         &   1.0 &   1.000000 \\
Chanin          &   1.0 &   0.636364 \\
Chanise         &   1.0 &   1.000000 \\
Chaniya         &   1.0 &   1.000000 \\
Channa          &   1.0 &   1.000000 \\
Channel         &   1.0 &   0.277778 \\
Channell        &   1.0 &   1.000000 \\
Channelle       &   1.0 &   1.000000 \\
Channing        &   1.0 &   0.208333 \\
Channon         &   1.0 &   0.833333 \\
Channy          &   1.0 &   1.000000 \\
Chantae         &   1.0 &   1.000000 \\
Chantal         &   1.0 &   0.084337 \\
Chantalle       &   1.0 &   1.000000 \\
Chantay         &   1.0 &   1.000000 \\
Chante          &   1.0 &   0.200000 \\
Chantel         &   1.0 &   0.056075 \\
Chantell        &   1.0 &   0.238095 \\
Chantelle       &   1.0 &   0.121951 \\
Chantil         &   1.0 &   1.000000 \\
Chara           &   1.0 &   0.714286 \\
Charae          &   1.0 &   1.000000 \\
Chardae         &   1.0 &   0.555556 \\
Charday         &   1.0 &   0.714286 \\
Charde          &   1.0 &   0.238095 \\
Chardonnay      &   1.0 &   0.600000 \\
Chari           &   1.0 &   0.750000 \\
Charice         &   1.0 &   1.000000 \\
Charis          &   1.0 &   0.315789 \\
Charisa         &   1.0 &   1.000000 \\
Charise         &   1.0 &   0.333333 \\
Charisma        &   1.0 &   0.176471 \\
Charissa        &   1.0 &   0.192308 \\
Charisse        &   1.0 &   0.192308 \\
Charito         &   1.0 &   1.000000 \\
Charity         &   1.0 &   0.055556 \\
Charla          &   1.0 &   0.434783 \\
Charlaine       &   1.0 &   1.000000 \\
Charlee         &   1.0 &   0.595238 \\
Charleen        &   1.0 &   0.142857 \\
Charleigh       &   1.0 &   0.260870 \\
Charlena        &   1.0 &   0.750000 \\
Charlene        &   1.0 &   0.063545 \\
Charles         &   1.0 &   0.357143 \\
Charlesetta     &   1.0 &   0.714286 \\
Charleston      &   1.0 &   0.545455 \\
Charlette       &   1.0 &   0.416667 \\
Charley         &   1.0 &   0.500000 \\
Charli          &   1.0 &   0.962264 \\
Charlie         &   1.0 &   0.953917 \\
Charline        &   1.0 &   0.333333 \\
Charlisa        &   1.0 &   1.000000 \\
Charlise        &   1.0 &   0.800000 \\
Charlize        &   1.0 &   0.181818 \\
Charlotte       &   1.0 &   0.761871 \\
Charly          &   1.0 &   0.600000 \\
Charlyn         &   1.0 &   0.555556 \\
Charlyne        &   1.0 &   0.833333 \\
Charlynn        &   1.0 &   1.000000 \\
Charm           &   1.0 &   0.555556 \\
Charmain        &   1.0 &   0.714286 \\
Charmaine       &   1.0 &   0.108696 \\
Charmane        &   1.0 &   1.000000 \\
Charmayne       &   1.0 &   0.714286 \\
Charmian        &   1.0 &   1.000000 \\
Charnae         &   1.0 &   1.000000 \\
Charnesha       &   1.0 &   1.000000 \\
Charolette      &   1.0 &   0.714286 \\
Charon          &   1.0 &   0.666667 \\
Charvi          &   1.0 &   0.714286 \\
Charyl          &   1.0 &   0.833333 \\
Chase           &   1.0 &   0.214286 \\
Chasity         &   1.0 &   0.217391 \\
Chassidy        &   1.0 &   0.714286 \\
Chastelin       &   1.0 &   1.000000 \\
Chastelyn       &   1.0 &   1.000000 \\
Chastity        &   1.0 &   0.250000 \\
Chau            &   1.0 &   0.555556 \\
Chaundra        &   1.0 &   1.000000 \\
Chaunte         &   1.0 &   1.000000 \\
Chauntel        &   1.0 &   1.000000 \\
Chava           &   1.0 &   1.000000 \\
Chavon          &   1.0 &   0.428571 \\
Chavonne        &   1.0 &   0.714286 \\
Chaya           &   1.0 &   0.611111 \\
Chayanne        &   1.0 &   0.625000 \\
Chayla          &   1.0 &   1.000000 \\
Chayse          &   1.0 &   0.666667 \\
Chaz            &   1.0 &   1.000000 \\
Che             &   1.0 &   1.000000 \\
Chee            &   1.0 &   0.818182 \\
Chekesha        &   1.0 &   0.555556 \\
Chela           &   1.0 &   1.000000 \\
Chelby          &   1.0 &   1.000000 \\
Chelcie         &   1.0 &   0.600000 \\
Chelcy          &   1.0 &   1.000000 \\
Chelesa         &   1.0 &   1.000000 \\
Chelle          &   1.0 &   1.000000 \\
Chelsa          &   1.0 &   0.461538 \\
Chelse          &   1.0 &   1.000000 \\
Chelsea         &   1.0 &   0.050576 \\
Chelsee         &   1.0 &   0.250000 \\
Chelsey         &   1.0 &   0.024390 \\
Chelsi          &   1.0 &   0.178571 \\
Chelsie         &   1.0 &   0.070588 \\
Chelsy          &   1.0 &   0.238095 \\
Chenay          &   1.0 &   1.000000 \\
Chenelle        &   1.0 &   0.833333 \\
Cheng           &   1.0 &   1.000000 \\
Chenin          &   1.0 &   1.000000 \\
Chenoa          &   1.0 &   0.461538 \\
Cher            &   1.0 &   0.238095 \\
Chere           &   1.0 &   0.384615 \\
Cheree          &   1.0 &   0.333333 \\
Cherelle        &   1.0 &   0.352941 \\
Cherene         &   1.0 &   1.000000 \\
Cherese         &   1.0 &   1.000000 \\
Cheri           &   1.0 &   0.029412 \\
Cherice         &   1.0 &   0.333333 \\
Cherie          &   1.0 &   0.036232 \\
Cherika         &   1.0 &   1.000000 \\
Cherilyn        &   1.0 &   0.250000 \\
Cherisa         &   1.0 &   1.000000 \\
Cherise         &   1.0 &   0.166667 \\
Cherish         &   1.0 &   0.163265 \\
Cherisse        &   1.0 &   0.555556 \\
Cherlene        &   1.0 &   1.000000 \\
Cherly          &   1.0 &   1.000000 \\
Cherlyn         &   1.0 &   0.416667 \\
Cherokee        &   1.0 &   0.357143 \\
Cheron          &   1.0 &   1.000000 \\
Cherrell        &   1.0 &   1.000000 \\
Cherrelle       &   1.0 &   0.333333 \\
Cherri          &   1.0 &   0.208333 \\
Cherrie         &   1.0 &   0.263158 \\
Cherrish        &   1.0 &   0.833333 \\
Cherry          &   1.0 &   0.208333 \\
Cherryl         &   1.0 &   0.529412 \\
Chery           &   1.0 &   1.000000 \\
Cheryal         &   1.0 &   1.000000 \\
Cheryl          &   1.0 &   0.003819 \\
Cheryle         &   1.0 &   0.135135 \\
Cherylene       &   1.0 &   1.000000 \\
Cheryll         &   1.0 &   0.400000 \\
Chesney         &   1.0 &   0.833333 \\
Chessa          &   1.0 &   1.000000 \\
Chevelle        &   1.0 &   0.500000 \\
Chevon          &   1.0 &   1.000000 \\
Chevonne        &   1.0 &   0.900000 \\
Chevy           &   1.0 &   1.000000 \\
Cheyann         &   1.0 &   0.384615 \\
Cheyanna        &   1.0 &   0.555556 \\
Cheyanne        &   1.0 &   0.097345 \\
Cheyene         &   1.0 &   0.625000 \\
Cheyenna        &   1.0 &   0.714286 \\
Cheyenne        &   1.0 &   0.083955 \\
Chi             &   1.0 &   0.357143 \\
Chia            &   1.0 &   0.384615 \\
Chiamaka        &   1.0 &   1.000000 \\
Chiana          &   1.0 &   1.000000 \\
Chiara          &   1.0 &   0.590909 \\
Chidera         &   1.0 &   1.000000 \\
Chieko          &   1.0 &   0.625000 \\
Chihiro         &   1.0 &   1.000000 \\
Chika           &   1.0 &   1.000000 \\
Chimamanda      &   1.0 &   0.714286 \\
Chimene         &   1.0 &   1.000000 \\
Chimere         &   1.0 &   1.000000 \\
China           &   1.0 &   0.192308 \\
Chinyere        &   1.0 &   1.000000 \\
Chioma          &   1.0 &   0.875000 \\
Chiquita        &   1.0 &   1.000000 \\
Chirstina       &   1.0 &   0.833333 \\
Chisa           &   1.0 &   1.000000 \\
Chisom          &   1.0 &   0.714286 \\
Chistina        &   1.0 &   0.875000 \\
Chiyeko         &   1.0 &   0.625000 \\
Chiyo           &   1.0 &   1.000000 \\
Chiyoko         &   1.0 &   0.294118 \\
Chizara         &   1.0 &   1.000000 \\
Chizaram        &   1.0 &   0.857143 \\
Chizuko         &   1.0 &   0.454545 \\
Chloe           &   1.0 &   0.557257 \\
Chloee          &   1.0 &   1.000000 \\
Chloey          &   1.0 &   0.857143 \\
Chloie          &   1.0 &   0.500000 \\
Chole           &   1.0 &   0.357143 \\
Chong           &   1.0 &   0.888889 \\
Choua           &   1.0 &   0.466667 \\
Chris           &   1.0 &   0.025907 \\
Chrisette       &   1.0 &   1.000000 \\
Chrissie        &   1.0 &   0.625000 \\
Chrissy         &   1.0 &   0.125000 \\
Christa         &   1.0 &   0.036496 \\
Christabel      &   1.0 &   0.833333 \\
Christabella    &   1.0 &   1.000000 \\
Christabelle    &   1.0 &   0.833333 \\
Christal        &   1.0 &   0.170732 \\
Christan        &   1.0 &   1.000000 \\
Christeen       &   1.0 &   0.625000 \\
Christel        &   1.0 &   0.277778 \\
Christelle      &   1.0 &   0.857143 \\
Christen        &   1.0 &   0.117647 \\
Christena       &   1.0 &   1.000000 \\
Christene       &   1.0 &   0.333333 \\
Christi         &   1.0 &   0.064103 \\
Christian       &   1.0 &   0.036364 \\
Christiana      &   1.0 &   0.086207 \\
Christiane      &   1.0 &   0.571429 \\
Christianna     &   1.0 &   0.461538 \\
Christianne     &   1.0 &   0.416667 \\
Christie        &   1.0 &   0.024876 \\
Christin        &   1.0 &   0.089286 \\
Christina       &   1.0 &   0.020791 \\
Christinamarie  &   1.0 &   1.000000 \\
Christine       &   1.0 &   0.020145 \\
Christinejoy    &   1.0 &   1.000000 \\
Christinia      &   1.0 &   1.000000 \\
Christle        &   1.0 &   1.000000 \\
Christna        &   1.0 &   1.000000 \\
Christol        &   1.0 &   1.000000 \\
Christopher     &   1.0 &   0.115385 \\
Christy         &   1.0 &   0.015385 \\
Christyn        &   1.0 &   1.000000 \\
Christyna       &   1.0 &   0.714286 \\
Christyne       &   1.0 &   1.000000 \\
Chrys           &   1.0 &   1.000000 \\
Chrysta         &   1.0 &   1.000000 \\
Chrystal        &   1.0 &   0.064103 \\
Chrystie        &   1.0 &   1.000000 \\
Chrystina       &   1.0 &   0.600000 \\
Chrystle        &   1.0 &   1.000000 \\
Chua            &   1.0 &   1.000000 \\
Chyann          &   1.0 &   0.833333 \\
Chyanne         &   1.0 &   0.333333 \\
Chyenne         &   1.0 &   0.411765 \\
Chyna           &   1.0 &   0.235294 \\
Chynna          &   1.0 &   0.312500 \\
Chyrl           &   1.0 &   1.000000 \\
Ciana           &   1.0 &   0.409091 \\
Cianna          &   1.0 &   0.225806 \\
Ciara           &   1.0 &   0.061947 \\
Ciarah          &   1.0 &   1.000000 \\
Ciarra          &   1.0 &   0.333333 \\
Cicely          &   1.0 &   0.227273 \\
Cici            &   1.0 &   1.000000 \\
Ciclali         &   1.0 &   1.000000 \\
Ciclaly         &   1.0 &   1.000000 \\
Cidney          &   1.0 &   0.555556 \\
Ciel            &   1.0 &   0.416667 \\
Ciela           &   1.0 &   0.833333 \\
Ciella          &   1.0 &   1.000000 \\
Cielo           &   1.0 &   0.923913 \\
Ciena           &   1.0 &   0.600000 \\
Cienna          &   1.0 &   0.408163 \\
Ciera           &   1.0 &   0.096774 \\
Cierra          &   1.0 &   0.031847 \\
Cierrah         &   1.0 &   1.000000 \\
Ciji            &   1.0 &   0.388889 \\
Cilicia         &   1.0 &   1.000000 \\
Cina            &   1.0 &   1.000000 \\
Cinamon         &   1.0 &   1.000000 \\
Cinda           &   1.0 &   0.428571 \\
Cinde           &   1.0 &   1.000000 \\
Cindee          &   1.0 &   0.833333 \\
Cinderella      &   1.0 &   1.000000 \\
Cindi           &   1.0 &   0.092308 \\
Cindia          &   1.0 &   0.625000 \\
Cindie          &   1.0 &   0.428571 \\
Cindra          &   1.0 &   0.875000 \\
Cindy           &   1.0 &   0.009206 \\
Cinnamon        &   1.0 &   0.192308 \\
Cinthia         &   1.0 &   0.055556 \\
Cinthya         &   1.0 &   0.290323 \\
Cintia          &   1.0 &   0.178571 \\
Cipriana        &   1.0 &   1.000000 \\
Cira            &   1.0 &   1.000000 \\
Circe           &   1.0 &   0.800000 \\
Cirilla         &   1.0 &   1.000000 \\
Citlali         &   1.0 &   0.228070 \\
Citlalic        &   1.0 &   0.545455 \\
Citlalli        &   1.0 &   0.054878 \\
Citlally        &   1.0 &   0.150000 \\
Citlaly         &   1.0 &   0.073171 \\
Clair           &   1.0 &   0.600000 \\
Claira          &   1.0 &   0.600000 \\
Claire          &   1.0 &   0.466454 \\
Clairissa       &   1.0 &   1.000000 \\
Clara           &   1.0 &   0.762295 \\
Clarabelle      &   1.0 &   1.000000 \\
Clare           &   1.0 &   0.462963 \\
Claribel        &   1.0 &   0.387097 \\
Clarice         &   1.0 &   0.185185 \\
Clarie          &   1.0 &   1.000000 \\
Clarinda        &   1.0 &   1.000000 \\
Claris          &   1.0 &   1.000000 \\
Clarisa         &   1.0 &   0.052632 \\
Clarise         &   1.0 &   0.714286 \\
Clarissa        &   1.0 &   0.173913 \\
Clarisse        &   1.0 &   0.500000 \\
Clarita         &   1.0 &   0.857143 \\
Clarity         &   1.0 &   0.700000 \\
Claritza        &   1.0 &   0.750000 \\
Clarivel        &   1.0 &   1.000000 \\
Clariza         &   1.0 &   0.625000 \\
Clarke          &   1.0 &   0.909091 \\
Clarrisa        &   1.0 &   1.000000 \\
Clarrissa       &   1.0 &   1.000000 \\
Claryssa        &   1.0 &   0.857143 \\
Claudette       &   1.0 &   0.111111 \\
Claudia         &   1.0 &   0.037627 \\
Claudina        &   1.0 &   1.000000 \\
Claudine        &   1.0 &   0.111111 \\
Claudio         &   1.0 &   1.000000 \\
Clea            &   1.0 &   1.000000 \\
Clementina      &   1.0 &   0.714286 \\
Clementine      &   1.0 &   0.833333 \\
Cleo            &   1.0 &   1.000000 \\
Cleopatra       &   1.0 &   0.818182 \\
Cleotilde       &   1.0 &   1.000000 \\
Cleta           &   1.0 &   0.500000 \\
Clifford        &   1.0 &   1.000000 \\
Clio            &   1.0 &   0.600000 \\
Cloe            &   1.0 &   0.217391 \\
Cloey           &   1.0 &   0.400000 \\
Cloie           &   1.0 &   0.714286 \\
Clorinda        &   1.0 &   1.000000 \\
Clorissa        &   1.0 &   1.000000 \\
Clotilde        &   1.0 &   0.714286 \\
Clover          &   1.0 &   0.609756 \\
Clowie          &   1.0 &   1.000000 \\
Cobie           &   1.0 &   1.000000 \\
Coco            &   1.0 &   0.741935 \\
Codi            &   1.0 &   0.500000 \\
Codie           &   1.0 &   0.700000 \\
Cody            &   1.0 &   0.225806 \\
Colbie          &   1.0 &   0.653846 \\
Colby           &   1.0 &   0.692308 \\
Cole            &   1.0 &   1.000000 \\
Coleen          &   1.0 &   0.070423 \\
Colene          &   1.0 &   1.000000 \\
Colette         &   1.0 &   0.895161 \\
Colin           &   1.0 &   1.000000 \\
Colleen         &   1.0 &   0.012766 \\
Collette        &   1.0 &   0.176471 \\
Collins         &   1.0 &   0.923077 \\
Columba         &   1.0 &   1.000000 \\
Comfort         &   1.0 &   1.000000 \\
Concepcion      &   1.0 &   0.172414 \\
Conception      &   1.0 &   0.714286 \\
Concetta        &   1.0 &   0.714286 \\
Concha          &   1.0 &   0.230769 \\
Conchita        &   1.0 &   0.625000 \\
Coni            &   1.0 &   1.000000 \\
Conni           &   1.0 &   0.750000 \\
Connie          &   1.0 &   0.014159 \\
Connor          &   1.0 &   0.538462 \\
Conny           &   1.0 &   1.000000 \\
Consepcion      &   1.0 &   1.000000 \\
Constance       &   1.0 &   0.043321 \\
Constanza       &   1.0 &   0.727273 \\
Consuela        &   1.0 &   0.555556 \\
Consuelo        &   1.0 &   0.045045 \\
Contessa        &   1.0 &   0.555556 \\
Cookie          &   1.0 &   0.875000 \\
Cooper          &   1.0 &   0.818182 \\
Cora            &   1.0 &   0.795775 \\
Corah           &   1.0 &   1.000000 \\
Coraima         &   1.0 &   0.054945 \\
Coral           &   1.0 &   0.392857 \\
Coralee         &   1.0 &   0.750000 \\
Coralie         &   1.0 &   0.541667 \\
Coraline        &   1.0 &   0.772727 \\
Coralyn         &   1.0 &   1.000000 \\
Coralynn        &   1.0 &   1.000000 \\
Corayma         &   1.0 &   0.727273 \\
Corazon         &   1.0 &   1.000000 \\
Corby           &   1.0 &   1.000000 \\
Cordelia        &   1.0 &   0.814815 \\
Coreen          &   1.0 &   0.529412 \\
Coreena         &   1.0 &   1.000000 \\
Corena          &   1.0 &   0.555556 \\
Corene          &   1.0 &   1.000000 \\
Coretta         &   1.0 &   0.636364 \\
Corey           &   1.0 &   0.104167 \\
Cori            &   1.0 &   0.181818 \\
Corie           &   1.0 &   0.172414 \\
Corin           &   1.0 &   0.294118 \\
Corina          &   1.0 &   0.053435 \\
Corine          &   1.0 &   0.238095 \\
Corinn          &   1.0 &   0.714286 \\
Corinna         &   1.0 &   0.250000 \\
Corinne         &   1.0 &   0.128000 \\
Corinthia       &   1.0 &   1.000000 \\
Corissa         &   1.0 &   0.500000 \\
Corliss         &   1.0 &   0.571429 \\
Cornelia        &   1.0 &   0.384615 \\
Corra           &   1.0 &   1.000000 \\
Correna         &   1.0 &   1.000000 \\
Correne         &   1.0 &   1.000000 \\
Corri           &   1.0 &   1.000000 \\
Corrie          &   1.0 &   0.131579 \\
Corrin          &   1.0 &   0.538462 \\
Corrina         &   1.0 &   0.106383 \\
Corrine         &   1.0 &   0.108696 \\
Corrinna        &   1.0 &   1.000000 \\
Corrinne        &   1.0 &   0.714286 \\
Corryn          &   1.0 &   0.875000 \\
Cortnee         &   1.0 &   0.625000 \\
Cortney         &   1.0 &   0.056180 \\
Cortni          &   1.0 &   1.000000 \\
Cortnie         &   1.0 &   0.750000 \\
Corvette        &   1.0 &   1.000000 \\
Cory            &   1.0 &   0.108696 \\
Coryn           &   1.0 &   0.625000 \\
Corynn          &   1.0 &   0.833333 \\
Cosette         &   1.0 &   0.636364 \\
Cosima          &   1.0 &   0.750000 \\
Coty            &   1.0 &   0.857143 \\
Coua            &   1.0 &   1.000000 \\
Courney         &   1.0 &   1.000000 \\
Courtenay       &   1.0 &   1.000000 \\
Courteney       &   1.0 &   0.238095 \\
Courtnee        &   1.0 &   0.461538 \\
Courtney        &   1.0 &   0.007576 \\
Courtni         &   1.0 &   0.833333 \\
Courtnie        &   1.0 &   0.277778 \\
Courtny         &   1.0 &   1.000000 \\
Cozette         &   1.0 &   0.833333 \\
Craig           &   1.0 &   1.000000 \\
Cree            &   1.0 &   0.500000 \\
Crimson         &   1.0 &   1.000000 \\
Cris            &   1.0 &   0.500000 \\
Criselda        &   1.0 &   1.000000 \\
Crisol          &   1.0 &   0.454545 \\
Crissy          &   1.0 &   0.350000 \\
Crista          &   1.0 &   0.185185 \\
Cristabel       &   1.0 &   1.000000 \\
Cristal         &   1.0 &   0.032491 \\
Cristalle       &   1.0 &   1.000000 \\
Cristel         &   1.0 &   0.227273 \\
Cristela        &   1.0 &   1.000000 \\
Cristen         &   1.0 &   0.500000 \\
Cristi          &   1.0 &   0.350000 \\
Cristian        &   1.0 &   0.142857 \\
Cristiana       &   1.0 &   0.555556 \\
Cristie         &   1.0 &   0.384615 \\
Cristin         &   1.0 &   0.200000 \\
Cristina        &   1.0 &   0.050794 \\
Cristine        &   1.0 &   0.166667 \\
Cristy          &   1.0 &   0.125000 \\
Cristyn         &   1.0 &   1.000000 \\
Crosby          &   1.0 &   1.000000 \\
Cruz            &   1.0 &   0.294118 \\
Crysta          &   1.0 &   0.291667 \\
Crystal         &   1.0 &   0.020106 \\
Crystall        &   1.0 &   1.000000 \\
Crystalyn       &   1.0 &   0.750000 \\
Crystel         &   1.0 &   0.700000 \\
Crystelle       &   1.0 &   1.000000 \\
Crystina        &   1.0 &   0.636364 \\
Crystle         &   1.0 &   0.333333 \\
Curtis          &   1.0 &   1.000000 \\
Cyan            &   1.0 &   0.384615 \\
Cyana           &   1.0 &   1.000000 \\
Cyann           &   1.0 &   1.000000 \\
Cyanne          &   1.0 &   1.000000 \\
Cybele          &   1.0 &   1.000000 \\
Cybil           &   1.0 &   0.833333 \\
Cyd             &   1.0 &   1.000000 \\
Cydnee          &   1.0 &   1.000000 \\
Cydney          &   1.0 &   0.240000 \\
Cyerra          &   1.0 &   0.833333 \\
Cymone          &   1.0 &   1.000000 \\
Cynde           &   1.0 &   1.000000 \\
Cyndee          &   1.0 &   0.428571 \\
Cyndi           &   1.0 &   0.142857 \\
Cyndie          &   1.0 &   0.461538 \\
Cyndle          &   1.0 &   1.000000 \\
Cyndy           &   1.0 &   0.300000 \\
Cyntha          &   1.0 &   1.000000 \\
Cynthia         &   1.0 &   0.013992 \\
Cynthya         &   1.0 &   1.000000 \\
Cyntia          &   1.0 &   0.714286 \\
Cypress         &   1.0 &   0.600000 \\
Cyra            &   1.0 &   0.625000 \\
Cyrena          &   1.0 &   0.714286 \\
Cyrene          &   1.0 &   1.000000 \\
Cyrine          &   1.0 &   1.000000 \\
Cyrstal         &   1.0 &   0.384615 \\
Cystal          &   1.0 &   1.000000 \\
Cytlaly         &   1.0 &   1.000000 \\
Czarina         &   1.0 &   1.000000 \\
Dacia           &   1.0 &   0.555556 \\
Daeja           &   1.0 &   0.555556 \\
Daejah          &   1.0 &   1.000000 \\
Daelyn          &   1.0 &   0.500000 \\
Daelynn         &   1.0 &   0.875000 \\
Daena           &   1.0 &   0.833333 \\
Daenerys        &   1.0 &   0.727273 \\
Daesha          &   1.0 &   1.000000 \\
Daffne          &   1.0 &   1.000000 \\
Dafina          &   1.0 &   1.000000 \\
Dafne           &   1.0 &   0.360000 \\
Dafnee          &   1.0 &   1.000000 \\
Dafney          &   1.0 &   1.000000 \\
Dagmar          &   1.0 &   1.000000 \\
Dagny           &   1.0 &   0.357143 \\
Dahlia          &   1.0 &   1.000000 \\
Dahlila         &   1.0 &   0.555556 \\
Daiana          &   1.0 &   0.333333 \\
Daicy           &   1.0 &   1.000000 \\
Daija           &   1.0 &   0.192308 \\
Daijah          &   1.0 &   0.380952 \\
Daila           &   1.0 &   1.000000 \\
Dailani         &   1.0 &   1.000000 \\
Dailany         &   1.0 &   1.000000 \\
Dailin          &   1.0 &   1.000000 \\
Dailyn          &   1.0 &   0.687500 \\
Dailynn         &   1.0 &   1.000000 \\
Daina           &   1.0 &   0.500000 \\
Daira           &   1.0 &   0.156250 \\
Daisey          &   1.0 &   0.250000 \\
Daisha          &   1.0 &   0.375000 \\
Daisi           &   1.0 &   1.000000 \\
Daisia          &   1.0 &   1.000000 \\
Daisie          &   1.0 &   0.625000 \\
Daissy          &   1.0 &   0.714286 \\
Daisy           &   1.0 &   0.279399 \\
Daizhane        &   1.0 &   1.000000 \\
Daizy           &   1.0 &   0.437500 \\
Daja            &   1.0 &   0.736842 \\
Dajah           &   1.0 &   0.250000 \\
Dajanae         &   1.0 &   0.384615 \\
Dajia           &   1.0 &   1.000000 \\
Dakoda          &   1.0 &   0.714286 \\
Dakota          &   1.0 &   0.647059 \\
Dakotah         &   1.0 &   0.666667 \\
Daksha          &   1.0 &   1.000000 \\
Dalana          &   1.0 &   1.000000 \\
Dalani          &   1.0 &   1.000000 \\
Dalarie         &   1.0 &   1.000000 \\
Dalary          &   1.0 &   0.620690 \\
Dalayza         &   1.0 &   1.000000 \\
Dale            &   1.0 &   0.073529 \\
Daleah          &   1.0 &   1.000000 \\
Dalena          &   1.0 &   0.277778 \\
Dalene          &   1.0 &   0.357143 \\
Dalet           &   1.0 &   1.000000 \\
Daleth          &   1.0 &   0.750000 \\
Dalett          &   1.0 &   1.000000 \\
Dalette         &   1.0 &   1.000000 \\
Dalexa          &   1.0 &   0.714286 \\
Daleysa         &   1.0 &   0.181818 \\
Daleyssa        &   1.0 &   0.857143 \\
Daleyza         &   1.0 &   0.496104 \\
Dalia           &   1.0 &   0.363636 \\
Daliah          &   1.0 &   0.400000 \\
Dalida          &   1.0 &   1.000000 \\
Dalila          &   1.0 &   0.339286 \\
Dalilah         &   1.0 &   0.468750 \\
Dalina          &   1.0 &   0.700000 \\
Dalisa          &   1.0 &   1.000000 \\
Dalisha         &   1.0 &   1.000000 \\
Daliyah         &   1.0 &   0.857143 \\
Dallana         &   1.0 &   0.357143 \\
Dallanara       &   1.0 &   1.000000 \\
Dallas          &   1.0 &   0.482143 \\
Dallis          &   1.0 &   1.000000 \\
Dalya           &   1.0 &   1.000000 \\
Dalyla          &   1.0 &   0.454545 \\
Dalylah         &   1.0 &   0.875000 \\
Damali          &   1.0 &   1.000000 \\
Damara          &   1.0 &   1.000000 \\
Damaris         &   1.0 &   0.194444 \\
Damariz         &   1.0 &   0.555556 \\
Damary          &   1.0 &   1.000000 \\
Damarys         &   1.0 &   1.000000 \\
Damia           &   1.0 &   1.000000 \\
Damian          &   1.0 &   1.000000 \\
Damiana         &   1.0 &   0.625000 \\
Damisha         &   1.0 &   1.000000 \\
Damita          &   1.0 &   0.833333 \\
Damiyah         &   1.0 &   1.000000 \\
Damon           &   1.0 &   1.000000 \\
Damya           &   1.0 &   0.750000 \\
Dana            &   1.0 &   0.063559 \\
Danae           &   1.0 &   0.320000 \\
Danah           &   1.0 &   1.000000 \\
Danai           &   1.0 &   1.000000 \\
Danaly          &   1.0 &   1.000000 \\
Danay           &   1.0 &   0.714286 \\
Danaya          &   1.0 &   1.000000 \\
Danee           &   1.0 &   0.714286 \\
Daneen          &   1.0 &   0.411765 \\
Daneille        &   1.0 &   1.000000 \\
Daneisha        &   1.0 &   1.000000 \\
Danell          &   1.0 &   0.500000 \\
Danelle         &   1.0 &   0.131579 \\
Danelly         &   1.0 &   0.285714 \\
Danely          &   1.0 &   0.416667 \\
Danesha         &   1.0 &   1.000000 \\
Danessa         &   1.0 &   0.600000 \\
Danett          &   1.0 &   1.000000 \\
Danetta         &   1.0 &   1.000000 \\
Danette         &   1.0 &   0.120000 \\
Dani            &   1.0 &   1.000000 \\
Dania           &   1.0 &   0.646341 \\
Daniah          &   1.0 &   1.000000 \\
Danica          &   1.0 &   0.162651 \\
Danice          &   1.0 &   1.000000 \\
Daniel          &   1.0 &   0.089286 \\
Daniela         &   1.0 &   0.276451 \\
Daniele         &   1.0 &   0.526316 \\
Daniell         &   1.0 &   0.500000 \\
Daniella        &   1.0 &   0.443089 \\
Danielle        &   1.0 &   0.042564 \\
Daniesha        &   1.0 &   1.000000 \\
Danika          &   1.0 &   0.190476 \\
Danille         &   1.0 &   0.714286 \\
Danilynn        &   1.0 &   1.000000 \\
Danine          &   1.0 &   1.000000 \\
Danise          &   1.0 &   0.625000 \\
Danisha         &   1.0 &   0.294118 \\
Danita          &   1.0 &   0.178571 \\
Danity          &   1.0 &   0.833333 \\
Danitza         &   1.0 &   0.529412 \\
Daniya          &   1.0 &   1.000000 \\
Daniyah         &   1.0 &   0.750000 \\
Dann            &   1.0 &   0.714286 \\
Danna           &   1.0 &   0.454839 \\
Dannae          &   1.0 &   1.000000 \\
Dannah          &   1.0 &   1.000000 \\
Danne           &   1.0 &   1.000000 \\
Dannell         &   1.0 &   1.000000 \\
Dannelle        &   1.0 &   1.000000 \\
Dannette        &   1.0 &   0.300000 \\
Danni           &   1.0 &   0.375000 \\
Dannia          &   1.0 &   0.714286 \\
Dannica         &   1.0 &   1.000000 \\
Dannie          &   1.0 &   1.000000 \\
Danniela        &   1.0 &   1.000000 \\
Dannielle       &   1.0 &   0.208333 \\
Dannika         &   1.0 &   0.857143 \\
Danny           &   1.0 &   0.625000 \\
Dannya          &   1.0 &   1.000000 \\
Dany            &   1.0 &   0.875000 \\
Danya           &   1.0 &   0.400000 \\
Danyale         &   1.0 &   0.833333 \\
Danyel          &   1.0 &   0.384615 \\
Danyela         &   1.0 &   1.000000 \\
Danyell         &   1.0 &   0.857143 \\
Danyelle        &   1.0 &   0.307692 \\
Dao             &   1.0 &   1.000000 \\
Daphane         &   1.0 &   1.000000 \\
Daphine         &   1.0 &   1.000000 \\
Daphne          &   1.0 &   0.936306 \\
Daphnee         &   1.0 &   0.857143 \\
Daphney         &   1.0 &   0.416667 \\
Daphnie         &   1.0 &   0.555556 \\
Dara            &   1.0 &   0.571429 \\
Darah           &   1.0 &   1.000000 \\
Darbie          &   1.0 &   1.000000 \\
Darby           &   1.0 &   0.195122 \\
Darcel          &   1.0 &   0.714286 \\
Darcell         &   1.0 &   1.000000 \\
Darcey          &   1.0 &   0.700000 \\
Darci           &   1.0 &   0.218750 \\
Darcie          &   1.0 &   0.250000 \\
Darcy           &   1.0 &   0.159574 \\
Darely          &   1.0 &   1.000000 \\
Daria           &   1.0 &   0.562500 \\
Darian          &   1.0 &   0.096386 \\
Dariana         &   1.0 &   0.340909 \\
Darianna        &   1.0 &   0.583333 \\
Darice          &   1.0 &   0.625000 \\
Dariela         &   1.0 &   1.000000 \\
Darien          &   1.0 &   0.428571 \\
Darienne        &   1.0 &   1.000000 \\
Darilyn         &   1.0 &   1.000000 \\
Darin           &   1.0 &   1.000000 \\
Darina          &   1.0 &   0.636364 \\
Darion          &   1.0 &   0.714286 \\
Dariya          &   1.0 &   1.000000 \\
Darla           &   1.0 &   0.194444 \\
Darlah          &   1.0 &   0.625000 \\
Darlean         &   1.0 &   1.000000 \\
Darleen         &   1.0 &   0.250000 \\
Darlena         &   1.0 &   0.750000 \\
Darlene         &   1.0 &   0.058182 \\
Darleth         &   1.0 &   1.000000 \\
Darlette        &   1.0 &   1.000000 \\
Darlin          &   1.0 &   0.636364 \\
Darline         &   1.0 &   0.208333 \\
Darling         &   1.0 &   0.700000 \\
Darlyn          &   1.0 &   0.684211 \\
Darlyne         &   1.0 &   0.625000 \\
Darlynn         &   1.0 &   0.714286 \\
Darnell         &   1.0 &   0.500000 \\
Darnesha        &   1.0 &   1.000000 \\
Darnisha        &   1.0 &   0.416667 \\
Darrah          &   1.0 &   1.000000 \\
Darrell         &   1.0 &   1.000000 \\
Darrian         &   1.0 &   0.375000 \\
Darryl          &   1.0 &   0.714286 \\
Darya           &   1.0 &   0.611111 \\
Daryana         &   1.0 &   1.000000 \\
Daryl           &   1.0 &   0.333333 \\
Daryn           &   1.0 &   0.555556 \\
Dasani          &   1.0 &   0.555556 \\
Dasha           &   1.0 &   0.218750 \\
Dashanique      &   1.0 &   1.000000 \\
Dashawn         &   1.0 &   1.000000 \\
Dasia           &   1.0 &   0.421053 \\
Daun            &   1.0 &   1.000000 \\
Dava            &   1.0 &   0.833333 \\
Davette         &   1.0 &   1.000000 \\
Davey           &   1.0 &   1.000000 \\
Davi            &   1.0 &   1.000000 \\
Davia           &   1.0 &   1.000000 \\
Daviana         &   1.0 &   0.750000 \\
Davianna        &   1.0 &   0.833333 \\
David           &   1.0 &   0.102041 \\
Davida          &   1.0 &   0.500000 \\
Davie           &   1.0 &   1.000000 \\
Davina          &   1.0 &   0.887640 \\
Davinah         &   1.0 &   1.000000 \\
Davisha         &   1.0 &   1.000000 \\
Davonna         &   1.0 &   0.714286 \\
Davy            &   1.0 &   1.000000 \\
Dawana          &   1.0 &   1.000000 \\
Dawn            &   1.0 &   0.010363 \\
Dawna           &   1.0 &   0.156863 \\
Dawne           &   1.0 &   0.545455 \\
Dawnell         &   1.0 &   1.000000 \\
Dawnelle        &   1.0 &   0.750000 \\
Dawnette        &   1.0 &   0.416667 \\
Dawnielle       &   1.0 &   0.714286 \\
Dawnn           &   1.0 &   1.000000 \\
Dawson          &   1.0 &   1.000000 \\
Daya            &   1.0 &   0.777778 \\
Dayami          &   1.0 &   0.115385 \\
Dayamy          &   1.0 &   1.000000 \\
Dayan           &   1.0 &   1.000000 \\
Dayana          &   1.0 &   0.553299 \\
Dayanara        &   1.0 &   0.052632 \\
Dayani          &   1.0 &   0.461538 \\
Dayanira        &   1.0 &   1.000000 \\
Dayanna         &   1.0 &   0.203704 \\
Dayannara       &   1.0 &   0.833333 \\
Dayanne         &   1.0 &   1.000000 \\
Dayelin         &   1.0 &   1.000000 \\
Dayja           &   1.0 &   0.833333 \\
Dayjah          &   1.0 &   1.000000 \\
Dayla           &   1.0 &   0.727273 \\
Daylani         &   1.0 &   0.777778 \\
Dayle           &   1.0 &   0.416667 \\
Dayleen         &   1.0 &   1.000000 \\
Daylene         &   1.0 &   1.000000 \\
Daylin          &   1.0 &   0.958333 \\
Dayna           &   1.0 &   0.163636 \\
Dayra           &   1.0 &   0.500000 \\
Dayrin          &   1.0 &   0.545455 \\
Daysha          &   1.0 &   0.636364 \\
Daysi           &   1.0 &   0.275862 \\
Daysia          &   1.0 &   0.714286 \\
Daysy           &   1.0 &   0.500000 \\
Dazhane         &   1.0 &   1.000000 \\
De              &   1.0 &   1.000000 \\
Dea             &   1.0 &   0.500000 \\
Deadra          &   1.0 &   0.833333 \\
Dean            &   1.0 &   1.000000 \\
Deana           &   1.0 &   0.037975 \\
Deandra         &   1.0 &   0.172414 \\
Deandrea        &   1.0 &   0.833333 \\
Deane           &   1.0 &   1.000000 \\
Deangela        &   1.0 &   1.000000 \\
Deanie          &   1.0 &   1.000000 \\
Deann           &   1.0 &   0.112903 \\
Deanna          &   1.0 &   0.008446 \\
Deanne          &   1.0 &   0.050000 \\
Deasia          &   1.0 &   0.666667 \\
Deatrice        &   1.0 &   1.000000 \\
Deb             &   1.0 &   1.000000 \\
Debanhi         &   1.0 &   1.000000 \\
Debanhy         &   1.0 &   1.000000 \\
Debbe           &   1.0 &   1.000000 \\
Debbi           &   1.0 &   0.108696 \\
Debbie          &   1.0 &   0.002815 \\
Debborah        &   1.0 &   0.857143 \\
Debbra          &   1.0 &   0.138889 \\
Debby           &   1.0 &   0.068966 \\
Debera          &   1.0 &   0.388889 \\
Deberah         &   1.0 &   0.625000 \\
Debhora         &   1.0 &   1.000000 \\
Debi            &   1.0 &   0.058824 \\
Debie           &   1.0 &   1.000000 \\
Debora          &   1.0 &   0.044715 \\
Deborah         &   1.0 &   0.009852 \\
Deboraha        &   1.0 &   1.000000 \\
Deborha         &   1.0 &   0.333333 \\
Deborra         &   1.0 &   0.833333 \\
Deborrah        &   1.0 &   0.500000 \\
Debra           &   1.0 &   0.001260 \\
Debrah          &   1.0 &   0.208333 \\
Debroah         &   1.0 &   1.000000 \\
Deby            &   1.0 &   0.833333 \\
December        &   1.0 &   0.833333 \\
Decklyn         &   1.0 &   1.000000 \\
Declan          &   1.0 &   1.000000 \\
Dede            &   1.0 &   0.250000 \\
Dedra           &   1.0 &   0.357143 \\
Dee             &   1.0 &   0.071429 \\
Deeana          &   1.0 &   1.000000 \\
Deeann          &   1.0 &   0.238095 \\
Deeanna         &   1.0 &   0.384615 \\
Deeanne         &   1.0 &   0.857143 \\
Deedee          &   1.0 &   0.250000 \\
Deedra          &   1.0 &   0.500000 \\
Deeksha         &   1.0 &   0.833333 \\
Deema           &   1.0 &   1.000000 \\
Deena           &   1.0 &   0.059406 \\
Deepa           &   1.0 &   1.000000 \\
Deepika         &   1.0 &   1.000000 \\
Deethya         &   1.0 &   1.000000 \\
Deetta          &   1.0 &   1.000000 \\
Deette          &   1.0 &   0.833333 \\
Deetya          &   1.0 &   1.000000 \\
Deeya           &   1.0 &   1.000000 \\
Defne           &   1.0 &   0.500000 \\
Deicy           &   1.0 &   0.714286 \\
Deidra          &   1.0 &   0.307692 \\
Deidre          &   1.0 &   0.139535 \\
Deija           &   1.0 &   0.555556 \\
Deilany         &   1.0 &   1.000000 \\
Deirdra         &   1.0 &   1.000000 \\
Deirdre         &   1.0 &   0.078125 \\
Deise           &   1.0 &   1.000000 \\
Deisi           &   1.0 &   0.200000 \\
Deisy           &   1.0 &   0.133333 \\
Deitra          &   1.0 &   0.714286 \\
Deja            &   1.0 &   0.062500 \\
Dejah           &   1.0 &   0.161290 \\
Dejanae         &   1.0 &   0.277778 \\
Dejanay         &   1.0 &   1.000000 \\
Dejane          &   1.0 &   1.000000 \\
Dejanee         &   1.0 &   0.500000 \\
Dejanique       &   1.0 &   0.833333 \\
Dejia           &   1.0 &   1.000000 \\
Delaila         &   1.0 &   0.461538 \\
Delailah        &   1.0 &   0.750000 \\
Delaina         &   1.0 &   0.833333 \\
Delaine         &   1.0 &   0.500000 \\
Delainey        &   1.0 &   0.625000 \\
Delana          &   1.0 &   0.384615 \\
Delaney         &   1.0 &   0.416667 \\
Delani          &   1.0 &   1.000000 \\
Delanie         &   1.0 &   0.555556 \\
Delany          &   1.0 &   1.000000 \\
Delara          &   1.0 &   1.000000 \\
Delayla         &   1.0 &   1.000000 \\
Delayna         &   1.0 &   0.833333 \\
Delaynie        &   1.0 &   1.000000 \\
Delayza         &   1.0 &   1.000000 \\
Delena          &   1.0 &   0.500000 \\
Delene          &   1.0 &   0.857143 \\
Deleyza         &   1.0 &   1.000000 \\
Delfina         &   1.0 &   0.352941 \\
Delia           &   1.0 &   0.144231 \\
Delicia         &   1.0 &   0.416667 \\
Delila          &   1.0 &   0.250000 \\
Delilah         &   1.0 &   0.797297 \\
Delina          &   1.0 &   0.636364 \\
Delinah         &   1.0 &   1.000000 \\
Delinda         &   1.0 &   0.636364 \\
Delisa          &   1.0 &   0.416667 \\
Delise          &   1.0 &   1.000000 \\
Dell            &   1.0 &   1.000000 \\
Della           &   1.0 &   0.276923 \\
Delma           &   1.0 &   0.555556 \\
Delmi           &   1.0 &   0.714286 \\
Delmy           &   1.0 &   0.357143 \\
Delois          &   1.0 &   0.625000 \\
Delora          &   1.0 &   0.833333 \\
Delores         &   1.0 &   0.078947 \\
Deloris         &   1.0 &   0.206897 \\
Delphia         &   1.0 &   0.833333 \\
Delphina        &   1.0 &   1.000000 \\
Delphine        &   1.0 &   0.454545 \\
Delsa           &   1.0 &   1.000000 \\
Delta           &   1.0 &   1.000000 \\
Delyla          &   1.0 &   0.600000 \\
Delylah         &   1.0 &   0.344828 \\
Delynn          &   1.0 &   0.833333 \\
Demarie         &   1.0 &   1.000000 \\
Demaris         &   1.0 &   1.000000 \\
Demetra         &   1.0 &   0.500000 \\
Demetria        &   1.0 &   0.193548 \\
Demetrius       &   1.0 &   1.000000 \\
Demi            &   1.0 &   0.746988 \\
Demiana         &   1.0 &   1.000000 \\
Demitra         &   1.0 &   1.000000 \\
Demitria        &   1.0 &   0.750000 \\
Demiyah         &   1.0 &   1.000000 \\
Dempsey         &   1.0 &   1.000000 \\
Dena            &   1.0 &   0.032258 \\
Denae           &   1.0 &   0.300000 \\
Denali          &   1.0 &   0.916667 \\
Denay           &   1.0 &   1.000000 \\
Dene            &   1.0 &   1.000000 \\
Denean          &   1.0 &   0.666667 \\
Denee           &   1.0 &   0.416667 \\
Deneen          &   1.0 &   0.088608 \\
Denell          &   1.0 &   0.875000 \\
Denelle         &   1.0 &   0.625000 \\
Denene          &   1.0 &   0.833333 \\
Denese          &   1.0 &   0.500000 \\
Denesha         &   1.0 &   1.000000 \\
Denette         &   1.0 &   0.500000 \\
Denia           &   1.0 &   0.666667 \\
Denice          &   1.0 &   0.138462 \\
Deniece         &   1.0 &   0.888889 \\
Denielle        &   1.0 &   1.000000 \\
Denika          &   1.0 &   1.000000 \\
Denim           &   1.0 &   1.000000 \\
Denine          &   1.0 &   0.615385 \\
Denis           &   1.0 &   0.545455 \\
Denisa          &   1.0 &   1.000000 \\
Denise          &   1.0 &   0.021263 \\
Denisha         &   1.0 &   0.214286 \\
Denishia        &   1.0 &   1.000000 \\
Denisse         &   1.0 &   0.280899 \\
Denita          &   1.0 &   0.500000 \\
Deniz           &   1.0 &   1.000000 \\
Denna           &   1.0 &   0.545455 \\
Dennice         &   1.0 &   0.625000 \\
Dennis          &   1.0 &   0.600000 \\
Dennise         &   1.0 &   0.151515 \\
Denny           &   1.0 &   1.000000 \\
Denver          &   1.0 &   0.800000 \\
Denyce          &   1.0 &   1.000000 \\
Denys           &   1.0 &   0.625000 \\
Denyse          &   1.0 &   0.555556 \\
Deon            &   1.0 &   0.555556 \\
Deondra         &   1.0 &   0.857143 \\
Deonna          &   1.0 &   0.384615 \\
Der             &   1.0 &   0.416667 \\
Derek           &   1.0 &   0.857143 \\
Derinda         &   1.0 &   1.000000 \\
Derricka        &   1.0 &   1.000000 \\
Derrisha        &   1.0 &   1.000000 \\
Desarae         &   1.0 &   0.454545 \\
Desaray         &   1.0 &   0.833333 \\
Desaree         &   1.0 &   1.000000 \\
Deserae         &   1.0 &   0.555556 \\
Deseray         &   1.0 &   0.555556 \\
Desere          &   1.0 &   1.000000 \\
Deseree         &   1.0 &   0.294118 \\
Deserie         &   1.0 &   0.272727 \\
Desha           &   1.0 &   1.000000 \\
Deshanae        &   1.0 &   1.000000 \\
Deshawn         &   1.0 &   0.500000 \\
Deshawna        &   1.0 &   1.000000 \\
Deshonna        &   1.0 &   1.000000 \\
Desi            &   1.0 &   0.928571 \\
Desirae         &   1.0 &   0.060000 \\
Desiray         &   1.0 &   0.363636 \\
Desire          &   1.0 &   0.250000 \\
Desirea         &   1.0 &   0.357143 \\
Desiree         &   1.0 &   0.086059 \\
Desirey         &   1.0 &   0.600000 \\
Desirie         &   1.0 &   1.000000 \\
Desree          &   1.0 &   1.000000 \\
Dessa           &   1.0 &   1.000000 \\
Dessie          &   1.0 &   1.000000 \\
Destanee        &   1.0 &   0.857143 \\
Destani         &   1.0 &   0.555556 \\
Destanie        &   1.0 &   0.315789 \\
Destany         &   1.0 &   0.233333 \\
Desteni         &   1.0 &   1.000000 \\
Destenie        &   1.0 &   0.833333 \\
Desteny         &   1.0 &   0.098039 \\
Destine         &   1.0 &   0.555556 \\
Destinee        &   1.0 &   0.062500 \\
Destiney        &   1.0 &   0.122807 \\
Destini         &   1.0 &   0.166667 \\
Destinie        &   1.0 &   0.208333 \\
Destiny         &   1.0 &   0.117162 \\
Destynee        &   1.0 &   0.833333 \\
Destyni         &   1.0 &   0.555556 \\
Destynie        &   1.0 &   1.000000 \\
Detra           &   1.0 &   0.700000 \\
Deva            &   1.0 &   0.666667 \\
Devan           &   1.0 &   0.142857 \\
Devani          &   1.0 &   0.750000 \\
Devanie         &   1.0 &   1.000000 \\
Devany          &   1.0 &   0.700000 \\
Deven           &   1.0 &   0.454545 \\
Devera          &   1.0 &   1.000000 \\
Devi            &   1.0 &   0.625000 \\
Devika          &   1.0 &   0.714286 \\
Devin           &   1.0 &   0.101695 \\
Devina          &   1.0 &   0.642857 \\
Devine          &   1.0 &   0.500000 \\
Deviny          &   1.0 &   1.000000 \\
Devon           &   1.0 &   0.094891 \\
Devona          &   1.0 &   0.777778 \\
Devonna         &   1.0 &   0.857143 \\
Devonne         &   1.0 &   0.555556 \\
Devora          &   1.0 &   1.000000 \\
Devorah         &   1.0 &   0.625000 \\
Devra           &   1.0 &   1.000000 \\
Devyn           &   1.0 &   0.448980 \\
Devynn          &   1.0 &   0.500000 \\
Deyanira        &   1.0 &   0.078125 \\
Deysi           &   1.0 &   0.428571 \\
Deysy           &   1.0 &   0.714286 \\
Dezarae         &   1.0 &   0.625000 \\
Dezaray         &   1.0 &   1.000000 \\
Dezaree         &   1.0 &   1.000000 \\
Dezerae         &   1.0 &   1.000000 \\
Dezeray         &   1.0 &   1.000000 \\
Dezhane         &   1.0 &   1.000000 \\
Dezi            &   1.0 &   1.000000 \\
Dezirae         &   1.0 &   0.312500 \\
Deziray         &   1.0 &   1.000000 \\
Dezire          &   1.0 &   0.833333 \\
Deziree         &   1.0 &   0.294118 \\
Dhalia          &   1.0 &   0.625000 \\
Dhamar          &   1.0 &   1.000000 \\
Dhana           &   1.0 &   0.555556 \\
Dhanya          &   1.0 &   1.000000 \\
Dhara           &   1.0 &   1.000000 \\
Dharma          &   1.0 &   0.600000 \\
Dhriti          &   1.0 &   1.000000 \\
Dhwani          &   1.0 &   1.000000 \\
Dia             &   1.0 &   0.583333 \\
Diahann         &   1.0 &   0.857143 \\
Diala           &   1.0 &   1.000000 \\
Diamante        &   1.0 &   1.000000 \\
Diamond         &   1.0 &   0.083333 \\
Diamonique      &   1.0 &   0.833333 \\
Dian            &   1.0 &   0.218750 \\
Diana           &   1.0 &   0.180045 \\
Diandra         &   1.0 &   0.250000 \\
Diane           &   1.0 &   0.006105 \\
Dianelly        &   1.0 &   1.000000 \\
Diann           &   1.0 &   0.166667 \\
Dianna          &   1.0 &   0.061453 \\
Dianne          &   1.0 &   0.019763 \\
Diannia         &   1.0 &   1.000000 \\
Diara           &   1.0 &   1.000000 \\
Diasy           &   1.0 &   1.000000 \\
Diedra          &   1.0 &   0.600000 \\
Diedre          &   1.0 &   0.384615 \\
Diego           &   1.0 &   0.600000 \\
Diem            &   1.0 &   0.454545 \\
Dierdre         &   1.0 &   0.500000 \\
Dietra          &   1.0 &   1.000000 \\
Dilara          &   1.0 &   1.000000 \\
Dilcia          &   1.0 &   1.000000 \\
Dilia           &   1.0 &   1.000000 \\
Dillan          &   1.0 &   1.000000 \\
Dillon          &   1.0 &   0.600000 \\
Dilpreet        &   1.0 &   1.000000 \\
Dilynn          &   1.0 &   0.714286 \\
Dimitra         &   1.0 &   1.000000 \\
Dimond          &   1.0 &   1.000000 \\
Dimple          &   1.0 &   1.000000 \\
Dina            &   1.0 &   0.049587 \\
Dinah           &   1.0 &   0.272727 \\
Dinora          &   1.0 &   0.428571 \\
Dinorah         &   1.0 &   0.833333 \\
Dion            &   1.0 &   0.714286 \\
Diona           &   1.0 &   0.857143 \\
Dione           &   1.0 &   0.315789 \\
Dionicia        &   1.0 &   1.000000 \\
Dionna          &   1.0 &   0.411765 \\
Dionne          &   1.0 &   0.069767 \\
Dior            &   1.0 &   0.764706 \\
Dioselina       &   1.0 &   1.000000 \\
Disa            &   1.0 &   1.000000 \\
Disha           &   1.0 &   0.500000 \\
Ditya           &   1.0 &   1.000000 \\
Diva            &   1.0 &   0.625000 \\
Divina          &   1.0 &   0.555556 \\
Divine          &   1.0 &   0.562500 \\
Divinity        &   1.0 &   1.000000 \\
Divisha         &   1.0 &   0.625000 \\
Divya           &   1.0 &   0.166667 \\
Dixie           &   1.0 &   0.148936 \\
Diya            &   1.0 &   0.410714 \\
Djuana          &   1.0 &   0.384615 \\
Djuna           &   1.0 &   1.000000 \\
Dodie           &   1.0 &   0.555556 \\
Dody            &   1.0 &   1.000000 \\
Dollie          &   1.0 &   0.500000 \\
Dolly           &   1.0 &   0.391304 \\
Dolores         &   1.0 &   0.039049 \\
Domanique       &   1.0 &   1.000000 \\
Domenica        &   1.0 &   0.090909 \\
Domenique       &   1.0 &   0.857143 \\
Dominga         &   1.0 &   0.555556 \\
Domingue        &   1.0 &   1.000000 \\
Dominic         &   1.0 &   0.545455 \\
Dominica        &   1.0 &   0.500000 \\
Dominigue       &   1.0 &   1.000000 \\
Dominika        &   1.0 &   1.000000 \\
Dominique       &   1.0 &   0.033816 \\
Domino          &   1.0 &   1.000000 \\
Dominque        &   1.0 &   0.236842 \\
Domonique       &   1.0 &   0.204082 \\
Dona            &   1.0 &   0.127273 \\
Donald          &   1.0 &   0.583333 \\
Donalee         &   1.0 &   1.000000 \\
Donatella       &   1.0 &   1.000000 \\
Dondi           &   1.0 &   1.000000 \\
Doneisha        &   1.0 &   1.000000 \\
Donell          &   1.0 &   0.625000 \\
Donella         &   1.0 &   1.000000 \\
Donelle         &   1.0 &   0.666667 \\
Donetta         &   1.0 &   0.714286 \\
Donette         &   1.0 &   0.555556 \\
Doni            &   1.0 &   1.000000 \\
Donia           &   1.0 &   0.625000 \\
Donica          &   1.0 &   1.000000 \\
Donielle        &   1.0 &   0.692308 \\
Donisha         &   1.0 &   0.500000 \\
Donita          &   1.0 &   0.384615 \\
Donna           &   1.0 &   0.010189 \\
Donnalee        &   1.0 &   1.000000 \\
Donnell         &   1.0 &   1.000000 \\
Donnetta        &   1.0 &   1.000000 \\
Donnette        &   1.0 &   0.625000 \\
Donnie          &   1.0 &   0.555556 \\
Donnisha        &   1.0 &   1.000000 \\
Donya           &   1.0 &   0.227273 \\
Donyale         &   1.0 &   0.857143 \\
Dora            &   1.0 &   0.087591 \\
Dorcas          &   1.0 &   1.000000 \\
Dore            &   1.0 &   1.000000 \\
Doree           &   1.0 &   0.714286 \\
Doreen          &   1.0 &   0.026455 \\
Doreena         &   1.0 &   1.000000 \\
Doreene         &   1.0 &   1.000000 \\
Dorena          &   1.0 &   0.666667 \\
Dorene          &   1.0 &   0.128205 \\
Doretha         &   1.0 &   0.555556 \\
Doretta         &   1.0 &   0.384615 \\
Dori            &   1.0 &   0.146341 \\
Doria           &   1.0 &   0.750000 \\
Dorian          &   1.0 &   0.300000 \\
Dorie           &   1.0 &   0.538462 \\
Dorina          &   1.0 &   1.000000 \\
Dorinda         &   1.0 &   0.230769 \\
Dorine          &   1.0 &   0.454545 \\
Doris           &   1.0 &   0.065169 \\
Dorise          &   1.0 &   1.000000 \\
Dorismar        &   1.0 &   1.000000 \\
Dorita          &   1.0 &   1.000000 \\
Dorla           &   1.0 &   1.000000 \\
Dorothea        &   1.0 &   0.205128 \\
Dorothey        &   1.0 &   1.000000 \\
Dorothy         &   1.0 &   0.043478 \\
Dorri           &   1.0 &   1.000000 \\
Dorrie          &   1.0 &   0.625000 \\
Dorris          &   1.0 &   0.357143 \\
Dorsa           &   1.0 &   1.000000 \\
Dortha          &   1.0 &   1.000000 \\
Dorthea         &   1.0 &   0.857143 \\
Dorthy          &   1.0 &   0.416667 \\
Dory            &   1.0 &   1.000000 \\
Dottie          &   1.0 &   0.600000 \\
Doua            &   1.0 &   1.000000 \\
Douglas         &   1.0 &   0.833333 \\
Dove            &   1.0 &   0.692308 \\
Dovie           &   1.0 &   1.000000 \\
Draya           &   1.0 &   0.714286 \\
Drea            &   1.0 &   0.666667 \\
Dream           &   1.0 &   1.000000 \\
Dreama          &   1.0 &   1.000000 \\
Dreanna         &   1.0 &   1.000000 \\
Drena           &   1.0 &   1.000000 \\
Drew            &   1.0 &   0.921569 \\
Drina           &   1.0 &   1.000000 \\
Drinda          &   1.0 &   1.000000 \\
Dru             &   1.0 &   0.888889 \\
Drucilla        &   1.0 &   0.454545 \\
Drusilla        &   1.0 &   0.714286 \\
Dua             &   1.0 &   1.000000 \\
Duaa            &   1.0 &   1.000000 \\
Dulce           &   1.0 &   0.197101 \\
Dulcemaria      &   1.0 &   0.217391 \\
Dulcie          &   1.0 &   1.000000 \\
Dulcinea        &   1.0 &   1.000000 \\
Dulse           &   1.0 &   0.833333 \\
Dung            &   1.0 &   0.666667 \\
Dunia           &   1.0 &   1.000000 \\
Dunya           &   1.0 &   1.000000 \\
Dusti           &   1.0 &   0.750000 \\
Dustie          &   1.0 &   0.875000 \\
Dustin          &   1.0 &   0.416667 \\
Dusty           &   1.0 &   0.250000 \\
Duyen           &   1.0 &   1.000000 \\
Dwan            &   1.0 &   1.000000 \\
Dwana           &   1.0 &   1.000000 \\
Dyamond         &   1.0 &   1.000000 \\
Dyan            &   1.0 &   0.363636 \\
Dyana           &   1.0 &   0.428571 \\
Dyanara         &   1.0 &   1.000000 \\
Dyane           &   1.0 &   1.000000 \\
Dyani           &   1.0 &   0.600000 \\
Dyann           &   1.0 &   1.000000 \\
Dyanna          &   1.0 &   0.437500 \\
Dyanne          &   1.0 &   0.833333 \\
Dylan           &   1.0 &   0.605839 \\
Dylann          &   1.0 &   0.500000 \\
Dyllan          &   1.0 &   1.000000 \\
Dymond          &   1.0 &   0.666667 \\
Dynasty         &   1.0 &   0.555556 \\
Earleen         &   1.0 &   0.714286 \\
Earlene         &   1.0 &   0.161290 \\
Earline         &   1.0 &   0.384615 \\
Earnestine      &   1.0 &   0.555556 \\
Easton          &   1.0 &   1.000000 \\
Eastyn          &   1.0 &   1.000000 \\
Ebelia          &   1.0 &   1.000000 \\
Ebelin          &   1.0 &   1.000000 \\
Ebelina         &   1.0 &   1.000000 \\
Ebone           &   1.0 &   0.625000 \\
Ebonee          &   1.0 &   0.500000 \\
Eboney          &   1.0 &   1.000000 \\
Eboni           &   1.0 &   0.217391 \\
Ebonie          &   1.0 &   0.294118 \\
Ebony           &   1.0 &   0.024876 \\
Echo            &   1.0 &   0.450000 \\
Eda             &   1.0 &   0.384615 \\
Eddie           &   1.0 &   0.545455 \\
Edeline         &   1.0 &   0.833333 \\
Edelmira        &   1.0 &   0.416667 \\
Edelyn          &   1.0 &   1.000000 \\
Eden            &   1.0 &   0.757322 \\
Edgar           &   1.0 &   0.263158 \\
Edid            &   1.0 &   1.000000 \\
Edie            &   1.0 &   0.212121 \\
Edit            &   1.0 &   0.875000 \\
Edith           &   1.0 &   0.223684 \\
Edlin           &   1.0 &   0.200000 \\
Edlyn           &   1.0 &   0.500000 \\
Edna            &   1.0 &   0.031447 \\
Ednita          &   1.0 &   1.000000 \\
Edris           &   1.0 &   1.000000 \\
Eduardo         &   1.0 &   0.571429 \\
Edward          &   1.0 &   0.727273 \\
Edwin           &   1.0 &   0.555556 \\
Edwina          &   1.0 &   0.184211 \\
Edyn            &   1.0 &   1.000000 \\
Edyth           &   1.0 &   0.714286 \\
Edythe          &   1.0 &   0.300000 \\
Eesha           &   1.0 &   0.533333 \\
Eevee           &   1.0 &   0.812500 \\
Eevie           &   1.0 &   1.000000 \\
Effie           &   1.0 &   0.583333 \\
Egypt           &   1.0 &   0.833333 \\
Ehlani          &   1.0 &   0.671053 \\
Eiko            &   1.0 &   0.700000 \\
Eila            &   1.0 &   0.611111 \\
Eilah           &   1.0 &   0.833333 \\
Eileen          &   1.0 &   0.344828 \\
Eilene          &   1.0 &   0.625000 \\
Eiliana         &   1.0 &   1.000000 \\
Eiliyah         &   1.0 &   1.000000 \\
Eilleen         &   1.0 &   1.000000 \\
Eily            &   1.0 &   1.000000 \\
Eilyn           &   1.0 &   1.000000 \\
Eimi            &   1.0 &   1.000000 \\
Eimy            &   1.0 &   0.933333 \\
Eira            &   1.0 &   0.625000 \\
Eirene          &   1.0 &   1.000000 \\
Eisley          &   1.0 &   0.666667 \\
Eiza            &   1.0 &   1.000000 \\
Ekam            &   1.0 &   0.416667 \\
Ekaterina       &   1.0 &   0.555556 \\
Eknoor          &   1.0 &   1.000000 \\
Ela             &   1.0 &   0.666667 \\
Elah            &   1.0 &   0.714286 \\
Elahi           &   1.0 &   1.000000 \\
Elahni          &   1.0 &   1.000000 \\
Elaina          &   1.0 &   0.814815 \\
Elaine          &   1.0 &   0.190157 \\
Elainna         &   1.0 &   1.000000 \\
Elan            &   1.0 &   0.714286 \\
Elana           &   1.0 &   0.370370 \\
Elane           &   1.0 &   1.000000 \\
Elani           &   1.0 &   1.000000 \\
Elanie          &   1.0 &   0.642857 \\
Elanna          &   1.0 &   1.000000 \\
Elanor          &   1.0 &   1.000000 \\
Elany           &   1.0 &   1.000000 \\
Elara           &   1.0 &   0.909091 \\
Elaura          &   1.0 &   1.000000 \\
Elaya           &   1.0 &   1.000000 \\
Elayah          &   1.0 &   1.000000 \\
Elayna          &   1.0 &   0.480000 \\
Elayne          &   1.0 &   0.500000 \\
Elba            &   1.0 &   0.263158 \\
Elbia           &   1.0 &   0.833333 \\
Elda            &   1.0 &   0.227273 \\
Eldana          &   1.0 &   1.000000 \\
Eldora          &   1.0 &   0.666667 \\
Elea            &   1.0 &   1.000000 \\
Eleana          &   1.0 &   0.615385 \\
Eleanna         &   1.0 &   1.000000 \\
Eleanor         &   1.0 &   0.937500 \\
Eleanora        &   1.0 &   0.833333 \\
Eleanore        &   1.0 &   0.409091 \\
Electra         &   1.0 &   1.000000 \\
Eleen           &   1.0 &   1.000000 \\
Eleena          &   1.0 &   1.000000 \\
Eleia           &   1.0 &   1.000000 \\
Eleina          &   1.0 &   0.666667 \\
Elektra         &   1.0 &   0.461538 \\
Elen            &   1.0 &   1.000000 \\
Elena           &   1.0 &   1.000000 \\
Elene           &   1.0 &   0.833333 \\
Eleni           &   1.0 &   0.766667 \\
Elenna          &   1.0 &   1.000000 \\
Elenoa          &   1.0 &   1.000000 \\
Elenor          &   1.0 &   1.000000 \\
Elenore         &   1.0 &   0.600000 \\
Eleny           &   1.0 &   0.875000 \\
Eleonor         &   1.0 &   0.714286 \\
Eleonora        &   1.0 &   0.714286 \\
Eleonore        &   1.0 &   0.875000 \\
Eleora          &   1.0 &   1.000000 \\
Elesha          &   1.0 &   1.000000 \\
Elexa           &   1.0 &   0.384615 \\
Elexia          &   1.0 &   0.857143 \\
Elexis          &   1.0 &   0.250000 \\
Elexus          &   1.0 &   0.368421 \\
Eleyna          &   1.0 &   1.000000 \\
Eli             &   1.0 &   0.636364 \\
Elia            &   1.0 &   0.720000 \\
Eliah           &   1.0 &   0.833333 \\
Elian           &   1.0 &   1.000000 \\
Eliana          &   1.0 &   1.000000 \\
Elianah         &   1.0 &   0.384615 \\
Eliane          &   1.0 &   1.000000 \\
Eliani          &   1.0 &   0.666667 \\
Elianna         &   1.0 &   1.000000 \\
Eliannah        &   1.0 &   1.000000 \\
Elianny         &   1.0 &   0.500000 \\
Eliany          &   1.0 &   1.000000 \\
Elicia          &   1.0 &   0.588235 \\
Elida           &   1.0 &   0.296296 \\
Elide           &   1.0 &   0.270270 \\
Elideth         &   1.0 &   1.000000 \\
Elidia          &   1.0 &   0.714286 \\
Elie            &   1.0 &   0.833333 \\
Elienai         &   1.0 &   0.777778 \\
Eliette         &   1.0 &   1.000000 \\
Elif            &   1.0 &   0.578947 \\
Elijah          &   1.0 &   0.727273 \\
Elika           &   1.0 &   0.875000 \\
Elin            &   1.0 &   0.280000 \\
Elina           &   1.0 &   0.943662 \\
Eline           &   1.0 &   1.000000 \\
Elinor          &   1.0 &   0.183673 \\
Elinore         &   1.0 &   0.750000 \\
Eliora          &   1.0 &   0.545455 \\
Eliot           &   1.0 &   1.000000 \\
Elis            &   1.0 &   0.833333 \\
Elisa           &   1.0 &   0.669421 \\
Elisabel        &   1.0 &   1.000000 \\
Elisabet        &   1.0 &   0.444444 \\
Elisabeth       &   1.0 &   0.230216 \\
Elisabetta      &   1.0 &   1.000000 \\
Elise           &   1.0 &   0.588679 \\
Eliset          &   1.0 &   1.000000 \\
Elisha          &   1.0 &   0.166667 \\
Elisheva        &   1.0 &   1.000000 \\
Elishia         &   1.0 &   0.833333 \\
Elisia          &   1.0 &   0.545455 \\
Elissa          &   1.0 &   0.432836 \\
Elisse          &   1.0 &   0.857143 \\
Elita           &   1.0 &   1.000000 \\
Eliya           &   1.0 &   1.000000 \\
Eliyah          &   1.0 &   0.545455 \\
Eliyana         &   1.0 &   0.800000 \\
Eliyanah        &   1.0 &   0.851852 \\
Eliyanna        &   1.0 &   1.000000 \\
Eliz            &   1.0 &   1.000000 \\
Eliza           &   1.0 &   0.816901 \\
Elizabella      &   1.0 &   0.600000 \\
Elizabet        &   1.0 &   0.250000 \\
Elizabeth       &   1.0 &   0.218474 \\
Elizah          &   1.0 &   0.666667 \\
Elizaveta       &   1.0 &   0.857143 \\
Elize           &   1.0 &   0.857143 \\
Elizebeth       &   1.0 &   0.666667 \\
Elizeth         &   1.0 &   1.000000 \\
Elke            &   1.0 &   0.400000 \\
Ella            &   1.0 &   0.721436 \\
Ellah           &   1.0 &   0.600000 \\
Ellaina         &   1.0 &   1.000000 \\
Ellamae         &   1.0 &   1.000000 \\
Ellamarie       &   1.0 &   1.000000 \\
Ellani          &   1.0 &   1.000000 \\
Ellanie         &   1.0 &   1.000000 \\
Ellanor         &   1.0 &   1.000000 \\
Ellanora        &   1.0 &   1.000000 \\
Ellanore        &   1.0 &   1.000000 \\
Ellarae         &   1.0 &   0.833333 \\
Ellarie         &   1.0 &   1.000000 \\
Ellarose        &   1.0 &   0.642857 \\
Elle            &   1.0 &   0.832117 \\
Elleana         &   1.0 &   1.000000 \\
Elleanna        &   1.0 &   1.000000 \\
Elleanor        &   1.0 &   1.000000 \\
Ellee           &   1.0 &   0.555556 \\
Ellen           &   1.0 &   0.060870 \\
Ellena          &   1.0 &   0.500000 \\
Elleri          &   1.0 &   1.000000 \\
Ellerie         &   1.0 &   1.000000 \\
Ellery          &   1.0 &   0.500000 \\
Ellesse         &   1.0 &   0.750000 \\
Elley           &   1.0 &   1.000000 \\
Elli            &   1.0 &   0.388889 \\
Ellia           &   1.0 &   0.782609 \\
Elliana         &   1.0 &   0.819672 \\
Ellianna        &   1.0 &   0.944444 \\
Ellianne        &   1.0 &   1.000000 \\
Ellie           &   1.0 &   1.000000 \\
Ellieana        &   1.0 &   0.857143 \\
Elliemae        &   1.0 &   0.833333 \\
Ellierose       &   1.0 &   1.000000 \\
Elliette        &   1.0 &   0.562500 \\
Ellina          &   1.0 &   0.833333 \\
Ellington       &   1.0 &   0.461538 \\
Ellinor         &   1.0 &   1.000000 \\
Elliot          &   1.0 &   0.500000 \\
Elliott         &   1.0 &   0.490566 \\
Elliotte        &   1.0 &   0.857143 \\
Ellis           &   1.0 &   0.750000 \\
Ellisa          &   1.0 &   1.000000 \\
Ellise          &   1.0 &   1.000000 \\
Ellison         &   1.0 &   0.526316 \\
Ellora          &   1.0 &   0.615385 \\
Ellory          &   1.0 &   1.000000 \\
Ellowyn         &   1.0 &   1.000000 \\
Elly            &   1.0 &   0.485714 \\
Ellyana         &   1.0 &   0.533333 \\
Ellyanna        &   1.0 &   0.714286 \\
Ellyn           &   1.0 &   0.500000 \\
Ellyse          &   1.0 &   0.500000 \\
Ellyssa         &   1.0 &   1.000000 \\
Elma            &   1.0 &   0.250000 \\
Elmira          &   1.0 &   1.000000 \\
Elna            &   1.0 &   0.875000 \\
Elnora          &   1.0 &   0.875000 \\
Elodia          &   1.0 &   0.750000 \\
Elodie          &   1.0 &   1.000000 \\
Elody           &   1.0 &   1.000000 \\
Eloisa          &   1.0 &   0.593750 \\
Eloise          &   1.0 &   1.000000 \\
Elona           &   1.0 &   1.000000 \\
Elonda          &   1.0 &   0.714286 \\
Elonna          &   1.0 &   1.000000 \\
Elora           &   1.0 &   0.833333 \\
Elouise         &   1.0 &   1.000000 \\
Elowen          &   1.0 &   1.000000 \\
Elowyn          &   1.0 &   0.578947 \\
Elsa            &   1.0 &   0.208531 \\
Elsi            &   1.0 &   0.555556 \\
Elsie           &   1.0 &   0.917293 \\
Elspeth         &   1.0 &   0.857143 \\
Elsy            &   1.0 &   0.659574 \\
Elva            &   1.0 &   0.134615 \\
Elvera          &   1.0 &   0.285714 \\
Elvia           &   1.0 &   0.114943 \\
Elvina          &   1.0 &   1.000000 \\
Elvira          &   1.0 &   0.220930 \\
Ely             &   1.0 &   0.555556 \\
Elyana          &   1.0 &   1.000000 \\
Elyanna         &   1.0 &   0.680000 \\
Elyce           &   1.0 &   1.000000 \\
Elyn            &   1.0 &   1.000000 \\
Elynn           &   1.0 &   1.000000 \\
Elynor          &   1.0 &   1.000000 \\
Elysa           &   1.0 &   0.666667 \\
Elyse           &   1.0 &   0.386364 \\
Elysha          &   1.0 &   0.625000 \\
Elysia          &   1.0 &   0.590909 \\
Elyssa          &   1.0 &   0.173913 \\
Elysse          &   1.0 &   0.700000 \\
Elyssia         &   1.0 &   0.857143 \\
Elyza           &   1.0 &   0.666667 \\
Elyzabeth       &   1.0 &   1.000000 \\
Elza            &   1.0 &   1.000000 \\
Ema             &   1.0 &   0.551724 \\
Emaan           &   1.0 &   1.000000 \\
Emalee          &   1.0 &   0.315789 \\
Emalie          &   1.0 &   0.454545 \\
Emaly           &   1.0 &   0.500000 \\
Emalyn          &   1.0 &   0.538462 \\
Emalynn         &   1.0 &   1.000000 \\
Eman            &   1.0 &   0.857143 \\
Emani           &   1.0 &   1.000000 \\
Emanuel         &   1.0 &   1.000000 \\
Emanuela        &   1.0 &   1.000000 \\
Emari           &   1.0 &   1.000000 \\
Emarie          &   1.0 &   0.500000 \\
Emaya           &   1.0 &   1.000000 \\
Ember           &   1.0 &   0.981481 \\
Emberlee        &   1.0 &   0.888889 \\
Emberleigh      &   1.0 &   1.000000 \\
Emberly         &   1.0 &   0.727273 \\
Emberlyn        &   1.0 &   0.777778 \\
Emberlynn       &   1.0 &   1.000000 \\
Embry           &   1.0 &   1.000000 \\
Emelda          &   1.0 &   0.714286 \\
Emeli           &   1.0 &   0.217391 \\
Emelia          &   1.0 &   0.630769 \\
Emelie          &   1.0 &   0.350000 \\
Emelin          &   1.0 &   0.466667 \\
Emelina         &   1.0 &   0.636364 \\
Emeline         &   1.0 &   0.437500 \\
Emelly          &   1.0 &   0.875000 \\
Emely           &   1.0 &   0.304075 \\
Emelyn          &   1.0 &   0.285714 \\
Emerald         &   1.0 &   0.924528 \\
Emeraude        &   1.0 &   1.000000 \\
Emeri           &   1.0 &   1.000000 \\
Emerie          &   1.0 &   1.000000 \\
Emerita         &   1.0 &   1.000000 \\
Emerly          &   1.0 &   0.857143 \\
Emersen         &   1.0 &   1.000000 \\
Emerson         &   1.0 &   0.702128 \\
Emersyn         &   1.0 &   0.741176 \\
Emersynn        &   1.0 &   1.000000 \\
Emery           &   1.0 &   1.000000 \\
Emi             &   1.0 &   1.000000 \\
Emie            &   1.0 &   1.000000 \\
Emika           &   1.0 &   0.833333 \\
Emiko           &   1.0 &   0.800000 \\
Emilce          &   1.0 &   1.000000 \\
Emile           &   1.0 &   1.000000 \\
Emilee          &   1.0 &   0.176471 \\
Emiley          &   1.0 &   0.428571 \\
Emili           &   1.0 &   0.500000 \\
Emilia          &   1.0 &   0.892905 \\
Emiliana        &   1.0 &   0.800000 \\
Emilianna       &   1.0 &   1.000000 \\
Emilie          &   1.0 &   0.190000 \\
Emillee         &   1.0 &   1.000000 \\
Emillia         &   1.0 &   1.000000 \\
Emillie         &   1.0 &   1.000000 \\
Emilly          &   1.0 &   0.384615 \\
Emilse          &   1.0 &   1.000000 \\
Emily           &   1.0 &   0.323185 \\
Emilyann        &   1.0 &   0.857143 \\
Emilyanne       &   1.0 &   1.000000 \\
Emilyn          &   1.0 &   0.600000 \\
Emilyrose       &   1.0 &   1.000000 \\
Emina           &   1.0 &   1.000000 \\
Emira           &   1.0 &   1.000000 \\
Emireth         &   1.0 &   0.857143 \\
Emiyah          &   1.0 &   0.714286 \\
Emma            &   1.0 &   0.756089 \\
Emmagrace       &   1.0 &   1.000000 \\
Emmah           &   1.0 &   0.636364 \\
Emmajean        &   1.0 &   1.000000 \\
Emmalee         &   1.0 &   0.285714 \\
Emmaleigh       &   1.0 &   0.700000 \\
Emmalia         &   1.0 &   1.000000 \\
Emmalie         &   1.0 &   0.857143 \\
Emmalin         &   1.0 &   1.000000 \\
Emmalina        &   1.0 &   0.333333 \\
Emmaline        &   1.0 &   0.538462 \\
Emmaly          &   1.0 &   0.555556 \\
Emmalyn         &   1.0 &   0.475410 \\
Emmalynn        &   1.0 &   0.392857 \\
Emmamarie       &   1.0 &   0.833333 \\
Emmanuel        &   1.0 &   1.000000 \\
Emmanuelle      &   1.0 &   0.714286 \\
Emmarae         &   1.0 &   1.000000 \\
Emmarie         &   1.0 &   0.800000 \\
Emmarose        &   1.0 &   0.428571 \\
Emme            &   1.0 &   0.677419 \\
Emmelia         &   1.0 &   1.000000 \\
Emmeline        &   1.0 &   0.473684 \\
Emmely          &   1.0 &   0.454545 \\
Emmelyn         &   1.0 &   0.625000 \\
Emmerie         &   1.0 &   1.000000 \\
Emmerson        &   1.0 &   0.357143 \\
Emmersyn        &   1.0 &   1.000000 \\
Emmery          &   1.0 &   1.000000 \\
Emmi            &   1.0 &   0.714286 \\
Emmie           &   1.0 &   0.661017 \\
Emmily          &   1.0 &   0.545455 \\
Emmilyn         &   1.0 &   1.000000 \\
Emmy            &   1.0 &   0.948052 \\
Emmylou         &   1.0 &   0.666667 \\
Emoni           &   1.0 &   0.545455 \\
Emonie          &   1.0 &   1.000000 \\
Emori           &   1.0 &   0.666667 \\
Emorie          &   1.0 &   1.000000 \\
Emory           &   1.0 &   0.837838 \\
Empress         &   1.0 &   0.727273 \\
Emree           &   1.0 &   1.000000 \\
Emri            &   1.0 &   1.000000 \\
Emrie           &   1.0 &   0.538462 \\
Emry            &   1.0 &   0.500000 \\
Emrys           &   1.0 &   1.000000 \\
Emsley          &   1.0 &   0.833333 \\
Emunah          &   1.0 &   1.000000 \\
Emy             &   1.0 &   0.642857 \\
Emya            &   1.0 &   1.000000 \\
Emylee          &   1.0 &   1.000000 \\
Emylia          &   1.0 &   1.000000 \\
Ena             &   1.0 &   0.700000 \\
Enalina         &   1.0 &   1.000000 \\
Enaya           &   1.0 &   1.000000 \\
Encarnacion     &   1.0 &   1.000000 \\
Enedina         &   1.0 &   0.350000 \\
Eneida          &   1.0 &   0.714286 \\
Enes            &   1.0 &   1.000000 \\
Engracia        &   1.0 &   1.000000 \\
Enid            &   1.0 &   0.411765 \\
Eniya           &   1.0 &   1.000000 \\
Eniyah          &   1.0 &   1.000000 \\
Enjoli          &   1.0 &   0.333333 \\
Enna            &   1.0 &   1.000000 \\
Enola           &   1.0 &   1.000000 \\
Enrique         &   1.0 &   0.833333 \\
Enriqueta       &   1.0 &   0.666667 \\
Ensley          &   1.0 &   0.640000 \\
Envy            &   1.0 &   0.600000 \\
Enya            &   1.0 &   0.437500 \\
Eowyn           &   1.0 &   1.000000 \\
Epiphany        &   1.0 &   1.000000 \\
Era             &   1.0 &   0.714286 \\
Erabella        &   1.0 &   1.000000 \\
Eraina          &   1.0 &   1.000000 \\
Erandi          &   1.0 &   0.666667 \\
Erandy          &   1.0 &   0.500000 \\
Erendida        &   1.0 &   0.600000 \\
Erendira        &   1.0 &   0.222222 \\
Eri             &   1.0 &   1.000000 \\
Eriana          &   1.0 &   0.545455 \\
Erianna         &   1.0 &   0.750000 \\
Eric            &   1.0 &   0.230769 \\
Erica           &   1.0 &   0.017530 \\
Ericca          &   1.0 &   0.875000 \\
Erick           &   1.0 &   0.461538 \\
Ericka          &   1.0 &   0.060345 \\
Eriel           &   1.0 &   1.000000 \\
Erielle         &   1.0 &   1.000000 \\
Erienne         &   1.0 &   0.833333 \\
Erik            &   1.0 &   0.600000 \\
Erika           &   1.0 &   0.027853 \\
Erikah          &   1.0 &   1.000000 \\
Erikka          &   1.0 &   0.555556 \\
Eriko           &   1.0 &   1.000000 \\
Erilyn          &   1.0 &   1.000000 \\
Erin            &   1.0 &   0.032925 \\
Erina           &   1.0 &   0.777778 \\
Erinn           &   1.0 &   0.260870 \\
Eris            &   1.0 &   0.705882 \\
Erleen          &   1.0 &   1.000000 \\
Erlene          &   1.0 &   0.833333 \\
Erlinda         &   1.0 &   0.218182 \\
Erline          &   1.0 &   1.000000 \\
Erma            &   1.0 &   0.135135 \\
Ermalinda       &   1.0 &   0.625000 \\
Ermelinda       &   1.0 &   0.384615 \\
Erminia         &   1.0 &   0.833333 \\
Erna            &   1.0 &   0.600000 \\
Ernest          &   1.0 &   1.000000 \\
Ernestina       &   1.0 &   0.178571 \\
Ernestine       &   1.0 &   0.170732 \\
Ernesto         &   1.0 &   1.000000 \\
Errin           &   1.0 &   0.625000 \\
Erryn           &   1.0 &   1.000000 \\
Eryka           &   1.0 &   0.466667 \\
Erykah          &   1.0 &   0.200000 \\
Eryn            &   1.0 &   0.166667 \\
Erynn           &   1.0 &   0.888889 \\
Erza            &   1.0 &   0.714286 \\
Esabella        &   1.0 &   0.400000 \\
Esbeidy         &   1.0 &   0.545455 \\
Esha            &   1.0 &   0.121951 \\
Eshaal          &   1.0 &   1.000000 \\
Eshal           &   1.0 &   0.692308 \\
Esli            &   1.0 &   0.727273 \\
Esly            &   1.0 &   0.625000 \\
Esmae           &   1.0 &   1.000000 \\
Esmay           &   1.0 &   0.857143 \\
Esme            &   1.0 &   1.000000 \\
Esmee           &   1.0 &   0.588235 \\
Esmeralda       &   1.0 &   0.222917 \\
Esmeray         &   1.0 &   1.000000 \\
Esmerelda       &   1.0 &   0.500000 \\
Esmirna         &   1.0 &   1.000000 \\
Esperansa       &   1.0 &   0.714286 \\
Esperanza       &   1.0 &   0.471074 \\
Esra            &   1.0 &   1.000000 \\
Essence         &   1.0 &   0.255814 \\
Essica          &   1.0 &   1.000000 \\
Essie           &   1.0 &   1.000000 \\
Esteban         &   1.0 &   1.000000 \\
Estee           &   1.0 &   1.000000 \\
Estefani        &   1.0 &   0.058824 \\
Estefania       &   1.0 &   0.118421 \\
Estefanie       &   1.0 &   0.428571 \\
Estefany        &   1.0 &   0.223529 \\
Esteffany       &   1.0 &   1.000000 \\
Estela          &   1.0 &   0.562500 \\
Estella         &   1.0 &   1.000000 \\
Estelle         &   1.0 &   0.450980 \\
Estephani       &   1.0 &   1.000000 \\
Estephania      &   1.0 &   0.312500 \\
Estephanie      &   1.0 &   0.173913 \\
Estephany       &   1.0 &   0.294118 \\
Ester           &   1.0 &   0.500000 \\
Esthefany       &   1.0 &   1.000000 \\
Esthela         &   1.0 &   0.666667 \\
Esther          &   1.0 &   0.527881 \\
Estrella        &   1.0 &   0.332288 \\
Estrellita      &   1.0 &   0.583333 \\
Estreya         &   1.0 &   1.000000 \\
Eternity        &   1.0 &   0.600000 \\
Ethel           &   1.0 &   0.038760 \\
Ethelyn         &   1.0 &   0.857143 \\
Ethyl           &   1.0 &   1.000000 \\
Etsuko          &   1.0 &   0.833333 \\
Etta            &   1.0 &   0.423077 \\
Eudora          &   1.0 &   1.000000 \\
Eugena          &   1.0 &   1.000000 \\
Eugene          &   1.0 &   1.000000 \\
Eugenia         &   1.0 &   0.119048 \\
Eugenie         &   1.0 &   0.500000 \\
Eugina          &   1.0 &   1.000000 \\
Eula            &   1.0 &   0.545455 \\
Eulalia         &   1.0 &   1.000000 \\
Eun             &   1.0 &   1.000000 \\
Euna            &   1.0 &   1.000000 \\
Eunice          &   1.0 &   0.265060 \\
Eunique         &   1.0 &   1.000000 \\
Eura            &   1.0 &   1.000000 \\
Eva             &   1.0 &   0.589655 \\
Evah            &   1.0 &   0.714286 \\
Evalee          &   1.0 &   1.000000 \\
Evalena         &   1.0 &   1.000000 \\
Evalette        &   1.0 &   1.000000 \\
Evalin          &   1.0 &   1.000000 \\
Evalina         &   1.0 &   0.964286 \\
Evaline         &   1.0 &   1.000000 \\
Evaluna         &   1.0 &   1.000000 \\
Evalyn          &   1.0 &   0.466667 \\
Evalyna         &   1.0 &   1.000000 \\
Evalynn         &   1.0 &   0.318182 \\
Evamarie        &   1.0 &   0.857143 \\
Evan            &   1.0 &   0.407407 \\
Evana           &   1.0 &   1.000000 \\
Evanee          &   1.0 &   1.000000 \\
Evangaline      &   1.0 &   1.000000 \\
Evangelia       &   1.0 &   1.000000 \\
Evangelina      &   1.0 &   0.406780 \\
Evangeline      &   1.0 &   0.827957 \\
Evangelyn       &   1.0 &   0.833333 \\
Evani           &   1.0 &   1.000000 \\
Evania          &   1.0 &   1.000000 \\
Evanie          &   1.0 &   0.625000 \\
Evanjelina      &   1.0 &   1.000000 \\
Evann           &   1.0 &   1.000000 \\
Evanna          &   1.0 &   0.666667 \\
Evany           &   1.0 &   0.384615 \\
Evarose         &   1.0 &   1.000000 \\
Eve             &   1.0 &   0.595960 \\
Evee            &   1.0 &   1.000000 \\
Eveleen         &   1.0 &   1.000000 \\
Eveleigh        &   1.0 &   1.000000 \\
Evelen          &   1.0 &   1.000000 \\
Evelett         &   1.0 &   1.000000 \\
Evelia          &   1.0 &   0.304348 \\
Evelin          &   1.0 &   0.140187 \\
Evelina         &   1.0 &   0.382353 \\
Eveline         &   1.0 &   0.454545 \\
Evely           &   1.0 &   1.000000 \\
Evelyn          &   1.0 &   0.824517 \\
Evelyne         &   1.0 &   0.384615 \\
Evelynn         &   1.0 &   0.810127 \\
Evelynne        &   1.0 &   0.700000 \\
Evenny          &   1.0 &   1.000000 \\
Eveny           &   1.0 &   1.000000 \\
Ever            &   1.0 &   0.900000 \\
Everest         &   1.0 &   0.866667 \\
Everett         &   1.0 &   1.000000 \\
Everette        &   1.0 &   1.000000 \\
Everlee         &   1.0 &   0.912281 \\
Everleigh       &   1.0 &   0.909091 \\
Everley         &   1.0 &   0.789474 \\
Everli          &   1.0 &   1.000000 \\
Everlie         &   1.0 &   0.600000 \\
Everly          &   1.0 &   0.835644 \\
Everlyn         &   1.0 &   0.769231 \\
Everlynn        &   1.0 &   1.000000 \\
Evett           &   1.0 &   1.000000 \\
Evette          &   1.0 &   0.551724 \\
Evey            &   1.0 &   0.666667 \\
Eviana          &   1.0 &   0.545455 \\
Evianna         &   1.0 &   0.384615 \\
Evie            &   1.0 &   0.965812 \\
Evin            &   1.0 &   1.000000 \\
Evita           &   1.0 &   0.185185 \\
Evoleht         &   1.0 &   1.000000 \\
Evolet          &   1.0 &   0.186047 \\
Evoleth         &   1.0 &   0.857143 \\
Evolette        &   1.0 &   1.000000 \\
Evon            &   1.0 &   0.500000 \\
Evonna          &   1.0 &   1.000000 \\
Evonne          &   1.0 &   0.294118 \\
Evony           &   1.0 &   0.700000 \\
Evvie           &   1.0 &   1.000000 \\
Evy             &   1.0 &   0.636364 \\
Evyn            &   1.0 &   0.875000 \\
Eydie           &   1.0 &   0.750000 \\
Eyleen          &   1.0 &   1.000000 \\
Ezabella        &   1.0 &   1.000000 \\
Ezlyn           &   1.0 &   0.583333 \\
Ezlynn          &   1.0 &   0.625000 \\
Ezme            &   1.0 &   1.000000 \\
Ezmeralda       &   1.0 &   0.555556 \\
Ezra            &   1.0 &   1.000000 \\
Ezrah           &   1.0 &   1.000000 \\
Ezri            &   1.0 &   0.636364 \\
Eztli           &   1.0 &   1.000000 \\
Fabian          &   1.0 &   0.875000 \\
Fabiana         &   1.0 &   0.555556 \\
Fabienne        &   1.0 &   1.000000 \\
Fabiola         &   1.0 &   0.040936 \\
Fadumo          &   1.0 &   1.000000 \\
Fae             &   1.0 &   0.833333 \\
Fahm            &   1.0 &   1.000000 \\
Faith           &   1.0 &   0.286645 \\
Faiza           &   1.0 &   1.000000 \\
Faizah          &   1.0 &   1.000000 \\
Falicia         &   1.0 &   0.625000 \\
Faline          &   1.0 &   1.000000 \\
Falisha         &   1.0 &   0.727273 \\
Fallon          &   1.0 &   0.510638 \\
Fallyn          &   1.0 &   0.666667 \\
Falon           &   1.0 &   0.461538 \\
Falyn           &   1.0 &   1.000000 \\
Falynn          &   1.0 &   1.000000 \\
Fancy           &   1.0 &   0.714286 \\
Fannie          &   1.0 &   0.384615 \\
Fanny           &   1.0 &   0.257143 \\
Fantasia        &   1.0 &   0.666667 \\
Fara            &   1.0 &   1.000000 \\
Farah           &   1.0 &   0.931034 \\
Faren           &   1.0 &   1.000000 \\
Farida          &   1.0 &   0.666667 \\
Fariha          &   1.0 &   1.000000 \\
Farm            &   1.0 &   0.625000 \\
Farrah          &   1.0 &   0.161616 \\
Farren          &   1.0 &   0.714286 \\
Fatema          &   1.0 &   0.555556 \\
Fathima         &   1.0 &   1.000000 \\
Fatima          &   1.0 &   0.312634 \\
Fatimah         &   1.0 &   0.352941 \\
Fatma           &   1.0 &   1.000000 \\
Faustina        &   1.0 &   0.833333 \\
Faustine        &   1.0 &   1.000000 \\
Faviola         &   1.0 &   0.125000 \\
Fawn            &   1.0 &   0.225806 \\
Fay             &   1.0 &   0.269231 \\
Faye            &   1.0 &   1.000000 \\
Fayth           &   1.0 &   0.833333 \\
Faythe          &   1.0 &   0.857143 \\
Feather         &   1.0 &   0.625000 \\
Febe            &   1.0 &   1.000000 \\
Felecia         &   1.0 &   0.379310 \\
Felica          &   1.0 &   0.714286 \\
Felice          &   1.0 &   0.470588 \\
Felicia         &   1.0 &   0.018237 \\
Feliciana       &   1.0 &   1.000000 \\
Felicita        &   1.0 &   1.000000 \\
Felicitas       &   1.0 &   0.384615 \\
Felicity        &   1.0 &   0.611111 \\
Felina          &   1.0 &   1.000000 \\
Felipa          &   1.0 &   0.500000 \\
Felipe          &   1.0 &   1.000000 \\
Felisa          &   1.0 &   0.384615 \\
Felisha         &   1.0 &   0.121951 \\
Felix           &   1.0 &   1.000000 \\
Feliza          &   1.0 &   1.000000 \\
Fenix           &   1.0 &   1.000000 \\
Fergie          &   1.0 &   1.000000 \\
Fern            &   1.0 &   0.410256 \\
Fernanda        &   1.0 &   0.401554 \\
Fernando        &   1.0 &   0.562500 \\
Ferne           &   1.0 &   0.384615 \\
Fey             &   1.0 &   1.000000 \\
Fia             &   1.0 &   1.000000 \\
Fiadh           &   1.0 &   1.000000 \\
Fianna          &   1.0 &   1.000000 \\
Fidelia         &   1.0 &   1.000000 \\
Finlee          &   1.0 &   1.000000 \\
Finley          &   1.0 &   0.600000 \\
Finn            &   1.0 &   1.000000 \\
Finnley         &   1.0 &   0.681818 \\
Fion            &   1.0 &   0.833333 \\
Fiona           &   1.0 &   0.366071 \\
Fionna          &   1.0 &   0.750000 \\
Fiora           &   1.0 &   0.833333 \\
Fiorela         &   1.0 &   0.416667 \\
Fiorella        &   1.0 &   0.500000 \\
Fiza            &   1.0 &   0.625000 \\
Flannery        &   1.0 &   1.000000 \\
Flavia          &   1.0 &   0.714286 \\
Flor            &   1.0 &   0.266667 \\
Flora           &   1.0 &   0.903846 \\
Florence        &   1.0 &   0.184564 \\
Florencia       &   1.0 &   0.454545 \\
Florene         &   1.0 &   0.625000 \\
Florentina      &   1.0 &   0.555556 \\
Floretta        &   1.0 &   1.000000 \\
Florinda        &   1.0 &   0.625000 \\
Florine         &   1.0 &   0.666667 \\
Florita         &   1.0 &   1.000000 \\
Flory           &   1.0 &   1.000000 \\
Flossie         &   1.0 &   1.000000 \\
Floy            &   1.0 &   0.714286 \\
Flynn           &   1.0 &   1.000000 \\
Folasade        &   1.0 &   1.000000 \\
Fonda           &   1.0 &   0.454545 \\
Forever         &   1.0 &   1.000000 \\
Forrest         &   1.0 &   0.833333 \\
Fran            &   1.0 &   0.416667 \\
Francella       &   1.0 &   1.000000 \\
Francely        &   1.0 &   1.000000 \\
Francene        &   1.0 &   1.000000 \\
Frances         &   1.0 &   0.155009 \\
Francesca       &   1.0 &   0.605042 \\
Franceska       &   1.0 &   1.000000 \\
Francessca      &   1.0 &   1.000000 \\
Franchesca      &   1.0 &   0.156250 \\
Francheska      &   1.0 &   0.454545 \\
Francia         &   1.0 &   0.357143 \\
Francie         &   1.0 &   0.583333 \\
Francina        &   1.0 &   1.000000 \\
Francine        &   1.0 &   0.084906 \\
Francis         &   1.0 &   0.196078 \\
Francisca       &   1.0 &   0.114754 \\
Francisco       &   1.0 &   0.368421 \\
Francoise       &   1.0 &   1.000000 \\
Frania          &   1.0 &   1.000000 \\
Frank           &   1.0 &   0.454545 \\
Frankee         &   1.0 &   0.714286 \\
Franki          &   1.0 &   0.857143 \\
Frankie         &   1.0 &   0.902439 \\
Freda           &   1.0 &   0.208333 \\
Fredda          &   1.0 &   0.833333 \\
Freddie         &   1.0 &   0.500000 \\
Frederica       &   1.0 &   0.714286 \\
Fredericka      &   1.0 &   1.000000 \\
Fredricka       &   1.0 &   1.000000 \\
Freedom         &   1.0 &   0.625000 \\
Freida          &   1.0 &   0.500000 \\
Freja           &   1.0 &   0.750000 \\
Freya           &   1.0 &   1.000000 \\
Freyja          &   1.0 &   1.000000 \\
Frida           &   1.0 &   0.496241 \\
Frieda          &   1.0 &   0.250000 \\
Fritzi          &   1.0 &   1.000000 \\
Fujiko          &   1.0 &   1.000000 \\
Fumi            &   1.0 &   0.916667 \\
Fumie           &   1.0 &   1.000000 \\
Fumiko          &   1.0 &   0.360000 \\
Fumiye          &   1.0 &   0.777778 \\
Fusako          &   1.0 &   1.000000 \\
Fusaye          &   1.0 &   0.714286 \\
Gabbanelli      &   1.0 &   0.625000 \\
Gabby           &   1.0 &   0.692308 \\
Gabiela         &   1.0 &   1.000000 \\
Gabriel         &   1.0 &   0.142857 \\
Gabriela        &   1.0 &   0.122449 \\
Gabriele        &   1.0 &   0.500000 \\
Gabriell        &   1.0 &   1.000000 \\
Gabriella       &   1.0 &   0.423469 \\
Gabrielle       &   1.0 &   0.049702 \\
Gabryella       &   1.0 &   0.833333 \\
Gaby            &   1.0 &   0.400000 \\
Gae             &   1.0 &   1.000000 \\
Gagandeep       &   1.0 &   1.000000 \\
Gaia            &   1.0 &   1.000000 \\
Gail            &   1.0 &   0.006702 \\
Gaile           &   1.0 &   0.750000 \\
Gal             &   1.0 &   1.000000 \\
Gala            &   1.0 &   0.650000 \\
Galadriel       &   1.0 &   1.000000 \\
Galaxy          &   1.0 &   0.833333 \\
Gale            &   1.0 &   0.050000 \\
Galen           &   1.0 &   1.000000 \\
Gali            &   1.0 &   1.000000 \\
Galia           &   1.0 &   0.625000 \\
Galilea         &   1.0 &   0.688525 \\
Galileah        &   1.0 &   0.454545 \\
Galilee         &   1.0 &   0.500000 \\
Galina          &   1.0 &   1.000000 \\
Galya           &   1.0 &   1.000000 \\
Gao             &   1.0 &   0.750000 \\
Gardenia        &   1.0 &   0.135135 \\
Gari            &   1.0 &   1.000000 \\
Garine          &   1.0 &   1.000000 \\
Garnet          &   1.0 &   1.000000 \\
Garrett         &   1.0 &   1.000000 \\
Gary            &   1.0 &   0.500000 \\
Gauri           &   1.0 &   0.857143 \\
Gavriela        &   1.0 &   0.714286 \\
Gay             &   1.0 &   0.070423 \\
Gayane          &   1.0 &   0.666667 \\
Gayathri        &   1.0 &   1.000000 \\
Gayatri         &   1.0 &   1.000000 \\
Gaye            &   1.0 &   0.285714 \\
Gayl            &   1.0 &   1.000000 \\
Gayla           &   1.0 &   0.125000 \\
Gayle           &   1.0 &   0.016287 \\
Gayleen         &   1.0 &   1.000000 \\
Gaylen          &   1.0 &   1.000000 \\
Gaylene         &   1.0 &   0.238095 \\
Gaylyn          &   1.0 &   1.000000 \\
Gaylynn         &   1.0 &   1.000000 \\
Gaynell         &   1.0 &   1.000000 \\
Geanna          &   1.0 &   1.000000 \\
Geannie         &   1.0 &   1.000000 \\
Gearldine       &   1.0 &   0.833333 \\
Geena           &   1.0 &   0.230769 \\
Geetika         &   1.0 &   1.000000 \\
Gelsey          &   1.0 &   0.777778 \\
Gema            &   1.0 &   0.733333 \\
Gemini          &   1.0 &   1.000000 \\
Gemma           &   1.0 &   1.000000 \\
Gena            &   1.0 &   0.067568 \\
Genavie         &   1.0 &   0.833333 \\
Genavieve       &   1.0 &   0.466667 \\
Gene            &   1.0 &   0.315789 \\
Genea           &   1.0 &   0.185185 \\
Genecis         &   1.0 &   1.000000 \\
Genee           &   1.0 &   1.000000 \\
Genelle         &   1.0 &   0.666667 \\
Genese          &   1.0 &   1.000000 \\
Genesee         &   1.0 &   0.833333 \\
Genesis         &   1.0 &   0.577628 \\
Genesiss        &   1.0 &   1.000000 \\
Genessa         &   1.0 &   0.833333 \\
Genessis        &   1.0 &   0.333333 \\
Genesys         &   1.0 &   1.000000 \\
Geneva          &   1.0 &   0.465116 \\
Genevie         &   1.0 &   0.833333 \\
Genevieve       &   1.0 &   0.846491 \\
Genevive        &   1.0 &   0.714286 \\
Genevy          &   1.0 &   0.714286 \\
Genia           &   1.0 &   0.888889 \\
Genice          &   1.0 &   0.833333 \\
Genie           &   1.0 &   0.454545 \\
Genine          &   1.0 &   0.833333 \\
Genise          &   1.0 &   1.000000 \\
Genisis         &   1.0 &   0.600000 \\
Genna           &   1.0 &   0.272727 \\
Gennesis        &   1.0 &   0.357143 \\
Gennifer        &   1.0 &   0.500000 \\
Genny           &   1.0 &   0.857143 \\
Genova          &   1.0 &   0.636364 \\
Genoveva        &   1.0 &   0.500000 \\
Gentry          &   1.0 &   1.000000 \\
Georgann        &   1.0 &   0.555556 \\
Georganna       &   1.0 &   1.000000 \\
Georganne       &   1.0 &   0.777778 \\
George          &   1.0 &   0.636364 \\
Georgeann       &   1.0 &   0.833333 \\
Georgeanna      &   1.0 &   1.000000 \\
Georgeanne      &   1.0 &   1.000000 \\
Georgene        &   1.0 &   0.400000 \\
Georgetta       &   1.0 &   0.500000 \\
Georgette       &   1.0 &   0.205882 \\
Georgi          &   1.0 &   1.000000 \\
Georgia         &   1.0 &   0.825581 \\
Georgiana       &   1.0 &   0.812500 \\
Georgiann       &   1.0 &   0.857143 \\
Georgianna      &   1.0 &   0.352941 \\
Georgianne      &   1.0 &   0.714286 \\
Georgie         &   1.0 &   1.000000 \\
Georgina        &   1.0 &   0.589474 \\
Georgine        &   1.0 &   1.000000 \\
Geovana         &   1.0 &   1.000000 \\
Geovanna        &   1.0 &   0.375000 \\
Ger             &   1.0 &   0.545455 \\
Gerald          &   1.0 &   1.000000 \\
Geraldine       &   1.0 &   0.142222 \\
Geraldy         &   1.0 &   1.000000 \\
Geralyn         &   1.0 &   0.312500 \\
Gerardo         &   1.0 &   0.600000 \\
Gerda           &   1.0 &   1.000000 \\
Geri            &   1.0 &   0.177778 \\
Gerilyn         &   1.0 &   1.000000 \\
Germaine        &   1.0 &   0.454545 \\
Gerri           &   1.0 &   0.192308 \\
Gerrie          &   1.0 &   0.357143 \\
Gerry           &   1.0 &   0.200000 \\
Gertrude        &   1.0 &   0.042373 \\
Geselle         &   1.0 &   0.500000 \\
Gesenia         &   1.0 &   1.000000 \\
Gesselle        &   1.0 &   1.000000 \\
Gessica         &   1.0 &   1.000000 \\
Getsemani       &   1.0 &   0.388889 \\
Ghalia          &   1.0 &   1.000000 \\
Ghazal          &   1.0 &   1.000000 \\
Gia             &   1.0 &   0.916256 \\
Giada           &   1.0 &   0.370370 \\
Giana           &   1.0 &   0.648649 \\
Gianella        &   1.0 &   0.538462 \\
Gianelle        &   1.0 &   1.000000 \\
Gianina         &   1.0 &   0.900000 \\
Gianna          &   1.0 &   0.722585 \\
Giannah         &   1.0 &   0.700000 \\
Gianni          &   1.0 &   0.600000 \\
Giannina        &   1.0 &   1.000000 \\
Giavana         &   1.0 &   1.000000 \\
Giavanna        &   1.0 &   0.826087 \\
Giavonna        &   1.0 &   1.000000 \\
Gicela          &   1.0 &   1.000000 \\
Gidget          &   1.0 &   0.400000 \\
Gigi            &   1.0 &   0.254237 \\
Gilbert         &   1.0 &   1.000000 \\
Gilberto        &   1.0 &   1.000000 \\
Gilda           &   1.0 &   0.161290 \\
Gillian         &   1.0 &   0.090164 \\
Gilma           &   1.0 &   1.000000 \\
Gimena          &   1.0 &   0.545455 \\
Gina            &   1.0 &   0.017291 \\
Ginamarie       &   1.0 &   0.833333 \\
Ginelle         &   1.0 &   1.000000 \\
Ginette         &   1.0 &   0.714286 \\
Ginger          &   1.0 &   0.081081 \\
Gini            &   1.0 &   0.714286 \\
Ginna           &   1.0 &   0.833333 \\
Ginnie          &   1.0 &   1.000000 \\
Ginny           &   1.0 &   0.206897 \\
Gioia           &   1.0 &   0.833333 \\
Gionna          &   1.0 &   1.000000 \\
Giorgia         &   1.0 &   0.833333 \\
Giovana         &   1.0 &   0.388889 \\
Giovanna        &   1.0 &   0.264706 \\
Giovanni        &   1.0 &   0.545455 \\
Gisel           &   1.0 &   0.135135 \\
Gisela          &   1.0 &   0.108108 \\
Gisele          &   1.0 &   0.149254 \\
Gisell          &   1.0 &   0.131579 \\
Gisella         &   1.0 &   0.277778 \\
Giselle         &   1.0 &   0.250855 \\
Gissel          &   1.0 &   0.117647 \\
Gissela         &   1.0 &   0.625000 \\
Gissele         &   1.0 &   0.285714 \\
Gissell         &   1.0 &   0.116279 \\
Gisselle        &   1.0 &   0.105422 \\
Gita            &   1.0 &   1.000000 \\
Giulia          &   1.0 &   1.000000 \\
Giuliana        &   1.0 &   0.213836 \\
Giulianna       &   1.0 &   0.280000 \\
Giulietta       &   1.0 &   0.416667 \\
Gizel           &   1.0 &   0.416667 \\
Gizell          &   1.0 &   1.000000 \\
Gizelle         &   1.0 &   0.062500 \\
Gizzelle        &   1.0 &   1.000000 \\
Gladis          &   1.0 &   0.119048 \\
Gladys          &   1.0 &   0.035897 \\
Glee            &   1.0 &   1.000000 \\
Glenda          &   1.0 &   0.030000 \\
Glendora        &   1.0 &   1.000000 \\
Glendy          &   1.0 &   0.857143 \\
Glenisha        &   1.0 &   1.000000 \\
Glenn           &   1.0 &   0.833333 \\
Glenna          &   1.0 &   0.131579 \\
Glennda         &   1.0 &   1.000000 \\
Glennis         &   1.0 &   1.000000 \\
Glenora         &   1.0 &   1.000000 \\
Glinda          &   1.0 &   0.545455 \\
Glori           &   1.0 &   1.000000 \\
Gloria          &   1.0 &   0.063616 \\
Gloriana        &   1.0 &   1.000000 \\
Glory           &   1.0 &   0.533333 \\
Glynda          &   1.0 &   1.000000 \\
Glynis          &   1.0 &   0.178571 \\
Glynnis         &   1.0 &   1.000000 \\
Goddess         &   1.0 &   1.000000 \\
Gohar           &   1.0 &   1.000000 \\
Golda           &   1.0 &   1.000000 \\
Golden          &   1.0 &   0.750000 \\
Goldie          &   1.0 &   1.000000 \\
Goretti         &   1.0 &   1.000000 \\
Grabiela        &   1.0 &   0.384615 \\
Grace           &   1.0 &   0.448517 \\
Graceann        &   1.0 &   1.000000 \\
Gracee          &   1.0 &   0.600000 \\
Gracelin        &   1.0 &   1.000000 \\
Gracelyn        &   1.0 &   0.653846 \\
Gracelynn       &   1.0 &   0.522727 \\
Gracen          &   1.0 &   0.875000 \\
Gracey          &   1.0 &   0.615385 \\
Graci           &   1.0 &   1.000000 \\
Gracia          &   1.0 &   1.000000 \\
Gracie          &   1.0 &   0.479798 \\
Graciela        &   1.0 &   0.152174 \\
Graciella       &   1.0 &   0.636364 \\
Gracy           &   1.0 &   0.666667 \\
Gracyn          &   1.0 &   0.384615 \\
Grasiela        &   1.0 &   1.000000 \\
Graviela        &   1.0 &   1.000000 \\
Gray            &   1.0 &   0.750000 \\
Grayce          &   1.0 &   0.700000 \\
Graycen         &   1.0 &   1.000000 \\
Graysen         &   1.0 &   1.000000 \\
Grayson         &   1.0 &   0.300000 \\
Grecia          &   1.0 &   0.456790 \\
Gredmarie       &   1.0 &   1.000000 \\
Greeicy         &   1.0 &   1.000000 \\
Greenlee        &   1.0 &   1.000000 \\
Greer           &   1.0 &   0.625000 \\
Gregoria        &   1.0 &   0.700000 \\
Gregory         &   1.0 &   0.416667 \\
Greicy          &   1.0 &   1.000000 \\
Greidy          &   1.0 &   1.000000 \\
Greidys         &   1.0 &   0.567568 \\
Gresia          &   1.0 &   1.000000 \\
Greta           &   1.0 &   0.390244 \\
Gretchen        &   1.0 &   0.044444 \\
Gretel          &   1.0 &   0.615385 \\
Grethel         &   1.0 &   1.000000 \\
Gretta          &   1.0 &   0.714286 \\
Grettel         &   1.0 &   0.705882 \\
Grey            &   1.0 &   0.615385 \\
Greydis         &   1.0 &   0.391304 \\
Greyson         &   1.0 &   1.000000 \\
Gricel          &   1.0 &   0.119048 \\
Gricelda        &   1.0 &   0.166667 \\
Grier           &   1.0 &   1.000000 \\
Grisel          &   1.0 &   0.064815 \\
Griselda        &   1.0 &   0.080745 \\
Grisell         &   1.0 &   1.000000 \\
Griselle        &   1.0 &   1.000000 \\
Grissel         &   1.0 &   0.555556 \\
Grizel          &   1.0 &   1.000000 \\
Grizelda        &   1.0 &   1.000000 \\
Guadalupe       &   1.0 &   0.092033 \\
Guadelupe       &   1.0 &   1.000000 \\
Gudalupe        &   1.0 &   1.000000 \\
Gudelia         &   1.0 &   1.000000 \\
Guiliana        &   1.0 &   0.454545 \\
Guillermina     &   1.0 &   0.200000 \\
Guillermo       &   1.0 &   1.000000 \\
Guinevere       &   1.0 &   0.428571 \\
Guisela         &   1.0 &   1.000000 \\
Guiselle        &   1.0 &   0.714286 \\
Gulianna        &   1.0 &   1.000000 \\
Gurasees        &   1.0 &   1.000000 \\
Gurbani         &   1.0 &   1.000000 \\
Gurjot          &   1.0 &   1.000000 \\
Gurleen         &   1.0 &   0.444444 \\
Gurnaaz         &   1.0 &   1.000000 \\
Gurneet         &   1.0 &   1.000000 \\
Gurnoor         &   1.0 &   0.300000 \\
Gurpreet        &   1.0 &   0.555556 \\
Gurseerat       &   1.0 &   1.000000 \\
Gursirat        &   1.0 &   1.000000 \\
Gustavo         &   1.0 &   0.625000 \\
Gwen            &   1.0 &   0.722222 \\
Gwenda          &   1.0 &   0.500000 \\
Gwendalyn       &   1.0 &   0.600000 \\
Gwendolyn       &   1.0 &   0.343915 \\
Gwendolynn      &   1.0 &   1.000000 \\
Gweneth         &   1.0 &   0.500000 \\
Gwenevere       &   1.0 &   1.000000 \\
Gwenivere       &   1.0 &   1.000000 \\
Gwenn           &   1.0 &   1.000000 \\
Gwenyth         &   1.0 &   0.280000 \\
Gwyn            &   1.0 &   0.375000 \\
Gwyneth         &   1.0 &   0.473684 \\
Gwynn           &   1.0 &   1.000000 \\
Gwynne          &   1.0 &   1.000000 \\
Gwynneth        &   1.0 &   1.000000 \\
Gyanna          &   1.0 &   1.000000 \\
Gypsy           &   1.0 &   0.666667 \\
Gyselle         &   1.0 &   0.888889 \\
Gyzelle         &   1.0 &   0.714286 \\
Ha              &   1.0 &   0.411765 \\
Haasini         &   1.0 &   1.000000 \\
Habiba          &   1.0 &   1.000000 \\
Hadar           &   1.0 &   1.000000 \\
Hadasa          &   1.0 &   0.538462 \\
Hadasha         &   1.0 &   1.000000 \\
Hadassa         &   1.0 &   0.500000 \\
Hadassah        &   1.0 &   0.615385 \\
Haddie          &   1.0 &   0.750000 \\
Hade            &   1.0 &   1.000000 \\
Hadeel          &   1.0 &   0.833333 \\
Hadia           &   1.0 &   1.000000 \\
Hadiya          &   1.0 &   1.000000 \\
Hadlee          &   1.0 &   0.555556 \\
Hadleigh        &   1.0 &   0.500000 \\
Hadley          &   1.0 &   0.551181 \\
Hae             &   1.0 &   1.000000 \\
Haeley          &   1.0 &   1.000000 \\
Hafsa           &   1.0 &   1.000000 \\
Hafsah          &   1.0 &   1.000000 \\
Haide           &   1.0 &   0.555556 \\
Haidee          &   1.0 &   0.500000 \\
Haiden          &   1.0 &   0.666667 \\
Haidi           &   1.0 &   1.000000 \\
Haidy           &   1.0 &   0.857143 \\
Haidyn          &   1.0 &   1.000000 \\
Haifa           &   1.0 &   1.000000 \\
Haile           &   1.0 &   0.454545 \\
Hailee          &   1.0 &   0.266667 \\
Hailei          &   1.0 &   1.000000 \\
Haileigh        &   1.0 &   0.714286 \\
Hailey          &   1.0 &   0.462273 \\
Haili           &   1.0 &   1.000000 \\
Hailie          &   1.0 &   0.301587 \\
Haille          &   1.0 &   1.000000 \\
Hailley         &   1.0 &   1.000000 \\
Haillie         &   1.0 &   1.000000 \\
Haily           &   1.0 &   0.214286 \\
Hailyn          &   1.0 &   1.000000 \\
Haisley         &   1.0 &   0.837209 \\
Haizel          &   1.0 &   1.000000 \\
Haizley         &   1.0 &   0.857143 \\
Hajar           &   1.0 &   1.000000 \\
Hala            &   1.0 &   0.857143 \\
Halee           &   1.0 &   0.416667 \\
Haleema         &   1.0 &   0.833333 \\
Haleigh         &   1.0 &   0.117647 \\
Halen           &   1.0 &   1.000000 \\
Halena          &   1.0 &   0.714286 \\
Haley           &   1.0 &   0.082489 \\
Hali            &   1.0 &   0.185185 \\
Halia           &   1.0 &   0.900000 \\
Halie           &   1.0 &   0.125000 \\
Halima          &   1.0 &   0.916667 \\
Halina          &   1.0 &   0.833333 \\
Halle           &   1.0 &   0.661017 \\
Hallee          &   1.0 &   1.000000 \\
Hallel          &   1.0 &   1.000000 \\
Halley          &   1.0 &   0.210526 \\
Halli           &   1.0 &   1.000000 \\
Hallie          &   1.0 &   0.840000 \\
Halo            &   1.0 &   0.875000 \\
Halsey          &   1.0 &   0.833333 \\
Halston         &   1.0 &   0.666667 \\
Halyn           &   1.0 &   1.000000 \\
Han             &   1.0 &   0.777778 \\
Hana            &   1.0 &   0.797980 \\
Hanah           &   1.0 &   0.642857 \\
Hanako          &   1.0 &   1.000000 \\
Hanalei         &   1.0 &   0.833333 \\
Hanan           &   1.0 &   0.888889 \\
Haneen          &   1.0 &   0.555556 \\
Hang            &   1.0 &   0.263158 \\
Hanh            &   1.0 &   0.333333 \\
Hania           &   1.0 &   0.600000 \\
Haniya          &   1.0 &   0.500000 \\
Hanna           &   1.0 &   0.372973 \\
Hannah          &   1.0 &   0.255124 \\
Hannahgrace     &   1.0 &   1.000000 \\
Hannaley        &   1.0 &   1.000000 \\
Hannan          &   1.0 &   0.833333 \\
Hannia          &   1.0 &   0.230769 \\
Hannya          &   1.0 &   1.000000 \\
Hansika         &   1.0 &   0.625000 \\
Harbor          &   1.0 &   0.714286 \\
Hareem          &   1.0 &   0.833333 \\
Hargun          &   1.0 &   0.538462 \\
Harini          &   1.0 &   0.714286 \\
Harir           &   1.0 &   1.000000 \\
Harjot          &   1.0 &   1.000000 \\
Harkiran        &   1.0 &   1.000000 \\
Harlan          &   1.0 &   0.833333 \\
Harlee          &   1.0 &   0.442623 \\
Harleen         &   1.0 &   0.310345 \\
Harleigh        &   1.0 &   0.562500 \\
Harlem          &   1.0 &   0.833333 \\
Harlene         &   1.0 &   0.428571 \\
Harley          &   1.0 &   0.674286 \\
Harleyquinn     &   1.0 &   1.000000 \\
Harli           &   1.0 &   0.625000 \\
Harlie          &   1.0 &   0.642857 \\
Harlo           &   1.0 &   1.000000 \\
Harlow          &   1.0 &   0.863014 \\
Harlowe         &   1.0 &   0.800000 \\
Harlyn          &   1.0 &   0.368421 \\
Harlynn         &   1.0 &   0.555556 \\
Harman          &   1.0 &   1.000000 \\
Harmehar        &   1.0 &   1.000000 \\
Harmonee        &   1.0 &   0.714286 \\
Harmoney        &   1.0 &   1.000000 \\
Harmoni         &   1.0 &   0.529412 \\
Harmonie        &   1.0 &   0.538462 \\
Harmony         &   1.0 &   0.785185 \\
Harnaaz         &   1.0 &   1.000000 \\
Harneet         &   1.0 &   0.857143 \\
Harnoor         &   1.0 &   0.615385 \\
Harold          &   1.0 &   1.000000 \\
Harper          &   1.0 &   0.702427 \\
Harpreet        &   1.0 &   0.500000 \\
Harriet         &   1.0 &   0.214286 \\
Harriett        &   1.0 &   0.375000 \\
Harriette       &   1.0 &   0.466667 \\
Harshini        &   1.0 &   1.000000 \\
Harshita        &   1.0 &   1.000000 \\
Harsimran       &   1.0 &   1.000000 \\
Hartley         &   1.0 &   0.625000 \\
Haru            &   1.0 &   1.000000 \\
Haruka          &   1.0 &   0.555556 \\
Haruko          &   1.0 &   0.263158 \\
Harumi          &   1.0 &   1.000000 \\
Haruna          &   1.0 &   1.000000 \\
Haruye          &   1.0 &   0.416667 \\
Harveen         &   1.0 &   1.000000 \\
Hasina          &   1.0 &   1.000000 \\
Hasini          &   1.0 &   0.416667 \\
Hasmik          &   1.0 &   0.500000 \\
Hathaway        &   1.0 &   1.000000 \\
Hatsuko         &   1.0 &   1.000000 \\
Hatsumi         &   1.0 &   1.000000 \\
Hatsuye         &   1.0 &   1.000000 \\
Hattie          &   1.0 &   0.720000 \\
Hava            &   1.0 &   1.000000 \\
Havana          &   1.0 &   0.615385 \\
Havanna         &   1.0 &   0.714286 \\
Haven           &   1.0 &   0.840426 \\
Havyn           &   1.0 &   0.833333 \\
Hawa            &   1.0 &   0.812500 \\
Haya            &   1.0 &   0.722222 \\
Hayat           &   1.0 &   1.000000 \\
Hayde           &   1.0 &   0.833333 \\
Haydee          &   1.0 &   0.214286 \\
Hayden          &   1.0 &   0.349794 \\
Hayes           &   1.0 &   0.461538 \\
Hayle           &   1.0 &   0.555556 \\
Haylee          &   1.0 &   0.186207 \\
Haylei          &   1.0 &   0.857143 \\
Hayleigh        &   1.0 &   0.333333 \\
Haylen          &   1.0 &   0.357143 \\
Hayley          &   1.0 &   0.144654 \\
Hayli           &   1.0 &   1.000000 \\
Haylie          &   1.0 &   0.087912 \\
Haylin          &   1.0 &   1.000000 \\
Hayven          &   1.0 &   0.500000 \\
Hayzel          &   1.0 &   1.000000 \\
Hazel           &   1.0 &   1.000000 \\
Hazell          &   1.0 &   1.000000 \\
Hazelle         &   1.0 &   0.857143 \\
Hazelyn         &   1.0 &   1.000000 \\
Heather         &   1.0 &   0.009926 \\
Heaven          &   1.0 &   0.382550 \\
Heavenlee       &   1.0 &   0.857143 \\
Heavenly        &   1.0 &   0.526316 \\
Heavyn          &   1.0 &   0.714286 \\
Heba            &   1.0 &   0.555556 \\
Hebe            &   1.0 &   1.000000 \\
Hector          &   1.0 &   0.333333 \\
Hedy            &   1.0 &   0.600000 \\
Heela           &   1.0 &   1.000000 \\
Heena           &   1.0 &   1.000000 \\
Heer            &   1.0 &   1.000000 \\
Heide           &   1.0 &   0.384615 \\
Heidi           &   1.0 &   0.140678 \\
Heidie          &   1.0 &   1.000000 \\
Heidy           &   1.0 &   0.236559 \\
Heiley          &   1.0 &   0.625000 \\
Heily           &   1.0 &   0.538462 \\
Heiress         &   1.0 &   0.888889 \\
Helaina         &   1.0 &   0.833333 \\
Helaine         &   1.0 &   1.000000 \\
Helayna         &   1.0 &   1.000000 \\
Helen           &   1.0 &   0.085366 \\
Helena          &   1.0 &   0.686957 \\
Helene          &   1.0 &   0.138889 \\
Helga           &   1.0 &   0.750000 \\
Hellen          &   1.0 &   0.260870 \\
Helyn           &   1.0 &   1.000000 \\
Hena            &   1.0 &   1.000000 \\
Hendrix         &   1.0 &   0.636364 \\
Henessy         &   1.0 &   0.500000 \\
Henley          &   1.0 &   0.476190 \\
Henna           &   1.0 &   0.352941 \\
Hennessey       &   1.0 &   0.714286 \\
Hennessy        &   1.0 &   0.264706 \\
Hennesy         &   1.0 &   1.000000 \\
Henrietta       &   1.0 &   0.106383 \\
Henry           &   1.0 &   0.454545 \\
Hensley         &   1.0 &   0.312500 \\
Hera            &   1.0 &   0.818182 \\
Herlinda        &   1.0 &   0.388889 \\
Hermelinda      &   1.0 &   0.384615 \\
Hermila         &   1.0 &   1.000000 \\
Herminia        &   1.0 &   0.400000 \\
Hermione        &   1.0 &   0.647059 \\
Hester          &   1.0 &   0.833333 \\
Hether          &   1.0 &   1.000000 \\
Heydi           &   1.0 &   0.777778 \\
Heydy           &   1.0 &   1.000000 \\
Hiba            &   1.0 &   0.909091 \\
Hideko          &   1.0 &   0.666667 \\
Hiedi           &   1.0 &   0.700000 \\
Hien            &   1.0 &   0.416667 \\
Hikari          &   1.0 &   0.750000 \\
Hila            &   1.0 &   1.000000 \\
Hilaria         &   1.0 &   1.000000 \\
Hilarie         &   1.0 &   1.000000 \\
Hilary          &   1.0 &   0.047945 \\
Hilda           &   1.0 &   0.101852 \\
Hiliana         &   1.0 &   1.000000 \\
Hillarie        &   1.0 &   1.000000 \\
Hillary         &   1.0 &   0.051282 \\
Hillery         &   1.0 &   1.000000 \\
Hilma           &   1.0 &   1.000000 \\
Himani          &   1.0 &   1.000000 \\
Hina            &   1.0 &   0.625000 \\
Hinata          &   1.0 &   0.875000 \\
Hira            &   1.0 &   1.000000 \\
Hiraya          &   1.0 &   1.000000 \\
Hiroko          &   1.0 &   0.833333 \\
Hiromi          &   1.0 &   0.555556 \\
Hisako          &   1.0 &   0.312500 \\
Hisaye          &   1.0 &   0.833333 \\
Hiya            &   1.0 &   1.000000 \\
Hiyab           &   1.0 &   1.000000 \\
Hlee            &   1.0 &   1.000000 \\
Hoa             &   1.0 &   0.312500 \\
Hoai            &   1.0 &   1.000000 \\
Hoang           &   1.0 &   1.000000 \\
Holiday         &   1.0 &   0.750000 \\
Holland         &   1.0 &   0.641026 \\
Hollee          &   1.0 &   0.833333 \\
Holley          &   1.0 &   0.545455 \\
Holli           &   1.0 &   0.375000 \\
Hollie          &   1.0 &   0.125000 \\
Hollis          &   1.0 &   0.727273 \\
Holly           &   1.0 &   0.129897 \\
Hollyann        &   1.0 &   0.714286 \\
Hollyn          &   1.0 &   0.666667 \\
Honesty         &   1.0 &   0.761905 \\
Honey           &   1.0 &   0.840000 \\
Hong            &   1.0 &   0.500000 \\
Honor           &   1.0 &   0.588235 \\
Hoorain         &   1.0 &   0.875000 \\
Hope            &   1.0 &   0.723164 \\
Hopie           &   1.0 &   1.000000 \\
Hortencia       &   1.0 &   0.222222 \\
Hortense        &   1.0 &   0.263158 \\
Hortensia       &   1.0 &   0.178571 \\
Hosanna         &   1.0 &   0.588235 \\
Hosna           &   1.0 &   1.000000 \\
Houa            &   1.0 &   0.636364 \\
Hripsime        &   1.0 &   1.000000 \\
Huda            &   1.0 &   0.705882 \\
Hudson          &   1.0 &   0.450000 \\
Hudsyn          &   1.0 &   1.000000 \\
Hugo            &   1.0 &   1.000000 \\
Hulda           &   1.0 &   1.000000 \\
Hunter          &   1.0 &   0.452055 \\
Huntley         &   1.0 &   1.000000 \\
Huong           &   1.0 &   0.294118 \\
Husna           &   1.0 &   0.666667 \\
Huxley          &   1.0 &   1.000000 \\
Huyen           &   1.0 &   0.625000 \\
Hydeia          &   1.0 &   1.000000 \\
Ia              &   1.0 &   1.000000 \\
Ian             &   1.0 &   0.454545 \\
Iana            &   1.0 &   1.000000 \\
Ianna           &   1.0 &   0.454545 \\
Ibeth           &   1.0 &   0.454545 \\
Icel            &   1.0 &   1.000000 \\
Icela           &   1.0 &   0.625000 \\
Icey            &   1.0 &   1.000000 \\
Icsel           &   1.0 &   1.000000 \\
Ida             &   1.0 &   0.235294 \\
Idalia          &   1.0 &   0.526316 \\
Idalie          &   1.0 &   0.625000 \\
Idalis          &   1.0 &   0.315789 \\
Idaly           &   1.0 &   0.227273 \\
Idalys          &   1.0 &   1.000000 \\
Idania          &   1.0 &   0.545455 \\
Idell           &   1.0 &   1.000000 \\
Idella          &   1.0 &   1.000000 \\
Ieasha          &   1.0 &   1.000000 \\
Iesha           &   1.0 &   0.111111 \\
Ieshia          &   1.0 &   1.000000 \\
Ignacia         &   1.0 &   0.833333 \\
Ikea            &   1.0 &   0.625000 \\
Ila             &   1.0 &   0.818182 \\
Ilah            &   1.0 &   0.777778 \\
Ilaisaane       &   1.0 &   1.000000 \\
Ilana           &   1.0 &   0.304348 \\
Ilani           &   1.0 &   0.714286 \\
Ilany           &   1.0 &   1.000000 \\
Ilaria          &   1.0 &   1.000000 \\
Ilce            &   1.0 &   1.000000 \\
Ilda            &   1.0 &   0.384615 \\
Ilea            &   1.0 &   1.000000 \\
Ileana          &   1.0 &   0.216667 \\
Ileanna         &   1.0 &   0.750000 \\
Ileen           &   1.0 &   0.388889 \\
Ileene          &   1.0 &   0.875000 \\
Ilena           &   1.0 &   1.000000 \\
Ilene           &   1.0 &   0.162791 \\
Ilette          &   1.0 &   1.000000 \\
Ilia            &   1.0 &   1.000000 \\
Iliana          &   1.0 &   0.417219 \\
Ilianna         &   1.0 &   0.428571 \\
Illeana         &   1.0 &   1.000000 \\
Illiana         &   1.0 &   0.733333 \\
Illyana         &   1.0 &   1.000000 \\
Ilona           &   1.0 &   0.416667 \\
Ilsa            &   1.0 &   1.000000 \\
Ilse            &   1.0 &   0.341463 \\
Ily             &   1.0 &   0.928571 \\
Ilyana          &   1.0 &   0.642857 \\
Ilyanna         &   1.0 &   1.000000 \\
Ilyssa          &   1.0 &   0.500000 \\
Ima             &   1.0 &   1.000000 \\
Imaan           &   1.0 &   0.777778 \\
Imalay          &   1.0 &   1.000000 \\
Iman            &   1.0 &   0.344828 \\
Imani           &   1.0 &   0.438017 \\
Imara           &   1.0 &   1.000000 \\
Imari           &   1.0 &   0.454545 \\
Imee            &   1.0 &   1.000000 \\
Imelda          &   1.0 &   0.102941 \\
Imogen          &   1.0 &   0.642857 \\
Imogene         &   1.0 &   0.277778 \\
Imunique        &   1.0 &   0.833333 \\
Ina             &   1.0 &   0.631579 \\
Inaaya          &   1.0 &   0.735294 \\
Inara           &   1.0 &   1.000000 \\
Inari           &   1.0 &   1.000000 \\
Inaya           &   1.0 &   1.000000 \\
Inayah          &   1.0 &   0.384615 \\
Inayat          &   1.0 &   0.888889 \\
Indi            &   1.0 &   0.750000 \\
India           &   1.0 &   0.327273 \\
Indiana         &   1.0 &   0.818182 \\
Indica          &   1.0 &   0.833333 \\
Indie           &   1.0 &   0.918367 \\
Indigo          &   1.0 &   0.675676 \\
Indira          &   1.0 &   0.611111 \\
Indra           &   1.0 &   1.000000 \\
Indy            &   1.0 &   1.000000 \\
Indya           &   1.0 &   0.416667 \\
Ines            &   1.0 &   0.552632 \\
Inessa          &   1.0 &   0.416667 \\
Inez            &   1.0 &   0.305085 \\
Infinity        &   1.0 &   0.714286 \\
Inga            &   1.0 &   0.333333 \\
Inge            &   1.0 &   0.714286 \\
Inger           &   1.0 &   0.437500 \\
Ingrid          &   1.0 &   0.157025 \\
Ingris          &   1.0 &   1.000000 \\
Inika           &   1.0 &   0.714286 \\
Iniya           &   1.0 &   0.750000 \\
Inna            &   1.0 &   0.625000 \\
Inori           &   1.0 &   1.000000 \\
Io              &   1.0 &   1.000000 \\
Iola            &   1.0 &   0.500000 \\
Iolana          &   1.0 &   1.000000 \\
Iona            &   1.0 &   1.000000 \\
Ione            &   1.0 &   0.384615 \\
Iqra            &   1.0 &   0.769231 \\
Ira             &   1.0 &   1.000000 \\
Iracema         &   1.0 &   1.000000 \\
Iraida          &   1.0 &   1.000000 \\
Irais           &   1.0 &   0.461538 \\
Iran            &   1.0 &   0.400000 \\
Irania          &   1.0 &   0.147059 \\
Irasema         &   1.0 &   0.642857 \\
Irazema         &   1.0 &   1.000000 \\
Ireland         &   1.0 &   0.288889 \\
Irelia          &   1.0 &   1.000000 \\
Irelyn          &   1.0 &   0.500000 \\
Irelynn         &   1.0 &   0.428571 \\
Irena           &   1.0 &   0.666667 \\
Irene           &   1.0 &   0.208920 \\
Ireri           &   1.0 &   1.000000 \\
Irha            &   1.0 &   1.000000 \\
Iriana          &   1.0 &   0.555556 \\
Iridian         &   1.0 &   0.079545 \\
Iridiana        &   1.0 &   0.240000 \\
Irie            &   1.0 &   0.571429 \\
Irina           &   1.0 &   0.809524 \\
Iris            &   1.0 &   1.000000 \\
Irlanda         &   1.0 &   0.750000 \\
Irma            &   1.0 &   0.059459 \\
Isa             &   1.0 &   0.782609 \\
Isaac           &   1.0 &   0.833333 \\
Isabel          &   1.0 &   0.370690 \\
Isabela         &   1.0 &   0.494565 \\
Isabell         &   1.0 &   0.222222 \\
Isabella        &   1.0 &   0.513938 \\
Isabellah       &   1.0 &   1.000000 \\
Isabellamarie   &   1.0 &   0.454545 \\
Isabelle        &   1.0 &   0.464692 \\
Isadora         &   1.0 &   0.592593 \\
Isaiah          &   1.0 &   0.833333 \\
Isamar          &   1.0 &   0.035000 \\
Isamara         &   1.0 &   1.000000 \\
Isaura          &   1.0 &   0.227273 \\
Isel            &   1.0 &   0.200000 \\
Isela           &   1.0 &   0.434783 \\
Isella          &   1.0 &   0.625000 \\
Isha            &   1.0 &   0.820513 \\
Ishana          &   1.0 &   1.000000 \\
Ishani          &   1.0 &   0.562500 \\
Ishanvi         &   1.0 &   0.461538 \\
Ishika          &   1.0 &   0.538462 \\
Ishita          &   1.0 &   0.555556 \\
Isidora         &   1.0 &   1.000000 \\
Isis            &   1.0 &   0.078947 \\
Isla            &   1.0 &   0.910632 \\
Islah           &   1.0 &   0.625000 \\
Island          &   1.0 &   0.833333 \\
Islay           &   1.0 &   1.000000 \\
Isley           &   1.0 &   0.700000 \\
Ismar           &   1.0 &   1.000000 \\
Ismenia         &   1.0 &   1.000000 \\
Ismerai         &   1.0 &   0.875000 \\
Isobel          &   1.0 &   0.333333 \\
Isolde          &   1.0 &   0.833333 \\
Isra            &   1.0 &   1.000000 \\
Israa           &   1.0 &   1.000000 \\
Israel          &   1.0 &   0.461538 \\
Issa            &   1.0 &   0.894737 \\
Issabela        &   1.0 &   0.833333 \\
Issabella       &   1.0 &   0.260870 \\
Issis           &   1.0 &   1.000000 \\
Isys            &   1.0 &   1.000000 \\
Isyss           &   1.0 &   0.312500 \\
Italia          &   1.0 &   0.459459 \\
Italy           &   1.0 &   0.692308 \\
Itati           &   1.0 &   0.500000 \\
Itcel           &   1.0 &   1.000000 \\
Ithzel          &   1.0 &   0.833333 \\
Itsel           &   1.0 &   0.357143 \\
Itxel           &   1.0 &   1.000000 \\
Itza            &   1.0 &   0.857143 \\
Itzabella       &   1.0 &   0.555556 \\
Itzae           &   1.0 &   0.785714 \\
Itzamara        &   1.0 &   0.500000 \\
Itzanami        &   1.0 &   1.000000 \\
Itzayana        &   1.0 &   0.806452 \\
Itzayanna       &   1.0 &   1.000000 \\
Itzel           &   1.0 &   0.522013 \\
Itzela          &   1.0 &   1.000000 \\
Itzell          &   1.0 &   1.000000 \\
Itzelle         &   1.0 &   1.000000 \\
Itzia           &   1.0 &   0.533333 \\
Itzy            &   1.0 &   1.000000 \\
Iva             &   1.0 &   0.437500 \\
Ivan            &   1.0 &   0.416667 \\
Ivana           &   1.0 &   0.578947 \\
Ivania          &   1.0 &   0.600000 \\
Ivanka          &   1.0 &   0.387755 \\
Ivanna          &   1.0 &   0.738562 \\
Ivannia         &   1.0 &   1.000000 \\
Ivee            &   1.0 &   1.000000 \\
Ivet            &   1.0 &   0.857143 \\
Iveth           &   1.0 &   0.583333 \\
Ivett           &   1.0 &   0.375000 \\
Ivette          &   1.0 &   0.099502 \\
Ivey            &   1.0 &   0.777778 \\
Ivie            &   1.0 &   0.888889 \\
Ivon            &   1.0 &   0.222222 \\
Ivone           &   1.0 &   0.500000 \\
Ivonna          &   1.0 &   1.000000 \\
Ivonne          &   1.0 &   0.057692 \\
Ivori           &   1.0 &   1.000000 \\
Ivory           &   1.0 &   0.794118 \\
Ivry            &   1.0 &   1.000000 \\
Ivy             &   1.0 &   0.981609 \\
Ivyrose         &   1.0 &   0.857143 \\
Ixchel          &   1.0 &   0.290323 \\
Ixel            &   1.0 &   1.000000 \\
Iyana           &   1.0 &   0.217391 \\
Iyanah          &   1.0 &   0.833333 \\
Iyanna          &   1.0 &   0.565217 \\
Iyannah         &   1.0 &   1.000000 \\
Iyari           &   1.0 &   1.000000 \\
Iyla            &   1.0 &   1.000000 \\
Iylah           &   1.0 &   1.000000 \\
Iyonna          &   1.0 &   0.833333 \\
Iza             &   1.0 &   1.000000 \\
Izabel          &   1.0 &   0.540541 \\
Izabela         &   1.0 &   0.454545 \\
Izabell         &   1.0 &   0.357143 \\
Izabella        &   1.0 &   0.359773 \\
Izabellah       &   1.0 &   1.000000 \\
Izabelle        &   1.0 &   0.264706 \\
Izamar          &   1.0 &   0.106383 \\
Izamary         &   1.0 &   1.000000 \\
Izel            &   1.0 &   0.500000 \\
Izela           &   1.0 &   1.000000 \\
Izellah         &   1.0 &   1.000000 \\
Izelle          &   1.0 &   1.000000 \\
Izumi           &   1.0 &   1.000000 \\
Izzabella       &   1.0 &   0.461538 \\
Izzah           &   1.0 &   1.000000 \\
Izzy            &   1.0 &   1.000000 \\
Jaanai          &   1.0 &   1.000000 \\
Jaanvi          &   1.0 &   1.000000 \\
Jacalyn         &   1.0 &   0.240000 \\
Jace            &   1.0 &   1.000000 \\
Jacee           &   1.0 &   1.000000 \\
Jacelyn         &   1.0 &   0.600000 \\
Jacey           &   1.0 &   0.241379 \\
Jaci            &   1.0 &   0.714286 \\
Jacie           &   1.0 &   1.000000 \\
Jacinda         &   1.0 &   0.156250 \\
Jacinta         &   1.0 &   0.700000 \\
Jack            &   1.0 &   0.714286 \\
Jackelin        &   1.0 &   0.320000 \\
Jackeline       &   1.0 &   0.127451 \\
Jackelyn        &   1.0 &   0.107692 \\
Jackelyne       &   1.0 &   0.800000 \\
Jacki           &   1.0 &   0.315789 \\
Jackie          &   1.0 &   0.025070 \\
Jacklin         &   1.0 &   0.625000 \\
Jackline        &   1.0 &   1.000000 \\
Jacklyn         &   1.0 &   0.068966 \\
Jacklynn        &   1.0 &   0.416667 \\
Jackqueline     &   1.0 &   1.000000 \\
Jackson         &   1.0 &   0.833333 \\
Jacky           &   1.0 &   1.000000 \\
Jaclyn          &   1.0 &   0.034602 \\
Jaclynn         &   1.0 &   0.333333 \\
Jacob           &   1.0 &   0.375000 \\
Jacolyn         &   1.0 &   1.000000 \\
Jacqlyn         &   1.0 &   0.750000 \\
Jacqualine      &   1.0 &   1.000000 \\
Jacqualyn       &   1.0 &   1.000000 \\
Jacque          &   1.0 &   0.434783 \\
Jacqueleen      &   1.0 &   1.000000 \\
Jacquelene      &   1.0 &   0.777778 \\
Jacquelin       &   1.0 &   0.123077 \\
Jacquelina      &   1.0 &   1.000000 \\
Jacqueline      &   1.0 &   0.071749 \\
Jacquelyn       &   1.0 &   0.081340 \\
Jacquelyne      &   1.0 &   0.400000 \\
Jacquelynn      &   1.0 &   0.260870 \\
Jacquelynne     &   1.0 &   1.000000 \\
Jacqui          &   1.0 &   1.000000 \\
Jacquie         &   1.0 &   0.277778 \\
Jacquiline      &   1.0 &   0.714286 \\
Jacquilyn       &   1.0 &   1.000000 \\
Jacquline       &   1.0 &   0.230769 \\
Jacqulyn        &   1.0 &   0.312500 \\
Jacy            &   1.0 &   0.583333 \\
Jada            &   1.0 &   0.073171 \\
Jadah           &   1.0 &   0.466667 \\
Jadalyn         &   1.0 &   0.833333 \\
Jadalynn        &   1.0 &   1.000000 \\
Jade            &   1.0 &   0.950758 \\
Jadelyn         &   1.0 &   0.388889 \\
Jadelynn        &   1.0 &   0.777778 \\
Jaden           &   1.0 &   0.035211 \\
Jadey           &   1.0 &   1.000000 \\
Jadeyn          &   1.0 &   0.500000 \\
Jadie           &   1.0 &   1.000000 \\
Jadin           &   1.0 &   0.714286 \\
Jadine          &   1.0 &   1.000000 \\
Jadira          &   1.0 &   1.000000 \\
Jadore          &   1.0 &   0.777778 \\
Jadyn           &   1.0 &   0.089431 \\
Jadynn          &   1.0 &   0.625000 \\
Jadzia          &   1.0 &   1.000000 \\
Jae             &   1.0 &   1.000000 \\
Jaeda           &   1.0 &   0.178571 \\
Jaeden          &   1.0 &   0.500000 \\
Jaedyn          &   1.0 &   0.230769 \\
Jael            &   1.0 &   0.260870 \\
Jaela           &   1.0 &   0.333333 \\
Jaelah          &   1.0 &   0.625000 \\
Jaelani         &   1.0 &   1.000000 \\
Jaeleen         &   1.0 &   0.750000 \\
Jaelen          &   1.0 &   0.625000 \\
Jaelene         &   1.0 &   0.411765 \\
Jaeliana        &   1.0 &   1.000000 \\
Jaelin          &   1.0 &   0.833333 \\
Jaelle          &   1.0 &   0.714286 \\
Jaelyn          &   1.0 &   0.230159 \\
Jaelynn         &   1.0 &   0.552239 \\
Jaelynne        &   1.0 &   1.000000 \\
Jagger          &   1.0 &   1.000000 \\
Jahaira         &   1.0 &   0.217391 \\
Jahayra         &   1.0 &   1.000000 \\
Jahira          &   1.0 &   1.000000 \\
Jahna           &   1.0 &   1.000000 \\
Jahnae          &   1.0 &   1.000000 \\
Jahnavi         &   1.0 &   0.625000 \\
Jahniya         &   1.0 &   1.000000 \\
Jahzara         &   1.0 &   0.384615 \\
Jai             &   1.0 &   0.500000 \\
Jaia            &   1.0 &   0.875000 \\
Jaida           &   1.0 &   0.139535 \\
Jaidah          &   1.0 &   0.714286 \\
Jaidan          &   1.0 &   0.888889 \\
Jaide           &   1.0 &   0.555556 \\
Jaiden          &   1.0 &   0.095238 \\
Jaidin          &   1.0 &   1.000000 \\
Jaidy           &   1.0 &   1.000000 \\
Jaidyn          &   1.0 &   0.122807 \\
Jaidynn         &   1.0 &   1.000000 \\
Jaila           &   1.0 &   0.357143 \\
Jailah          &   1.0 &   0.454545 \\
Jailani         &   1.0 &   1.000000 \\
Jaileen         &   1.0 &   0.333333 \\
Jailene         &   1.0 &   0.078740 \\
Jailenne        &   1.0 &   1.000000 \\
Jailia          &   1.0 &   1.000000 \\
Jailin          &   1.0 &   1.000000 \\
Jailine         &   1.0 &   0.227273 \\
Jailyn          &   1.0 &   0.629630 \\
Jailyne         &   1.0 &   0.700000 \\
Jailynn         &   1.0 &   0.357143 \\
Jaime           &   1.0 &   0.014608 \\
Jaimee          &   1.0 &   0.222222 \\
Jaimi           &   1.0 &   0.411765 \\
Jaimie          &   1.0 &   0.063291 \\
Jaimy           &   1.0 &   0.833333 \\
Jaina           &   1.0 &   0.500000 \\
Jaira           &   1.0 &   1.000000 \\
Jaiya           &   1.0 &   1.000000 \\
Jaiyana         &   1.0 &   1.000000 \\
Jakayla         &   1.0 &   0.625000 \\
Jake            &   1.0 &   1.000000 \\
Jakelin         &   1.0 &   0.833333 \\
Jakeline        &   1.0 &   0.416667 \\
Jakelyn         &   1.0 &   0.500000 \\
Jakia           &   1.0 &   1.000000 \\
Jala            &   1.0 &   0.833333 \\
Jalani          &   1.0 &   1.000000 \\
Jalaya          &   1.0 &   0.454545 \\
Jalayah         &   1.0 &   0.705882 \\
Jalea           &   1.0 &   1.000000 \\
Jaleah          &   1.0 &   0.666667 \\
Jaleen          &   1.0 &   1.000000 \\
Jaleesa         &   1.0 &   0.194444 \\
Jalen           &   1.0 &   0.466667 \\
Jalena          &   1.0 &   1.000000 \\
Jalene          &   1.0 &   0.454545 \\
Jalessa         &   1.0 &   0.833333 \\
Jaleyah         &   1.0 &   1.000000 \\
Jaleyza         &   1.0 &   1.000000 \\
Jalia           &   1.0 &   0.625000 \\
Jaliah          &   1.0 &   0.833333 \\
Jalina          &   1.0 &   1.000000 \\
Jalisa          &   1.0 &   0.162162 \\
Jalissa         &   1.0 &   0.200000 \\
Jaliyah         &   1.0 &   0.500000 \\
Jalyn           &   1.0 &   0.272727 \\
Jalynn          &   1.0 &   0.312500 \\
Jalyssa         &   1.0 &   0.411765 \\
Jamaica         &   1.0 &   0.583333 \\
Jamara          &   1.0 &   1.000000 \\
Jamaya          &   1.0 &   1.000000 \\
Jamee           &   1.0 &   0.500000 \\
Jameela         &   1.0 &   0.714286 \\
Jameelah        &   1.0 &   0.312500 \\
Jameika         &   1.0 &   1.000000 \\
Jameisha        &   1.0 &   1.000000 \\
Jameka          &   1.0 &   0.625000 \\
Jamela          &   1.0 &   0.714286 \\
Jamelia         &   1.0 &   0.833333 \\
James           &   1.0 &   0.200000 \\
Jamesha         &   1.0 &   0.388889 \\
Jameson         &   1.0 &   0.500000 \\
Jamey           &   1.0 &   0.437500 \\
Jami            &   1.0 &   0.058140 \\
Jamia           &   1.0 &   1.000000 \\
Jamiah          &   1.0 &   0.714286 \\
Jamica          &   1.0 &   0.714286 \\
Jamie           &   1.0 &   0.057319 \\
Jamielee        &   1.0 &   0.833333 \\
Jamika          &   1.0 &   0.600000 \\
Jamila          &   1.0 &   0.210526 \\
Jamilah         &   1.0 &   0.416667 \\
Jamile          &   1.0 &   1.000000 \\
Jamilee         &   1.0 &   1.000000 \\
Jamilet         &   1.0 &   0.230769 \\
Jamileth        &   1.0 &   0.380952 \\
Jamilett        &   1.0 &   1.000000 \\
Jamilette       &   1.0 &   0.857143 \\
Jamilex         &   1.0 &   0.666667 \\
Jamilla         &   1.0 &   0.714286 \\
Jamillah        &   1.0 &   0.833333 \\
Jamine          &   1.0 &   1.000000 \\
Jamisha         &   1.0 &   0.625000 \\
Jamison         &   1.0 &   0.384615 \\
Jamiya          &   1.0 &   1.000000 \\
Jamiyah         &   1.0 &   0.750000 \\
Jammie          &   1.0 &   0.192308 \\
Jamya           &   1.0 &   0.454545 \\
Jan             &   1.0 &   0.014663 \\
Jana            &   1.0 &   0.092715 \\
Janae           &   1.0 &   0.212766 \\
Janai           &   1.0 &   0.357143 \\
Janalee         &   1.0 &   0.454545 \\
Janan           &   1.0 &   1.000000 \\
Janani          &   1.0 &   1.000000 \\
Janay           &   1.0 &   0.092593 \\
Janaya          &   1.0 &   0.294118 \\
Janayah         &   1.0 &   0.833333 \\
Janaye          &   1.0 &   0.545455 \\
Jane            &   1.0 &   0.220000 \\
Janea           &   1.0 &   1.000000 \\
Janean          &   1.0 &   0.384615 \\
Janee           &   1.0 &   0.178571 \\
Janeen          &   1.0 &   0.152174 \\
Janel           &   1.0 &   0.160714 \\
Janele          &   1.0 &   1.000000 \\
Janeli          &   1.0 &   0.454545 \\
Janell          &   1.0 &   0.122807 \\
Janella         &   1.0 &   1.000000 \\
Janelle         &   1.0 &   0.319328 \\
Janelli         &   1.0 &   1.000000 \\
Janelly         &   1.0 &   0.448276 \\
Janely          &   1.0 &   0.171429 \\
Janene          &   1.0 &   0.304348 \\
Janesa          &   1.0 &   1.000000 \\
Janese          &   1.0 &   1.000000 \\
Janesha         &   1.0 &   1.000000 \\
Janessa         &   1.0 &   0.262712 \\
Janet           &   1.0 &   0.015424 \\
Janete          &   1.0 &   1.000000 \\
Janeth          &   1.0 &   0.246377 \\
Janett          &   1.0 &   0.156250 \\
Janetta         &   1.0 &   0.625000 \\
Janette         &   1.0 &   0.036145 \\
Janey           &   1.0 &   0.384615 \\
Jani            &   1.0 &   0.555556 \\
Jania           &   1.0 &   0.294118 \\
Janiah          &   1.0 &   0.388889 \\
Janica          &   1.0 &   1.000000 \\
Janice          &   1.0 &   0.014691 \\
Janie           &   1.0 &   0.152941 \\
Janiece         &   1.0 &   0.500000 \\
Janiel          &   1.0 &   1.000000 \\
Janielle        &   1.0 &   0.777778 \\
Janika          &   1.0 &   1.000000 \\
Janin           &   1.0 &   1.000000 \\
Janina          &   1.0 &   0.277778 \\
Janine          &   1.0 &   0.041885 \\
Janira          &   1.0 &   0.500000 \\
Janis           &   1.0 &   0.018405 \\
Janisa          &   1.0 &   1.000000 \\
Janise          &   1.0 &   0.714286 \\
Janisha         &   1.0 &   0.500000 \\
Janissa         &   1.0 &   0.833333 \\
Janita          &   1.0 &   1.000000 \\
Janiya          &   1.0 &   0.243243 \\
Janiyah         &   1.0 &   0.468085 \\
Jann            &   1.0 &   0.227273 \\
Janna           &   1.0 &   0.204082 \\
Jannah          &   1.0 &   0.470588 \\
Jannat          &   1.0 &   1.000000 \\
Jannel          &   1.0 &   0.625000 \\
Jannell         &   1.0 &   0.625000 \\
Jannelle        &   1.0 &   0.818182 \\
Jannely         &   1.0 &   0.714286 \\
Jannessa        &   1.0 &   1.000000 \\
Jannet          &   1.0 &   0.125000 \\
Jannete         &   1.0 &   1.000000 \\
Janneth         &   1.0 &   0.454545 \\
Jannett         &   1.0 &   0.500000 \\
Jannette        &   1.0 &   0.111111 \\
Janney          &   1.0 &   0.333333 \\
Jannie          &   1.0 &   0.333333 \\
Jannifer        &   1.0 &   0.714286 \\
Jannine         &   1.0 &   0.625000 \\
Janny           &   1.0 &   0.384615 \\
January         &   1.0 &   0.272727 \\
Janvi           &   1.0 &   1.000000 \\
Jany            &   1.0 &   1.000000 \\
Janya           &   1.0 &   1.000000 \\
Janyah          &   1.0 &   0.666667 \\
Janyce          &   1.0 &   0.416667 \\
Japji           &   1.0 &   0.636364 \\
Japleen         &   1.0 &   1.000000 \\
Jaquay          &   1.0 &   1.000000 \\
Jaquelin        &   1.0 &   0.148148 \\
Jaqueline       &   1.0 &   0.026954 \\
Jaquelinne      &   1.0 &   0.555556 \\
Jaquelyn        &   1.0 &   0.194444 \\
Jaquelyne       &   1.0 &   0.625000 \\
Jared           &   1.0 &   1.000000 \\
Jareli          &   1.0 &   0.600000 \\
Jarelly         &   1.0 &   0.833333 \\
Jarely          &   1.0 &   0.238095 \\
Jarethzy        &   1.0 &   1.000000 \\
Jaretzi         &   1.0 &   0.500000 \\
Jaretzy         &   1.0 &   0.269231 \\
Jariah          &   1.0 &   1.000000 \\
Jaritza         &   1.0 &   0.294118 \\
Jariyah         &   1.0 &   1.000000 \\
Jasdeep         &   1.0 &   1.000000 \\
Jaselle         &   1.0 &   0.545455 \\
Jasia           &   1.0 &   1.000000 \\
Jasibe          &   1.0 &   1.000000 \\
Jasimine        &   1.0 &   1.000000 \\
Jasira          &   1.0 &   1.000000 \\
Jaskiran        &   1.0 &   1.000000 \\
Jaskirat        &   1.0 &   1.000000 \\
Jasleen         &   1.0 &   0.220339 \\
Jaslene         &   1.0 &   0.044715 \\
Jaslin          &   1.0 &   0.666667 \\
Jasline         &   1.0 &   0.461538 \\
Jaslyn          &   1.0 &   0.279070 \\
Jaslyne         &   1.0 &   0.545455 \\
Jaslynn         &   1.0 &   0.217391 \\
Jaslynne        &   1.0 &   1.000000 \\
Jasmaine        &   1.0 &   1.000000 \\
Jasmarie        &   1.0 &   1.000000 \\
Jasmeen         &   1.0 &   0.294118 \\
Jasmeet         &   1.0 &   0.750000 \\
Jasmen          &   1.0 &   0.750000 \\
Jasmin          &   1.0 &   0.058043 \\
Jasmina         &   1.0 &   0.500000 \\
Jasmine         &   1.0 &   0.139104 \\
Jasminemarie    &   1.0 &   1.000000 \\
Jasminerose     &   1.0 &   1.000000 \\
Jasminne        &   1.0 &   0.833333 \\
Jasmyn          &   1.0 &   0.125000 \\
Jasmyne         &   1.0 &   0.416667 \\
Jasneet         &   1.0 &   1.000000 \\
Jasnoor         &   1.0 &   1.000000 \\
Jason           &   1.0 &   0.135135 \\
Jasper          &   1.0 &   1.000000 \\
Jaspreet        &   1.0 &   0.461538 \\
Jassmin         &   1.0 &   0.625000 \\
Jassmine        &   1.0 &   1.000000 \\
Jatziri         &   1.0 &   0.636364 \\
Jatziry         &   1.0 &   0.384615 \\
Javier          &   1.0 &   0.333333 \\
Javon           &   1.0 &   1.000000 \\
Javonna         &   1.0 &   1.000000 \\
Jawanna         &   1.0 &   1.000000 \\
Jaxon           &   1.0 &   1.000000 \\
Jay             &   1.0 &   0.454545 \\
Jaya            &   1.0 &   0.464286 \\
Jayah           &   1.0 &   0.714286 \\
Jayana          &   1.0 &   0.714286 \\
Jayani          &   1.0 &   0.625000 \\
Jayanna         &   1.0 &   1.000000 \\
Jayce           &   1.0 &   0.625000 \\
Jaycee          &   1.0 &   0.578947 \\
Jayci           &   1.0 &   0.555556 \\
Jaycie          &   1.0 &   0.277778 \\
Jayda           &   1.0 &   0.257143 \\
Jaydah          &   1.0 &   0.461538 \\
Jaydalynn       &   1.0 &   1.000000 \\
Jaydan          &   1.0 &   1.000000 \\
Jayde           &   1.0 &   0.333333 \\
Jaydee          &   1.0 &   0.545455 \\
Jaydeen         &   1.0 &   1.000000 \\
Jayden          &   1.0 &   0.045662 \\
Jaydenn         &   1.0 &   1.000000 \\
Jaydin          &   1.0 &   0.312500 \\
Jaydn           &   1.0 &   1.000000 \\
Jaydy           &   1.0 &   0.625000 \\
Jaydyn          &   1.0 &   0.800000 \\
Jaye            &   1.0 &   0.416667 \\
Jayla           &   1.0 &   0.579399 \\
Jaylah          &   1.0 &   0.177419 \\
Jaylan          &   1.0 &   1.000000 \\
Jaylani         &   1.0 &   1.000000 \\
Jaylanie        &   1.0 &   1.000000 \\
Jaylanni        &   1.0 &   1.000000 \\
Jaylean         &   1.0 &   0.700000 \\
Jaylee          &   1.0 &   0.255319 \\
Jayleen         &   1.0 &   0.527473 \\
Jayleene        &   1.0 &   0.454545 \\
Jaylen          &   1.0 &   0.500000 \\
Jaylene         &   1.0 &   0.427928 \\
Jaylenn         &   1.0 &   1.000000 \\
Jaylenne        &   1.0 &   0.600000 \\
Jayli           &   1.0 &   1.000000 \\
Jayliah         &   1.0 &   1.000000 \\
Jayliana        &   1.0 &   1.000000 \\
Jaylie          &   1.0 &   0.454545 \\
Jaylin          &   1.0 &   0.688312 \\
Jaylina         &   1.0 &   1.000000 \\
Jayline         &   1.0 &   0.647059 \\
Jaylinn         &   1.0 &   0.636364 \\
Jaylyn          &   1.0 &   0.243243 \\
Jaylynn         &   1.0 &   0.126984 \\
Jaylynne        &   1.0 &   1.000000 \\
Jayme           &   1.0 &   0.075758 \\
Jaymee          &   1.0 &   0.666667 \\
Jaymi           &   1.0 &   0.777778 \\
Jaymie          &   1.0 &   0.272727 \\
Jayna           &   1.0 &   0.217391 \\
Jaynae          &   1.0 &   1.000000 \\
Jayne           &   1.0 &   0.123457 \\
Jaynee          &   1.0 &   1.000000 \\
Jaynie          &   1.0 &   0.714286 \\
Jazalyn         &   1.0 &   1.000000 \\
Jazel           &   1.0 &   0.416667 \\
Jazell          &   1.0 &   1.000000 \\
Jazelle         &   1.0 &   0.239130 \\
Jazira          &   1.0 &   1.000000 \\
Jazleen         &   1.0 &   0.240000 \\
Jazlene         &   1.0 &   0.083333 \\
Jazlin          &   1.0 &   0.300000 \\
Jazline         &   1.0 &   0.500000 \\
Jazlyn          &   1.0 &   0.639752 \\
Jazlyne         &   1.0 &   0.416667 \\
Jazlynn         &   1.0 &   0.395062 \\
Jazlynne        &   1.0 &   0.750000 \\
Jazman          &   1.0 &   1.000000 \\
Jazmeen         &   1.0 &   1.000000 \\
Jazmen          &   1.0 &   0.461538 \\
Jazmene         &   1.0 &   1.000000 \\
Jazmin          &   1.0 &   0.159539 \\
Jazmine         &   1.0 &   0.139896 \\
Jazmyn          &   1.0 &   0.120690 \\
Jazmyne         &   1.0 &   0.162162 \\
Jazmynn         &   1.0 &   1.000000 \\
Jazz            &   1.0 &   1.000000 \\
Jazzel          &   1.0 &   1.000000 \\
Jazzelle        &   1.0 &   0.555556 \\
Jazzleen        &   1.0 &   1.000000 \\
Jazzlene        &   1.0 &   0.555556 \\
Jazzlin         &   1.0 &   1.000000 \\
Jazzlyn         &   1.0 &   0.490909 \\
Jazzlynn        &   1.0 &   0.333333 \\
Jazzmen         &   1.0 &   1.000000 \\
Jazzmin         &   1.0 &   0.225806 \\
Jazzmine        &   1.0 &   0.166667 \\
Jazzmyn         &   1.0 &   0.857143 \\
Jazzy           &   1.0 &   1.000000 \\
Jean            &   1.0 &   0.028261 \\
Jeana           &   1.0 &   0.259259 \\
Jeanae          &   1.0 &   1.000000 \\
Jeane           &   1.0 &   0.384615 \\
Jeaneen         &   1.0 &   1.000000 \\
Jeanell         &   1.0 &   1.000000 \\
Jeanelle        &   1.0 &   0.368421 \\
Jeanene         &   1.0 &   0.818182 \\
Jeaneth         &   1.0 &   1.000000 \\
Jeanett         &   1.0 &   0.625000 \\
Jeanetta        &   1.0 &   0.416667 \\
Jeanette        &   1.0 &   0.016867 \\
Jeani           &   1.0 &   1.000000 \\
Jeanice         &   1.0 &   0.857143 \\
Jeanie          &   1.0 &   0.125000 \\
Jeanine         &   1.0 &   0.062500 \\
Jeanmarie       &   1.0 &   1.000000 \\
Jeanna          &   1.0 &   0.176471 \\
Jeanne          &   1.0 &   0.017493 \\
Jeannett        &   1.0 &   1.000000 \\
Jeannetta       &   1.0 &   1.000000 \\
Jeannette       &   1.0 &   0.052239 \\
Jeannie         &   1.0 &   0.038462 \\
Jeannine        &   1.0 &   0.056818 \\
Jeanny          &   1.0 &   1.000000 \\
Jeena           &   1.0 &   1.000000 \\
Jeffery         &   1.0 &   1.000000 \\
Jeffrey         &   1.0 &   0.894737 \\
Jehan           &   1.0 &   1.000000 \\
Jehilyn         &   1.0 &   1.000000 \\
Jeidy           &   1.0 &   1.000000 \\
Jeilyn          &   1.0 &   1.000000 \\
Jeimy           &   1.0 &   1.000000 \\
Jelani          &   1.0 &   1.000000 \\
Jelena          &   1.0 &   0.500000 \\
Jelisa          &   1.0 &   0.625000 \\
Jelissa         &   1.0 &   1.000000 \\
Jema            &   1.0 &   1.000000 \\
Jemima          &   1.0 &   0.555556 \\
Jemimah         &   1.0 &   1.000000 \\
Jemma           &   1.0 &   0.763636 \\
Jena            &   1.0 &   0.081967 \\
Jenae           &   1.0 &   0.206897 \\
Jenah           &   1.0 &   1.000000 \\
Jenai           &   1.0 &   0.714286 \\
Jenalee         &   1.0 &   1.000000 \\
Jenalyn         &   1.0 &   1.000000 \\
Jenasis         &   1.0 &   1.000000 \\
Jenavee         &   1.0 &   0.857143 \\
Jenaveve        &   1.0 &   1.000000 \\
Jenavie         &   1.0 &   0.555556 \\
Jenavieve       &   1.0 &   0.294118 \\
Jenay           &   1.0 &   0.454545 \\
Jenaya          &   1.0 &   0.666667 \\
Jene            &   1.0 &   0.454545 \\
Jenea           &   1.0 &   0.666667 \\
Jenean          &   1.0 &   0.833333 \\
Jenee           &   1.0 &   0.277778 \\
Jeneen          &   1.0 &   0.888889 \\
Jenefer         &   1.0 &   1.000000 \\
Jeneffer        &   1.0 &   1.000000 \\
Jenel           &   1.0 &   1.000000 \\
Jenell          &   1.0 &   0.500000 \\
Jenelle         &   1.0 &   0.125000 \\
Jenene          &   1.0 &   1.000000 \\
Jenesis         &   1.0 &   0.813953 \\
Jenessa         &   1.0 &   0.304348 \\
Jenessy         &   1.0 &   1.000000 \\
Jenesys         &   1.0 &   1.000000 \\
Jenette         &   1.0 &   0.428571 \\
Jeneva          &   1.0 &   1.000000 \\
Jenevie         &   1.0 &   0.833333 \\
Jenevieve       &   1.0 &   0.333333 \\
Jeni            &   1.0 &   0.217391 \\
Jenica          &   1.0 &   0.466667 \\
Jenice          &   1.0 &   0.500000 \\
Jenicka         &   1.0 &   0.185185 \\
Jenie           &   1.0 &   0.833333 \\
Jeniece         &   1.0 &   1.000000 \\
Jenifer         &   1.0 &   0.041667 \\
Jeniffer        &   1.0 &   0.200000 \\
Jenika          &   1.0 &   1.000000 \\
Jenilee         &   1.0 &   0.294118 \\
Jenille         &   1.0 &   1.000000 \\
Jenin           &   1.0 &   0.625000 \\
Jenine          &   1.0 &   0.312500 \\
Jenipher        &   1.0 &   1.000000 \\
Jenise          &   1.0 &   0.294118 \\
Jenisis         &   1.0 &   1.000000 \\
Jenita          &   1.0 &   1.000000 \\
Jenna           &   1.0 &   0.088924 \\
Jennae          &   1.0 &   0.625000 \\
Jennafer        &   1.0 &   0.357143 \\
Jennah          &   1.0 &   0.227273 \\
Jennalyn        &   1.0 &   0.555556 \\
Jennavecia      &   1.0 &   0.500000 \\
Jennavie        &   1.0 &   0.555556 \\
Jennavieve      &   1.0 &   1.000000 \\
Jenne           &   1.0 &   0.857143 \\
Jennefer        &   1.0 &   0.277778 \\
Jennel          &   1.0 &   1.000000 \\
Jennell         &   1.0 &   0.555556 \\
Jennelle        &   1.0 &   0.500000 \\
Jennessa        &   1.0 &   0.545455 \\
Jennessy        &   1.0 &   1.000000 \\
Jennett         &   1.0 &   1.000000 \\
Jennette        &   1.0 &   0.357143 \\
Jenney          &   1.0 &   1.000000 \\
Jennfier        &   1.0 &   1.000000 \\
Jenni           &   1.0 &   0.187500 \\
Jennica         &   1.0 &   0.533333 \\
Jennie          &   1.0 &   0.121429 \\
Jenniefer       &   1.0 &   1.000000 \\
Jennifer        &   1.0 &   0.018796 \\
Jenniferann     &   1.0 &   1.000000 \\
Jenniffer       &   1.0 &   0.208333 \\
Jennifier       &   1.0 &   1.000000 \\
Jennika         &   1.0 &   1.000000 \\
Jennilee        &   1.0 &   0.625000 \\
Jennine         &   1.0 &   0.454545 \\
Jennipher       &   1.0 &   1.000000 \\
Jenny           &   1.0 &   0.071618 \\
Jennyfer        &   1.0 &   0.230769 \\
Jensen          &   1.0 &   0.368421 \\
Jensine         &   1.0 &   0.625000 \\
Jeny            &   1.0 &   1.000000 \\
Jenyfer         &   1.0 &   0.833333 \\
Jera            &   1.0 &   1.000000 \\
Jeraldin        &   1.0 &   1.000000 \\
Jeraldine       &   1.0 &   0.384615 \\
Jeralyn         &   1.0 &   1.000000 \\
Jere            &   1.0 &   1.000000 \\
Jeremy          &   1.0 &   0.500000 \\
Jerene          &   1.0 &   1.000000 \\
Jeri            &   1.0 &   0.030675 \\
Jerica          &   1.0 &   0.156250 \\
Jericha         &   1.0 &   1.000000 \\
Jerika          &   1.0 &   0.400000 \\
Jerilyn         &   1.0 &   0.217391 \\
Jerilynn        &   1.0 &   0.461538 \\
Jerlene         &   1.0 &   1.000000 \\
Jermaine        &   1.0 &   1.000000 \\
Jerri           &   1.0 &   0.084746 \\
Jerrica         &   1.0 &   0.171429 \\
Jerrie          &   1.0 &   0.294118 \\
Jerrika         &   1.0 &   0.857143 \\
Jerrilyn        &   1.0 &   0.800000 \\
Jerry           &   1.0 &   0.134615 \\
Jersey          &   1.0 &   0.388889 \\
Jerusalem       &   1.0 &   1.000000 \\
Jerusha         &   1.0 &   0.555556 \\
Jeryl           &   1.0 &   0.384615 \\
Jeselle         &   1.0 &   1.000000 \\
Jesenia         &   1.0 &   0.094340 \\
Jesiah          &   1.0 &   0.384615 \\
Jesica          &   1.0 &   0.090909 \\
Jesika          &   1.0 &   0.333333 \\
Jeslyn          &   1.0 &   0.565217 \\
Jessa           &   1.0 &   0.204082 \\
Jessabelle      &   1.0 &   1.000000 \\
Jessalyn        &   1.0 &   0.230769 \\
Jessalynn       &   1.0 &   1.000000 \\
Jessamyn        &   1.0 &   0.750000 \\
Jesscia         &   1.0 &   0.833333 \\
Jesse           &   1.0 &   0.195652 \\
Jesseca         &   1.0 &   0.416667 \\
Jessel          &   1.0 &   1.000000 \\
Jesselle        &   1.0 &   0.625000 \\
Jesselyn        &   1.0 &   0.833333 \\
Jessenia        &   1.0 &   0.103448 \\
Jessi           &   1.0 &   0.321429 \\
Jessia          &   1.0 &   0.857143 \\
Jessica         &   1.0 &   0.013955 \\
Jessicaann      &   1.0 &   0.750000 \\
Jessicamae      &   1.0 &   1.000000 \\
Jessicamarie    &   1.0 &   1.000000 \\
Jessicca        &   1.0 &   0.666667 \\
Jessie          &   1.0 &   0.435374 \\
Jessika         &   1.0 &   0.085714 \\
Jessilyn        &   1.0 &   1.000000 \\
Jesslyn         &   1.0 &   0.533333 \\
Jesslynn        &   1.0 &   1.000000 \\
Jessy           &   1.0 &   0.384615 \\
Jessyca         &   1.0 &   0.437500 \\
Jessyka         &   1.0 &   0.500000 \\
Jestine         &   1.0 &   0.625000 \\
Jesus           &   1.0 &   0.175000 \\
Jesusita        &   1.0 &   1.000000 \\
Jesyka          &   1.0 &   1.000000 \\
Jett            &   1.0 &   1.000000 \\
Jetzabel        &   1.0 &   1.000000 \\
Jewel           &   1.0 &   0.289855 \\
Jewelia         &   1.0 &   0.500000 \\
Jeweliana       &   1.0 &   1.000000 \\
Jewelisa        &   1.0 &   1.000000 \\
Jewelissa       &   1.0 &   1.000000 \\
Jewell          &   1.0 &   0.428571 \\
Jewels          &   1.0 &   0.454545 \\
Jeyla           &   1.0 &   1.000000 \\
Jeylah          &   1.0 &   1.000000 \\
Jeylin          &   1.0 &   1.000000 \\
Jezabel         &   1.0 &   0.888889 \\
Jezabelle       &   1.0 &   1.000000 \\
Jezebel         &   1.0 &   0.615385 \\
Jezebelle       &   1.0 &   1.000000 \\
Jezel           &   1.0 &   1.000000 \\
Jezelle         &   1.0 &   0.600000 \\
Jezreel         &   1.0 &   1.000000 \\
Jhanvi          &   1.0 &   1.000000 \\
Jhene           &   1.0 &   0.771930 \\
Jhoana          &   1.0 &   0.384615 \\
Jhoanna         &   1.0 &   0.700000 \\
Jhoselyn        &   1.0 &   1.000000 \\
Jhovana         &   1.0 &   1.000000 \\
Ji              &   1.0 &   1.000000 \\
Jia             &   1.0 &   1.000000 \\
Jiana           &   1.0 &   0.933333 \\
Jianna          &   1.0 &   0.794118 \\
Jiaqi           &   1.0 &   1.000000 \\
Jiayi           &   1.0 &   0.833333 \\
Jiayue          &   1.0 &   1.000000 \\
Jihan           &   1.0 &   1.000000 \\
Jil             &   1.0 &   0.833333 \\
Jilian          &   1.0 &   1.000000 \\
Jill            &   1.0 &   0.006519 \\
Jillene         &   1.0 &   0.833333 \\
Jillian         &   1.0 &   0.044521 \\
Jilliana        &   1.0 &   1.000000 \\
Jilliane        &   1.0 &   1.000000 \\
Jilliann        &   1.0 &   1.000000 \\
Jillianne       &   1.0 &   0.833333 \\
Jimena          &   1.0 &   0.560311 \\
Jimin           &   1.0 &   1.000000 \\
Jimmie          &   1.0 &   0.333333 \\
Jimmy           &   1.0 &   0.857143 \\
Jin             &   1.0 &   1.000000 \\
Jina            &   1.0 &   0.416667 \\
Jind            &   1.0 &   1.000000 \\
Jinelle         &   1.0 &   1.000000 \\
Jinny           &   1.0 &   0.833333 \\
Jinora          &   1.0 &   1.000000 \\
Jinx            &   1.0 &   1.000000 \\
Jireh           &   1.0 &   0.777778 \\
Jisel           &   1.0 &   0.416667 \\
Jisela          &   1.0 &   0.600000 \\
Jisell          &   1.0 &   1.000000 \\
Jiselle         &   1.0 &   0.195122 \\
Jissel          &   1.0 &   0.312500 \\
Jissell         &   1.0 &   1.000000 \\
Jisselle        &   1.0 &   0.500000 \\
Jiya            &   1.0 &   0.458333 \\
Jizel           &   1.0 &   0.833333 \\
Jizelle         &   1.0 &   0.136364 \\
Jlynn           &   1.0 &   1.000000 \\
Jniyah          &   1.0 &   1.000000 \\
Jo              &   1.0 &   0.011848 \\
Joan            &   1.0 &   0.017972 \\
Joana           &   1.0 &   0.098485 \\
Joandra         &   1.0 &   1.000000 \\
Joane           &   1.0 &   0.714286 \\
Joani           &   1.0 &   1.000000 \\
Joanie          &   1.0 &   0.304348 \\
Joann           &   1.0 &   0.029740 \\
Joanna          &   1.0 &   0.168504 \\
Joannamarie     &   1.0 &   1.000000 \\
Joanne          &   1.0 &   0.042654 \\
Joannie         &   1.0 &   0.500000 \\
Joaquina        &   1.0 &   1.000000 \\
Jobina          &   1.0 &   1.000000 \\
Jocabed         &   1.0 &   1.000000 \\
Jocelin         &   1.0 &   0.166667 \\
Joceline        &   1.0 &   0.087719 \\
Jocell          &   1.0 &   1.000000 \\
Jocely          &   1.0 &   1.000000 \\
Jocelyn         &   1.0 &   0.123270 \\
Jocelyne        &   1.0 &   0.024096 \\
Jocelynn        &   1.0 &   0.196970 \\
Jocelynne       &   1.0 &   0.583333 \\
Jocilyn         &   1.0 &   1.000000 \\
Joclyn          &   1.0 &   1.000000 \\
Jodeci          &   1.0 &   0.714286 \\
Jodee           &   1.0 &   0.500000 \\
Jodene          &   1.0 &   1.000000 \\
Jodi            &   1.0 &   0.019305 \\
Jodie           &   1.0 &   0.068966 \\
Jodine          &   1.0 &   0.750000 \\
Jody            &   1.0 &   0.023923 \\
Joe             &   1.0 &   0.357143 \\
Joeann          &   1.0 &   1.000000 \\
Joei            &   1.0 &   1.000000 \\
Joel            &   1.0 &   0.642857 \\
Joelene         &   1.0 &   0.714286 \\
Joelie          &   1.0 &   1.000000 \\
Joell           &   1.0 &   0.444444 \\
Joella          &   1.0 &   0.600000 \\
Joelle          &   1.0 &   0.579710 \\
Joellen         &   1.0 &   0.400000 \\
Joellyn         &   1.0 &   1.000000 \\
Joely           &   1.0 &   0.384615 \\
Joetta          &   1.0 &   0.416667 \\
Joette          &   1.0 &   0.454545 \\
Joey            &   1.0 &   0.969697 \\
Johana          &   1.0 &   0.289474 \\
Johanna         &   1.0 &   0.173554 \\
Johannah        &   1.0 &   0.500000 \\
John            &   1.0 &   0.138889 \\
Johna           &   1.0 &   0.333333 \\
Johnae          &   1.0 &   0.714286 \\
Johnanna        &   1.0 &   1.000000 \\
Johnathan       &   1.0 &   1.000000 \\
Johnesha        &   1.0 &   1.000000 \\
Johnetta        &   1.0 &   0.625000 \\
Johnette        &   1.0 &   0.857143 \\
Johnisha        &   1.0 &   0.545455 \\
Johnna          &   1.0 &   0.122449 \\
Johnnie         &   1.0 &   0.296296 \\
Johnny          &   1.0 &   0.750000 \\
Joi             &   1.0 &   0.192308 \\
Joie            &   1.0 &   0.444444 \\
Jojo            &   1.0 &   0.625000 \\
Jolanda         &   1.0 &   0.833333 \\
Jolee           &   1.0 &   0.333333 \\
Joleen          &   1.0 &   0.206897 \\
Joleena         &   1.0 &   1.000000 \\
Jolena          &   1.0 &   1.000000 \\
Jolene          &   1.0 &   0.825397 \\
Jolette         &   1.0 &   0.161290 \\
Joley           &   1.0 &   1.000000 \\
Joli            &   1.0 &   0.833333 \\
Jolie           &   1.0 &   0.518987 \\
Jolin           &   1.0 &   0.714286 \\
Jolina          &   1.0 &   0.312500 \\
Jolinda         &   1.0 &   1.000000 \\
Joline          &   1.0 &   0.700000 \\
Jolisa          &   1.0 &   1.000000 \\
Jolissa         &   1.0 &   0.833333 \\
Jolyn           &   1.0 &   0.750000 \\
Jolyne          &   1.0 &   1.000000 \\
Jolynn          &   1.0 &   0.466667 \\
Jomayra         &   1.0 &   1.000000 \\
Jon             &   1.0 &   1.000000 \\
Jona            &   1.0 &   1.000000 \\
Jonae           &   1.0 &   0.454545 \\
Jonah           &   1.0 &   1.000000 \\
Jonathan        &   1.0 &   0.189189 \\
Jonathon        &   1.0 &   1.000000 \\
Jonda           &   1.0 &   1.000000 \\
Jonell          &   1.0 &   0.714286 \\
Jonelle         &   1.0 &   0.277778 \\
Jonette         &   1.0 &   0.454545 \\
Joni            &   1.0 &   0.077419 \\
Jonie           &   1.0 &   0.833333 \\
Jonique         &   1.0 &   1.000000 \\
Jonisha         &   1.0 &   1.000000 \\
Jonna           &   1.0 &   0.185185 \\
Jonni           &   1.0 &   0.625000 \\
Jonnie          &   1.0 &   0.615385 \\
Jonte           &   1.0 &   1.000000 \\
Jood            &   1.0 &   1.000000 \\
Jordan          &   1.0 &   0.122124 \\
Jordana         &   1.0 &   0.444444 \\
Jordann         &   1.0 &   0.500000 \\
Jordanna        &   1.0 &   1.000000 \\
Jordanne        &   1.0 &   1.000000 \\
Jorden          &   1.0 &   0.388889 \\
Jordin          &   1.0 &   0.098039 \\
Jordon          &   1.0 &   0.615385 \\
Jordy           &   1.0 &   1.000000 \\
Jordyn          &   1.0 &   0.421053 \\
Jordynn         &   1.0 &   0.432432 \\
Jorene          &   1.0 &   1.000000 \\
Jorge           &   1.0 &   0.263158 \\
Jorgina         &   1.0 &   1.000000 \\
Jori            &   1.0 &   0.714286 \\
Jorja           &   1.0 &   0.600000 \\
Jorley          &   1.0 &   1.000000 \\
Jory            &   1.0 &   1.000000 \\
Josabet         &   1.0 &   0.777778 \\
Josabeth        &   1.0 &   0.833333 \\
Josalyn         &   1.0 &   0.545455 \\
Josalynn        &   1.0 &   1.000000 \\
Joscelin        &   1.0 &   1.000000 \\
Joscelyn        &   1.0 &   0.238095 \\
Jose            &   1.0 &   0.116667 \\
Josefa          &   1.0 &   0.555556 \\
Josefina        &   1.0 &   0.210526 \\
Josefine        &   1.0 &   0.555556 \\
Joselin         &   1.0 &   0.136364 \\
Joseline        &   1.0 &   0.076923 \\
Joselinne       &   1.0 &   1.000000 \\
Joselle         &   1.0 &   1.000000 \\
Joselyn         &   1.0 &   0.084142 \\
Joselyne        &   1.0 &   0.324324 \\
Joselynn        &   1.0 &   0.545455 \\
Joseph          &   1.0 &   0.147059 \\
Josephina       &   1.0 &   0.642857 \\
Josephine       &   1.0 &   0.774510 \\
Josephyne       &   1.0 &   1.000000 \\
Josette         &   1.0 &   0.140000 \\
Josey           &   1.0 &   1.000000 \\
Joshlyn         &   1.0 &   1.000000 \\
Joshua          &   1.0 &   0.135135 \\
Josie           &   1.0 &   1.000000 \\
Josilyn         &   1.0 &   0.714286 \\
Joslin          &   1.0 &   0.833333 \\
Joslyn          &   1.0 &   0.157895 \\
Joslynn         &   1.0 &   0.238095 \\
Jossalyn        &   1.0 &   1.000000 \\
Josselin        &   1.0 &   0.500000 \\
Josseline       &   1.0 &   0.454545 \\
Josselyn        &   1.0 &   0.188679 \\
Josselyne       &   1.0 &   1.000000 \\
Jossie          &   1.0 &   0.500000 \\
Josslyn         &   1.0 &   0.230769 \\
Josslynn        &   1.0 &   1.000000 \\
Josue           &   1.0 &   0.714286 \\
Joua            &   1.0 &   1.000000 \\
Joud            &   1.0 &   1.000000 \\
Jourdan         &   1.0 &   0.260870 \\
Jourdyn         &   1.0 &   1.000000 \\
Journee         &   1.0 &   0.949367 \\
Journei         &   1.0 &   0.700000 \\
Journey         &   1.0 &   0.615385 \\
Journi          &   1.0 &   0.636364 \\
Journie         &   1.0 &   0.727273 \\
Journii         &   1.0 &   1.000000 \\
Jovan           &   1.0 &   0.263158 \\
Jovana          &   1.0 &   0.127660 \\
Jovani          &   1.0 &   1.000000 \\
Jovanna         &   1.0 &   0.103896 \\
Jovi            &   1.0 &   0.692308 \\
Jovie           &   1.0 &   1.000000 \\
Jovita          &   1.0 &   0.312500 \\
Jovon           &   1.0 &   1.000000 \\
Jovonna         &   1.0 &   0.833333 \\
Joy             &   1.0 &   0.410480 \\
Joya            &   1.0 &   0.500000 \\
Joyanna         &   1.0 &   1.000000 \\
Joyce           &   1.0 &   0.052557 \\
Joycelin        &   1.0 &   1.000000 \\
Joycelyn        &   1.0 &   0.500000 \\
Joye            &   1.0 &   0.833333 \\
Joylyn          &   1.0 &   1.000000 \\
Jozelyn         &   1.0 &   0.750000 \\
Jozette         &   1.0 &   0.833333 \\
Jozie           &   1.0 &   1.000000 \\
Jozlyn          &   1.0 &   0.416667 \\
Jream           &   1.0 &   1.000000 \\
Juan            &   1.0 &   0.257143 \\
Juana           &   1.0 &   0.224299 \\
Juanisha        &   1.0 &   1.000000 \\
Juanita         &   1.0 &   0.047619 \\
Jubilee         &   1.0 &   0.580645 \\
Judah           &   1.0 &   1.000000 \\
Jude            &   1.0 &   0.551724 \\
Judee           &   1.0 &   0.555556 \\
Judi            &   1.0 &   0.058824 \\
Judie           &   1.0 &   0.096154 \\
Judit           &   1.0 &   0.454545 \\
Judith          &   1.0 &   0.030670 \\
Judy            &   1.0 &   0.007491 \\
Judyth          &   1.0 &   0.375000 \\
Julee           &   1.0 &   0.294118 \\
Julene          &   1.0 &   0.454545 \\
Jules           &   1.0 &   0.647059 \\
Juli            &   1.0 &   0.083333 \\
Julia           &   1.0 &   0.268683 \\
Julian          &   1.0 &   0.277778 \\
Juliana         &   1.0 &   0.374494 \\
Juliane         &   1.0 &   0.384615 \\
Juliann         &   1.0 &   0.269231 \\
Julianna        &   1.0 &   0.534810 \\
Julianne        &   1.0 &   0.141509 \\
Julie           &   1.0 &   0.029891 \\
Julieana        &   1.0 &   0.416667 \\
Julieann        &   1.0 &   0.333333 \\
Julieanna       &   1.0 &   0.411765 \\
Julieanne       &   1.0 &   0.461538 \\
Juliene         &   1.0 &   1.000000 \\
Julienne        &   1.0 &   0.571429 \\
Juliet          &   1.0 &   0.672131 \\
Julieta         &   1.0 &   1.000000 \\
Julieth         &   1.0 &   0.588235 \\
Juliett         &   1.0 &   1.000000 \\
Julietta        &   1.0 &   1.000000 \\
Juliette        &   1.0 &   1.000000 \\
Julina          &   1.0 &   1.000000 \\
Julio           &   1.0 &   0.555556 \\
Julisa          &   1.0 &   0.077922 \\
Julissa         &   1.0 &   0.166307 \\
Julitza         &   1.0 &   1.000000 \\
Juliza          &   1.0 &   0.583333 \\
Julizza         &   1.0 &   1.000000 \\
Jullian         &   1.0 &   0.700000 \\
Julliana        &   1.0 &   0.500000 \\
Jullianna       &   1.0 &   1.000000 \\
Jullie          &   1.0 &   1.000000 \\
Jully           &   1.0 &   1.000000 \\
July            &   1.0 &   1.000000 \\
Julya           &   1.0 &   1.000000 \\
Julyana         &   1.0 &   0.625000 \\
Julyanna        &   1.0 &   0.833333 \\
Julyssa         &   1.0 &   0.411765 \\
Jumana          &   1.0 &   1.000000 \\
Juna            &   1.0 &   0.818182 \\
June            &   1.0 &   0.517615 \\
Juni            &   1.0 &   1.000000 \\
Junia           &   1.0 &   0.636364 \\
Junie           &   1.0 &   0.714286 \\
Juniper         &   1.0 &   1.000000 \\
Junko           &   1.0 &   1.000000 \\
Juno            &   1.0 &   0.826087 \\
Jupiter         &   1.0 &   0.900000 \\
Jurea           &   1.0 &   1.000000 \\
Jurnee          &   1.0 &   0.714286 \\
Jury            &   1.0 &   0.888889 \\
Justeen         &   1.0 &   0.555556 \\
Justene         &   1.0 &   1.000000 \\
Justice         &   1.0 &   0.362500 \\
Justin          &   1.0 &   0.185185 \\
Justina         &   1.0 &   0.066667 \\
Justine         &   1.0 &   0.017045 \\
Justis          &   1.0 &   1.000000 \\
Justus          &   1.0 &   1.000000 \\
Justyce         &   1.0 &   0.461538 \\
Justyne         &   1.0 &   0.555556 \\
Juvia           &   1.0 &   1.000000 \\
Jyl             &   1.0 &   1.000000 \\
Jyoti           &   1.0 &   1.000000 \\
Ka              &   1.0 &   0.300000 \\
Kaaliyah        &   1.0 &   1.000000 \\
Kaaren          &   1.0 &   0.250000 \\
Kaavya          &   1.0 &   1.000000 \\
Kabao           &   1.0 &   1.000000 \\
Kabrina         &   1.0 &   1.000000 \\
Kacee           &   1.0 &   0.500000 \\
Kacey           &   1.0 &   0.513514 \\
Kachina         &   1.0 &   1.000000 \\
Kaci            &   1.0 &   0.318182 \\
Kacia           &   1.0 &   1.000000 \\
Kacie           &   1.0 &   0.208333 \\
Kacy            &   1.0 &   0.260870 \\
Kadance         &   1.0 &   1.000000 \\
Kadee           &   1.0 &   1.000000 \\
Kaden           &   1.0 &   0.357143 \\
Kadence         &   1.0 &   0.240000 \\
Kadi            &   1.0 &   1.000000 \\
Kadie           &   1.0 &   0.312500 \\
Kadijah         &   1.0 &   1.000000 \\
Kadisha         &   1.0 &   1.000000 \\
Kady            &   1.0 &   0.294118 \\
Kadyn           &   1.0 &   0.538462 \\
Kadynce         &   1.0 &   0.875000 \\
Kaede           &   1.0 &   1.000000 \\
Kaedence        &   1.0 &   1.000000 \\
Kaedyn          &   1.0 &   0.875000 \\
Kaela           &   1.0 &   0.086207 \\
Kaelah          &   1.0 &   0.750000 \\
Kaelani         &   1.0 &   0.875000 \\
Kaelee          &   1.0 &   0.416667 \\
Kaeleigh        &   1.0 &   1.000000 \\
Kaeley          &   1.0 &   0.700000 \\
Kaeli           &   1.0 &   0.684211 \\
Kaelie          &   1.0 &   0.714286 \\
Kaelin          &   1.0 &   0.294118 \\
Kaelly          &   1.0 &   1.000000 \\
Kaely           &   1.0 &   0.416667 \\
Kaelyn          &   1.0 &   0.056604 \\
Kaelynn         &   1.0 &   0.290323 \\
Kaelynne        &   1.0 &   1.000000 \\
Kaetlyn         &   1.0 &   0.833333 \\
Kahealani       &   1.0 &   1.000000 \\
Kahla           &   1.0 &   1.000000 \\
Kahlan          &   1.0 &   0.500000 \\
Kahlani         &   1.0 &   0.450000 \\
Kahlen          &   1.0 &   1.000000 \\
Kahlia          &   1.0 &   1.000000 \\
Kai             &   1.0 &   0.733333 \\
Kaia            &   1.0 &   0.996078 \\
Kaiah           &   1.0 &   1.000000 \\
Kaida           &   1.0 &   1.000000 \\
Kaiden          &   1.0 &   0.461538 \\
Kaidence        &   1.0 &   0.333333 \\
Kaidyn          &   1.0 &   1.000000 \\
Kaija           &   1.0 &   0.714286 \\
Kaila           &   1.0 &   0.200000 \\
Kailah          &   1.0 &   0.227273 \\
Kailan          &   1.0 &   0.625000 \\
Kailana         &   1.0 &   0.700000 \\
Kailani         &   1.0 &   0.874396 \\
Kailanie        &   1.0 &   1.000000 \\
Kailany         &   1.0 &   1.000000 \\
Kaile           &   1.0 &   0.625000 \\
Kailea          &   1.0 &   0.571429 \\
Kaileah         &   1.0 &   0.714286 \\
Kailee          &   1.0 &   0.172840 \\
Kaileen         &   1.0 &   1.000000 \\
Kailei          &   1.0 &   0.714286 \\
Kaileia         &   1.0 &   1.000000 \\
Kaileigh        &   1.0 &   0.411765 \\
Kailen          &   1.0 &   0.833333 \\
Kailene         &   1.0 &   1.000000 \\
Kailey          &   1.0 &   0.210227 \\
Kaili           &   1.0 &   0.162162 \\
Kailia          &   1.0 &   1.000000 \\
Kailie          &   1.0 &   0.312500 \\
Kailin          &   1.0 &   0.700000 \\
Kaily           &   1.0 &   0.222222 \\
Kailyn          &   1.0 &   0.184211 \\
Kailynn         &   1.0 &   0.217391 \\
Kaimana         &   1.0 &   1.000000 \\
Kaina           &   1.0 &   1.000000 \\
Kaira           &   1.0 &   0.612245 \\
Kairi           &   1.0 &   0.754717 \\
Kaisha          &   1.0 &   1.000000 \\
Kaisley         &   1.0 &   0.583333 \\
Kaithlyn        &   1.0 &   1.000000 \\
Kaitlan         &   1.0 &   0.357143 \\
Kaitlen         &   1.0 &   0.555556 \\
Kaitlin         &   1.0 &   0.018462 \\
Kaitlyn         &   1.0 &   0.094595 \\
Kaitlyne        &   1.0 &   1.000000 \\
Kaitlynn        &   1.0 &   0.084507 \\
Kaitlynne       &   1.0 &   0.625000 \\
Kaiulani        &   1.0 &   0.625000 \\
Kaiya           &   1.0 &   0.578125 \\
Kaiyah          &   1.0 &   0.666667 \\
Kaja            &   1.0 &   1.000000 \\
Kajal           &   1.0 &   0.555556 \\
Kala            &   1.0 &   0.172414 \\
Kalah           &   1.0 &   1.000000 \\
Kalani          &   1.0 &   0.797297 \\
Kalanie         &   1.0 &   1.000000 \\
Kalaya          &   1.0 &   0.833333 \\
Kalayah         &   1.0 &   1.000000 \\
Kalea           &   1.0 &   0.826087 \\
Kaleah          &   1.0 &   0.411765 \\
Kalee           &   1.0 &   0.333333 \\
Kaleen          &   1.0 &   0.750000 \\
Kaleena         &   1.0 &   0.121951 \\
Kaleesi         &   1.0 &   0.833333 \\
Kalei           &   1.0 &   0.375000 \\
Kaleia          &   1.0 &   0.941176 \\
Kaleigh         &   1.0 &   0.116279 \\
Kaleigha        &   1.0 &   1.000000 \\
Kalen           &   1.0 &   0.666667 \\
Kalena          &   1.0 &   0.476190 \\
Kalene          &   1.0 &   0.333333 \\
Kaley           &   1.0 &   0.200000 \\
Kaleya          &   1.0 &   1.000000 \\
Kali            &   1.0 &   0.575000 \\
Kalia           &   1.0 &   0.631579 \\
Kaliah          &   1.0 &   0.611111 \\
Kaliana         &   1.0 &   0.777778 \\
Kalie           &   1.0 &   0.217391 \\
Kalila          &   1.0 &   0.333333 \\
Kalilah         &   1.0 &   1.000000 \\
Kalin           &   1.0 &   0.777778 \\
Kalina          &   1.0 &   0.806452 \\
Kalise          &   1.0 &   1.000000 \\
Kalisha         &   1.0 &   1.000000 \\
Kalissa         &   1.0 &   0.833333 \\
Kalista         &   1.0 &   0.411765 \\
Kaliya          &   1.0 &   1.000000 \\
Kaliyah         &   1.0 &   0.714286 \\
Kaliyanei       &   1.0 &   1.000000 \\
Kalleigh        &   1.0 &   1.000000 \\
Kalli           &   1.0 &   0.388889 \\
Kallie          &   1.0 &   0.384615 \\
Kalliope        &   1.0 &   0.666667 \\
Kallista        &   1.0 &   0.714286 \\
Kalliyan        &   1.0 &   1.000000 \\
Kally           &   1.0 &   0.750000 \\
Kaloni          &   1.0 &   1.000000 \\
Kalyn           &   1.0 &   0.172414 \\
Kalynn          &   1.0 &   0.466667 \\
Kalynne         &   1.0 &   1.000000 \\
Kalyssa         &   1.0 &   0.555556 \\
Kalysta         &   1.0 &   0.500000 \\
Kam             &   1.0 &   0.722222 \\
Kama            &   1.0 &   0.625000 \\
Kamaiyah        &   1.0 &   0.545455 \\
Kamala          &   1.0 &   0.416667 \\
Kamani          &   1.0 &   0.857143 \\
Kamara          &   1.0 &   0.800000 \\
Kamari          &   1.0 &   0.571429 \\
Kamaria         &   1.0 &   0.900000 \\
Kamaya          &   1.0 &   0.357143 \\
Kambree         &   1.0 &   1.000000 \\
Kambria         &   1.0 &   0.400000 \\
Kamden          &   1.0 &   1.000000 \\
Kamdyn          &   1.0 &   0.750000 \\
Kamea           &   1.0 &   0.750000 \\
Kameelah        &   1.0 &   0.583333 \\
Kamela          &   1.0 &   0.384615 \\
Kameron         &   1.0 &   0.208333 \\
Kameryn         &   1.0 &   0.416667 \\
Kamesha         &   1.0 &   1.000000 \\
Kami            &   1.0 &   0.241379 \\
Kamia           &   1.0 &   0.700000 \\
Kamiah          &   1.0 &   0.416667 \\
Kamie           &   1.0 &   0.875000 \\
Kamika          &   1.0 &   1.000000 \\
Kamila          &   1.0 &   0.952381 \\
Kamilah         &   1.0 &   0.731707 \\
Kamilla         &   1.0 &   0.470588 \\
Kamillah        &   1.0 &   1.000000 \\
Kamille         &   1.0 &   0.535714 \\
Kamisha         &   1.0 &   0.625000 \\
Kamiya          &   1.0 &   0.600000 \\
Kamiyah         &   1.0 &   1.000000 \\
Kammie          &   1.0 &   1.000000 \\
Kamora          &   1.0 &   0.434783 \\
Kamori          &   1.0 &   1.000000 \\
Kamri           &   1.0 &   1.000000 \\
Kamrie          &   1.0 &   1.000000 \\
Kamrin          &   1.0 &   1.000000 \\
Kamron          &   1.0 &   1.000000 \\
Kamry           &   1.0 &   1.000000 \\
Kamryn          &   1.0 &   0.295455 \\
Kamrynn         &   1.0 &   0.833333 \\
Kamya           &   1.0 &   0.437500 \\
Kamyah          &   1.0 &   1.000000 \\
Kamyla          &   1.0 &   0.357143 \\
Kamylah         &   1.0 &   1.000000 \\
Kana            &   1.0 &   1.000000 \\
Kanani          &   1.0 &   0.666667 \\
Kandace         &   1.0 &   0.205882 \\
Kandee          &   1.0 &   0.714286 \\
Kandi           &   1.0 &   0.208333 \\
Kandice         &   1.0 &   0.157895 \\
Kandie          &   1.0 &   1.000000 \\
Kandis          &   1.0 &   0.400000 \\
Kandra          &   1.0 &   0.600000 \\
Kandy           &   1.0 &   0.222222 \\
Kandyce         &   1.0 &   0.833333 \\
Kaneisha        &   1.0 &   1.000000 \\
Kanesha         &   1.0 &   0.625000 \\
Kang            &   1.0 &   0.714286 \\
Kanika          &   1.0 &   0.400000 \\
Kanisha         &   1.0 &   0.312500 \\
Kaniya          &   1.0 &   0.454545 \\
Kaniyah         &   1.0 &   0.750000 \\
Kanna           &   1.0 &   1.000000 \\
Kanon           &   1.0 &   1.000000 \\
Kansas          &   1.0 &   1.000000 \\
Kanwal          &   1.0 &   1.000000 \\
Kao             &   1.0 &   0.666667 \\
Kaori           &   1.0 &   0.500000 \\
Kaoru           &   1.0 &   0.625000 \\
Kapri           &   1.0 &   1.000000 \\
Kara            &   1.0 &   0.117647 \\
Karah           &   1.0 &   0.500000 \\
Karalee         &   1.0 &   1.000000 \\
Karalyn         &   1.0 &   1.000000 \\
Karalynn        &   1.0 &   1.000000 \\
Karan           &   1.0 &   0.227273 \\
Karee           &   1.0 &   0.545455 \\
Kareemah        &   1.0 &   1.000000 \\
Kareen          &   1.0 &   0.750000 \\
Kareena         &   1.0 &   0.428571 \\
Karel           &   1.0 &   0.714286 \\
Kareli          &   1.0 &   0.583333 \\
Karelly         &   1.0 &   1.000000 \\
Karely          &   1.0 &   0.785714 \\
Karem           &   1.0 &   0.857143 \\
Karen           &   1.0 &   0.010587 \\
Karena          &   1.0 &   0.185185 \\
Karene          &   1.0 &   1.000000 \\
Karenna         &   1.0 &   0.500000 \\
Karess          &   1.0 &   1.000000 \\
Karessa         &   1.0 &   1.000000 \\
Karey           &   1.0 &   0.615385 \\
Kari            &   1.0 &   0.023923 \\
Karia           &   1.0 &   1.000000 \\
Kariana         &   1.0 &   1.000000 \\
Kariann         &   1.0 &   0.833333 \\
Karianne        &   1.0 &   0.625000 \\
Karicia         &   1.0 &   1.000000 \\
Karie           &   1.0 &   0.156250 \\
Karilyn         &   1.0 &   1.000000 \\
Karima          &   1.0 &   0.555556 \\
Karime          &   1.0 &   0.114754 \\
Karin           &   1.0 &   0.036765 \\
Karina          &   1.0 &   0.065256 \\
Karine          &   1.0 &   0.357143 \\
Karinna         &   1.0 &   0.421053 \\
Karinne         &   1.0 &   1.000000 \\
Karis           &   1.0 &   0.142857 \\
Karisa          &   1.0 &   0.275862 \\
Karisha         &   1.0 &   1.000000 \\
Karishma        &   1.0 &   0.294118 \\
Karisma         &   1.0 &   0.368421 \\
Karissa         &   1.0 &   0.049505 \\
Karizma         &   1.0 &   0.333333 \\
Karl            &   1.0 &   0.857143 \\
Karla           &   1.0 &   0.157609 \\
Karlee          &   1.0 &   0.186047 \\
Karleen         &   1.0 &   0.636364 \\
Karleigh        &   1.0 &   0.416667 \\
Karlene         &   1.0 &   0.357143 \\
Karley          &   1.0 &   0.193548 \\
Karli           &   1.0 &   0.096154 \\
Karlie          &   1.0 &   0.155556 \\
Karly           &   1.0 &   0.166667 \\
Karlyn          &   1.0 &   0.714286 \\
Karma           &   1.0 &   0.333333 \\
Karmen          &   1.0 &   0.333333 \\
Karmina         &   1.0 &   0.833333 \\
Karna           &   1.0 &   0.263158 \\
Karol           &   1.0 &   0.093023 \\
Karole          &   1.0 &   0.833333 \\
Karolina        &   1.0 &   0.617647 \\
Karoline        &   1.0 &   0.444444 \\
Karolyn         &   1.0 &   0.461538 \\
Karolynn        &   1.0 &   1.000000 \\
Karon           &   1.0 &   0.238095 \\
Karra           &   1.0 &   1.000000 \\
Karrah          &   1.0 &   1.000000 \\
Karren          &   1.0 &   0.571429 \\
Karri           &   1.0 &   0.166667 \\
Karrie          &   1.0 &   0.142857 \\
Karrin          &   1.0 &   1.000000 \\
Karrina         &   1.0 &   1.000000 \\
Karrington      &   1.0 &   1.000000 \\
Karrisa         &   1.0 &   1.000000 \\
Karry           &   1.0 &   0.666667 \\
Karsen          &   1.0 &   0.714286 \\
Karson          &   1.0 &   0.714286 \\
Karsyn          &   1.0 &   0.931034 \\
Karter          &   1.0 &   0.500000 \\
Kary            &   1.0 &   0.357143 \\
Karyl           &   1.0 &   0.500000 \\
Karyme          &   1.0 &   0.084746 \\
Karyn           &   1.0 &   0.060241 \\
Karyna          &   1.0 &   0.400000 \\
Karyssa         &   1.0 &   0.700000 \\
Kasandra        &   1.0 &   0.060870 \\
Kasara          &   1.0 &   1.000000 \\
Kasarah         &   1.0 &   1.000000 \\
Kasaundra       &   1.0 &   1.000000 \\
Kasey           &   1.0 &   0.148148 \\
Kasha           &   1.0 &   0.875000 \\
Kashia          &   1.0 &   1.000000 \\
Kashish         &   1.0 &   1.000000 \\
Kashmere        &   1.0 &   1.000000 \\
Kashmir         &   1.0 &   1.000000 \\
Kashvi          &   1.0 &   0.750000 \\
Kasi            &   1.0 &   0.714286 \\
Kasia           &   1.0 &   0.666667 \\
Kasie           &   1.0 &   0.294118 \\
Kasondra        &   1.0 &   0.500000 \\
Kassandra       &   1.0 &   0.076175 \\
Kassaundra      &   1.0 &   0.555556 \\
Kassey          &   1.0 &   1.000000 \\
Kassi           &   1.0 &   0.800000 \\
Kassia          &   1.0 &   1.000000 \\
Kassidi         &   1.0 &   0.625000 \\
Kassidy         &   1.0 &   0.369565 \\
Kassie          &   1.0 &   0.185185 \\
Kassondra       &   1.0 &   0.500000 \\
Kassy           &   1.0 &   0.714286 \\
Katalaya        &   1.0 &   0.857143 \\
Katalea         &   1.0 &   1.000000 \\
Kataleah        &   1.0 &   0.750000 \\
Kataleia        &   1.0 &   1.000000 \\
Kataleya        &   1.0 &   0.750000 \\
Kataleyah       &   1.0 &   0.555556 \\
Katalia         &   1.0 &   1.000000 \\
Katalina        &   1.0 &   1.000000 \\
Katalyna        &   1.0 &   0.875000 \\
Katana          &   1.0 &   0.615385 \\
Katara          &   1.0 &   1.000000 \\
Katarina        &   1.0 &   0.103448 \\
Kate            &   1.0 &   0.198276 \\
Katee           &   1.0 &   0.555556 \\
Kateland        &   1.0 &   1.000000 \\
Kateleen        &   1.0 &   1.000000 \\
Katelen         &   1.0 &   1.000000 \\
Katelin         &   1.0 &   0.148936 \\
Katelyn         &   1.0 &   0.056701 \\
Katelyne        &   1.0 &   1.000000 \\
Katelynn        &   1.0 &   0.079365 \\
Katelynne       &   1.0 &   0.727273 \\
Kateri          &   1.0 &   0.555556 \\
Katerin         &   1.0 &   0.692308 \\
Katerina        &   1.0 &   0.432432 \\
Katerine        &   1.0 &   0.454545 \\
Katey           &   1.0 &   0.416667 \\
Kathaleen       &   1.0 &   0.384615 \\
Kathaleya       &   1.0 &   1.000000 \\
Kathalina       &   1.0 &   0.500000 \\
Katharina       &   1.0 &   0.461538 \\
Katharine       &   1.0 &   0.068627 \\
Katharyn        &   1.0 &   0.857143 \\
Kathe           &   1.0 &   0.636364 \\
Katheleen       &   1.0 &   1.000000 \\
Kathelyn        &   1.0 &   1.000000 \\
Katherin        &   1.0 &   0.375000 \\
Katherina       &   1.0 &   0.416667 \\
Katherine       &   1.0 &   0.151854 \\
Katherinne      &   1.0 &   1.000000 \\
Kathern         &   1.0 &   0.625000 \\
Katheryn        &   1.0 &   0.116279 \\
Katheryne       &   1.0 &   0.416667 \\
Kathey          &   1.0 &   1.000000 \\
Kathi           &   1.0 &   0.065217 \\
Kathia          &   1.0 &   0.222222 \\
Kathie          &   1.0 &   0.065789 \\
Kathleen        &   1.0 &   0.007861 \\
Kathleena       &   1.0 &   1.000000 \\
Kathleene       &   1.0 &   1.000000 \\
Kathlene        &   1.0 &   0.227273 \\
Kathlyn         &   1.0 &   0.294118 \\
Kathlynn        &   1.0 &   1.000000 \\
Kathren         &   1.0 &   1.000000 \\
Kathrin         &   1.0 &   0.625000 \\
Kathrina        &   1.0 &   0.555556 \\
Kathrine        &   1.0 &   0.146341 \\
Kathryn         &   1.0 &   0.028264 \\
Kathryne        &   1.0 &   0.400000 \\
Kathrynn        &   1.0 &   1.000000 \\
Kathy           &   1.0 &   0.010018 \\
Kathya          &   1.0 &   0.318182 \\
Kathyleen       &   1.0 &   1.000000 \\
Kathyrn         &   1.0 &   0.461538 \\
Kati            &   1.0 &   0.250000 \\
Katia           &   1.0 &   0.153846 \\
Katiana         &   1.0 &   0.625000 \\
Katie           &   1.0 &   0.062745 \\
Katieann        &   1.0 &   1.000000 \\
Katilyn         &   1.0 &   0.857143 \\
Katina          &   1.0 &   0.046296 \\
Katiria         &   1.0 &   1.000000 \\
Katisha         &   1.0 &   1.000000 \\
Katja           &   1.0 &   0.714286 \\
Katlin          &   1.0 &   0.185185 \\
Katlyn          &   1.0 &   0.079365 \\
Katlynn         &   1.0 &   0.263158 \\
Katniss         &   1.0 &   0.833333 \\
Katrena         &   1.0 &   0.714286 \\
Katriana        &   1.0 &   1.000000 \\
Katrice         &   1.0 &   0.500000 \\
Katriel         &   1.0 &   1.000000 \\
Katrin          &   1.0 &   0.714286 \\
Katrina         &   1.0 &   0.064444 \\
Katrine         &   1.0 &   1.000000 \\
Katryn          &   1.0 &   1.000000 \\
Katryna         &   1.0 &   0.857143 \\
Kattaleya       &   1.0 &   0.777778 \\
Kattie          &   1.0 &   0.666667 \\
Kattleya        &   1.0 &   0.625000 \\
Katty           &   1.0 &   0.583333 \\
Katy            &   1.0 &   0.146667 \\
Katya           &   1.0 &   0.194444 \\
Kaula           &   1.0 &   1.000000 \\
Kavita          &   1.0 &   0.625000 \\
Kavya           &   1.0 &   0.714286 \\
Kay             &   1.0 &   0.026316 \\
Kaya            &   1.0 &   0.515152 \\
Kayah           &   1.0 &   1.000000 \\
Kayal           &   1.0 &   1.000000 \\
Kayan           &   1.0 &   1.000000 \\
Kayana          &   1.0 &   0.625000 \\
Kayanna         &   1.0 &   0.714286 \\
Kayce           &   1.0 &   0.600000 \\
Kaycee          &   1.0 &   0.774194 \\
Kayci           &   1.0 &   0.555556 \\
Kaycie          &   1.0 &   0.428571 \\
Kayda           &   1.0 &   1.000000 \\
Kaydance        &   1.0 &   0.500000 \\
Kaydee          &   1.0 &   0.461538 \\
Kayden          &   1.0 &   0.082192 \\
Kaydence        &   1.0 &   0.233333 \\
Kaydin          &   1.0 &   0.857143 \\
Kaydince        &   1.0 &   1.000000 \\
Kaye            &   1.0 &   0.217391 \\
Kayla           &   1.0 &   0.145139 \\
Kaylah          &   1.0 &   0.230769 \\
Kaylamarie      &   1.0 &   0.666667 \\
Kaylan          &   1.0 &   0.666667 \\
Kaylana         &   1.0 &   0.625000 \\
Kaylani         &   1.0 &   0.969512 \\
Kaylanie        &   1.0 &   0.400000 \\
Kaylany         &   1.0 &   1.000000 \\
Kayle           &   1.0 &   0.178571 \\
Kaylea          &   1.0 &   0.800000 \\
Kayleah         &   1.0 &   1.000000 \\
Kaylee          &   1.0 &   0.293907 \\
Kayleeann       &   1.0 &   1.000000 \\
Kayleen         &   1.0 &   0.423077 \\
Kayleena        &   1.0 &   0.714286 \\
Kaylei          &   1.0 &   0.333333 \\
Kayleigh        &   1.0 &   0.414286 \\
Kaylen          &   1.0 &   0.162791 \\
Kaylena         &   1.0 &   1.000000 \\
Kaylene         &   1.0 &   0.344828 \\
Kayley          &   1.0 &   0.218182 \\
Kayli           &   1.0 &   0.121951 \\
Kaylia          &   1.0 &   1.000000 \\
Kayliana        &   1.0 &   0.750000 \\
Kaylie          &   1.0 &   0.203046 \\
Kayliegh        &   1.0 &   1.000000 \\
Kaylin          &   1.0 &   0.160000 \\
Kaylina         &   1.0 &   0.818182 \\
Kaylinn         &   1.0 &   1.000000 \\
Kayloni         &   1.0 &   0.714286 \\
Kaylonnie       &   1.0 &   1.000000 \\
Kayly           &   1.0 &   0.555556 \\
Kaylyn          &   1.0 &   0.140000 \\
Kaylynn         &   1.0 &   0.291667 \\
Kaylynne        &   1.0 &   1.000000 \\
Kayra           &   1.0 &   0.714286 \\
Kaysee          &   1.0 &   1.000000 \\
Kaysha          &   1.0 &   0.625000 \\
Kaysie          &   1.0 &   0.714286 \\
Kaytee          &   1.0 &   1.000000 \\
Kaytie          &   1.0 &   1.000000 \\
Kaytlin         &   1.0 &   0.238095 \\
Kaytlyn         &   1.0 &   0.428571 \\
Kaytlynn        &   1.0 &   0.454545 \\
Kazandra        &   1.0 &   0.666667 \\
Kazue           &   1.0 &   1.000000 \\
Kazuko          &   1.0 &   0.161290 \\
Kazuye          &   1.0 &   0.714286 \\
Kazzandra       &   1.0 &   1.000000 \\
Kc              &   1.0 &   0.777778 \\
Kea             &   1.0 &   1.000000 \\
Keagan          &   1.0 &   0.714286 \\
Keaira          &   1.0 &   0.454545 \\
Keala           &   1.0 &   0.294118 \\
Kealani         &   1.0 &   0.416667 \\
Keana           &   1.0 &   0.142857 \\
Keandra         &   1.0 &   0.625000 \\
Keani           &   1.0 &   0.857143 \\
Keanna          &   1.0 &   0.304348 \\
Keara           &   1.0 &   0.285714 \\
Kearra          &   1.0 &   1.000000 \\
Keaton          &   1.0 &   0.625000 \\
Kebrina         &   1.0 &   1.000000 \\
Kecia           &   1.0 &   0.093750 \\
Keeana          &   1.0 &   1.000000 \\
Keegan          &   1.0 &   0.454545 \\
Keeley          &   1.0 &   0.450000 \\
Keeli           &   1.0 &   1.000000 \\
Keelie          &   1.0 &   1.000000 \\
Keelin          &   1.0 &   0.833333 \\
Keely           &   1.0 &   0.189189 \\
Keena           &   1.0 &   0.500000 \\
Keera           &   1.0 &   1.000000 \\
Keerat          &   1.0 &   0.714286 \\
Keerthana       &   1.0 &   0.714286 \\
Keesha          &   1.0 &   0.666667 \\
Keeva           &   1.0 &   1.000000 \\
Kehlani         &   1.0 &   1.000000 \\
Kehlany         &   1.0 &   0.714286 \\
Keiana          &   1.0 &   0.454545 \\
Keianna         &   1.0 &   0.714286 \\
Keiara          &   1.0 &   0.857143 \\
Keidy           &   1.0 &   1.000000 \\
Keiko           &   1.0 &   0.384615 \\
Keila           &   1.0 &   0.228571 \\
Keilah          &   1.0 &   0.411765 \\
Keilana         &   1.0 &   0.416667 \\
Keilani         &   1.0 &   0.972222 \\
Keilany         &   1.0 &   0.869565 \\
Keilee          &   1.0 &   1.000000 \\
Keiley          &   1.0 &   0.833333 \\
Keili           &   1.0 &   0.750000 \\
Keilly          &   1.0 &   0.454545 \\
Keily           &   1.0 &   0.750000 \\
Keilyn          &   1.0 &   0.818182 \\
Keiona          &   1.0 &   1.000000 \\
Keionna         &   1.0 &   1.000000 \\
Keira           &   1.0 &   0.236041 \\
Keirah          &   1.0 &   1.000000 \\
Keirra          &   1.0 &   0.714286 \\
Keirsten        &   1.0 &   1.000000 \\
Keiry           &   1.0 &   0.227273 \\
Keisa           &   1.0 &   0.714286 \\
Keisha          &   1.0 &   0.071429 \\
Keisi           &   1.0 &   1.000000 \\
Keisy           &   1.0 &   0.714286 \\
Keith           &   1.0 &   0.888889 \\
Keitha          &   1.0 &   1.000000 \\
Kekoa           &   1.0 &   1.000000 \\
Kela            &   1.0 &   1.000000 \\
Kelani          &   1.0 &   0.888889 \\
Kelcee          &   1.0 &   1.000000 \\
Kelcey          &   1.0 &   0.185185 \\
Kelci           &   1.0 &   0.600000 \\
Kelcie          &   1.0 &   0.263158 \\
Kelcy           &   1.0 &   0.857143 \\
Kelee           &   1.0 &   1.000000 \\
Kelene          &   1.0 &   1.000000 \\
Keli            &   1.0 &   0.312500 \\
Kelia           &   1.0 &   0.857143 \\
Kelis           &   1.0 &   0.263158 \\
Kelle           &   1.0 &   0.333333 \\
Kellee          &   1.0 &   0.214286 \\
Kelleen         &   1.0 &   0.454545 \\
Kellen          &   1.0 &   0.500000 \\
Kellene         &   1.0 &   0.625000 \\
Kelley          &   1.0 &   0.042781 \\
Kelli           &   1.0 &   0.020000 \\
Kelliann        &   1.0 &   1.000000 \\
Kellianne       &   1.0 &   1.000000 \\
Kellie          &   1.0 &   0.023529 \\
Kellsie         &   1.0 &   0.714286 \\
Kelly           &   1.0 &   0.026735 \\
Kellyanne       &   1.0 &   0.833333 \\
Kellye          &   1.0 &   0.428571 \\
Kellyn          &   1.0 &   0.416667 \\
Kelsea          &   1.0 &   0.111111 \\
Kelsee          &   1.0 &   0.454545 \\
Kelsey          &   1.0 &   0.048897 \\
Kelsi           &   1.0 &   0.080645 \\
Kelsie          &   1.0 &   0.186813 \\
Kelsy           &   1.0 &   0.208333 \\
Kemberly        &   1.0 &   0.538462 \\
Kemily          &   1.0 &   1.000000 \\
Kemora          &   1.0 &   1.000000 \\
Kena            &   1.0 &   0.714286 \\
Kenadee         &   1.0 &   0.666667 \\
Kenadi          &   1.0 &   0.454545 \\
Kenadie         &   1.0 &   1.000000 \\
Kenda           &   1.0 &   0.545455 \\
Kendahl         &   1.0 &   1.000000 \\
Kendal          &   1.0 &   0.173913 \\
Kendall         &   1.0 &   0.378641 \\
Kendel          &   1.0 &   1.000000 \\
Kendell         &   1.0 &   0.888889 \\
Kendis          &   1.0 &   1.000000 \\
Kendra          &   1.0 &   0.192182 \\
Kendrah         &   1.0 &   1.000000 \\
Kendria         &   1.0 &   1.000000 \\
Kendy           &   1.0 &   1.000000 \\
Kendyl          &   1.0 &   0.380952 \\
Kendyll         &   1.0 &   0.750000 \\
Kenedi          &   1.0 &   1.000000 \\
Kenesha         &   1.0 &   0.714286 \\
Kenia           &   1.0 &   0.480620 \\
Kenisha         &   1.0 &   0.192308 \\
Keniya          &   1.0 &   1.000000 \\
Keniyah         &   1.0 &   1.000000 \\
Kenlee          &   1.0 &   1.000000 \\
Kenleigh        &   1.0 &   1.000000 \\
Kenley          &   1.0 &   0.343750 \\
Kenna           &   1.0 &   0.605263 \\
Kennadi         &   1.0 &   0.625000 \\
Kennadie        &   1.0 &   1.000000 \\
Kennady         &   1.0 &   1.000000 \\
Kennedi         &   1.0 &   0.411765 \\
Kennedie        &   1.0 &   1.000000 \\
Kennedy         &   1.0 &   0.752089 \\
Kennesha        &   1.0 &   1.000000 \\
Kenneth         &   1.0 &   0.461538 \\
Kennia          &   1.0 &   0.384615 \\
Kennisha        &   1.0 &   0.666667 \\
Kenny           &   1.0 &   1.000000 \\
Kennya          &   1.0 &   0.555556 \\
Kensey          &   1.0 &   1.000000 \\
Kensi           &   1.0 &   0.545455 \\
Kensie          &   1.0 &   0.636364 \\
Kensington      &   1.0 &   0.296296 \\
Kensley         &   1.0 &   0.694444 \\
Kenya           &   1.0 &   0.211009 \\
Kenyah          &   1.0 &   1.000000 \\
Kenyatta        &   1.0 &   0.555556 \\
Kenyetta        &   1.0 &   1.000000 \\
Kenza           &   1.0 &   0.777778 \\
Kenzi           &   1.0 &   0.500000 \\
Kenzie          &   1.0 &   0.448000 \\
Kenzington      &   1.0 &   0.857143 \\
Kenzlee         &   1.0 &   0.857143 \\
Kenzley         &   1.0 &   1.000000 \\
Kenzy           &   1.0 &   1.000000 \\
Keona           &   1.0 &   0.285714 \\
Keonna          &   1.0 &   1.000000 \\
Keosha          &   1.0 &   0.666667 \\
Kera            &   1.0 &   0.333333 \\
Keren           &   1.0 &   0.171429 \\
Kerensa         &   1.0 &   1.000000 \\
Keri            &   1.0 &   0.046358 \\
Keriann         &   1.0 &   0.833333 \\
Kerianne        &   1.0 &   0.750000 \\
Kerie           &   1.0 &   0.833333 \\
Kerin           &   1.0 &   0.555556 \\
Kerissa         &   1.0 &   0.833333 \\
Kerith          &   1.0 &   1.000000 \\
Kerra           &   1.0 &   1.000000 \\
Kerri           &   1.0 &   0.041322 \\
Kerrie          &   1.0 &   0.106383 \\
Kerrigan        &   1.0 &   1.000000 \\
Kerrin          &   1.0 &   1.000000 \\
Kerrington      &   1.0 &   1.000000 \\
Kerry           &   1.0 &   0.034091 \\
Kersten         &   1.0 &   1.000000 \\
Kerstin         &   1.0 &   0.428571 \\
Keryn           &   1.0 &   1.000000 \\
Kesha           &   1.0 &   0.185185 \\
Keshana         &   1.0 &   1.000000 \\
Keshia          &   1.0 &   0.242424 \\
Kesia           &   1.0 &   1.000000 \\
Kestrel         &   1.0 &   1.000000 \\
Keturah         &   1.0 &   0.750000 \\
Ketzaly         &   1.0 &   1.000000 \\
Keva            &   1.0 &   1.000000 \\
Kevin           &   1.0 &   0.142857 \\
Keya            &   1.0 &   0.700000 \\
Keyaira         &   1.0 &   1.000000 \\
Keyana          &   1.0 &   0.357143 \\
Keyanna         &   1.0 &   0.294118 \\
Keyara          &   1.0 &   0.700000 \\
Keyarah         &   1.0 &   1.000000 \\
Keyera          &   1.0 &   1.000000 \\
Keyla           &   1.0 &   0.542056 \\
Keylani         &   1.0 &   1.000000 \\
Keylee          &   1.0 &   0.384615 \\
Keylen          &   1.0 &   1.000000 \\
Keyli           &   1.0 &   0.600000 \\
Keylin          &   1.0 &   1.000000 \\
Keyly           &   1.0 &   1.000000 \\
Keyona          &   1.0 &   0.555556 \\
Keyonna         &   1.0 &   0.777778 \\
Keyra           &   1.0 &   0.428571 \\
Keyri           &   1.0 &   1.000000 \\
Keysha          &   1.0 &   0.625000 \\
Kezia           &   1.0 &   0.875000 \\
Keziah          &   1.0 &   0.700000 \\
Khadejah        &   1.0 &   1.000000 \\
Khadija         &   1.0 &   0.944444 \\
Khadijah        &   1.0 &   0.066667 \\
Khaila          &   1.0 &   0.700000 \\
Khalani         &   1.0 &   1.000000 \\
Khalea          &   1.0 &   1.000000 \\
Khaleesi        &   1.0 &   0.774648 \\
Khalessi        &   1.0 &   1.000000 \\
Khali           &   1.0 &   0.466667 \\
Khalia          &   1.0 &   0.611111 \\
Khaliah         &   1.0 &   0.833333 \\
Khalilah        &   1.0 &   0.375000 \\
Khaliyah        &   1.0 &   0.625000 \\
Khalyla         &   1.0 &   1.000000 \\
Khamani         &   1.0 &   1.000000 \\
Khamila         &   1.0 &   0.500000 \\
Khanh           &   1.0 &   0.625000 \\
Khara           &   1.0 &   0.555556 \\
Khari           &   1.0 &   0.647059 \\
Khayla          &   1.0 &   0.833333 \\
Khelani         &   1.0 &   1.000000 \\
Kherington      &   1.0 &   1.000000 \\
Khianna         &   1.0 &   1.000000 \\
Khiara          &   1.0 &   1.000000 \\
Khloe           &   1.0 &   0.196995 \\
Khloee          &   1.0 &   0.277778 \\
Khloey          &   1.0 &   0.888889 \\
Khloie          &   1.0 &   0.727273 \\
Khole           &   1.0 &   1.000000 \\
Khristina       &   1.0 &   0.625000 \\
Khristine       &   1.0 &   0.555556 \\
Khrystina       &   1.0 &   1.000000 \\
Khrystyna       &   1.0 &   1.000000 \\
Khrystyne       &   1.0 &   0.714286 \\
Khushi          &   1.0 &   0.333333 \\
Khushpreet      &   1.0 &   1.000000 \\
Khyla           &   1.0 &   0.625000 \\
Khylee          &   1.0 &   1.000000 \\
Khylie          &   1.0 &   1.000000 \\
Kia             &   1.0 &   0.172414 \\
Kiah            &   1.0 &   0.333333 \\
Kiahna          &   1.0 &   1.000000 \\
Kiaira          &   1.0 &   1.000000 \\
Kiana           &   1.0 &   0.307692 \\
Kiandra         &   1.0 &   1.000000 \\
Kianga          &   1.0 &   1.000000 \\
Kiani           &   1.0 &   0.500000 \\
Kianna          &   1.0 &   0.444444 \\
Kiannah         &   1.0 &   1.000000 \\
Kiara           &   1.0 &   0.464115 \\
Kiarah          &   1.0 &   0.600000 \\
Kiari           &   1.0 &   1.000000 \\
Kiarra          &   1.0 &   0.315789 \\
Kiaya           &   1.0 &   1.000000 \\
Kieara          &   1.0 &   1.000000 \\
Kiela           &   1.0 &   1.000000 \\
Kiele           &   1.0 &   0.750000 \\
Kiely           &   1.0 &   0.700000 \\
Kiera           &   1.0 &   0.315315 \\
Kierah          &   1.0 &   1.000000 \\
Kieran          &   1.0 &   0.750000 \\
Kierra          &   1.0 &   0.113636 \\
Kiersten        &   1.0 &   0.121951 \\
Kierstin        &   1.0 &   0.461538 \\
Kierstyn        &   1.0 &   0.500000 \\
Kiesha          &   1.0 &   0.500000 \\
Kieu            &   1.0 &   0.833333 \\
Kiki            &   1.0 &   0.833333 \\
Kikue           &   1.0 &   0.833333 \\
Kikuye          &   1.0 &   0.857143 \\
Kila            &   1.0 &   1.000000 \\
Kilani          &   1.0 &   1.000000 \\
Kilee           &   1.0 &   0.555556 \\
Kiley           &   1.0 &   0.071429 \\
Kim             &   1.0 &   0.006167 \\
Kimanh          &   1.0 &   1.000000 \\
Kimani          &   1.0 &   0.666667 \\
Kimara          &   1.0 &   1.000000 \\
Kimari          &   1.0 &   1.000000 \\
Kimaya          &   1.0 &   1.000000 \\
Kimbella        &   1.0 &   1.000000 \\
Kimber          &   1.0 &   0.500000 \\
Kimberely       &   1.0 &   0.352941 \\
Kimberle        &   1.0 &   0.600000 \\
Kimberlee       &   1.0 &   0.047170 \\
Kimberley       &   1.0 &   0.022059 \\
Kimberli        &   1.0 &   0.208333 \\
Kimberlie       &   1.0 &   0.172414 \\
Kimberlin       &   1.0 &   0.583333 \\
Kimberlina      &   1.0 &   1.000000 \\
Kimberly        &   1.0 &   0.091419 \\
Kimberlyann     &   1.0 &   0.714286 \\
Kimberlyanne    &   1.0 &   1.000000 \\
Kimberlyn       &   1.0 &   0.277778 \\
Kimberlynn      &   1.0 &   1.000000 \\
Kimbra          &   1.0 &   0.750000 \\
Kimi            &   1.0 &   0.277778 \\
Kimia           &   1.0 &   0.583333 \\
Kimie           &   1.0 &   1.000000 \\
Kimiko          &   1.0 &   0.352941 \\
Kimiya          &   1.0 &   0.714286 \\
Kimiye          &   1.0 &   0.600000 \\
Kimiyo          &   1.0 &   0.833333 \\
Kimm            &   1.0 &   0.833333 \\
Kimmi           &   1.0 &   1.000000 \\
Kimmie          &   1.0 &   0.416667 \\
Kimmy           &   1.0 &   0.625000 \\
Kimora          &   1.0 &   0.065789 \\
Kimya           &   1.0 &   0.555556 \\
Kina            &   1.0 &   1.000000 \\
Kinberly        &   1.0 &   0.833333 \\
Kindle          &   1.0 &   1.000000 \\
Kindra          &   1.0 &   0.200000 \\
Kingsley        &   1.0 &   1.000000 \\
Kinley          &   1.0 &   0.415385 \\
Kinsey          &   1.0 &   0.636364 \\
Kinslee         &   1.0 &   0.869565 \\
Kinsleigh       &   1.0 &   1.000000 \\
Kinsley         &   1.0 &   0.926724 \\
Kinza           &   1.0 &   1.000000 \\
Kinzie          &   1.0 &   0.625000 \\
Kinzlee         &   1.0 &   0.857143 \\
Kinzley         &   1.0 &   0.583333 \\
Kiona           &   1.0 &   0.875000 \\
Kionna          &   1.0 &   0.833333 \\
Kiora           &   1.0 &   1.000000 \\
Kira            &   1.0 &   0.682927 \\
Kirah           &   1.0 &   1.000000 \\
Kiran           &   1.0 &   0.357143 \\
Kirandeep       &   1.0 &   1.000000 \\
Kirby           &   1.0 &   0.375000 \\
Kiri            &   1.0 &   0.714286 \\
Kirin           &   1.0 &   0.857143 \\
Kirpa           &   1.0 &   1.000000 \\
Kirra           &   1.0 &   0.200000 \\
Kirsten         &   1.0 &   0.027778 \\
Kirsti          &   1.0 &   0.416667 \\
Kirstie         &   1.0 &   0.078125 \\
Kirstin         &   1.0 &   0.104167 \\
Kirsty          &   1.0 &   0.555556 \\
Kirstyn         &   1.0 &   0.333333 \\
Kirti           &   1.0 &   1.000000 \\
Kisha           &   1.0 &   0.147059 \\
Kit             &   1.0 &   1.000000 \\
Kitana          &   1.0 &   0.846154 \\
Kitt            &   1.0 &   1.000000 \\
Kittie          &   1.0 &   0.714286 \\
Kitty           &   1.0 &   0.258065 \\
Kitzia          &   1.0 &   0.454545 \\
Kiva            &   1.0 &   1.000000 \\
Kiya            &   1.0 &   0.357143 \\
Kiyah           &   1.0 &   0.666667 \\
Kiyana          &   1.0 &   0.625000 \\
Kiyanna         &   1.0 &   1.000000 \\
Kiyara          &   1.0 &   0.666667 \\
Kiyo            &   1.0 &   0.857143 \\
Kiyoko          &   1.0 &   0.172414 \\
Kiyomi          &   1.0 &   1.000000 \\
Kizzie          &   1.0 &   1.000000 \\
Kizzy           &   1.0 &   0.120690 \\
Kla             &   1.0 &   1.000000 \\
Klaire          &   1.0 &   0.888889 \\
Klara           &   1.0 &   0.833333 \\
Klarisa         &   1.0 &   0.750000 \\
Klarissa        &   1.0 &   0.184211 \\
Klaryssa        &   1.0 &   1.000000 \\
Klaudia         &   1.0 &   1.000000 \\
Kleo            &   1.0 &   1.000000 \\
Kloe            &   1.0 &   0.185185 \\
Kloee           &   1.0 &   1.000000 \\
Kloey           &   1.0 &   0.600000 \\
Kloie           &   1.0 &   0.750000 \\
Klynn           &   1.0 &   1.000000 \\
Koa             &   1.0 &   0.928571 \\
Kobe            &   1.0 &   1.000000 \\
Kobi            &   1.0 &   0.666667 \\
Kobie           &   1.0 &   1.000000 \\
Koda            &   1.0 &   1.000000 \\
Kodi            &   1.0 &   1.000000 \\
Kodie           &   1.0 &   0.555556 \\
Kody            &   1.0 &   1.000000 \\
Kohana          &   1.0 &   1.000000 \\
Kolbie          &   1.0 &   1.000000 \\
Kolby           &   1.0 &   1.000000 \\
Kolette         &   1.0 &   1.000000 \\
Kolina          &   1.0 &   1.000000 \\
Kolleen         &   1.0 &   0.625000 \\
Kollins         &   1.0 &   1.000000 \\
Kollyns         &   1.0 &   1.000000 \\
Komal           &   1.0 &   0.384615 \\
Komalpreet      &   1.0 &   1.000000 \\
Kona            &   1.0 &   1.000000 \\
Koni            &   1.0 &   1.000000 \\
Konnie          &   1.0 &   1.000000 \\
Kora            &   1.0 &   1.000000 \\
Koraima         &   1.0 &   0.600000 \\
Koral           &   1.0 &   0.857143 \\
Koraline        &   1.0 &   0.625000 \\
Koreen          &   1.0 &   1.000000 \\
Koren           &   1.0 &   0.500000 \\
Korena          &   1.0 &   0.833333 \\
Korey           &   1.0 &   1.000000 \\
Kori            &   1.0 &   0.388889 \\
Korie           &   1.0 &   0.428571 \\
Korin           &   1.0 &   0.500000 \\
Korina          &   1.0 &   0.147059 \\
Korinna         &   1.0 &   0.857143 \\
Korinne         &   1.0 &   0.714286 \\
Korissa         &   1.0 &   1.000000 \\
Korla           &   1.0 &   1.000000 \\
Korra           &   1.0 &   0.916667 \\
Korri           &   1.0 &   1.000000 \\
Korrie          &   1.0 &   0.857143 \\
Korrina         &   1.0 &   0.833333 \\
Korrine         &   1.0 &   1.000000 \\
Kortnee         &   1.0 &   1.000000 \\
Kortney         &   1.0 &   0.166667 \\
Kortni          &   1.0 &   0.833333 \\
Kortnie         &   1.0 &   1.000000 \\
Kory            &   1.0 &   0.454545 \\
Koryn           &   1.0 &   1.000000 \\
Koryna          &   1.0 &   1.000000 \\
Kourtnee        &   1.0 &   1.000000 \\
Kourtney        &   1.0 &   0.140351 \\
Kourtni         &   1.0 &   1.000000 \\
Koy             &   1.0 &   1.000000 \\
Kris            &   1.0 &   0.048544 \\
Krisann         &   1.0 &   1.000000 \\
Krisha          &   1.0 &   0.666667 \\
Krishna         &   1.0 &   0.200000 \\
Krisinda        &   1.0 &   1.000000 \\
Krislyn         &   1.0 &   1.000000 \\
Krislynn        &   1.0 &   1.000000 \\
Kriss           &   1.0 &   1.000000 \\
Krissy          &   1.0 &   0.312500 \\
Krista          &   1.0 &   0.022222 \\
Kristal         &   1.0 &   0.070423 \\
Kristan         &   1.0 &   0.285714 \\
Kriste          &   1.0 &   1.000000 \\
Kristeen        &   1.0 &   0.454545 \\
Kristeena       &   1.0 &   1.000000 \\
Kristel         &   1.0 &   0.162162 \\
Kristell        &   1.0 &   0.833333 \\
Kristelle       &   1.0 &   0.714286 \\
Kristen         &   1.0 &   0.017422 \\
Kristena        &   1.0 &   0.857143 \\
Kristene        &   1.0 &   0.714286 \\
Kristi          &   1.0 &   0.017730 \\
Kristian        &   1.0 &   0.142857 \\
Kristiana       &   1.0 &   0.217391 \\
Kristiane       &   1.0 &   1.000000 \\
Kristiann       &   1.0 &   1.000000 \\
Kristianna      &   1.0 &   0.600000 \\
Kristianne      &   1.0 &   1.000000 \\
Kristie         &   1.0 &   0.043165 \\
Kristin         &   1.0 &   0.007326 \\
Kristina        &   1.0 &   0.023222 \\
Kristinamarie   &   1.0 &   1.000000 \\
Kristine        &   1.0 &   0.016509 \\
Kristle         &   1.0 &   0.533333 \\
Kristol         &   1.0 &   1.000000 \\
Kriston         &   1.0 &   1.000000 \\
Kristopher      &   1.0 &   1.000000 \\
Kristy          &   1.0 &   0.012821 \\
Kristyl         &   1.0 &   1.000000 \\
Kristyn         &   1.0 &   0.120000 \\
Kristyna        &   1.0 &   0.714286 \\
Kristyne        &   1.0 &   0.714286 \\
Krithi          &   1.0 &   1.000000 \\
Kriti           &   1.0 &   0.750000 \\
Kritika         &   1.0 &   1.000000 \\
Krizia          &   1.0 &   0.571429 \\
Krupa           &   1.0 &   1.000000 \\
Krysta          &   1.0 &   0.098039 \\
Krystal         &   1.0 &   0.023704 \\
Krystale        &   1.0 &   1.000000 \\
Krystalyn       &   1.0 &   1.000000 \\
Krystalynn      &   1.0 &   1.000000 \\
Krystel         &   1.0 &   0.269231 \\
Krystelle       &   1.0 &   1.000000 \\
Krysten         &   1.0 &   0.172414 \\
Krysti          &   1.0 &   1.000000 \\
Krystiana       &   1.0 &   1.000000 \\
Krystie         &   1.0 &   1.000000 \\
Krystin         &   1.0 &   0.214286 \\
Krystina        &   1.0 &   0.109375 \\
Krystine        &   1.0 &   0.294118 \\
Krystle         &   1.0 &   0.014493 \\
Krystyn         &   1.0 &   1.000000 \\
Krystyna        &   1.0 &   0.470588 \\
Krystyne        &   1.0 &   1.000000 \\
Ksenia          &   1.0 &   1.000000 \\
Kya             &   1.0 &   0.461538 \\
Kyah            &   1.0 &   0.333333 \\
Kyaira          &   1.0 &   1.000000 \\
Kyana           &   1.0 &   0.400000 \\
Kyanna          &   1.0 &   0.384615 \\
Kyara           &   1.0 &   0.500000 \\
Kyarra          &   1.0 &   1.000000 \\
Kyia            &   1.0 &   1.000000 \\
Kyla            &   1.0 &   0.271795 \\
Kylah           &   1.0 &   0.187500 \\
Kylani          &   1.0 &   1.000000 \\
Kyle            &   1.0 &   0.156250 \\
Kylea           &   1.0 &   0.833333 \\
Kyleah          &   1.0 &   0.714286 \\
Kylee           &   1.0 &   0.107477 \\
Kyleen          &   1.0 &   0.714286 \\
Kylei           &   1.0 &   0.625000 \\
Kyleigh         &   1.0 &   0.098039 \\
Kylen           &   1.0 &   1.000000 \\
Kylene          &   1.0 &   0.259259 \\
Kyler           &   1.0 &   0.500000 \\
Kyley           &   1.0 &   0.454545 \\
Kyli            &   1.0 &   0.500000 \\
Kylia           &   1.0 &   0.714286 \\
Kylie           &   1.0 &   0.235387 \\
Kyliee          &   1.0 &   1.000000 \\
Kyliegh         &   1.0 &   1.000000 \\
Kylin           &   1.0 &   0.714286 \\
Kylynn          &   1.0 &   1.000000 \\
Kym             &   1.0 &   0.388889 \\
Kymani          &   1.0 &   0.857143 \\
Kymber          &   1.0 &   1.000000 \\
Kymberlee       &   1.0 &   1.000000 \\
Kymberli        &   1.0 &   1.000000 \\
Kymberly        &   1.0 &   0.250000 \\
Kymora          &   1.0 &   0.714286 \\
Kyna            &   1.0 &   0.833333 \\
Kyndal          &   1.0 &   0.545455 \\
Kyndall         &   1.0 &   0.263158 \\
Kyndra          &   1.0 &   0.400000 \\
Kynlee          &   1.0 &   0.400000 \\
Kynslee         &   1.0 &   0.700000 \\
Kynsley         &   1.0 &   1.000000 \\
Kynzie          &   1.0 &   1.000000 \\
Kynzlee         &   1.0 &   1.000000 \\
Kyoko           &   1.0 &   0.833333 \\
Kyomi           &   1.0 &   0.600000 \\
Kyra            &   1.0 &   0.373494 \\
Kyrah           &   1.0 &   0.384615 \\
Kyrie           &   1.0 &   0.666667 \\
Kyrsten         &   1.0 &   0.500000 \\
Kyrstie         &   1.0 &   1.000000 \\
Kyrstin         &   1.0 &   0.888889 \\
La              &   1.0 &   0.687500 \\
Laasya          &   1.0 &   0.666667 \\
Labrea          &   1.0 &   1.000000 \\
Lacee           &   1.0 &   0.333333 \\
Lacey           &   1.0 &   0.073276 \\
Lachelle        &   1.0 &   1.000000 \\
Laci            &   1.0 &   0.106383 \\
Lacie           &   1.0 &   0.156250 \\
Lacresha        &   1.0 &   0.500000 \\
Lacretia        &   1.0 &   1.000000 \\
Lacy            &   1.0 &   0.065789 \\
Ladana          &   1.0 &   1.000000 \\
Ladawn          &   1.0 &   0.500000 \\
Ladena          &   1.0 &   1.000000 \\
Ladona          &   1.0 &   1.000000 \\
Ladonna         &   1.0 &   0.090909 \\
Lady            &   1.0 &   0.454545 \\
Lael            &   1.0 &   0.500000 \\
Laela           &   1.0 &   0.454545 \\
Laelah          &   1.0 &   1.000000 \\
Laelani         &   1.0 &   1.000000 \\
Laelia          &   1.0 &   1.000000 \\
Lafondra        &   1.0 &   1.000000 \\
Lagina          &   1.0 &   1.000000 \\
Lahna           &   1.0 &   1.000000 \\
Lahoma          &   1.0 &   1.000000 \\
Lai             &   1.0 &   0.416667 \\
Laia            &   1.0 &   0.571429 \\
Laiah           &   1.0 &   1.000000 \\
Laiba           &   1.0 &   0.625000 \\
Laila           &   1.0 &   0.452675 \\
Lailah          &   1.0 &   0.400000 \\
Lailani         &   1.0 &   0.909091 \\
Lailany         &   1.0 &   0.857143 \\
Lailoni         &   1.0 &   0.714286 \\
Laina           &   1.0 &   0.454545 \\
Laine           &   1.0 &   0.619048 \\
Lainee          &   1.0 &   1.000000 \\
Lainey          &   1.0 &   1.000000 \\
Laini           &   1.0 &   1.000000 \\
Lainie          &   1.0 &   0.238095 \\
Laisa           &   1.0 &   0.875000 \\
Laisha          &   1.0 &   0.089552 \\
Laiyah          &   1.0 &   1.000000 \\
Laiza           &   1.0 &   0.312500 \\
Lajuana         &   1.0 &   0.500000 \\
Lakayla         &   1.0 &   1.000000 \\
Lake            &   1.0 &   0.615385 \\
Lakeesha        &   1.0 &   0.714286 \\
Lakeisha        &   1.0 &   0.100000 \\
Lakelyn         &   1.0 &   0.833333 \\
Laken           &   1.0 &   0.500000 \\
Lakendra        &   1.0 &   1.000000 \\
Lakenya         &   1.0 &   0.625000 \\
Lakesha         &   1.0 &   0.146341 \\
Lakeshia        &   1.0 &   0.263158 \\
Lakeya          &   1.0 &   0.714286 \\
Lakeysha        &   1.0 &   0.400000 \\
Lakia           &   1.0 &   0.625000 \\
Lakiesha        &   1.0 &   0.333333 \\
Lakisha         &   1.0 &   0.169231 \\
Lakishia        &   1.0 &   1.000000 \\
Lakita          &   1.0 &   1.000000 \\
Lakota          &   1.0 &   1.000000 \\
Lakshmi         &   1.0 &   0.600000 \\
Laksmi          &   1.0 &   1.000000 \\
Lalani          &   1.0 &   1.000000 \\
Lalanya         &   1.0 &   1.000000 \\
Lalena          &   1.0 &   0.625000 \\
Lalita          &   1.0 &   1.000000 \\
Lamar           &   1.0 &   0.454545 \\
Lamaya          &   1.0 &   1.000000 \\
Lamees          &   1.0 &   1.000000 \\
Lamia           &   1.0 &   1.000000 \\
Lamiya          &   1.0 &   1.000000 \\
Lamiyah         &   1.0 &   0.714286 \\
Lamona          &   1.0 &   1.000000 \\
Lamonica        &   1.0 &   0.625000 \\
Lamya           &   1.0 &   0.500000 \\
Lan             &   1.0 &   0.227273 \\
Lana            &   1.0 &   0.770115 \\
Lanae           &   1.0 &   0.466667 \\
Lanah           &   1.0 &   0.714286 \\
Lanai           &   1.0 &   0.833333 \\
Lanay           &   1.0 &   1.000000 \\
Lanaya          &   1.0 &   0.666667 \\
Landon          &   1.0 &   1.000000 \\
Landry          &   1.0 &   0.714286 \\
Landy           &   1.0 &   1.000000 \\
Landyn          &   1.0 &   0.750000 \\
Lane            &   1.0 &   0.545455 \\
Lanea           &   1.0 &   1.000000 \\
Lanee           &   1.0 &   1.000000 \\
Laneisha        &   1.0 &   0.555556 \\
Lanell          &   1.0 &   0.625000 \\
Lanesha         &   1.0 &   0.600000 \\
Laneshia        &   1.0 &   1.000000 \\
Lanessa         &   1.0 &   1.000000 \\
Lanetta         &   1.0 &   0.833333 \\
Lanette         &   1.0 &   0.181818 \\
Laney           &   1.0 &   0.411765 \\
Lani            &   1.0 &   0.605263 \\
Lania           &   1.0 &   0.714286 \\
Laniah          &   1.0 &   0.500000 \\
Lanie           &   1.0 &   0.416667 \\
Laniece         &   1.0 &   1.000000 \\
Lanika          &   1.0 &   1.000000 \\
Lanisha         &   1.0 &   0.384615 \\
Lanita          &   1.0 &   0.375000 \\
Laniya          &   1.0 &   0.312500 \\
Laniyah         &   1.0 &   0.260870 \\
Lanna           &   1.0 &   0.750000 \\
Lannette        &   1.0 &   0.555556 \\
Lannie          &   1.0 &   1.000000 \\
Lanora          &   1.0 &   0.833333 \\
Lany            &   1.0 &   1.000000 \\
Lanya           &   1.0 &   0.625000 \\
Lanyah          &   1.0 &   1.000000 \\
Laporsha        &   1.0 &   0.750000 \\
Laquesha        &   1.0 &   0.625000 \\
Laquinta        &   1.0 &   1.000000 \\
Laquisha        &   1.0 &   0.470588 \\
Laquita         &   1.0 &   0.461538 \\
Lara            &   1.0 &   0.333333 \\
Larae           &   1.0 &   0.333333 \\
Laraine         &   1.0 &   0.315789 \\
Laree           &   1.0 &   0.625000 \\
Lareen          &   1.0 &   1.000000 \\
Lareina         &   1.0 &   0.500000 \\
Laren           &   1.0 &   1.000000 \\
Larena          &   1.0 &   0.857143 \\
Larhonda        &   1.0 &   0.583333 \\
Lari            &   1.0 &   1.000000 \\
Lariah          &   1.0 &   0.357143 \\
Larina          &   1.0 &   1.000000 \\
Larisa          &   1.0 &   0.350000 \\
Larissa         &   1.0 &   0.189655 \\
Larita          &   1.0 &   0.625000 \\
Laritza         &   1.0 &   0.833333 \\
Lariyah         &   1.0 &   0.857143 \\
Lariza          &   1.0 &   1.000000 \\
Lark            &   1.0 &   0.312500 \\
Larkin          &   1.0 &   0.600000 \\
Laronda         &   1.0 &   0.263158 \\
Larraine        &   1.0 &   0.857143 \\
Larrie          &   1.0 &   1.000000 \\
Larry           &   1.0 &   0.555556 \\
Larue           &   1.0 &   1.000000 \\
Laruen          &   1.0 &   1.000000 \\
Laryssa         &   1.0 &   0.500000 \\
Lasandra        &   1.0 &   0.857143 \\
Lasaundra       &   1.0 &   1.000000 \\
Lasha           &   1.0 &   1.000000 \\
Lashae          &   1.0 &   0.500000 \\
Lashan          &   1.0 &   0.500000 \\
Lashana         &   1.0 &   0.833333 \\
Lashanae        &   1.0 &   0.727273 \\
Lashanay        &   1.0 &   1.000000 \\
Lashanda        &   1.0 &   0.347826 \\
Lashane         &   1.0 &   1.000000 \\
Lashanique      &   1.0 &   1.000000 \\
Lashanna        &   1.0 &   0.875000 \\
Lashanti        &   1.0 &   1.000000 \\
Lashaun         &   1.0 &   0.555556 \\
Lashaunda       &   1.0 &   0.714286 \\
Lashawn         &   1.0 &   0.079365 \\
Lashawna        &   1.0 &   0.875000 \\
Lashawnda       &   1.0 &   0.384615 \\
Lashay          &   1.0 &   0.357143 \\
Lashea          &   1.0 &   1.000000 \\
Lashell         &   1.0 &   0.714286 \\
Lashelle        &   1.0 &   1.000000 \\
Lashon          &   1.0 &   0.250000 \\
Lashonda        &   1.0 &   0.116279 \\
Lashondra       &   1.0 &   1.000000 \\
Lashone         &   1.0 &   1.000000 \\
Lashonna        &   1.0 &   1.000000 \\
Lashun          &   1.0 &   0.555556 \\
Lashunda        &   1.0 &   0.714286 \\
Lashundra       &   1.0 &   1.000000 \\
Lashune         &   1.0 &   1.000000 \\
Lasonya         &   1.0 &   1.000000 \\
Lasya           &   1.0 &   0.461538 \\
Latania         &   1.0 &   0.833333 \\
Latanya         &   1.0 &   0.063291 \\
Latara          &   1.0 &   0.461538 \\
Latasha         &   1.0 &   0.088235 \\
Latashia        &   1.0 &   0.600000 \\
Latausha        &   1.0 &   1.000000 \\
Latavia         &   1.0 &   1.000000 \\
Latecia         &   1.0 &   1.000000 \\
Lateefah        &   1.0 &   1.000000 \\
Lateisha        &   1.0 &   1.000000 \\
Latesha         &   1.0 &   0.416667 \\
Latia           &   1.0 &   0.750000 \\
Latiana         &   1.0 &   0.833333 \\
Laticia         &   1.0 &   0.357143 \\
Latiera         &   1.0 &   1.000000 \\
Latifah         &   1.0 &   1.000000 \\
Latijera        &   1.0 &   1.000000 \\
Latina          &   1.0 &   0.700000 \\
Latisha         &   1.0 &   0.081967 \\
Latishia        &   1.0 &   0.555556 \\
Latoi           &   1.0 &   1.000000 \\
Latonia         &   1.0 &   0.192308 \\
Latonja         &   1.0 &   0.714286 \\
Latonya         &   1.0 &   0.073529 \\
Latosha         &   1.0 &   0.368421 \\
Latoshia        &   1.0 &   1.000000 \\
Latoya          &   1.0 &   0.021930 \\
Latoyia         &   1.0 &   0.454545 \\
Latrece         &   1.0 &   1.000000 \\
Latrenda        &   1.0 &   1.000000 \\
Latrese         &   1.0 &   1.000000 \\
Latrice         &   1.0 &   0.119048 \\
Latricia        &   1.0 &   0.238095 \\
Latrina         &   1.0 &   0.375000 \\
Latrisha        &   1.0 &   0.666667 \\
Latunya         &   1.0 &   1.000000 \\
Launa           &   1.0 &   0.384615 \\
Laura           &   1.0 &   0.034725 \\
Lauraine        &   1.0 &   1.000000 \\
Laural          &   1.0 &   0.454545 \\
Lauralee        &   1.0 &   0.625000 \\
Lauran          &   1.0 &   0.714286 \\
Laure           &   1.0 &   0.750000 \\
Lauree          &   1.0 &   1.000000 \\
Laureen         &   1.0 &   0.157895 \\
Laurel          &   1.0 &   0.200000 \\
Lauren          &   1.0 &   0.040286 \\
Laurena         &   1.0 &   1.000000 \\
Laurene         &   1.0 &   0.277778 \\
Lauretta        &   1.0 &   0.388889 \\
Laurette        &   1.0 &   0.636364 \\
Laurey          &   1.0 &   1.000000 \\
Lauri           &   1.0 &   0.168675 \\
Laurice         &   1.0 &   0.857143 \\
Laurie          &   1.0 &   0.007049 \\
Laurin          &   1.0 &   0.875000 \\
Laurinda        &   1.0 &   0.500000 \\
Laurine         &   1.0 &   1.000000 \\
Laurissa        &   1.0 &   0.500000 \\
Laurita         &   1.0 &   1.000000 \\
Laury           &   1.0 &   1.000000 \\
Lauryn          &   1.0 &   0.093897 \\
Laurynn         &   1.0 &   0.833333 \\
Lavada          &   1.0 &   1.000000 \\
Lavanya         &   1.0 &   1.000000 \\
Laveah          &   1.0 &   1.000000 \\
Lavelle         &   1.0 &   1.000000 \\
Lavena          &   1.0 &   1.000000 \\
Lavender        &   1.0 &   1.000000 \\
Lavera          &   1.0 &   1.000000 \\
Lavern          &   1.0 &   0.500000 \\
Laverna         &   1.0 &   0.500000 \\
Laverne         &   1.0 &   0.096386 \\
Lavette         &   1.0 &   0.461538 \\
Lavina          &   1.0 &   0.666667 \\
Lavinia         &   1.0 &   0.666667 \\
Lavon           &   1.0 &   0.416667 \\
Lavonda         &   1.0 &   1.000000 \\
Lavonna         &   1.0 &   0.714286 \\
Lavonne         &   1.0 &   0.166667 \\
Lavren          &   1.0 &   1.000000 \\
Lawana          &   1.0 &   0.294118 \\
Lawanda         &   1.0 &   0.214286 \\
Lawanna         &   1.0 &   0.625000 \\
Lawren          &   1.0 &   0.857143 \\
Lawrence        &   1.0 &   1.000000 \\
Laya            &   1.0 &   0.500000 \\
Layah           &   1.0 &   0.833333 \\
Layal           &   1.0 &   0.777778 \\
Layan           &   1.0 &   1.000000 \\
Layana          &   1.0 &   0.714286 \\
Laycee          &   1.0 &   1.000000 \\
Layla           &   1.0 &   0.881900 \\
Laylah          &   1.0 &   0.577465 \\
Laylani         &   1.0 &   1.000000 \\
Laylanie        &   1.0 &   0.909091 \\
Layleen         &   1.0 &   1.000000 \\
Laylene         &   1.0 &   1.000000 \\
Layloni         &   1.0 &   1.000000 \\
Laylonie        &   1.0 &   1.000000 \\
Layna           &   1.0 &   0.777778 \\
Layne           &   1.0 &   1.000000 \\
Laynee          &   1.0 &   1.000000 \\
Laynie          &   1.0 &   1.000000 \\
Laysha          &   1.0 &   0.178571 \\
Layton          &   1.0 &   1.000000 \\
Le              &   1.0 &   0.416667 \\
Lea             &   1.0 &   0.627660 \\
Leah            &   1.0 &   0.627764 \\
Leahna          &   1.0 &   1.000000 \\
Leala           &   1.0 &   1.000000 \\
Lean            &   1.0 &   1.000000 \\
Leana           &   1.0 &   0.454545 \\
Leandra         &   1.0 &   0.250000 \\
Leandrea        &   1.0 &   1.000000 \\
Leane           &   1.0 &   1.000000 \\
Leann           &   1.0 &   0.080000 \\
Leanna          &   1.0 &   0.494382 \\
Leanne          &   1.0 &   0.125000 \\
Leanny          &   1.0 &   1.000000 \\
Leanora         &   1.0 &   0.750000 \\
Leanore         &   1.0 &   1.000000 \\
Leanza          &   1.0 &   1.000000 \\
Leasa           &   1.0 &   1.000000 \\
Leasia          &   1.0 &   1.000000 \\
Leatha          &   1.0 &   0.625000 \\
Leatrice        &   1.0 &   0.352941 \\
Lecia           &   1.0 &   0.625000 \\
Leda            &   1.0 &   0.625000 \\
Lee             &   1.0 &   0.038710 \\
Leea            &   1.0 &   1.000000 \\
Leeah           &   1.0 &   0.625000 \\
Leeana          &   1.0 &   0.625000 \\
Leeann          &   1.0 &   0.133333 \\
Leeanna         &   1.0 &   0.583333 \\
Leeanne         &   1.0 &   0.416667 \\
Leela           &   1.0 &   0.380952 \\
Leelah          &   1.0 &   0.750000 \\
Leelee          &   1.0 &   1.000000 \\
Leen            &   1.0 &   0.562500 \\
Leena           &   1.0 &   0.774194 \\
Leesa           &   1.0 &   0.350000 \\
Leesha          &   1.0 &   1.000000 \\
Leeya           &   1.0 &   1.000000 \\
Leeyah          &   1.0 &   1.000000 \\
Leeza           &   1.0 &   0.333333 \\
Legaci          &   1.0 &   1.000000 \\
Legacy          &   1.0 &   0.852941 \\
Legend          &   1.0 &   0.714286 \\
Lei             &   1.0 &   0.857143 \\
Leia            &   1.0 &   1.000000 \\
Leiah           &   1.0 &   0.466667 \\
Leiana          &   1.0 &   0.416667 \\
Leianna         &   1.0 &   0.625000 \\
Leida           &   1.0 &   1.000000 \\
Leidi           &   1.0 &   1.000000 \\
Leidy           &   1.0 &   0.416667 \\
Leigh           &   1.0 &   0.083333 \\
Leigha          &   1.0 &   0.333333 \\
Leighann        &   1.0 &   0.500000 \\
Leighanna       &   1.0 &   1.000000 \\
Leighanne       &   1.0 &   1.000000 \\
Leighla         &   1.0 &   0.555556 \\
Leighton        &   1.0 &   0.720000 \\
Leila           &   1.0 &   0.732673 \\
Leilah          &   1.0 &   0.647059 \\
Leilana         &   1.0 &   0.857143 \\
Leilanee        &   1.0 &   1.000000 \\
Leilaney        &   1.0 &   1.000000 \\
Leilani         &   1.0 &   0.984127 \\
Leilanie        &   1.0 &   0.307692 \\
Leilanni        &   1.0 &   0.888889 \\
Leilany         &   1.0 &   1.000000 \\
Leileen         &   1.0 &   1.000000 \\
Leilene         &   1.0 &   0.178571 \\
Leiloni         &   1.0 &   0.857143 \\
Leily           &   1.0 &   1.000000 \\
Leina           &   1.0 &   1.000000 \\
Leisa           &   1.0 &   0.222222 \\
Leisha          &   1.0 &   0.454545 \\
Leiya           &   1.0 &   1.000000 \\
Leiyah          &   1.0 &   1.000000 \\
Lekisha         &   1.0 &   1.000000 \\
Lela            &   1.0 &   0.260870 \\
Lelah           &   1.0 &   1.000000 \\
Lelani          &   1.0 &   0.500000 \\
Lelania         &   1.0 &   1.000000 \\
Lelia           &   1.0 &   0.636364 \\
Lemon           &   1.0 &   1.000000 \\
Lena            &   1.0 &   0.981651 \\
Lenae           &   1.0 &   1.000000 \\
Lenda           &   1.0 &   1.000000 \\
Lenee           &   1.0 &   1.000000 \\
Lenette         &   1.0 &   0.625000 \\
Leni            &   1.0 &   1.000000 \\
Lenina          &   1.0 &   1.000000 \\
Lenita          &   1.0 &   1.000000 \\
Lenna           &   1.0 &   0.937500 \\
Lennie          &   1.0 &   0.714286 \\
Lennix          &   1.0 &   1.000000 \\
Lennon          &   1.0 &   0.931818 \\
Lennox          &   1.0 &   0.950000 \\
Lenny           &   1.0 &   0.600000 \\
Lennyn          &   1.0 &   1.000000 \\
Lenora          &   1.0 &   0.500000 \\
Lenore          &   1.0 &   0.162162 \\
Lenox           &   1.0 &   1.000000 \\
Lenya           &   1.0 &   1.000000 \\
Leo             &   1.0 &   1.000000 \\
Leola           &   1.0 &   0.333333 \\
Leon            &   1.0 &   1.000000 \\
Leona           &   1.0 &   0.898734 \\
Leonarda        &   1.0 &   1.000000 \\
Leonardo        &   1.0 &   1.000000 \\
Leondra         &   1.0 &   0.833333 \\
Leone           &   1.0 &   0.625000 \\
Leonela         &   1.0 &   0.263158 \\
Leoni           &   1.0 &   1.000000 \\
Leonie          &   1.0 &   0.529412 \\
Leonila         &   1.0 &   1.000000 \\
Leonna          &   1.0 &   1.000000 \\
Leonor          &   1.0 &   0.269231 \\
Leonora         &   1.0 &   0.769231 \\
Leonore         &   1.0 &   0.625000 \\
Leora           &   1.0 &   0.466667 \\
Leota           &   1.0 &   0.400000 \\
Lesa            &   1.0 &   0.101695 \\
Lesha           &   1.0 &   0.833333 \\
Leshawn         &   1.0 &   1.000000 \\
Lesia           &   1.0 &   0.857143 \\
Leslee          &   1.0 &   0.200000 \\
Lesley          &   1.0 &   0.036530 \\
Lesli           &   1.0 &   0.135135 \\
Leslie          &   1.0 &   0.086777 \\
Leslieann       &   1.0 &   0.600000 \\
Lesly           &   1.0 &   0.104089 \\
Leslye          &   1.0 &   0.523810 \\
Lessie          &   1.0 &   1.000000 \\
Lesslie         &   1.0 &   0.555556 \\
Lessly          &   1.0 &   0.263158 \\
Leta            &   1.0 &   0.421053 \\
Letecia         &   1.0 &   0.714286 \\
Letha           &   1.0 &   0.333333 \\
Letica          &   1.0 &   1.000000 \\
Leticia         &   1.0 &   0.050114 \\
Letisha         &   1.0 &   0.416667 \\
Letisia         &   1.0 &   0.208333 \\
Letitia         &   1.0 &   0.200000 \\
Letrice         &   1.0 &   0.833333 \\
Letricia        &   1.0 &   1.000000 \\
Letticia        &   1.0 &   0.625000 \\
Lettie          &   1.0 &   1.000000 \\
Letty           &   1.0 &   0.500000 \\
Lety            &   1.0 &   1.000000 \\
Letzy           &   1.0 &   1.000000 \\
Levana          &   1.0 &   1.000000 \\
Levi            &   1.0 &   0.705882 \\
Levy            &   1.0 &   0.555556 \\
Lexa            &   1.0 &   0.294118 \\
Lexani          &   1.0 &   0.875000 \\
Lexi            &   1.0 &   0.628743 \\
Lexia           &   1.0 &   0.750000 \\
Lexie           &   1.0 &   0.567308 \\
Lexii           &   1.0 &   1.000000 \\
Lexine          &   1.0 &   1.000000 \\
Lexington       &   1.0 &   0.888889 \\
Lexis           &   1.0 &   0.421053 \\
Lexus           &   1.0 &   0.125000 \\
Lexxie          &   1.0 &   1.000000 \\
Lexy            &   1.0 &   0.321429 \\
Leya            &   1.0 &   0.875000 \\
Leyah           &   1.0 &   0.545455 \\
Leyda           &   1.0 &   0.857143 \\
Leydi           &   1.0 &   0.714286 \\
Leyla           &   1.0 &   0.899160 \\
Leylah          &   1.0 &   0.681818 \\
Leylani         &   1.0 &   1.000000 \\
Leylanie        &   1.0 &   0.555556 \\
Leylany         &   1.0 &   1.000000 \\
Leyna           &   1.0 &   0.138889 \\
Leyre           &   1.0 &   1.000000 \\
Leza            &   1.0 &   1.000000 \\
Lezli           &   1.0 &   1.000000 \\
Lezlie          &   1.0 &   0.357143 \\
Lezly           &   1.0 &   0.400000 \\
Lia             &   1.0 &   0.901786 \\
Liah            &   1.0 &   0.581395 \\
Liahna          &   1.0 &   1.000000 \\
Lian            &   1.0 &   0.500000 \\
Liana           &   1.0 &   0.859375 \\
Liandra         &   1.0 &   1.000000 \\
Liane           &   1.0 &   0.250000 \\
Liani           &   1.0 &   0.454545 \\
Lianna          &   1.0 &   0.673913 \\
Liannah         &   1.0 &   1.000000 \\
Lianne          &   1.0 &   0.333333 \\
Liara           &   1.0 &   0.714286 \\
Liat            &   1.0 &   1.000000 \\
Libby           &   1.0 &   0.652174 \\
Libertad        &   1.0 &   1.000000 \\
Liberty         &   1.0 &   0.473684 \\
Libni           &   1.0 &   0.833333 \\
Libra           &   1.0 &   1.000000 \\
Librada         &   1.0 &   1.000000 \\
Licet           &   1.0 &   1.000000 \\
Licette         &   1.0 &   0.833333 \\
Licia           &   1.0 &   0.666667 \\
Lida            &   1.0 &   0.625000 \\
Lidia           &   1.0 &   0.179775 \\
Liduvina        &   1.0 &   1.000000 \\
Lidya           &   1.0 &   1.000000 \\
Liel            &   1.0 &   0.625000 \\
Lielle          &   1.0 &   0.545455 \\
Lien            &   1.0 &   0.333333 \\
Liese           &   1.0 &   1.000000 \\
Liesel          &   1.0 &   0.857143 \\
Liesl           &   1.0 &   0.454545 \\
Liezl           &   1.0 &   1.000000 \\
Ligia           &   1.0 &   0.357143 \\
Lila            &   1.0 &   0.585859 \\
Lilac           &   1.0 &   1.000000 \\
Lilah           &   1.0 &   0.915493 \\
Lilee           &   1.0 &   0.714286 \\
Lili            &   1.0 &   0.550000 \\
Lilia           &   1.0 &   0.547619 \\
Lilian          &   1.0 &   0.554455 \\
Liliana         &   1.0 &   0.797101 \\
Liliane         &   1.0 &   0.625000 \\
Liliann         &   1.0 &   1.000000 \\
Lilianna        &   1.0 &   0.641026 \\
Lilianne        &   1.0 &   0.600000 \\
Lilibet         &   1.0 &   1.000000 \\
Lilibeth        &   1.0 &   0.283019 \\
Lilie           &   1.0 &   0.833333 \\
Lilieth         &   1.0 &   1.000000 \\
Lilit           &   1.0 &   0.600000 \\
Lilith          &   1.0 &   1.000000 \\
Liliya          &   1.0 &   1.000000 \\
Lilja           &   1.0 &   1.000000 \\
Lilla           &   1.0 &   0.555556 \\
Lillee          &   1.0 &   0.857143 \\
Lilli           &   1.0 &   0.421053 \\
Lillia          &   1.0 &   0.428571 \\
Lillian         &   1.0 &   0.542923 \\
Lilliana        &   1.0 &   0.578571 \\
Lilliann        &   1.0 &   0.714286 \\
Lillianna       &   1.0 &   0.371429 \\
Lillianne       &   1.0 &   0.833333 \\
Lillie          &   1.0 &   0.372093 \\
Lillith         &   1.0 &   0.416667 \\
Lilly           &   1.0 &   0.415020 \\
Lillyan         &   1.0 &   0.461538 \\
Lillyana        &   1.0 &   0.212766 \\
Lillyann        &   1.0 &   0.350000 \\
Lillyanna       &   1.0 &   0.187500 \\
Lillyanne       &   1.0 &   0.555556 \\
Lillybeth       &   1.0 &   1.000000 \\
Lilo            &   1.0 &   1.000000 \\
Lilou           &   1.0 &   0.857143 \\
Lilu            &   1.0 &   1.000000 \\
Lily            &   1.0 &   0.819495 \\
Lilya           &   1.0 &   0.714286 \\
Lilyan          &   1.0 &   0.277778 \\
Lilyana         &   1.0 &   0.443299 \\
Lilyanah        &   1.0 &   1.000000 \\
Lilyann         &   1.0 &   0.473684 \\
Lilyanna        &   1.0 &   0.468085 \\
Lilyanne        &   1.0 &   0.277778 \\
Lilybeth        &   1.0 &   0.600000 \\
Lilyrose        &   1.0 &   0.800000 \\
Lilyth          &   1.0 &   1.000000 \\
Limairy         &   1.0 &   1.000000 \\
Limayri         &   1.0 &   1.000000 \\
Lin             &   1.0 &   0.625000 \\
Lina            &   1.0 &   0.923077 \\
Lincoln         &   1.0 &   0.571429 \\
Lincy           &   1.0 &   1.000000 \\
Linda           &   1.0 &   0.005622 \\
Linden          &   1.0 &   0.777778 \\
Lindi           &   1.0 &   0.625000 \\
Lindley         &   1.0 &   1.000000 \\
Lindsay         &   1.0 &   0.022487 \\
Lindsee         &   1.0 &   1.000000 \\
Lindsey         &   1.0 &   0.017391 \\
Lindsi          &   1.0 &   1.000000 \\
Lindsie         &   1.0 &   0.555556 \\
Lindsy          &   1.0 &   0.294118 \\
Lindy           &   1.0 &   0.176471 \\
Linell          &   1.0 &   1.000000 \\
Linette         &   1.0 &   0.277778 \\
Ling            &   1.0 &   1.000000 \\
Linh            &   1.0 &   0.272727 \\
Linn            &   1.0 &   0.600000 \\
Linna           &   1.0 &   1.000000 \\
Linnae          &   1.0 &   1.000000 \\
Linnea          &   1.0 &   0.531250 \\
Linnette        &   1.0 &   0.714286 \\
Linsay          &   1.0 &   1.000000 \\
Linsey          &   1.0 &   0.192308 \\
Linsy           &   1.0 &   1.000000 \\
Linzy           &   1.0 &   1.000000 \\
Lior            &   1.0 &   0.833333 \\
Liora           &   1.0 &   0.785714 \\
Lira            &   1.0 &   1.000000 \\
Lirio           &   1.0 &   1.000000 \\
Liron           &   1.0 &   1.000000 \\
Lisa            &   1.0 &   0.011338 \\
Lisabeth        &   1.0 &   0.714286 \\
Lisamarie       &   1.0 &   0.250000 \\
Lisandra        &   1.0 &   0.300000 \\
Lisania         &   1.0 &   1.000000 \\
Lisanne         &   1.0 &   1.000000 \\
Lisbet          &   1.0 &   0.187500 \\
Lisbeth         &   1.0 &   0.089888 \\
Lise            &   1.0 &   0.217391 \\
Liset           &   1.0 &   0.172414 \\
Liseth          &   1.0 &   0.312500 \\
Lisett          &   1.0 &   0.333333 \\
Lisette         &   1.0 &   0.078740 \\
Lisha           &   1.0 &   0.545455 \\
Lissa           &   1.0 &   0.260870 \\
Lissandra       &   1.0 &   0.833333 \\
Lisset          &   1.0 &   0.225806 \\
Lissete         &   1.0 &   0.583333 \\
Lisseth         &   1.0 &   0.384615 \\
Lissett         &   1.0 &   0.461538 \\
Lissette        &   1.0 &   0.052632 \\
Lita            &   1.0 &   0.375000 \\
Lithzy          &   1.0 &   1.000000 \\
Litsy           &   1.0 &   1.000000 \\
Litzi           &   1.0 &   0.450000 \\
Litzy           &   1.0 &   0.048458 \\
Liv             &   1.0 &   0.795918 \\
Livana          &   1.0 &   1.000000 \\
Lively          &   1.0 &   0.500000 \\
Livi            &   1.0 &   1.000000 \\
Livia           &   1.0 &   0.690909 \\
Liviana         &   1.0 &   1.000000 \\
Livie           &   1.0 &   0.714286 \\
Livier          &   1.0 &   0.500000 \\
Liya            &   1.0 &   1.000000 \\
Liyah           &   1.0 &   0.240000 \\
Liyana          &   1.0 &   1.000000 \\
Liz             &   1.0 &   0.116279 \\
Liza            &   1.0 &   0.151899 \\
Lizabeth        &   1.0 &   0.185185 \\
Lizandra        &   1.0 &   0.555556 \\
Lizania         &   1.0 &   1.000000 \\
Lizanne         &   1.0 &   0.625000 \\
Lizbet          &   1.0 &   0.125000 \\
Lizbeth         &   1.0 &   0.069712 \\
Lizbett         &   1.0 &   1.000000 \\
Lizel           &   1.0 &   1.000000 \\
Lizet           &   1.0 &   0.083333 \\
Lizeth          &   1.0 &   0.141104 \\
Lizett          &   1.0 &   0.166667 \\
Lizette         &   1.0 &   0.035857 \\
Lizvet          &   1.0 &   1.000000 \\
Lizveth         &   1.0 &   1.000000 \\
Lizvette        &   1.0 &   1.000000 \\
Lizzet          &   1.0 &   0.318182 \\
Lizzete         &   1.0 &   1.000000 \\
Lizzeth         &   1.0 &   0.500000 \\
Lizzett         &   1.0 &   0.666667 \\
Lizzette        &   1.0 &   0.214286 \\
Lizzie          &   1.0 &   0.357143 \\
Lizzy           &   1.0 &   0.714286 \\
Lladira         &   1.0 &   1.000000 \\
Llesenia        &   1.0 &   0.454545 \\
Lloana          &   1.0 &   1.000000 \\
Lluliana        &   1.0 &   1.000000 \\
Lluvia          &   1.0 &   0.338983 \\
Loa             &   1.0 &   1.000000 \\
Loan            &   1.0 &   0.375000 \\
Logan           &   1.0 &   0.640351 \\
Logann          &   1.0 &   1.000000 \\
Loida           &   1.0 &   0.714286 \\
Lois            &   1.0 &   0.013369 \\
Lola            &   1.0 &   0.621053 \\
Lolah           &   1.0 &   1.000000 \\
Lolita          &   1.0 &   0.208333 \\
Lolly           &   1.0 &   1.000000 \\
Loma            &   1.0 &   1.000000 \\
Lona            &   1.0 &   0.428571 \\
Londa           &   1.0 &   0.545455 \\
London          &   1.0 &   0.326923 \\
Londyn          &   1.0 &   0.568807 \\
Londynn         &   1.0 &   0.529412 \\
Loni            &   1.0 &   0.133333 \\
Lonie           &   1.0 &   1.000000 \\
Lonna           &   1.0 &   0.312500 \\
Lonnie          &   1.0 &   0.277778 \\
Lora            &   1.0 &   0.068182 \\
Loraine         &   1.0 &   0.178571 \\
Loralee         &   1.0 &   0.555556 \\
Loralei         &   1.0 &   1.000000 \\
Loralie         &   1.0 &   1.000000 \\
Lorayne         &   1.0 &   1.000000 \\
Lore            &   1.0 &   0.500000 \\
Loreal          &   1.0 &   0.625000 \\
Loree           &   1.0 &   0.315789 \\
Loreen          &   1.0 &   0.227273 \\
Lorelai         &   1.0 &   0.860000 \\
Lorelei         &   1.0 &   0.716667 \\
Loreli          &   1.0 &   1.000000 \\
Lorely          &   1.0 &   0.833333 \\
Loren           &   1.0 &   0.142857 \\
Lorena          &   1.0 &   0.055838 \\
Lorenda         &   1.0 &   1.000000 \\
Lorene          &   1.0 &   0.127660 \\
Lorenia         &   1.0 &   1.000000 \\
Lorenza         &   1.0 &   0.666667 \\
Loretta         &   1.0 &   0.072727 \\
Lorette         &   1.0 &   1.000000 \\
Lori            &   1.0 &   0.008138 \\
Loria           &   1.0 &   1.000000 \\
Loriann         &   1.0 &   0.500000 \\
Lorianne        &   1.0 &   0.714286 \\
Lorie           &   1.0 &   0.040000 \\
Lorien          &   1.0 &   0.714286 \\
Loriene         &   1.0 &   1.000000 \\
Lorilee         &   1.0 &   0.833333 \\
Lorilei         &   1.0 &   1.000000 \\
Lorilyn         &   1.0 &   1.000000 \\
Lorin           &   1.0 &   0.416667 \\
Lorina          &   1.0 &   0.615385 \\
Lorinda         &   1.0 &   0.185185 \\
Lorine          &   1.0 &   0.777778 \\
Loris           &   1.0 &   1.000000 \\
Lorisa          &   1.0 &   1.000000 \\
Lorissa         &   1.0 &   0.454545 \\
Lorita          &   1.0 &   0.777778 \\
Lorna           &   1.0 &   0.051282 \\
Lorraina        &   1.0 &   1.000000 \\
Lorraine        &   1.0 &   0.045455 \\
Lorrayne        &   1.0 &   1.000000 \\
Lorre           &   1.0 &   1.000000 \\
Lorren          &   1.0 &   1.000000 \\
Lorrene         &   1.0 &   1.000000 \\
Lorretta        &   1.0 &   0.454545 \\
Lorri           &   1.0 &   0.079365 \\
Lorriane        &   1.0 &   1.000000 \\
Lorrie          &   1.0 &   0.046512 \\
Lorrine         &   1.0 &   1.000000 \\
Lorry           &   1.0 &   0.818182 \\
Lory            &   1.0 &   0.384615 \\
Loryn           &   1.0 &   0.500000 \\
Lotoya          &   1.0 &   1.000000 \\
Lottie          &   1.0 &   0.900000 \\
Lotus           &   1.0 &   0.545455 \\
Lou             &   1.0 &   0.226415 \\
Louann          &   1.0 &   0.454545 \\
Louanna         &   1.0 &   1.000000 \\
Louanne         &   1.0 &   0.833333 \\
Louella         &   1.0 &   0.500000 \\
Louie           &   1.0 &   1.000000 \\
Louis           &   1.0 &   0.833333 \\
Louisa          &   1.0 &   0.684211 \\
Louise          &   1.0 &   0.162791 \\
Loukisha        &   1.0 &   1.000000 \\
Louna           &   1.0 &   1.000000 \\
Loura           &   1.0 &   0.833333 \\
Lourdes         &   1.0 &   0.073770 \\
Lourie          &   1.0 &   0.714286 \\
Love            &   1.0 &   1.000000 \\
Loveah          &   1.0 &   0.857143 \\
Lovella         &   1.0 &   1.000000 \\
Lovely          &   1.0 &   0.782609 \\
Lovelyn         &   1.0 &   1.000000 \\
Lovelynn        &   1.0 &   1.000000 \\
Lovette         &   1.0 &   1.000000 \\
Lovina          &   1.0 &   1.000000 \\
Lowana          &   1.0 &   1.000000 \\
Loy             &   1.0 &   1.000000 \\
Loyal           &   1.0 &   1.000000 \\
Loyalty         &   1.0 &   0.680000 \\
Loyce           &   1.0 &   0.750000 \\
Ltanya          &   1.0 &   1.000000 \\
Lu              &   1.0 &   0.400000 \\
Lua             &   1.0 &   1.000000 \\
Luana           &   1.0 &   0.592593 \\
Luann           &   1.0 &   0.111111 \\
Luanna          &   1.0 &   0.750000 \\
Luanne          &   1.0 &   0.138889 \\
Luca            &   1.0 &   0.454545 \\
Lucas           &   1.0 &   1.000000 \\
Lucca           &   1.0 &   0.357143 \\
Lucciana        &   1.0 &   0.833333 \\
Lucerito        &   1.0 &   0.333333 \\
Lucero          &   1.0 &   0.251852 \\
Luci            &   1.0 &   0.636364 \\
Lucia           &   1.0 &   1.000000 \\
Luciana         &   1.0 &   0.928571 \\
Lucianna        &   1.0 &   0.450000 \\
Lucie           &   1.0 &   0.692308 \\
Lucienne        &   1.0 &   0.714286 \\
Lucila          &   1.0 &   0.227273 \\
Lucile          &   1.0 &   0.131579 \\
Lucille         &   1.0 &   0.338095 \\
Lucina          &   1.0 &   0.454545 \\
Lucinda         &   1.0 &   0.115789 \\
Lucine          &   1.0 &   0.666667 \\
Lucky           &   1.0 &   1.000000 \\
Lucrecia        &   1.0 &   0.666667 \\
Lucretia        &   1.0 &   0.352941 \\
Lucy            &   1.0 &   0.796767 \\
Ludivina        &   1.0 &   1.000000 \\
Luella          &   1.0 &   0.772727 \\
Luis            &   1.0 &   0.230769 \\
Luisa           &   1.0 &   0.633803 \\
Luisana         &   1.0 &   0.084507 \\
Luiza           &   1.0 &   0.625000 \\
Lujain          &   1.0 &   1.000000 \\
Luka            &   1.0 &   0.625000 \\
Luke            &   1.0 &   1.000000 \\
Lula            &   1.0 &   0.714286 \\
Lulu            &   1.0 &   0.933333 \\
Luma            &   1.0 &   1.000000 \\
Lumen           &   1.0 &   1.000000 \\
Lumi            &   1.0 &   1.000000 \\
Luna            &   1.0 &   1.000000 \\
Lunabella       &   1.0 &   1.000000 \\
Lunabelle       &   1.0 &   1.000000 \\
Lunah           &   1.0 &   1.000000 \\
Lunarose        &   1.0 &   1.000000 \\
Lunden          &   1.0 &   1.000000 \\
Lundyn          &   1.0 &   1.000000 \\
Lunna           &   1.0 &   0.833333 \\
Lupe            &   1.0 &   0.044025 \\
Lupie           &   1.0 &   1.000000 \\
Lupita          &   1.0 &   0.135593 \\
Lura            &   1.0 &   0.352941 \\
Lurdes          &   1.0 &   0.625000 \\
Lurline         &   1.0 &   1.000000 \\
Lus             &   1.0 &   0.416667 \\
Lusero          &   1.0 &   0.714286 \\
Lusia           &   1.0 &   0.666667 \\
Lusine          &   1.0 &   0.750000 \\
Luwana          &   1.0 &   1.000000 \\
Lux             &   1.0 &   0.450000 \\
Luxe            &   1.0 &   1.000000 \\
Luz             &   1.0 &   0.431034 \\
Luzelena        &   1.0 &   0.666667 \\
Luzia           &   1.0 &   1.000000 \\
Luzmaria        &   1.0 &   0.461538 \\
Ly              &   1.0 &   0.428571 \\
Lya             &   1.0 &   0.357143 \\
Lyah            &   1.0 &   1.000000 \\
Lyana           &   1.0 &   0.857143 \\
Lyanna          &   1.0 &   0.514286 \\
Lyanne          &   1.0 &   0.294118 \\
Lyda            &   1.0 &   1.000000 \\
Lydia           &   1.0 &   0.587302 \\
Lyla            &   1.0 &   0.956000 \\
Lylah           &   1.0 &   0.850746 \\
Lylia           &   1.0 &   1.000000 \\
Lyliana         &   1.0 &   1.000000 \\
Lyly            &   1.0 &   1.000000 \\
Lyn             &   1.0 &   0.113636 \\
Lyna            &   1.0 &   0.900000 \\
Lynae           &   1.0 &   0.714286 \\
Lynda           &   1.0 &   0.010121 \\
Lyndee          &   1.0 &   1.000000 \\
Lyndell         &   1.0 &   0.833333 \\
Lynden          &   1.0 &   1.000000 \\
Lyndi           &   1.0 &   0.714286 \\
Lyndia          &   1.0 &   1.000000 \\
Lyndie          &   1.0 &   1.000000 \\
Lyndsay         &   1.0 &   0.184211 \\
Lyndsey         &   1.0 &   0.079365 \\
Lyndsi          &   1.0 &   0.625000 \\
Lyndsie         &   1.0 &   0.375000 \\
Lyndsy          &   1.0 &   1.000000 \\
Lynea           &   1.0 &   1.000000 \\
Lynee           &   1.0 &   1.000000 \\
Lynell          &   1.0 &   0.454545 \\
Lynelle         &   1.0 &   0.416667 \\
Lynetta         &   1.0 &   0.555556 \\
Lynette         &   1.0 &   0.059113 \\
Lynn            &   1.0 &   0.012800 \\
Lynna           &   1.0 &   0.714286 \\
Lynnae          &   1.0 &   0.714286 \\
Lynnda          &   1.0 &   0.555556 \\
Lynne           &   1.0 &   0.022936 \\
Lynnea          &   1.0 &   0.714286 \\
Lynnel          &   1.0 &   1.000000 \\
Lynnell         &   1.0 &   1.000000 \\
Lynnetta        &   1.0 &   1.000000 \\
Lynnette        &   1.0 &   0.075000 \\
Lynsey          &   1.0 &   0.208333 \\
Lynsie          &   1.0 &   0.750000 \\
Lynzee          &   1.0 &   1.000000 \\
Lynzie          &   1.0 &   1.000000 \\
Lyra            &   1.0 &   1.000000 \\
Lyric           &   1.0 &   0.397436 \\
Lyrica          &   1.0 &   1.000000 \\
Lyrik           &   1.0 &   0.500000 \\
Lyriq           &   1.0 &   0.750000 \\
Lysa            &   1.0 &   1.000000 \\
Lysandra        &   1.0 &   0.750000 \\
Lysette         &   1.0 &   0.384615 \\
Lyssa           &   1.0 &   0.500000 \\
Lytzy           &   1.0 &   1.000000 \\
Lyvia           &   1.0 &   1.000000 \\
Lyza            &   1.0 &   1.000000 \\
Lyzeth          &   1.0 &   1.000000 \\
Lyzette         &   1.0 &   0.600000 \\
Ma              &   1.0 &   0.500000 \\
Maahi           &   1.0 &   1.000000 \\
Maaliyah        &   1.0 &   1.000000 \\
Maanvi          &   1.0 &   1.000000 \\
Maanya          &   1.0 &   0.700000 \\
Maayan          &   1.0 &   1.000000 \\
Mabel           &   1.0 &   0.927711 \\
Mabelle         &   1.0 &   1.000000 \\
Mable           &   1.0 &   0.192308 \\
Macaela         &   1.0 &   1.000000 \\
Macayla         &   1.0 &   0.545455 \\
Macee           &   1.0 &   0.857143 \\
Macey           &   1.0 &   0.218750 \\
Machele         &   1.0 &   1.000000 \\
Machelle        &   1.0 &   0.263158 \\
Maci            &   1.0 &   0.309859 \\
Macie           &   1.0 &   0.493506 \\
Maciel          &   1.0 &   0.833333 \\
Mackayla        &   1.0 &   0.777778 \\
Mackenna        &   1.0 &   0.178571 \\
Mackensie       &   1.0 &   1.000000 \\
Mackenzee       &   1.0 &   0.888889 \\
Mackenzi        &   1.0 &   1.000000 \\
Mackenzie       &   1.0 &   0.465241 \\
Mackenzy        &   1.0 &   0.714286 \\
Mackinzie       &   1.0 &   1.000000 \\
Macrina         &   1.0 &   1.000000 \\
Macy            &   1.0 &   0.225806 \\
Macyn           &   1.0 &   1.000000 \\
Madai           &   1.0 &   0.600000 \\
Madaleine       &   1.0 &   1.000000 \\
Madalena        &   1.0 &   1.000000 \\
Madalin         &   1.0 &   1.000000 \\
Madaline        &   1.0 &   0.375000 \\
Madalyn         &   1.0 &   0.226667 \\
Madalyne        &   1.0 &   0.833333 \\
Madalynn        &   1.0 &   0.372093 \\
Madalynne       &   1.0 &   1.000000 \\
Maddalena       &   1.0 &   1.000000 \\
Maddalyn        &   1.0 &   1.000000 \\
Maddelyn        &   1.0 &   0.454545 \\
Maddelynn       &   1.0 &   1.000000 \\
Madden          &   1.0 &   1.000000 \\
Maddie          &   1.0 &   1.000000 \\
Maddilyn        &   1.0 &   0.500000 \\
Maddilynn       &   1.0 &   1.000000 \\
Maddisen        &   1.0 &   1.000000 \\
Maddison        &   1.0 &   0.793388 \\
Maddisyn        &   1.0 &   0.714286 \\
Maddox          &   1.0 &   0.777778 \\
Maddy           &   1.0 &   1.000000 \\
Maddyn          &   1.0 &   0.750000 \\
Maddyson        &   1.0 &   1.000000 \\
Madeira         &   1.0 &   1.000000 \\
Madelaine       &   1.0 &   0.571429 \\
Madelein        &   1.0 &   1.000000 \\
Madeleine       &   1.0 &   0.641148 \\
Madelene        &   1.0 &   0.625000 \\
Madeley         &   1.0 &   1.000000 \\
Madelin         &   1.0 &   0.555556 \\
Madeline        &   1.0 &   0.569686 \\
Madelon         &   1.0 &   0.714286 \\
Madelyn         &   1.0 &   0.813063 \\
Madelyne        &   1.0 &   0.368421 \\
Madelynn        &   1.0 &   0.804878 \\
Madelynne       &   1.0 &   0.416667 \\
Madge           &   1.0 &   0.600000 \\
Madilyn         &   1.0 &   0.675926 \\
Madilynn        &   1.0 &   0.775510 \\
Madilynne       &   1.0 &   1.000000 \\
Madina          &   1.0 &   0.461538 \\
Madisen         &   1.0 &   0.189189 \\
Madison         &   1.0 &   0.376803 \\
Madisson        &   1.0 &   1.000000 \\
Madisyn         &   1.0 &   0.384615 \\
Madolyn         &   1.0 &   1.000000 \\
Madonna         &   1.0 &   0.238095 \\
Madylin         &   1.0 &   1.000000 \\
Madysen         &   1.0 &   0.352941 \\
Madysin         &   1.0 &   1.000000 \\
Madyson         &   1.0 &   0.130435 \\
Mae             &   1.0 &   0.775510 \\
Maegan          &   1.0 &   0.131579 \\
Maegen          &   1.0 &   0.875000 \\
Maeghan         &   1.0 &   1.000000 \\
Maela           &   1.0 &   1.000000 \\
Maelani         &   1.0 &   0.800000 \\
Maelee          &   1.0 &   0.833333 \\
Maelie          &   1.0 &   0.714286 \\
Maelle          &   1.0 &   1.000000 \\
Maelyn          &   1.0 &   1.000000 \\
Maelynn         &   1.0 &   0.777778 \\
Maeva           &   1.0 &   0.800000 \\
Maeve           &   1.0 &   1.000000 \\
Maeven          &   1.0 &   1.000000 \\
Magaby          &   1.0 &   1.000000 \\
Magali          &   1.0 &   0.134328 \\
Magally         &   1.0 &   0.875000 \\
Magaly          &   1.0 &   0.128571 \\
Magan           &   1.0 &   0.545455 \\
Magda           &   1.0 &   0.428571 \\
Magdalen        &   1.0 &   1.000000 \\
Magdalena       &   1.0 &   0.358491 \\
Magdalene       &   1.0 &   0.416667 \\
Magdelena       &   1.0 &   1.000000 \\
Magen           &   1.0 &   0.285714 \\
Maggi           &   1.0 &   1.000000 \\
Maggie          &   1.0 &   0.496000 \\
Maggy           &   1.0 &   0.714286 \\
Maghan          &   1.0 &   1.000000 \\
Magnolia        &   1.0 &   0.859504 \\
Maguadalupe     &   1.0 &   0.833333 \\
Maha            &   1.0 &   0.538462 \\
Mahala          &   1.0 &   1.000000 \\
Mahalia         &   1.0 &   0.727273 \\
Mahathi         &   1.0 &   0.833333 \\
Mahati          &   1.0 &   1.000000 \\
Mahayla         &   1.0 &   1.000000 \\
Mahealani       &   1.0 &   0.833333 \\
Maheen          &   1.0 &   1.000000 \\
Mahek           &   1.0 &   0.750000 \\
Mahelet         &   1.0 &   1.000000 \\
Mahi            &   1.0 &   0.583333 \\
Mahika          &   1.0 &   0.636364 \\
Mahima          &   1.0 &   0.600000 \\
Mahina          &   1.0 &   1.000000 \\
Mahira          &   1.0 &   0.454545 \\
Mahlia          &   1.0 &   0.714286 \\
Mahnoor         &   1.0 &   0.625000 \\
Mahogany        &   1.0 &   0.461538 \\
Mahsa           &   1.0 &   0.666667 \\
Mahum           &   1.0 &   1.000000 \\
Mai             &   1.0 &   0.086957 \\
Maia            &   1.0 &   0.862903 \\
Maiah           &   1.0 &   0.500000 \\
Maiana          &   1.0 &   1.000000 \\
Maida           &   1.0 &   0.500000 \\
Maija           &   1.0 &   1.000000 \\
Maika           &   1.0 &   0.714286 \\
Maila           &   1.0 &   1.000000 \\
Mailani         &   1.0 &   0.800000 \\
Maile           &   1.0 &   0.148148 \\
Mailee          &   1.0 &   0.555556 \\
Mailen          &   1.0 &   0.785714 \\
Mailey          &   1.0 &   0.500000 \\
Maili           &   1.0 &   1.000000 \\
Mailin          &   1.0 &   0.833333 \\
Maille          &   1.0 &   1.000000 \\
Maily           &   1.0 &   0.727273 \\
Mailyn          &   1.0 &   0.461538 \\
Maiquel         &   1.0 &   1.000000 \\
Maira           &   1.0 &   0.059259 \\
Mairani         &   1.0 &   1.000000 \\
Mairany         &   1.0 &   0.833333 \\
Mairead         &   1.0 &   0.833333 \\
Mairin          &   1.0 &   0.714286 \\
Mairyn          &   1.0 &   1.000000 \\
Maisee          &   1.0 &   1.000000 \\
Maisey          &   1.0 &   0.833333 \\
Maisha          &   1.0 &   0.142857 \\
Maisie          &   1.0 &   0.959459 \\
Maison          &   1.0 &   1.000000 \\
Maisy           &   1.0 &   1.000000 \\
Maisyn          &   1.0 &   0.909091 \\
Maite           &   1.0 &   0.455882 \\
Maiya           &   1.0 &   0.482759 \\
Maiyah          &   1.0 &   0.833333 \\
Maiyer          &   1.0 &   1.000000 \\
Maizie          &   1.0 &   0.916667 \\
Maizy           &   1.0 &   1.000000 \\
Maja            &   1.0 &   0.454545 \\
Majestic        &   1.0 &   1.000000 \\
Majesty         &   1.0 &   0.761905 \\
Makaela         &   1.0 &   0.230769 \\
Makaila         &   1.0 &   0.128205 \\
Makailah        &   1.0 &   1.000000 \\
Makaiya         &   1.0 &   1.000000 \\
Makala          &   1.0 &   0.294118 \\
Makalah         &   1.0 &   0.714286 \\
Makana          &   1.0 &   0.833333 \\
Makaya          &   1.0 &   1.000000 \\
Makayla         &   1.0 &   0.151618 \\
Makaylah        &   1.0 &   0.275862 \\
Makaylee        &   1.0 &   0.500000 \\
Makaylin        &   1.0 &   1.000000 \\
Makeba          &   1.0 &   1.000000 \\
Makeda          &   1.0 &   0.833333 \\
Makena          &   1.0 &   0.280899 \\
Makenah         &   1.0 &   1.000000 \\
Makenna         &   1.0 &   0.433121 \\
Makennah        &   1.0 &   1.000000 \\
Makenzee        &   1.0 &   1.000000 \\
Makenzi         &   1.0 &   0.416667 \\
Makenzie        &   1.0 &   0.278571 \\
Makenzy         &   1.0 &   1.000000 \\
Makeyla         &   1.0 &   0.600000 \\
Maki            &   1.0 &   1.000000 \\
Makiah          &   1.0 &   0.600000 \\
Makinley        &   1.0 &   0.750000 \\
Makinzie        &   1.0 &   0.714286 \\
Makisha         &   1.0 &   0.833333 \\
Makiya          &   1.0 &   0.833333 \\
Makiyah         &   1.0 &   0.714286 \\
Makya           &   1.0 &   1.000000 \\
Makyla          &   1.0 &   0.454545 \\
Makynzie        &   1.0 &   1.000000 \\
Mala            &   1.0 &   1.000000 \\
Malaak          &   1.0 &   1.000000 \\
Malai           &   1.0 &   1.000000 \\
Malaia          &   1.0 &   1.000000 \\
Malaika         &   1.0 &   0.294118 \\
Malaina         &   1.0 &   1.000000 \\
Malaiyah        &   1.0 &   1.000000 \\
Malak           &   1.0 &   0.520000 \\
Malana          &   1.0 &   0.555556 \\
Malani          &   1.0 &   1.000000 \\
Malanie         &   1.0 &   1.000000 \\
Malarie         &   1.0 &   1.000000 \\
Malaya          &   1.0 &   0.918367 \\
Malayah         &   1.0 &   0.696970 \\
Malayna         &   1.0 &   0.500000 \\
Malaynah        &   1.0 &   1.000000 \\
Malaysia        &   1.0 &   0.326087 \\
Malea           &   1.0 &   0.428571 \\
Maleah          &   1.0 &   0.403846 \\
Maleeha         &   1.0 &   1.000000 \\
Maleena         &   1.0 &   0.416667 \\
Maleia          &   1.0 &   1.000000 \\
Maleiah         &   1.0 &   1.000000 \\
Maleigha        &   1.0 &   1.000000 \\
Malena          &   1.0 &   0.361111 \\
Maleni          &   1.0 &   0.566667 \\
Maleny          &   1.0 &   0.250000 \\
Malerie         &   1.0 &   0.600000 \\
Maleya          &   1.0 &   1.000000 \\
Maleyah         &   1.0 &   0.833333 \\
Mali            &   1.0 &   0.714286 \\
Malia           &   1.0 &   0.579592 \\
Maliah          &   1.0 &   0.339623 \\
Malibu          &   1.0 &   1.000000 \\
Maliha          &   1.0 &   0.461538 \\
Malika          &   1.0 &   0.500000 \\
Malikah         &   1.0 &   1.000000 \\
Malillany       &   1.0 &   0.666667 \\
Malin           &   1.0 &   0.714286 \\
Malina          &   1.0 &   0.483871 \\
Malinda         &   1.0 &   0.239130 \\
Malini          &   1.0 &   1.000000 \\
Malisa          &   1.0 &   0.333333 \\
Malisha         &   1.0 &   1.000000 \\
Malissa         &   1.0 &   0.170732 \\
Maliya          &   1.0 &   0.631579 \\
Maliyah         &   1.0 &   0.441667 \\
Malka           &   1.0 &   1.000000 \\
Mallary         &   1.0 &   0.833333 \\
Mallika         &   1.0 &   1.000000 \\
Mallori         &   1.0 &   0.454545 \\
Mallorie        &   1.0 &   0.333333 \\
Mallory         &   1.0 &   0.148026 \\
Maloni          &   1.0 &   1.000000 \\
Malori          &   1.0 &   1.000000 \\
Malorie         &   1.0 &   0.240000 \\
Malory          &   1.0 &   0.384615 \\
Maly            &   1.0 &   0.625000 \\
Malynda         &   1.0 &   0.833333 \\
Malyssa         &   1.0 &   0.600000 \\
Mamie           &   1.0 &   0.208333 \\
Mana            &   1.0 &   0.500000 \\
Manaal          &   1.0 &   1.000000 \\
Manaia          &   1.0 &   0.875000 \\
Manal           &   1.0 &   0.750000 \\
Manar           &   1.0 &   0.833333 \\
Manasa          &   1.0 &   0.714286 \\
Manasi          &   1.0 &   0.857143 \\
Manasvi         &   1.0 &   0.312500 \\
Manda           &   1.0 &   0.555556 \\
Mandalyn        &   1.0 &   1.000000 \\
Mandee          &   1.0 &   0.666667 \\
Mandeep         &   1.0 &   0.714286 \\
Mandi           &   1.0 &   0.109091 \\
Mandie          &   1.0 &   0.238095 \\
Mandisa         &   1.0 &   0.714286 \\
Mandy           &   1.0 &   0.037736 \\
Mane            &   1.0 &   0.857143 \\
Maneh           &   1.0 &   1.000000 \\
Manette         &   1.0 &   1.000000 \\
Manha           &   1.0 &   0.875000 \\
Maniah          &   1.0 &   1.000000 \\
Manisha         &   1.0 &   0.625000 \\
Maniya          &   1.0 &   1.000000 \\
Maniyah         &   1.0 &   1.000000 \\
Manjot          &   1.0 &   1.000000 \\
Manmeet         &   1.0 &   1.000000 \\
Mannat          &   1.0 &   0.714286 \\
Manon           &   1.0 &   0.625000 \\
Manpreet        &   1.0 &   0.500000 \\
Manreet         &   1.0 &   0.583333 \\
Mansi           &   1.0 &   0.416667 \\
Mansirat        &   1.0 &   1.000000 \\
Manuel          &   1.0 &   0.312500 \\
Manuela         &   1.0 &   0.225806 \\
Manvi           &   1.0 &   1.000000 \\
Manvir          &   1.0 &   1.000000 \\
Manya           &   1.0 &   0.555556 \\
Mao             &   1.0 &   0.545455 \\
Maple           &   1.0 &   0.950000 \\
Mar             &   1.0 &   1.000000 \\
Mara            &   1.0 &   0.854545 \\
Marah           &   1.0 &   0.750000 \\
Maral           &   1.0 &   0.500000 \\
Maralee         &   1.0 &   0.833333 \\
Maralyn         &   1.0 &   1.000000 \\
Maram           &   1.0 &   0.625000 \\
Maranda         &   1.0 &   0.135135 \\
Maraya          &   1.0 &   0.454545 \\
Marayah         &   1.0 &   0.625000 \\
Marbella        &   1.0 &   0.666667 \\
Marcee          &   1.0 &   1.000000 \\
Marcel          &   1.0 &   1.000000 \\
Marcela         &   1.0 &   0.300000 \\
Marcelina       &   1.0 &   0.277778 \\
Marceline       &   1.0 &   0.981818 \\
Marcella        &   1.0 &   0.222222 \\
Marcelle        &   1.0 &   0.400000 \\
Marcellina      &   1.0 &   1.000000 \\
Marcene         &   1.0 &   1.000000 \\
Marcey          &   1.0 &   1.000000 \\
Marche          &   1.0 &   1.000000 \\
Marchelle       &   1.0 &   0.555556 \\
Marcheta        &   1.0 &   1.000000 \\
Marci           &   1.0 &   0.055556 \\
Marcia          &   1.0 &   0.022654 \\
Marcie          &   1.0 &   0.095238 \\
Marcine         &   1.0 &   1.000000 \\
Marco           &   1.0 &   0.857143 \\
Marcos          &   1.0 &   1.000000 \\
Marcus          &   1.0 &   1.000000 \\
Marcy           &   1.0 &   0.109375 \\
Mardel          &   1.0 &   1.000000 \\
Mardell         &   1.0 &   0.555556 \\
Mardi           &   1.0 &   0.714286 \\
Mareena         &   1.0 &   1.000000 \\
Mareli          &   1.0 &   0.043103 \\
Marelie         &   1.0 &   1.000000 \\
Marelin         &   1.0 &   0.625000 \\
Marelly         &   1.0 &   1.000000 \\
Marely          &   1.0 &   0.057522 \\
Marelyn         &   1.0 &   0.454545 \\
Maren           &   1.0 &   0.640000 \\
Marena          &   1.0 &   0.538462 \\
Maresa          &   1.0 &   0.714286 \\
Maressa         &   1.0 &   1.000000 \\
Margaret        &   1.0 &   0.083612 \\
Margarete       &   1.0 &   1.000000 \\
Margarett       &   1.0 &   1.000000 \\
Margarette      &   1.0 &   0.625000 \\
Margarita       &   1.0 &   0.077821 \\
Margarite       &   1.0 &   0.714286 \\
Margaux         &   1.0 &   0.833333 \\
Marge           &   1.0 &   0.555556 \\
Margeaux        &   1.0 &   0.833333 \\
Margene         &   1.0 &   0.833333 \\
Margery         &   1.0 &   0.142857 \\
Margie          &   1.0 &   0.044118 \\
Margit          &   1.0 &   0.833333 \\
Margo           &   1.0 &   0.343434 \\
Margot          &   1.0 &   0.974522 \\
Margret         &   1.0 &   0.263158 \\
Marguerita      &   1.0 &   0.625000 \\
Marguerite      &   1.0 &   0.056818 \\
Marguita        &   1.0 &   1.000000 \\
Margy           &   1.0 &   0.714286 \\
Mari            &   1.0 &   0.368421 \\
Maria           &   1.0 &   0.165338 \\
Mariaangelica   &   1.0 &   1.000000 \\
Mariabelen      &   1.0 &   1.000000 \\
Mariaceleste    &   1.0 &   1.000000 \\
Mariacristina   &   1.0 &   1.000000 \\
Mariade         &   1.0 &   0.857143 \\
Mariadejesus    &   1.0 &   0.238095 \\
Mariadel        &   1.0 &   1.000000 \\
Mariadelaluz    &   1.0 &   0.714286 \\
Mariadelcarmen  &   1.0 &   0.318182 \\
Mariadelosang   &   1.0 &   0.888889 \\
Mariadelosangel &   1.0 &   1.000000 \\
Mariadelourdes  &   1.0 &   1.000000 \\
Mariaelena      &   1.0 &   0.185185 \\
Mariaesther     &   1.0 &   1.000000 \\
Mariafernanda   &   1.0 &   0.380952 \\
Mariaguadalupe  &   1.0 &   0.147059 \\
Mariah          &   1.0 &   0.121777 \\
Mariaisabel     &   1.0 &   0.375000 \\
Mariajose       &   1.0 &   0.387755 \\
Mariali         &   1.0 &   1.000000 \\
Marializ        &   1.0 &   0.750000 \\
Marialuisa      &   1.0 &   0.428571 \\
Mariam          &   1.0 &   0.662162 \\
Mariama         &   1.0 &   0.833333 \\
Marian          &   1.0 &   0.097458 \\
Mariana         &   1.0 &   0.343808 \\
Mariangel       &   1.0 &   1.000000 \\
Mariangela      &   1.0 &   1.000000 \\
Mariann         &   1.0 &   0.263158 \\
Marianna        &   1.0 &   0.714286 \\
Marianne        &   1.0 &   0.043137 \\
Mariapaula      &   1.0 &   1.000000 \\
Mariateresa     &   1.0 &   0.714286 \\
Mariatheresa    &   1.0 &   1.000000 \\
Maribel         &   1.0 &   0.137466 \\
Maribell        &   1.0 &   0.625000 \\
Maribella       &   1.0 &   0.416667 \\
Maribelle       &   1.0 &   0.333333 \\
Maribeth        &   1.0 &   0.454545 \\
Marica          &   1.0 &   1.000000 \\
Maricar         &   1.0 &   1.000000 \\
Maricarmen      &   1.0 &   0.192308 \\
Maricel         &   1.0 &   0.416667 \\
Maricela        &   1.0 &   0.041494 \\
Maricella       &   1.0 &   0.263158 \\
Maricris        &   1.0 &   1.000000 \\
Maricruz        &   1.0 &   0.055556 \\
Maricsa         &   1.0 &   1.000000 \\
Marie           &   1.0 &   0.163551 \\
Mariel          &   1.0 &   0.372881 \\
Mariela         &   1.0 &   0.097959 \\
Mariele         &   1.0 &   1.000000 \\
Marielena       &   1.0 &   0.127660 \\
Marieli         &   1.0 &   0.833333 \\
Mariella        &   1.0 &   0.480000 \\
Marielle        &   1.0 &   0.360000 \\
Mariellen       &   1.0 &   1.000000 \\
Mariely         &   1.0 &   0.666667 \\
Marietta        &   1.0 &   0.263158 \\
Marifer         &   1.0 &   0.727273 \\
Marigold        &   1.0 &   0.966667 \\
Mariha          &   1.0 &   1.000000 \\
Marijane        &   1.0 &   1.000000 \\
Marijo          &   1.0 &   1.000000 \\
Marijose        &   1.0 &   1.000000 \\
Marika          &   1.0 &   0.368421 \\
Mariko          &   1.0 &   0.250000 \\
Marilee         &   1.0 &   0.185185 \\
Marilena        &   1.0 &   1.000000 \\
Marili          &   1.0 &   1.000000 \\
Marilin         &   1.0 &   0.312500 \\
Marillyn        &   1.0 &   0.857143 \\
Marilou         &   1.0 &   0.600000 \\
Marilouise      &   1.0 &   1.000000 \\
Marilu          &   1.0 &   0.225806 \\
Mariluz         &   1.0 &   1.000000 \\
Marily          &   1.0 &   0.428571 \\
Marilyn         &   1.0 &   0.089307 \\
Marilyne        &   1.0 &   1.000000 \\
Marilynn        &   1.0 &   0.155556 \\
Marilynne       &   1.0 &   0.625000 \\
Marimar         &   1.0 &   0.151515 \\
Marin           &   1.0 &   0.375000 \\
Marina          &   1.0 &   0.318182 \\
Marinda         &   1.0 &   1.000000 \\
Marine          &   1.0 &   0.615385 \\
Marinna         &   1.0 &   0.500000 \\
Mario           &   1.0 &   0.333333 \\
Marion          &   1.0 &   0.092784 \\
Maripaz         &   1.0 &   1.000000 \\
Mariposa        &   1.0 &   0.833333 \\
Maris           &   1.0 &   1.000000 \\
Marisa          &   1.0 &   0.033520 \\
Marisabel       &   1.0 &   0.800000 \\
Marisela        &   1.0 &   0.032258 \\
Marisella       &   1.0 &   1.000000 \\
Marisha         &   1.0 &   0.454545 \\
Mariska         &   1.0 &   1.000000 \\
Marisleysis     &   1.0 &   1.000000 \\
Marisol         &   1.0 &   0.183673 \\
Marison         &   1.0 &   1.000000 \\
Marissa         &   1.0 &   0.028628 \\
Marita          &   1.0 &   0.625000 \\
Maritere        &   1.0 &   1.000000 \\
Marites         &   1.0 &   0.833333 \\
Maritsa         &   1.0 &   0.333333 \\
Maritssa        &   1.0 &   0.833333 \\
Maritza         &   1.0 &   0.079872 \\
Marivel         &   1.0 &   0.142857 \\
Marivi          &   1.0 &   1.000000 \\
Marivic         &   1.0 &   1.000000 \\
Marixa          &   1.0 &   0.714286 \\
Mariya          &   1.0 &   0.416667 \\
Mariyah         &   1.0 &   0.239130 \\
Mariza          &   1.0 &   0.352941 \\
Marizela        &   1.0 &   1.000000 \\
Marizol         &   1.0 &   0.600000 \\
Marja           &   1.0 &   0.833333 \\
Marjan          &   1.0 &   1.000000 \\
Marjani         &   1.0 &   1.000000 \\
Marji           &   1.0 &   1.000000 \\
Marjie          &   1.0 &   1.000000 \\
Marjorie        &   1.0 &   0.056156 \\
Marjory         &   1.0 &   0.285714 \\
Mark            &   1.0 &   0.352941 \\
Markayla        &   1.0 &   0.833333 \\
Markeisha       &   1.0 &   0.454545 \\
Markesha        &   1.0 &   0.625000 \\
Marki           &   1.0 &   1.000000 \\
Markia          &   1.0 &   1.000000 \\
Markie          &   1.0 &   0.238095 \\
Markiesha       &   1.0 &   1.000000 \\
Markisha        &   1.0 &   0.500000 \\
Markita         &   1.0 &   0.900000 \\
Marla           &   1.0 &   0.035714 \\
Marlaina        &   1.0 &   0.555556 \\
Marlaine        &   1.0 &   1.000000 \\
Marlana         &   1.0 &   0.750000 \\
Marlane         &   1.0 &   1.000000 \\
Marlayna        &   1.0 &   1.000000 \\
Marlee          &   1.0 &   0.476923 \\
Marleen         &   1.0 &   0.137255 \\
Marleena        &   1.0 &   1.000000 \\
Marlei          &   1.0 &   1.000000 \\
Marleigh        &   1.0 &   0.769231 \\
Marlen          &   1.0 &   0.126582 \\
Marlena         &   1.0 &   0.090909 \\
Marlene         &   1.0 &   0.093333 \\
Marleni         &   1.0 &   0.500000 \\
Marlenne        &   1.0 &   0.384615 \\
Marleny         &   1.0 &   0.705882 \\
Marlet          &   1.0 &   1.000000 \\
Marley          &   1.0 &   0.326347 \\
Marli           &   1.0 &   0.444444 \\
Marlie          &   1.0 &   0.242424 \\
Marlies         &   1.0 &   1.000000 \\
Marlin          &   1.0 &   0.368421 \\
Marlina         &   1.0 &   0.454545 \\
Marlinda        &   1.0 &   1.000000 \\
Marline         &   1.0 &   1.000000 \\
Marlisa         &   1.0 &   1.000000 \\
Marlise         &   1.0 &   1.000000 \\
Marlisha        &   1.0 &   1.000000 \\
Marlissa        &   1.0 &   1.000000 \\
Marlo           &   1.0 &   0.172414 \\
Marlow          &   1.0 &   0.555556 \\
Marlowe         &   1.0 &   1.000000 \\
Marly           &   1.0 &   0.500000 \\
Marlyn          &   1.0 &   0.094340 \\
Marlyne         &   1.0 &   1.000000 \\
Marlynn         &   1.0 &   0.833333 \\
Marlys          &   1.0 &   0.384615 \\
Marna           &   1.0 &   0.500000 \\
Marne           &   1.0 &   0.555556 \\
Marnee          &   1.0 &   0.833333 \\
Marney          &   1.0 &   1.000000 \\
Marni           &   1.0 &   0.170732 \\
Marnie          &   1.0 &   0.086957 \\
Marnisha        &   1.0 &   1.000000 \\
Marnita         &   1.0 &   1.000000 \\
Marolyn         &   1.0 &   0.833333 \\
Marquelle       &   1.0 &   1.000000 \\
Marquesha       &   1.0 &   1.000000 \\
Marquetta       &   1.0 &   1.000000 \\
Marqui          &   1.0 &   1.000000 \\
Marquia         &   1.0 &   1.000000 \\
Marquis         &   1.0 &   1.000000 \\
Marquisha       &   1.0 &   0.294118 \\
Marquita        &   1.0 &   0.040650 \\
Marquitta       &   1.0 &   0.750000 \\
Marriah         &   1.0 &   0.714286 \\
Marrisa         &   1.0 &   0.357143 \\
Marrissa        &   1.0 &   0.294118 \\
Marry           &   1.0 &   1.000000 \\
Mars            &   1.0 &   1.000000 \\
Marsela         &   1.0 &   0.714286 \\
Marsha          &   1.0 &   0.011416 \\
Marshae         &   1.0 &   0.833333 \\
Marshay         &   1.0 &   0.555556 \\
Marsi           &   1.0 &   0.625000 \\
Marta           &   1.0 &   0.090909 \\
Martha          &   1.0 &   0.060790 \\
Marti           &   1.0 &   0.208333 \\
Martie          &   1.0 &   1.000000 \\
Martika         &   1.0 &   0.333333 \\
Martin          &   1.0 &   0.454545 \\
Martina         &   1.0 &   0.288462 \\
Martine         &   1.0 &   0.352941 \\
Martinique      &   1.0 &   0.833333 \\
Martisha        &   1.0 &   1.000000 \\
Martiza         &   1.0 &   0.461538 \\
Marty           &   1.0 &   0.227273 \\
Marva           &   1.0 &   0.250000 \\
Marvel          &   1.0 &   0.555556 \\
Marvella        &   1.0 &   1.000000 \\
Marvin          &   1.0 &   1.000000 \\
Marvis          &   1.0 &   1.000000 \\
Marwa           &   1.0 &   0.500000 \\
Marwah          &   1.0 &   1.000000 \\
Mary            &   1.0 &   0.026903 \\
Marya           &   1.0 &   0.500000 \\
Maryah          &   1.0 &   0.428571 \\
Maryalice       &   1.0 &   0.833333 \\
Maryam          &   1.0 &   0.900000 \\
Maryan          &   1.0 &   0.714286 \\
Maryana         &   1.0 &   0.714286 \\
Maryann         &   1.0 &   0.081395 \\
Maryanna        &   1.0 &   0.500000 \\
Maryanne        &   1.0 &   0.156250 \\
Marybel         &   1.0 &   0.545455 \\
Marybell        &   1.0 &   0.625000 \\
Marybella       &   1.0 &   1.000000 \\
Marybelle       &   1.0 &   0.750000 \\
Marybeth        &   1.0 &   0.500000 \\
Marycarmen      &   1.0 &   0.888889 \\
Marycruz        &   1.0 &   0.263158 \\
Marye           &   1.0 &   1.000000 \\
Maryelizabeth   &   1.0 &   0.666667 \\
Maryellen       &   1.0 &   0.263158 \\
Maryfer         &   1.0 &   1.000000 \\
Marygrace       &   1.0 &   0.555556 \\
Maryhelen       &   1.0 &   0.625000 \\
Maryjane        &   1.0 &   0.383178 \\
Maryjo          &   1.0 &   0.384615 \\
Marykate        &   1.0 &   0.727273 \\
Marykatherine   &   1.0 &   1.000000 \\
Maryl           &   1.0 &   1.000000 \\
Marylee         &   1.0 &   0.384615 \\
Marylin         &   1.0 &   0.333333 \\
Marylou         &   1.0 &   0.138889 \\
Marylouise      &   1.0 &   0.555556 \\
Marylu          &   1.0 &   0.500000 \\
Marylyn         &   1.0 &   0.454545 \\
Marylynn        &   1.0 &   0.750000 \\
Maryn           &   1.0 &   0.636364 \\
Maryrose        &   1.0 &   1.000000 \\
Marysa          &   1.0 &   0.625000 \\
Marysol         &   1.0 &   0.214286 \\
Maryssa         &   1.0 &   0.277778 \\
Marytza         &   1.0 &   1.000000 \\
Masa            &   1.0 &   1.000000 \\
Masae           &   1.0 &   1.000000 \\
Masako          &   1.0 &   0.192308 \\
Masaye          &   1.0 &   0.857143 \\
Masha           &   1.0 &   1.000000 \\
Mason           &   1.0 &   0.277778 \\
Massiel         &   1.0 &   0.333333 \\
Masyn           &   1.0 &   1.000000 \\
Matalie         &   1.0 &   1.000000 \\
Mataya          &   1.0 &   1.000000 \\
Matea           &   1.0 &   0.600000 \\
Mathea          &   1.0 &   1.000000 \\
Mathilda        &   1.0 &   0.615385 \\
Mathilde        &   1.0 &   0.714286 \\
Matilda         &   1.0 &   0.846154 \\
Matilde         &   1.0 &   0.600000 \\
Matisse         &   1.0 &   0.833333 \\
Matsuko         &   1.0 &   1.000000 \\
Matsuye         &   1.0 &   1.000000 \\
Mattea          &   1.0 &   0.312500 \\
Matthew         &   1.0 &   0.218750 \\
Mattie          &   1.0 &   0.409091 \\
Mattison        &   1.0 &   0.555556 \\
Maude           &   1.0 &   0.416667 \\
Maudie          &   1.0 &   0.833333 \\
Maura           &   1.0 &   0.297297 \\
Maureen         &   1.0 &   0.016575 \\
Maurene         &   1.0 &   1.000000 \\
Mauricia        &   1.0 &   1.000000 \\
Maurine         &   1.0 &   0.312500 \\
Maurissa        &   1.0 &   0.714286 \\
Maven           &   1.0 &   0.750000 \\
Maverick        &   1.0 &   1.000000 \\
Mavi            &   1.0 &   1.000000 \\
Mavis           &   1.0 &   1.000000 \\
Max             &   1.0 &   0.642857 \\
Maxene          &   1.0 &   1.000000 \\
Maxima          &   1.0 &   1.000000 \\
Maxine          &   1.0 &   0.656250 \\
Maxwell         &   1.0 &   0.700000 \\
May             &   1.0 &   0.344262 \\
Maya            &   1.0 &   0.830664 \\
Mayah           &   1.0 &   0.206897 \\
Mayan           &   1.0 &   0.555556 \\
Mayar           &   1.0 &   0.857143 \\
Mayari          &   1.0 &   1.000000 \\
Maybelle        &   1.0 &   0.615385 \\
Maybelline      &   1.0 &   0.500000 \\
Maycee          &   1.0 &   0.375000 \\
Maycie          &   1.0 &   0.750000 \\
Mayda           &   1.0 &   0.714286 \\
Mayela          &   1.0 &   0.384615 \\
Mayeli          &   1.0 &   0.900000 \\
Mayerli         &   1.0 &   0.615385 \\
Mayerly         &   1.0 &   0.625000 \\
Maygan          &   1.0 &   1.000000 \\
Mayla           &   1.0 &   0.529412 \\
Maylani         &   1.0 &   0.636364 \\
Maylea          &   1.0 &   1.000000 \\
Maylee          &   1.0 &   0.529412 \\
Mayleen         &   1.0 &   0.304348 \\
Maylen          &   1.0 &   0.388889 \\
Maylene         &   1.0 &   0.375000 \\
Mayli           &   1.0 &   1.000000 \\
Maylin          &   1.0 &   0.459459 \\
Mayline         &   1.0 &   1.000000 \\
Maylyn          &   1.0 &   1.000000 \\
Maylynn         &   1.0 &   1.000000 \\
Mayme           &   1.0 &   1.000000 \\
Mayra           &   1.0 &   0.035348 \\
Mayraalejandra  &   1.0 &   0.666667 \\
Mayrani         &   1.0 &   0.714286 \\
Mayrin          &   1.0 &   0.250000 \\
Maysa           &   1.0 &   0.833333 \\
Mayson          &   1.0 &   0.857143 \\
Mayte           &   1.0 &   0.142857 \\
Maytte          &   1.0 &   1.000000 \\
Mayumi          &   1.0 &   0.777778 \\
Mayuri          &   1.0 &   1.000000 \\
Mayvis          &   1.0 &   1.000000 \\
Mayzie          &   1.0 &   1.000000 \\
Mazie           &   1.0 &   1.000000 \\
Mazikeen        &   1.0 &   0.560000 \\
Mazzy           &   1.0 &   0.545455 \\
Mccall          &   1.0 &   0.777778 \\
Mckayla         &   1.0 &   0.283019 \\
Mckenna         &   1.0 &   0.290000 \\
Mckenzee        &   1.0 &   0.857143 \\
Mckenzi         &   1.0 &   0.857143 \\
Mckenzie        &   1.0 &   0.335484 \\
Mckinlee        &   1.0 &   1.000000 \\
Mckinley        &   1.0 &   0.534884 \\
Mckinzie        &   1.0 &   0.714286 \\
Mckynlee        &   1.0 &   1.000000 \\
Mckynzie        &   1.0 &   1.000000 \\
Mea             &   1.0 &   0.428571 \\
Meadow          &   1.0 &   1.000000 \\
Meagan          &   1.0 &   0.025000 \\
Meagen          &   1.0 &   0.454545 \\
Meaghan         &   1.0 &   0.140000 \\
Meah            &   1.0 &   0.400000 \\
Meara           &   1.0 &   0.555556 \\
Mecca           &   1.0 &   0.600000 \\
Mechele         &   1.0 &   0.700000 \\
Mechelle        &   1.0 &   0.161290 \\
Medha           &   1.0 &   0.538462 \\
Medina          &   1.0 &   0.555556 \\
Mee             &   1.0 &   0.312500 \\
Meegan          &   1.0 &   0.833333 \\
Meeka           &   1.0 &   1.000000 \\
Meela           &   1.0 &   1.000000 \\
Meelah          &   1.0 &   1.000000 \\
Meena           &   1.0 &   0.705882 \\
Meenakshi       &   1.0 &   1.000000 \\
Meera           &   1.0 &   0.822222 \\
Meesha          &   1.0 &   1.000000 \\
Meeya           &   1.0 &   1.000000 \\
Meg             &   1.0 &   0.294118 \\
Megan           &   1.0 &   0.020916 \\
Meganelizabeth  &   1.0 &   1.000000 \\
Megann          &   1.0 &   0.555556 \\
Megen           &   1.0 &   0.714286 \\
Meggan          &   1.0 &   0.277778 \\
Megha           &   1.0 &   0.357143 \\
Meghan          &   1.0 &   0.071839 \\
Meghana         &   1.0 &   0.500000 \\
Meghann         &   1.0 &   0.372093 \\
Meghna          &   1.0 &   0.615385 \\
Megin           &   1.0 &   1.000000 \\
Megumi          &   1.0 &   0.700000 \\
Megyn           &   1.0 &   1.000000 \\
Mehak           &   1.0 &   0.666667 \\
Mehar           &   1.0 &   1.000000 \\
Mehek           &   1.0 &   1.000000 \\
Meher           &   1.0 &   0.550000 \\
Mehgan          &   1.0 &   0.714286 \\
Mehr            &   1.0 &   0.857143 \\
Mehreen         &   1.0 &   0.625000 \\
Mei             &   1.0 &   1.000000 \\
Meichele        &   1.0 &   1.000000 \\
Meika           &   1.0 &   1.000000 \\
Meila           &   1.0 &   1.000000 \\
Meilan          &   1.0 &   1.000000 \\
Meilani         &   1.0 &   0.605634 \\
Meilany         &   1.0 &   0.625000 \\
Meili           &   1.0 &   0.833333 \\
Meiling         &   1.0 &   1.000000 \\
Meily           &   1.0 &   1.000000 \\
Meilyn          &   1.0 &   1.000000 \\
Meira           &   1.0 &   0.625000 \\
Meisha          &   1.0 &   0.625000 \\
Meka            &   1.0 &   0.833333 \\
Mekayla         &   1.0 &   0.700000 \\
Mekenna         &   1.0 &   1.000000 \\
Mekenzie        &   1.0 &   0.833333 \\
Meklit          &   1.0 &   1.000000 \\
Mel             &   1.0 &   1.000000 \\
Mela            &   1.0 &   0.833333 \\
Melaina         &   1.0 &   0.555556 \\
Melaine         &   1.0 &   0.545455 \\
Melana          &   1.0 &   1.000000 \\
Melane          &   1.0 &   1.000000 \\
Melanee         &   1.0 &   0.454545 \\
Melaney         &   1.0 &   0.625000 \\
Melani          &   1.0 &   0.560976 \\
Melania         &   1.0 &   0.621622 \\
Melanie         &   1.0 &   0.387008 \\
Melanieann      &   1.0 &   1.000000 \\
Melannie        &   1.0 &   0.285714 \\
Melanny         &   1.0 &   0.263158 \\
Melany          &   1.0 &   0.372449 \\
Melanye         &   1.0 &   0.714286 \\
Melayah         &   1.0 &   1.000000 \\
Melayna         &   1.0 &   1.000000 \\
Melba           &   1.0 &   0.153846 \\
Mele            &   1.0 &   0.333333 \\
Melea           &   1.0 &   1.000000 \\
Meleah          &   1.0 &   0.700000 \\
Meleane         &   1.0 &   1.000000 \\
Meleena         &   1.0 &   1.000000 \\
Melek           &   1.0 &   0.909091 \\
Melena          &   1.0 &   0.500000 \\
Meleni          &   1.0 &   1.000000 \\
Melenie         &   1.0 &   0.416667 \\
Meleny          &   1.0 &   0.277778 \\
Melessa         &   1.0 &   1.000000 \\
Melia           &   1.0 &   0.212766 \\
Meliah          &   1.0 &   0.714286 \\
Melida          &   1.0 &   0.714286 \\
Melika          &   1.0 &   1.000000 \\
Melina          &   1.0 &   0.500000 \\
Melinda         &   1.0 &   0.056300 \\
Meline          &   1.0 &   0.625000 \\
Melinna         &   1.0 &   0.666667 \\
Melisa          &   1.0 &   0.060241 \\
Melisha         &   1.0 &   1.000000 \\
Meliss          &   1.0 &   1.000000 \\
Melissa         &   1.0 &   0.055888 \\
Melissaann      &   1.0 &   0.875000 \\
Melissia        &   1.0 &   0.833333 \\
Melita          &   1.0 &   0.833333 \\
Meliyah         &   1.0 &   1.000000 \\
Meliza          &   1.0 &   0.250000 \\
Melizza         &   1.0 &   1.000000 \\
Mellanie        &   1.0 &   0.461538 \\
Mellany         &   1.0 &   1.000000 \\
Mellisa         &   1.0 &   0.178571 \\
Mellissa        &   1.0 &   0.138889 \\
Melodee         &   1.0 &   0.285714 \\
Melodi          &   1.0 &   0.700000 \\
Melodie         &   1.0 &   0.163265 \\
Melody          &   1.0 &   0.938650 \\
Melodye         &   1.0 &   1.000000 \\
Melonee         &   1.0 &   1.000000 \\
Meloney         &   1.0 &   1.000000 \\
Melonie         &   1.0 &   0.294118 \\
Melony          &   1.0 &   0.156250 \\
Melrose         &   1.0 &   0.388889 \\
Melva           &   1.0 &   0.250000 \\
Melvina         &   1.0 &   0.857143 \\
Melyna          &   1.0 &   1.000000 \\
Melynda         &   1.0 &   0.461538 \\
Melysa          &   1.0 &   1.000000 \\
Melyssa         &   1.0 &   0.138889 \\
Memory          &   1.0 &   0.857143 \\
Memphis         &   1.0 &   0.466667 \\
Mena            &   1.0 &   0.500000 \\
Mendi           &   1.0 &   1.000000 \\
Mendy           &   1.0 &   0.333333 \\
Menucha         &   1.0 &   1.000000 \\
Mera            &   1.0 &   1.000000 \\
Meranda         &   1.0 &   0.368421 \\
Merari          &   1.0 &   0.238095 \\
Merary          &   1.0 &   0.625000 \\
Merced          &   1.0 &   1.000000 \\
Mercede         &   1.0 &   1.000000 \\
Mercedes        &   1.0 &   0.097561 \\
Mercedez        &   1.0 &   0.363636 \\
Merci           &   1.0 &   0.750000 \\
Mercie          &   1.0 &   1.000000 \\
Mercy           &   1.0 &   0.595745 \\
Meredith        &   1.0 &   0.325203 \\
Merelyn         &   1.0 &   1.000000 \\
Meri            &   1.0 &   0.333333 \\
Meriah          &   1.0 &   0.800000 \\
Merida          &   1.0 &   0.636364 \\
Merideth        &   1.0 &   0.625000 \\
Meridith        &   1.0 &   0.500000 \\
Merilee         &   1.0 &   0.500000 \\
Merilyn         &   1.0 &   0.545455 \\
Merina          &   1.0 &   1.000000 \\
Merisa          &   1.0 &   1.000000 \\
Merissa         &   1.0 &   0.153846 \\
Merle           &   1.0 &   0.208333 \\
Merlene         &   1.0 &   0.666667 \\
Merlin          &   1.0 &   0.500000 \\
Merlina         &   1.0 &   0.700000 \\
Merly           &   1.0 &   0.833333 \\
Merlyn          &   1.0 &   0.750000 \\
Merna           &   1.0 &   1.000000 \\
Meron           &   1.0 &   1.000000 \\
Merri           &   1.0 &   0.555556 \\
Merrie          &   1.0 &   0.400000 \\
Merrilee        &   1.0 &   0.375000 \\
Merrily         &   1.0 &   0.294118 \\
Merrilyn        &   1.0 &   0.833333 \\
Merritt         &   1.0 &   0.625000 \\
Merry           &   1.0 &   0.166667 \\
Mery            &   1.0 &   1.000000 \\
Meryem          &   1.0 &   1.000000 \\
Meryl           &   1.0 &   0.461538 \\
Mesha           &   1.0 &   0.625000 \\
Meshell         &   1.0 &   1.000000 \\
Meshelle        &   1.0 &   1.000000 \\
Messiah         &   1.0 &   0.833333 \\
Meta            &   1.0 &   0.857143 \\
Metzli          &   1.0 &   0.470588 \\
Metztli         &   1.0 &   1.000000 \\
Meuy            &   1.0 &   0.833333 \\
Mey             &   1.0 &   1.000000 \\
Meya            &   1.0 &   0.555556 \\
Meyah           &   1.0 &   0.833333 \\
Meylin          &   1.0 &   1.000000 \\
Meztli          &   1.0 &   0.416667 \\
Mi              &   1.0 &   0.555556 \\
Mia             &   1.0 &   0.655749 \\
Miaa            &   1.0 &   1.000000 \\
Miabella        &   1.0 &   0.428571 \\
Miabelle        &   1.0 &   1.000000 \\
Miagrace        &   1.0 &   1.000000 \\
Miah            &   1.0 &   0.333333 \\
Miaisabella     &   1.0 &   1.000000 \\
Mialani         &   1.0 &   0.714286 \\
Miamor          &   1.0 &   0.833333 \\
Miana           &   1.0 &   0.833333 \\
Miangel         &   1.0 &   1.000000 \\
Miani           &   1.0 &   1.000000 \\
Mianna          &   1.0 &   0.833333 \\
Miarose         &   1.0 &   0.500000 \\
Mica            &   1.0 &   0.666667 \\
Micaela         &   1.0 &   0.456522 \\
Micah           &   1.0 &   0.297297 \\
Micaiah         &   1.0 &   1.000000 \\
Micayla         &   1.0 &   0.240000 \\
Micel           &   1.0 &   1.000000 \\
Michael         &   1.0 &   0.067568 \\
Michaela        &   1.0 &   0.137584 \\
Michaele        &   1.0 &   0.333333 \\
Michaella       &   1.0 &   0.333333 \\
Michaelle       &   1.0 &   0.500000 \\
Michaelyn       &   1.0 &   1.000000 \\
Michaila        &   1.0 &   1.000000 \\
Michal          &   1.0 &   0.357143 \\
Michala         &   1.0 &   0.777778 \\
Michayla        &   1.0 &   0.384615 \\
Micheal         &   1.0 &   0.461538 \\
Micheala        &   1.0 &   0.750000 \\
Michel          &   1.0 &   0.400000 \\
Michela         &   1.0 &   0.444444 \\
Michele         &   1.0 &   0.004500 \\
Michelene       &   1.0 &   1.000000 \\
Micheline       &   1.0 &   0.636364 \\
Michell         &   1.0 &   0.151515 \\
Michella        &   1.0 &   0.833333 \\
Michelle        &   1.0 &   0.045407 \\
Michelleann     &   1.0 &   1.000000 \\
Michellee       &   1.0 &   1.000000 \\
Michi           &   1.0 &   0.857143 \\
Michiko         &   1.0 &   0.294118 \\
Michiye         &   1.0 &   1.000000 \\
Michon          &   1.0 &   1.000000 \\
Mickaela        &   1.0 &   0.833333 \\
Mickayla        &   1.0 &   0.538462 \\
Mickey          &   1.0 &   0.333333 \\
Micki           &   1.0 &   0.294118 \\
Mickie          &   1.0 &   0.357143 \\
Micky           &   1.0 &   1.000000 \\
Micole          &   1.0 &   0.714286 \\
Midge           &   1.0 &   1.000000 \\
Midori          &   1.0 &   0.312500 \\
Mieko           &   1.0 &   1.000000 \\
Miel            &   1.0 &   1.000000 \\
Miesha          &   1.0 &   0.108696 \\
Miette          &   1.0 &   1.000000 \\
Migdalia        &   1.0 &   1.000000 \\
Mignon          &   1.0 &   0.700000 \\
Miguel          &   1.0 &   0.238095 \\
Mihika          &   1.0 &   0.692308 \\
Mihira          &   1.0 &   0.636364 \\
Miia            &   1.0 &   1.000000 \\
Mija            &   1.0 &   1.000000 \\
Mika            &   1.0 &   0.653061 \\
Mikaela         &   1.0 &   0.454545 \\
Mikaella        &   1.0 &   1.000000 \\
Mikah           &   1.0 &   0.625000 \\
Mikaila         &   1.0 &   0.208333 \\
Mikala          &   1.0 &   0.291667 \\
Mikalah         &   1.0 &   0.333333 \\
Mikayla         &   1.0 &   0.242690 \\
Mikaylah        &   1.0 &   0.277778 \\
Mike            &   1.0 &   0.833333 \\
Mikeila         &   1.0 &   1.000000 \\
Mikeisha        &   1.0 &   1.000000 \\
Mikel           &   1.0 &   1.000000 \\
Mikela          &   1.0 &   0.454545 \\
Mikell          &   1.0 &   1.000000 \\
Mikelle         &   1.0 &   0.833333 \\
Mikenna         &   1.0 &   0.416667 \\
Mikenzie        &   1.0 &   0.833333 \\
Mikeyla         &   1.0 &   0.833333 \\
Mikhaela        &   1.0 &   0.833333 \\
Mikhaila        &   1.0 &   0.833333 \\
Miki            &   1.0 &   0.500000 \\
Mikia           &   1.0 &   1.000000 \\
Mikki           &   1.0 &   0.500000 \\
Miko            &   1.0 &   0.750000 \\
Miku            &   1.0 &   0.714286 \\
Mikyla          &   1.0 &   0.888889 \\
Mila            &   1.0 &   0.830626 \\
Milagro         &   1.0 &   0.500000 \\
Milagros        &   1.0 &   0.330275 \\
Milah           &   1.0 &   0.591837 \\
Milan           &   1.0 &   0.614035 \\
Milana          &   1.0 &   0.746667 \\
Milani          &   1.0 &   1.000000 \\
Milania         &   1.0 &   0.225806 \\
Milanie         &   1.0 &   0.714286 \\
Milarose        &   1.0 &   1.000000 \\
Milca           &   1.0 &   0.571429 \\
Mildred         &   1.0 &   0.022293 \\
Mileah          &   1.0 &   1.000000 \\
Milee           &   1.0 &   0.714286 \\
Mileena         &   1.0 &   1.000000 \\
Mileidy         &   1.0 &   1.000000 \\
Milena          &   1.0 &   0.653846 \\
Milenia         &   1.0 &   1.000000 \\
Milenka         &   1.0 &   1.000000 \\
Miles           &   1.0 &   1.000000 \\
Miley           &   1.0 &   0.092958 \\
Mileydi         &   1.0 &   1.000000 \\
Mili            &   1.0 &   0.416667 \\
Miliana         &   1.0 &   0.833333 \\
Miliani         &   1.0 &   0.416667 \\
Mililani        &   1.0 &   1.000000 \\
Milinda         &   1.0 &   1.000000 \\
Milissa         &   1.0 &   0.277778 \\
Milka           &   1.0 &   1.000000 \\
Milla           &   1.0 &   0.375000 \\
Millenia        &   1.0 &   1.000000 \\
Miller          &   1.0 &   0.900000 \\
Milli           &   1.0 &   1.000000 \\
Milliana        &   1.0 &   0.750000 \\
Millicent       &   1.0 &   0.571429 \\
Millie          &   1.0 &   1.000000 \\
Milly           &   1.0 &   0.454545 \\
Milo            &   1.0 &   0.833333 \\
Mily            &   1.0 &   0.416667 \\
Milynn          &   1.0 &   1.000000 \\
Mimi            &   1.0 &   0.138889 \\
Min             &   1.0 &   1.000000 \\
Mina            &   1.0 &   1.000000 \\
Minahil         &   1.0 &   1.000000 \\
Minami          &   1.0 &   0.833333 \\
Minda           &   1.0 &   1.000000 \\
Mindee          &   1.0 &   1.000000 \\
Mindi           &   1.0 &   0.227273 \\
Mindy           &   1.0 &   0.034749 \\
Minerva         &   1.0 &   0.333333 \\
Minette         &   1.0 &   1.000000 \\
Minh            &   1.0 &   0.750000 \\
Minha           &   1.0 &   1.000000 \\
Minka           &   1.0 &   0.857143 \\
Minna           &   1.0 &   0.800000 \\
Minnie          &   1.0 &   0.121951 \\
Mio             &   1.0 &   0.714286 \\
Mira            &   1.0 &   1.000000 \\
Miraal          &   1.0 &   1.000000 \\
Mirabai         &   1.0 &   1.000000 \\
Mirabel         &   1.0 &   1.000000 \\
Mirabella       &   1.0 &   0.615385 \\
Mirabelle       &   1.0 &   0.833333 \\
Miracle         &   1.0 &   0.948276 \\
Mirai           &   1.0 &   0.727273 \\
Miral           &   1.0 &   1.000000 \\
Miranda         &   1.0 &   0.168971 \\
Miraya          &   1.0 &   0.687500 \\
Mirca           &   1.0 &   1.000000 \\
Mireille        &   1.0 &   0.666667 \\
Mirella         &   1.0 &   0.112676 \\
Mirely          &   1.0 &   1.000000 \\
Mireya          &   1.0 &   0.224390 \\
Mirha           &   1.0 &   1.000000 \\
Miri            &   1.0 &   1.000000 \\
Miriah          &   1.0 &   0.294118 \\
Miriam          &   1.0 &   0.253125 \\
Mirian          &   1.0 &   0.108696 \\
Mirielle        &   1.0 &   1.000000 \\
Mirka           &   1.0 &   0.375000 \\
Mirna           &   1.0 &   0.107143 \\
Miroslava       &   1.0 &   0.500000 \\
Mirta           &   1.0 &   0.555556 \\
Mirtha          &   1.0 &   0.625000 \\
Miryam          &   1.0 &   0.384615 \\
Mirza           &   1.0 &   1.000000 \\
Misa            &   1.0 &   0.714286 \\
Misaki          &   1.0 &   1.000000 \\
Misako          &   1.0 &   0.833333 \\
Misao           &   1.0 &   0.636364 \\
Mischa          &   1.0 &   0.214286 \\
Mischelle       &   1.0 &   0.857143 \\
Misel           &   1.0 &   1.000000 \\
Misha           &   1.0 &   0.518519 \\
Mishel          &   1.0 &   1.000000 \\
Mishell         &   1.0 &   1.000000 \\
Mishelle        &   1.0 &   1.000000 \\
Mishika         &   1.0 &   0.727273 \\
Mishka          &   1.0 &   0.578947 \\
Misk            &   1.0 &   1.000000 \\
Missy           &   1.0 &   0.200000 \\
Misti           &   1.0 &   0.218750 \\
Mistica         &   1.0 &   1.000000 \\
Mistie          &   1.0 &   0.428571 \\
Misty           &   1.0 &   0.012594 \\
Mitali          &   1.0 &   1.000000 \\
Mitchell        &   1.0 &   0.454545 \\
Mitchelle       &   1.0 &   0.714286 \\
Mithra          &   1.0 &   1.000000 \\
Mitra           &   1.0 &   1.000000 \\
Mitsue          &   1.0 &   1.000000 \\
Mitsuko         &   1.0 &   0.588235 \\
Mitsuye         &   1.0 &   0.538462 \\
Mitsy           &   1.0 &   1.000000 \\
Mitzi           &   1.0 &   0.157895 \\
Mitzie          &   1.0 &   1.000000 \\
Mitzy           &   1.0 &   0.208333 \\
Miu             &   1.0 &   1.000000 \\
Mixtli          &   1.0 &   0.833333 \\
Miya            &   1.0 &   0.480000 \\
Miyah           &   1.0 &   0.347826 \\
Miyako          &   1.0 &   1.000000 \\
Miyana          &   1.0 &   1.000000 \\
Miye            &   1.0 &   1.000000 \\
Miyeko          &   1.0 &   0.666667 \\
Miyo            &   1.0 &   0.625000 \\
Miyoko          &   1.0 &   0.294118 \\
Miyu            &   1.0 &   0.714286 \\
Miyuki          &   1.0 &   1.000000 \\
Mizuki          &   1.0 &   0.875000 \\
Mliss           &   1.0 &   1.000000 \\
Moana           &   1.0 &   0.380952 \\
Modesta         &   1.0 &   0.714286 \\
Moesha          &   1.0 &   1.000000 \\
Mohini          &   1.0 &   1.000000 \\
Moira           &   1.0 &   0.250000 \\
Moises          &   1.0 &   1.000000 \\
Moksha          &   1.0 &   1.000000 \\
Molina          &   1.0 &   1.000000 \\
Molli           &   1.0 &   1.000000 \\
Mollie          &   1.0 &   0.166667 \\
Molly           &   1.0 &   0.163934 \\
Momoka          &   1.0 &   0.714286 \\
Mona            &   1.0 &   0.148936 \\
Monae           &   1.0 &   0.294118 \\
Monalisa        &   1.0 &   0.416667 \\
Monay           &   1.0 &   0.875000 \\
Moncerat        &   1.0 &   0.714286 \\
Moncerrat       &   1.0 &   0.705882 \\
Moncerrath      &   1.0 &   0.833333 \\
Moncia          &   1.0 &   0.833333 \\
Mone            &   1.0 &   0.833333 \\
Monee           &   1.0 &   0.833333 \\
Monesha         &   1.0 &   1.000000 \\
Monet           &   1.0 &   0.142857 \\
Monette         &   1.0 &   0.500000 \\
Monic           &   1.0 &   0.545455 \\
Monica          &   1.0 &   0.045858 \\
Monigue         &   1.0 &   0.454545 \\
Monik           &   1.0 &   1.000000 \\
Monika          &   1.0 &   0.113636 \\
Moniqua         &   1.0 &   1.000000 \\
Monique         &   1.0 &   0.015291 \\
Monisha         &   1.0 &   0.375000 \\
Monita          &   1.0 &   1.000000 \\
Monroe          &   1.0 &   0.983871 \\
Monse           &   1.0 &   1.000000 \\
Monserat        &   1.0 &   0.352941 \\
Monserath       &   1.0 &   0.500000 \\
Monserrat       &   1.0 &   0.246835 \\
Monserrath      &   1.0 &   0.214286 \\
Monserratt      &   1.0 &   0.750000 \\
Montana         &   1.0 &   0.365854 \\
Montanna        &   1.0 &   1.000000 \\
Monte           &   1.0 &   1.000000 \\
Montoya         &   1.0 &   0.833333 \\
Montserrat      &   1.0 &   0.273256 \\
Montserrath     &   1.0 &   1.000000 \\
Montzerrat      &   1.0 &   1.000000 \\
Monzeratt       &   1.0 &   1.000000 \\
Monzerrat       &   1.0 &   0.437500 \\
Monzerrath      &   1.0 &   0.833333 \\
Moon            &   1.0 &   1.000000 \\
Moorea          &   1.0 &   0.625000 \\
Moraima         &   1.0 &   1.000000 \\
Moranda         &   1.0 &   1.000000 \\
Morelia         &   1.0 &   0.185185 \\
Morena          &   1.0 &   1.000000 \\
Morgan          &   1.0 &   0.176730 \\
Morgana         &   1.0 &   1.000000 \\
Morgane         &   1.0 &   1.000000 \\
Morgann         &   1.0 &   1.000000 \\
Morganne        &   1.0 &   0.500000 \\
Morgen          &   1.0 &   0.384615 \\
Morghan         &   1.0 &   1.000000 \\
Morgyn          &   1.0 &   1.000000 \\
Moria           &   1.0 &   0.555556 \\
Moriah          &   1.0 &   0.140845 \\
Morning         &   1.0 &   1.000000 \\
Morrigan        &   1.0 &   0.666667 \\
Morrison        &   1.0 &   1.000000 \\
Moxie           &   1.0 &   1.000000 \\
Moya            &   1.0 &   1.000000 \\
Mui             &   1.0 &   1.000000 \\
Mulan           &   1.0 &   1.000000 \\
Muna            &   1.0 &   1.000000 \\
Muntaha         &   1.0 &   1.000000 \\
Muriel          &   1.0 &   0.049505 \\
Murphy          &   1.0 &   0.884615 \\
Mursal          &   1.0 &   1.000000 \\
Muskaan         &   1.0 &   0.454545 \\
Muskan          &   1.0 &   0.555556 \\
My              &   1.0 &   0.151515 \\
Mya             &   1.0 &   0.365132 \\
Myah            &   1.0 &   0.280000 \\
Mychaela        &   1.0 &   1.000000 \\
Mychal          &   1.0 &   1.000000 \\
Mychelle        &   1.0 &   0.500000 \\
Myeisha         &   1.0 &   0.545455 \\
Myesha          &   1.0 &   0.162162 \\
Myeshia         &   1.0 &   0.454545 \\
Myia            &   1.0 &   0.750000 \\
Myiah           &   1.0 &   0.833333 \\
Myiesha         &   1.0 &   0.600000 \\
Myisha          &   1.0 &   0.300000 \\
Myka            &   1.0 &   0.750000 \\
Mykaela         &   1.0 &   0.384615 \\
Mykah           &   1.0 &   0.700000 \\
Mykaila         &   1.0 &   0.833333 \\
Mykala          &   1.0 &   0.666667 \\
Mykayla         &   1.0 &   0.333333 \\
Mykel           &   1.0 &   1.000000 \\
Mykenzie        &   1.0 &   1.000000 \\
Mykia           &   1.0 &   1.000000 \\
Myla            &   1.0 &   1.000000 \\
Mylah           &   1.0 &   0.956044 \\
Mylani          &   1.0 &   1.000000 \\
Myleah          &   1.0 &   0.857143 \\
Mylee           &   1.0 &   0.108696 \\
Myleen          &   1.0 &   0.714286 \\
Mylene          &   1.0 &   0.384615 \\
Myley           &   1.0 &   1.000000 \\
Myli            &   1.0 &   1.000000 \\
Mylia           &   1.0 &   1.000000 \\
Mylie           &   1.0 &   0.179487 \\
Mylin           &   1.0 &   1.000000 \\
Mylinda         &   1.0 &   1.000000 \\
Mylinh          &   1.0 &   1.000000 \\
Mylissa         &   1.0 &   1.000000 \\
Myra            &   1.0 &   0.505155 \\
Myracle         &   1.0 &   0.833333 \\
Myrah           &   1.0 &   1.000000 \\
Myranda         &   1.0 &   0.242424 \\
Myriah          &   1.0 &   0.263158 \\
Myriam          &   1.0 &   0.227273 \\
Myrian          &   1.0 &   1.000000 \\
Myrissa         &   1.0 &   0.625000 \\
Myrka           &   1.0 &   0.208333 \\
Myrle           &   1.0 &   1.000000 \\
Myrna           &   1.0 &   0.052632 \\
Myrtle          &   1.0 &   0.074627 \\
Mysha           &   1.0 &   0.833333 \\
Myshia          &   1.0 &   1.000000 \\
Mysti           &   1.0 &   1.000000 \\
Mystic          &   1.0 &   1.000000 \\
Mystica         &   1.0 &   1.000000 \\
Mystique        &   1.0 &   1.000000 \\
Na              &   1.0 &   1.000000 \\
Naara           &   1.0 &   1.000000 \\
Naavya          &   1.0 &   1.000000 \\
Nabiha          &   1.0 &   1.000000 \\
Nabil           &   1.0 &   1.000000 \\
Nabila          &   1.0 &   0.833333 \\
Nachelle        &   1.0 &   1.000000 \\
Nacole          &   1.0 &   0.555556 \\
Nacy            &   1.0 &   1.000000 \\
Nada            &   1.0 &   0.500000 \\
Nadean          &   1.0 &   1.000000 \\
Nadeen          &   1.0 &   0.555556 \\
Nadene          &   1.0 &   1.000000 \\
Nadia           &   1.0 &   0.293286 \\
Nadiah          &   1.0 &   0.714286 \\
Nadina          &   1.0 &   1.000000 \\
Nadine          &   1.0 &   0.200000 \\
Nadira          &   1.0 &   1.000000 \\
Nadiya          &   1.0 &   0.384615 \\
Nadiyah         &   1.0 &   1.000000 \\
Nadja           &   1.0 &   1.000000 \\
Nadya           &   1.0 &   0.230769 \\
Naeemah         &   1.0 &   1.000000 \\
Naelani         &   1.0 &   1.000000 \\
Naevia          &   1.0 &   0.857143 \\
Nahal           &   1.0 &   1.000000 \\
Nahia           &   1.0 &   0.833333 \\
Nahla           &   1.0 &   0.333333 \\
Nahlia          &   1.0 &   1.000000 \\
Nahomi          &   1.0 &   0.388889 \\
Nahomy          &   1.0 &   0.526316 \\
Nai             &   1.0 &   0.454545 \\
Naia            &   1.0 &   1.000000 \\
Naiara          &   1.0 &   0.625000 \\
Naibe           &   1.0 &   0.750000 \\
Naidelin        &   1.0 &   0.240000 \\
Naidely         &   1.0 &   1.000000 \\
Naidelyn        &   1.0 &   0.193548 \\
Naiema          &   1.0 &   1.000000 \\
Naila           &   1.0 &   0.619048 \\
Nailah          &   1.0 &   0.791667 \\
Nailani         &   1.0 &   1.000000 \\
Nailea          &   1.0 &   0.461538 \\
Naileah         &   1.0 &   1.000000 \\
Naima           &   1.0 &   0.232558 \\
Naimah          &   1.0 &   0.333333 \\
Naina           &   1.0 &   0.428571 \\
Nainika         &   1.0 &   1.000000 \\
Naiomi          &   1.0 &   1.000000 \\
Naira           &   1.0 &   0.357143 \\
Nairi           &   1.0 &   0.583333 \\
Nairobi         &   1.0 &   1.000000 \\
Naisha          &   1.0 &   0.500000 \\
Naiya           &   1.0 &   0.294118 \\
Naiyah          &   1.0 &   0.600000 \\
Naizeth         &   1.0 &   1.000000 \\
Naja            &   1.0 &   0.666667 \\
Najae           &   1.0 &   1.000000 \\
Najah           &   1.0 &   0.545455 \\
Najla           &   1.0 &   1.000000 \\
Najwa           &   1.0 &   1.000000 \\
Nakayla         &   1.0 &   0.454545 \\
Nakeisha        &   1.0 &   1.000000 \\
Nakesha         &   1.0 &   1.000000 \\
Nakeya          &   1.0 &   1.000000 \\
Nakia           &   1.0 &   0.075949 \\
Nakisha         &   1.0 &   0.571429 \\
Nakita          &   1.0 &   0.350000 \\
Nakiya          &   1.0 &   1.000000 \\
Nala            &   1.0 &   0.837209 \\
Nalah           &   1.0 &   0.428571 \\
Nalani          &   1.0 &   1.000000 \\
Nalanie         &   1.0 &   0.714286 \\
Nalayah         &   1.0 &   1.000000 \\
Naleah          &   1.0 &   0.833333 \\
Nalee           &   1.0 &   1.000000 \\
Naleya          &   1.0 &   0.714286 \\
Nalia           &   1.0 &   0.666667 \\
Naliah          &   1.0 &   1.000000 \\
Nalina          &   1.0 &   1.000000 \\
Nalini          &   1.0 &   1.000000 \\
Naliyah         &   1.0 &   0.500000 \\
Nalleli         &   1.0 &   0.315789 \\
Nallely         &   1.0 &   0.055556 \\
Nalley          &   1.0 &   1.000000 \\
Naly            &   1.0 &   1.000000 \\
Namaari         &   1.0 &   1.000000 \\
Namari          &   1.0 &   1.000000 \\
Nami            &   1.0 &   1.000000 \\
Namiko          &   1.0 &   0.833333 \\
Namrata         &   1.0 &   1.000000 \\
Nan             &   1.0 &   0.263158 \\
Nana            &   1.0 &   0.555556 \\
Nanami          &   1.0 &   0.600000 \\
Nance           &   1.0 &   1.000000 \\
Nancee          &   1.0 &   0.666667 \\
Nancey          &   1.0 &   1.000000 \\
Nanci           &   1.0 &   0.146341 \\
Nancie          &   1.0 &   0.312500 \\
Nancy           &   1.0 &   0.018769 \\
Nandi           &   1.0 &   0.833333 \\
Nandini         &   1.0 &   0.500000 \\
Nandita         &   1.0 &   1.000000 \\
Nane            &   1.0 &   1.000000 \\
Nanette         &   1.0 &   0.050000 \\
Nani            &   1.0 &   0.833333 \\
Nannette        &   1.0 &   0.116279 \\
Nansi           &   1.0 &   0.583333 \\
Naoko           &   1.0 &   1.000000 \\
Naoma           &   1.0 &   1.000000 \\
Naomi           &   1.0 &   0.883858 \\
Naomie          &   1.0 &   0.727273 \\
Naomy           &   1.0 &   0.166667 \\
Nara            &   1.0 &   0.600000 \\
Narali          &   1.0 &   1.000000 \\
Naraly          &   1.0 &   0.312500 \\
Narda           &   1.0 &   1.000000 \\
Nare            &   1.0 &   1.000000 \\
Nareh           &   1.0 &   1.000000 \\
Narely          &   1.0 &   0.714286 \\
Nari            &   1.0 &   1.000000 \\
Nariah          &   1.0 &   0.526316 \\
Narine          &   1.0 &   0.714286 \\
Narissa         &   1.0 &   1.000000 \\
Nariyah         &   1.0 &   0.315789 \\
Nary            &   1.0 &   0.875000 \\
Naseeb          &   1.0 &   1.000000 \\
Naseem          &   1.0 &   0.833333 \\
Nashali         &   1.0 &   1.000000 \\
Nashaly         &   1.0 &   0.500000 \\
Nashira         &   1.0 &   1.000000 \\
Nashla          &   1.0 &   1.000000 \\
Nashley         &   1.0 &   1.000000 \\
Nashly          &   1.0 &   1.000000 \\
Nasia           &   1.0 &   0.833333 \\
Nasim           &   1.0 &   1.000000 \\
Nassim          &   1.0 &   1.000000 \\
Nastasha        &   1.0 &   1.000000 \\
Nastassia       &   1.0 &   0.636364 \\
Nastassja       &   1.0 &   0.700000 \\
Nasya           &   1.0 &   1.000000 \\
Natacha         &   1.0 &   0.555556 \\
Natalee         &   1.0 &   0.102941 \\
Nataley         &   1.0 &   0.625000 \\
Natali          &   1.0 &   0.206897 \\
Natalia         &   1.0 &   0.913866 \\
Nataliah        &   1.0 &   1.000000 \\
Natalie         &   1.0 &   0.223619 \\
Natalina        &   1.0 &   1.000000 \\
Nataliya        &   1.0 &   0.555556 \\
Natallie        &   1.0 &   0.750000 \\
Natally         &   1.0 &   0.384615 \\
Nataly          &   1.0 &   0.190647 \\
Natalya         &   1.0 &   0.338710 \\
Natalye         &   1.0 &   1.000000 \\
Natalyn         &   1.0 &   0.833333 \\
Natalynn        &   1.0 &   1.000000 \\
Natania         &   1.0 &   0.416667 \\
Natascha        &   1.0 &   0.750000 \\
Natasha         &   1.0 &   0.131403 \\
Natashia        &   1.0 &   0.352941 \\
Natasia         &   1.0 &   0.833333 \\
Natassha        &   1.0 &   1.000000 \\
Natassia        &   1.0 &   0.384615 \\
Natassja        &   1.0 &   1.000000 \\
Nathalee        &   1.0 &   1.000000 \\
Nathali         &   1.0 &   0.416667 \\
Nathalia        &   1.0 &   0.700000 \\
Nathalie        &   1.0 &   0.225989 \\
Nathaly         &   1.0 &   0.206897 \\
Nathalya        &   1.0 &   1.000000 \\
Nathalye        &   1.0 &   1.000000 \\
Nathan          &   1.0 &   0.600000 \\
Nathania        &   1.0 &   1.000000 \\
Nathaniel       &   1.0 &   1.000000 \\
Natisha         &   1.0 &   0.555556 \\
Natividad       &   1.0 &   0.416667 \\
Natosha         &   1.0 &   0.357143 \\
Natoya          &   1.0 &   1.000000 \\
Natsumi         &   1.0 &   1.000000 \\
Nattalie        &   1.0 &   1.000000 \\
Nattaly         &   1.0 &   1.000000 \\
Naudia          &   1.0 &   1.000000 \\
Nautica         &   1.0 &   0.368421 \\
Nautika         &   1.0 &   1.000000 \\
Nava            &   1.0 &   0.750000 \\
Navaeh          &   1.0 &   0.500000 \\
Navah           &   1.0 &   1.000000 \\
Navany          &   1.0 &   1.000000 \\
Navdeep         &   1.0 &   0.750000 \\
Naveah          &   1.0 &   1.000000 \\
Naveen          &   1.0 &   0.555556 \\
Naveyah         &   1.0 &   1.000000 \\
Navi            &   1.0 &   0.368421 \\
Navid           &   1.0 &   1.000000 \\
Navil           &   1.0 &   0.250000 \\
Navina          &   1.0 &   1.000000 \\
Navjot          &   1.0 &   0.625000 \\
Navneet         &   1.0 &   0.500000 \\
Navpreet        &   1.0 &   0.714286 \\
Navreet         &   1.0 &   1.000000 \\
Navy            &   1.0 &   1.000000 \\
Navya           &   1.0 &   0.629630 \\
Nawal           &   1.0 &   0.625000 \\
Naya            &   1.0 &   0.910000 \\
Nayah           &   1.0 &   0.666667 \\
Nayana          &   1.0 &   0.625000 \\
Nayara          &   1.0 &   1.000000 \\
Naydeen         &   1.0 &   1.000000 \\
Naydeli         &   1.0 &   1.000000 \\
Naydelin        &   1.0 &   0.169811 \\
Naydeline       &   1.0 &   0.714286 \\
Naydelyn        &   1.0 &   0.545455 \\
Nayeli          &   1.0 &   0.323160 \\
Nayelie         &   1.0 &   0.388889 \\
Nayelli         &   1.0 &   0.181818 \\
Nayelly         &   1.0 &   0.357143 \\
Nayely          &   1.0 &   0.049505 \\
Nayla           &   1.0 &   1.000000 \\
Naylah          &   1.0 &   0.666667 \\
Naylani         &   1.0 &   0.600000 \\
Naylea          &   1.0 &   0.727273 \\
Nayleah         &   1.0 &   1.000000 \\
Nayleen         &   1.0 &   0.625000 \\
Naylene         &   1.0 &   0.555556 \\
Nayomi          &   1.0 &   0.500000 \\
Nayra           &   1.0 &   1.000000 \\
Naysa           &   1.0 &   0.714286 \\
Nayzeth         &   1.0 &   0.208333 \\
Nazanin         &   1.0 &   0.833333 \\
Nazareth        &   1.0 &   1.000000 \\
Nazaria         &   1.0 &   1.000000 \\
Nazeli          &   1.0 &   1.000000 \\
Nazia           &   1.0 &   0.625000 \\
Nazli           &   1.0 &   0.750000 \\
Nazly           &   1.0 &   0.833333 \\
Nazyia          &   1.0 &   1.000000 \\
Nea             &   1.0 &   1.000000 \\
Neah            &   1.0 &   1.000000 \\
Neala           &   1.0 &   0.500000 \\
Nechama         &   1.0 &   1.000000 \\
Necole          &   1.0 &   0.500000 \\
Neda            &   1.0 &   0.272727 \\
Nedra           &   1.0 &   0.466667 \\
Neeka           &   1.0 &   1.000000 \\
Neela           &   1.0 &   0.375000 \\
Neelam          &   1.0 &   0.500000 \\
Neely           &   1.0 &   1.000000 \\
Neema           &   1.0 &   1.000000 \\
Neena           &   1.0 &   1.000000 \\
Neesha          &   1.0 &   1.000000 \\
Neeva           &   1.0 &   1.000000 \\
Nefertari       &   1.0 &   0.500000 \\
Nefertiti       &   1.0 &   1.000000 \\
Neftali         &   1.0 &   0.454545 \\
Neftaly         &   1.0 &   1.000000 \\
Negar           &   1.0 &   1.000000 \\
Negin           &   1.0 &   0.833333 \\
Neha            &   1.0 &   0.102041 \\
Nehemiah        &   1.0 &   1.000000 \\
Neiba           &   1.0 &   0.666667 \\
Neida           &   1.0 &   0.135135 \\
Neidy           &   1.0 &   1.000000 \\
Neila           &   1.0 &   0.600000 \\
Neira           &   1.0 &   1.000000 \\
Neisha          &   1.0 &   0.833333 \\
Neiva           &   1.0 &   0.545455 \\
Nekia           &   1.0 &   1.000000 \\
Neko            &   1.0 &   1.000000 \\
Nelda           &   1.0 &   0.250000 \\
Neli            &   1.0 &   0.833333 \\
Nelia           &   1.0 &   1.000000 \\
Nelida          &   1.0 &   0.294118 \\
Nell            &   1.0 &   0.909091 \\
Nella           &   1.0 &   0.777778 \\
Nelle           &   1.0 &   1.000000 \\
Nelli           &   1.0 &   0.714286 \\
Nellie          &   1.0 &   0.244898 \\
Nelly           &   1.0 &   0.246154 \\
Nely            &   1.0 &   0.466667 \\
Nemesis         &   1.0 &   0.857143 \\
Nena            &   1.0 &   0.312500 \\
Neoma           &   1.0 &   1.000000 \\
Neomi           &   1.0 &   0.666667 \\
Nereida         &   1.0 &   0.056180 \\
Nereyda         &   1.0 &   0.157895 \\
Neri            &   1.0 &   1.000000 \\
Neriah          &   1.0 &   0.500000 \\
Nerissa         &   1.0 &   0.461538 \\
Nery            &   1.0 &   0.833333 \\
Neslihan        &   1.0 &   1.000000 \\
Nessa           &   1.0 &   0.916667 \\
Neta            &   1.0 &   1.000000 \\
Netanya         &   1.0 &   0.833333 \\
Nethra          &   1.0 &   1.000000 \\
Netra           &   1.0 &   1.000000 \\
Nettie          &   1.0 &   0.263158 \\
Neva            &   1.0 &   0.388889 \\
Nevada          &   1.0 &   1.000000 \\
Nevaeh          &   1.0 &   0.509774 \\
Nevaeha         &   1.0 &   0.384615 \\
Nevah           &   1.0 &   1.000000 \\
Neve            &   1.0 &   0.533333 \\
Neveah          &   1.0 &   0.434783 \\
Neveen          &   1.0 &   1.000000 \\
Neviah          &   1.0 &   1.000000 \\
Neya            &   1.0 &   1.000000 \\
Neyda           &   1.0 &   0.421053 \\
Neyla           &   1.0 &   1.000000 \\
Neylan          &   1.0 &   1.000000 \\
Neyra           &   1.0 &   1.000000 \\
Neysa           &   1.0 &   0.714286 \\
Neyva           &   1.0 &   0.800000 \\
Nezuko          &   1.0 &   1.000000 \\
Nga             &   1.0 &   0.333333 \\
Ngan            &   1.0 &   1.000000 \\
Ngoc            &   1.0 &   0.277778 \\
Ngozi           &   1.0 &   1.000000 \\
Nguyet          &   1.0 &   1.000000 \\
Nhi             &   1.0 &   0.400000 \\
Nhia            &   1.0 &   1.000000 \\
Nhu             &   1.0 &   0.714286 \\
Nhung           &   1.0 &   0.777778 \\
Nia             &   1.0 &   0.509091 \\
Niah            &   1.0 &   0.428571 \\
Niamh           &   1.0 &   0.600000 \\
Niana           &   1.0 &   1.000000 \\
Niaomi          &   1.0 &   1.000000 \\
Niara           &   1.0 &   0.714286 \\
Niccole         &   1.0 &   0.461538 \\
Nichele         &   1.0 &   0.625000 \\
Nichelle        &   1.0 &   0.088235 \\
Nichol          &   1.0 &   0.222222 \\
Nichola         &   1.0 &   1.000000 \\
Nicholas        &   1.0 &   0.185185 \\
Nichole         &   1.0 &   0.029973 \\
Nicholette      &   1.0 &   0.416667 \\
Nicholl         &   1.0 &   1.000000 \\
Nicholle        &   1.0 &   0.357143 \\
Nicki           &   1.0 &   0.200000 \\
Nickie          &   1.0 &   0.384615 \\
Nickole         &   1.0 &   0.333333 \\
Nicky           &   1.0 &   0.500000 \\
Nico            &   1.0 &   0.588235 \\
Nicol           &   1.0 &   0.250000 \\
Nicola          &   1.0 &   0.368421 \\
Nicolasa        &   1.0 &   0.714286 \\
Nicole          &   1.0 &   0.065398 \\
Nicoleanne      &   1.0 &   1.000000 \\
Nicolemarie     &   1.0 &   1.000000 \\
Nicolet         &   1.0 &   1.000000 \\
Nicoletta       &   1.0 &   0.833333 \\
Nicolette       &   1.0 &   0.157895 \\
Nicolina        &   1.0 &   0.714286 \\
Nicolle         &   1.0 &   0.166667 \\
Nicollette      &   1.0 &   0.227273 \\
Nida            &   1.0 &   0.875000 \\
Nidhi           &   1.0 &   0.466667 \\
Nidia           &   1.0 &   0.119048 \\
Nidya           &   1.0 &   1.000000 \\
Niesha          &   1.0 &   0.588235 \\
Nieves          &   1.0 &   1.000000 \\
Nihan           &   1.0 &   0.500000 \\
Niharika        &   1.0 &   0.714286 \\
Nihira          &   1.0 &   1.000000 \\
Nijah           &   1.0 &   1.000000 \\
Nika            &   1.0 &   0.958333 \\
Nikayla         &   1.0 &   0.500000 \\
Nikesha         &   1.0 &   1.000000 \\
Nikeya          &   1.0 &   1.000000 \\
Nikhita         &   1.0 &   1.000000 \\
Niki            &   1.0 &   0.281250 \\
Nikia           &   1.0 &   0.294118 \\
Nikisha         &   1.0 &   0.750000 \\
Nikita          &   1.0 &   0.236364 \\
Nikitha         &   1.0 &   1.000000 \\
Nikka           &   1.0 &   0.714286 \\
Nikki           &   1.0 &   0.081633 \\
Nikkia          &   1.0 &   0.750000 \\
Nikkie          &   1.0 &   1.000000 \\
Nikkita         &   1.0 &   1.000000 \\
Nikkole         &   1.0 &   0.833333 \\
Niko            &   1.0 &   0.714286 \\
Nikol           &   1.0 &   0.625000 \\
Nikole          &   1.0 &   0.200000 \\
Nikolette       &   1.0 &   0.714286 \\
Nil             &   1.0 &   0.700000 \\
Nila            &   1.0 &   0.806452 \\
Nilah           &   1.0 &   0.888889 \\
Nilani          &   1.0 &   1.000000 \\
Nilda           &   1.0 &   0.555556 \\
Nile            &   1.0 &   1.000000 \\
Niloofar        &   1.0 &   1.000000 \\
Nima            &   1.0 &   1.000000 \\
Nimrat          &   1.0 &   0.642857 \\
Nimrit          &   1.0 &   1.000000 \\
Nina            &   1.0 &   0.639594 \\
Ninel           &   1.0 &   0.277778 \\
Ninette         &   1.0 &   0.555556 \\
Ninfa           &   1.0 &   0.454545 \\
Niobe           &   1.0 &   1.000000 \\
Niomi           &   1.0 &   0.750000 \\
Nira            &   1.0 &   0.750000 \\
Nirel           &   1.0 &   1.000000 \\
Nirvana         &   1.0 &   1.000000 \\
Nirvi           &   1.0 &   0.625000 \\
Nisa            &   1.0 &   0.333333 \\
Nisha           &   1.0 &   0.250000 \\
Nishika         &   1.0 &   1.000000 \\
Nishka          &   1.0 &   0.312500 \\
Nissa           &   1.0 &   0.357143 \\
Nissi           &   1.0 &   1.000000 \\
Nita            &   1.0 &   0.172414 \\
Nitara          &   1.0 &   0.636364 \\
Nitasha         &   1.0 &   1.000000 \\
Nithila         &   1.0 &   1.000000 \\
Nithya          &   1.0 &   0.750000 \\
Nitika          &   1.0 &   1.000000 \\
Nitya           &   1.0 &   0.461538 \\
Nitza           &   1.0 &   1.000000 \\
Niurka          &   1.0 &   0.857143 \\
Niva            &   1.0 &   1.000000 \\
Nivea           &   1.0 &   1.000000 \\
Niveah          &   1.0 &   1.000000 \\
Nivia           &   1.0 &   0.833333 \\
Nivisha         &   1.0 &   1.000000 \\
Nixie           &   1.0 &   0.857143 \\
Nixon           &   1.0 &   0.833333 \\
Niya            &   1.0 &   0.736842 \\
Niyah           &   1.0 &   0.434783 \\
Niyati          &   1.0 &   0.555556 \\
Niyla           &   1.0 &   1.000000 \\
Niylah          &   1.0 &   1.000000 \\
Niza            &   1.0 &   1.000000 \\
Nizhoni         &   1.0 &   1.000000 \\
Nneka           &   1.0 &   0.714286 \\
Noa             &   1.0 &   1.000000 \\
Noah            &   1.0 &   1.000000 \\
Noami           &   1.0 &   0.714286 \\
Nobu            &   1.0 &   1.000000 \\
Nobuko          &   1.0 &   0.384615 \\
Nocole          &   1.0 &   1.000000 \\
Noe             &   1.0 &   1.000000 \\
Noel            &   1.0 &   0.213115 \\
Noelani         &   1.0 &   0.697674 \\
Noeli           &   1.0 &   1.000000 \\
Noelia          &   1.0 &   0.378947 \\
Noell           &   1.0 &   1.000000 \\
Noella          &   1.0 &   0.533333 \\
Noelle          &   1.0 &   0.915663 \\
Noellie         &   1.0 &   1.000000 \\
Noelly          &   1.0 &   1.000000 \\
Noely           &   1.0 &   0.500000 \\
Noemi           &   1.0 &   0.596330 \\
Noemie          &   1.0 &   1.000000 \\
Noemy           &   1.0 &   0.228571 \\
Noga            &   1.0 &   1.000000 \\
Nohea           &   1.0 &   1.000000 \\
Noheli          &   1.0 &   1.000000 \\
Nohelia         &   1.0 &   1.000000 \\
Nohely          &   1.0 &   0.072165 \\
Nohemi          &   1.0 &   0.440000 \\
Nohemy          &   1.0 &   0.555556 \\
Nola            &   1.0 &   0.729730 \\
Nolani          &   1.0 &   1.000000 \\
Nomi            &   1.0 &   0.833333 \\
Nona            &   1.0 &   0.172414 \\
Noni            &   1.0 &   1.000000 \\
Nonie           &   1.0 &   1.000000 \\
Nonnie          &   1.0 &   1.000000 \\
Noor            &   1.0 &   1.000000 \\
Noora           &   1.0 &   0.500000 \\
Nora            &   1.0 &   0.990991 \\
Norah           &   1.0 &   0.675676 \\
Noralee         &   1.0 &   1.000000 \\
Noreen          &   1.0 &   0.134615 \\
Norene          &   1.0 &   0.437500 \\
Nori            &   1.0 &   0.960000 \\
Noriah          &   1.0 &   1.000000 \\
Noriko          &   1.0 &   1.000000 \\
Norine          &   1.0 &   0.500000 \\
Norita          &   1.0 &   1.000000 \\
Norma           &   1.0 &   0.041543 \\
Norman          &   1.0 &   1.000000 \\
Normani         &   1.0 &   0.833333 \\
Nou             &   1.0 &   0.269231 \\
Nour            &   1.0 &   1.000000 \\
Noura           &   1.0 &   0.900000 \\
Nova            &   1.0 &   1.000000 \\
Novah           &   1.0 &   0.896552 \\
Novalee         &   1.0 &   0.814815 \\
Novaleigh       &   1.0 &   1.000000 \\
Novalie         &   1.0 &   1.000000 \\
Novalynn        &   1.0 &   1.000000 \\
Novella         &   1.0 &   1.000000 \\
November        &   1.0 &   1.000000 \\
Noya            &   1.0 &   0.625000 \\
Nubia           &   1.0 &   0.266667 \\
Nuha            &   1.0 &   1.000000 \\
Nura            &   1.0 &   0.909091 \\
Nuri            &   1.0 &   0.409091 \\
Nuria           &   1.0 &   0.625000 \\
Nurvi           &   1.0 &   1.000000 \\
Nury            &   1.0 &   1.000000 \\
Nuvia           &   1.0 &   0.529412 \\
Nya             &   1.0 &   0.758621 \\
Nyah            &   1.0 &   0.241935 \\
Nyana           &   1.0 &   1.000000 \\
Nyanza          &   1.0 &   1.000000 \\
Nyari           &   1.0 &   1.000000 \\
Nyasia          &   1.0 &   1.000000 \\
Nycole          &   1.0 &   0.454545 \\
Nydia           &   1.0 &   0.217391 \\
Nyeli           &   1.0 &   1.000000 \\
Nyelli          &   1.0 &   1.000000 \\
Nyellie         &   1.0 &   1.000000 \\
Nyema           &   1.0 &   0.714286 \\
Nyemah          &   1.0 &   1.000000 \\
Nyesha          &   1.0 &   0.416667 \\
Nyima           &   1.0 &   1.000000 \\
Nyisha          &   1.0 &   0.875000 \\
Nyjah           &   1.0 &   1.000000 \\
Nykia           &   1.0 &   1.000000 \\
Nykole          &   1.0 &   1.000000 \\
Nyla            &   1.0 &   1.000000 \\
Nylah           &   1.0 &   0.638554 \\
Nylani          &   1.0 &   1.000000 \\
Nylee           &   1.0 &   1.000000 \\
Nyomi           &   1.0 &   0.923077 \\
Nyra            &   1.0 &   0.730159 \\
Nyree           &   1.0 &   0.454545 \\
Nysa            &   1.0 &   1.000000 \\
Nysha           &   1.0 &   1.000000 \\
Nyssa           &   1.0 &   0.294118 \\
Nyx             &   1.0 &   1.000000 \\
Oaklee          &   1.0 &   1.000000 \\
Oakleigh        &   1.0 &   1.000000 \\
Oakley          &   1.0 &   1.000000 \\
Oaklyn          &   1.0 &   1.000000 \\
Oaklynn         &   1.0 &   1.000000 \\
Oanh            &   1.0 &   0.700000 \\
Oasis           &   1.0 &   1.000000 \\
Obdulia         &   1.0 &   0.833333 \\
Ocean           &   1.0 &   1.000000 \\
Oceana          &   1.0 &   0.357143 \\
Oceanna         &   1.0 &   1.000000 \\
Octavia         &   1.0 &   0.716981 \\
October         &   1.0 &   0.714286 \\
Odalis          &   1.0 &   0.069767 \\
Odaliz          &   1.0 &   0.800000 \\
Odalys          &   1.0 &   0.039683 \\
Odelia          &   1.0 &   1.000000 \\
Odessa          &   1.0 &   0.294118 \\
Odette          &   1.0 &   0.809524 \\
Odilia          &   1.0 &   1.000000 \\
Odyssey         &   1.0 &   0.714286 \\
Ofelia          &   1.0 &   0.441860 \\
Ohana           &   1.0 &   1.000000 \\
Oksana          &   1.0 &   0.428571 \\
Ola             &   1.0 &   0.454545 \\
Olanna          &   1.0 &   1.000000 \\
Oleta           &   1.0 &   0.714286 \\
Olga            &   1.0 &   0.074766 \\
Oliana          &   1.0 &   1.000000 \\
Olimpia         &   1.0 &   0.833333 \\
Olina           &   1.0 &   1.000000 \\
Olinda          &   1.0 &   1.000000 \\
Oliva           &   1.0 &   0.454545 \\
Olive           &   1.0 &   1.000000 \\
Olivea          &   1.0 &   1.000000 \\
Olivia          &   1.0 &   0.835123 \\
Oliviagrace     &   1.0 &   1.000000 \\
Oliviah         &   1.0 &   0.714286 \\
Oliviamae       &   1.0 &   1.000000 \\
Oliviana        &   1.0 &   0.625000 \\
Oliviarose      &   1.0 &   0.625000 \\
Ollie           &   1.0 &   1.000000 \\
Olympia         &   1.0 &   0.411765 \\
Olyvia          &   1.0 &   0.416667 \\
Oma             &   1.0 &   1.000000 \\
Omar            &   1.0 &   0.333333 \\
Omunique        &   1.0 &   1.000000 \\
Ona             &   1.0 &   0.857143 \\
Ondine          &   1.0 &   1.000000 \\
Ondrea          &   1.0 &   1.000000 \\
Oneida          &   1.0 &   1.000000 \\
Onica           &   1.0 &   1.000000 \\
Onika           &   1.0 &   1.000000 \\
Onyx            &   1.0 &   1.000000 \\
Oona            &   1.0 &   0.500000 \\
Opal            &   1.0 &   1.000000 \\
Ophelia         &   1.0 &   1.000000 \\
Ora             &   1.0 &   0.333333 \\
Oralia          &   1.0 &   0.250000 \\
Ori             &   1.0 &   1.000000 \\
Oriana          &   1.0 &   0.370370 \\
Orianna         &   1.0 &   0.636364 \\
Orion           &   1.0 &   0.416667 \\
Orla            &   1.0 &   0.714286 \\
Orlene          &   1.0 &   1.000000 \\
Orly            &   1.0 &   0.750000 \\
Orquidea        &   1.0 &   1.000000 \\
Ortencia        &   1.0 &   1.000000 \\
Ortensia        &   1.0 &   1.000000 \\
Oscar           &   1.0 &   0.333333 \\
Osiris          &   1.0 &   0.250000 \\
Osmara          &   1.0 &   0.625000 \\
Otilia          &   1.0 &   0.416667 \\
Ottilie         &   1.0 &   1.000000 \\
Oviya           &   1.0 &   1.000000 \\
Owen            &   1.0 &   1.000000 \\
Oyuki           &   1.0 &   0.625000 \\
Ozzie           &   1.0 &   1.000000 \\
Ozzy            &   1.0 &   0.384615 \\
Pa              &   1.0 &   0.180000 \\
Pablo           &   1.0 &   1.000000 \\
Pachia          &   1.0 &   1.000000 \\
Padma           &   1.0 &   1.000000 \\
Paetyn          &   1.0 &   0.833333 \\
Page            &   1.0 &   0.312500 \\
Pahola          &   1.0 &   0.714286 \\
Pahoua          &   1.0 &   0.555556 \\
Paige           &   1.0 &   0.205534 \\
Paislee         &   1.0 &   0.571429 \\
Paisleigh       &   1.0 &   0.777778 \\
Paisley         &   1.0 &   0.749077 \\
Paiton          &   1.0 &   0.625000 \\
Paityn          &   1.0 &   0.233333 \\
Paitynn         &   1.0 &   1.000000 \\
Paizlee         &   1.0 &   1.000000 \\
Paizley         &   1.0 &   1.000000 \\
Pakou           &   1.0 &   1.000000 \\
Palak           &   1.0 &   1.000000 \\
Pallavi         &   1.0 &   0.555556 \\
Palma           &   1.0 &   1.000000 \\
Palmer          &   1.0 &   0.959184 \\
Palmira         &   1.0 &   0.545455 \\
Paloma          &   1.0 &   0.643478 \\
Pam             &   1.0 &   0.014577 \\
Pamala          &   1.0 &   0.147059 \\
Pamela          &   1.0 &   0.004955 \\
Pamelia         &   1.0 &   0.714286 \\
Pamella         &   1.0 &   0.350000 \\
Pamila          &   1.0 &   1.000000 \\
Pamla           &   1.0 &   1.000000 \\
Pandora         &   1.0 &   0.384615 \\
Pang            &   1.0 &   0.238095 \\
Panhia          &   1.0 &   0.625000 \\
Pansy           &   1.0 &   0.875000 \\
Paola           &   1.0 &   0.117207 \\
Paolina         &   1.0 &   1.000000 \\
Paradise        &   1.0 &   0.666667 \\
Pari            &   1.0 &   0.384615 \\
Paris           &   1.0 &   0.122047 \\
Parisa          &   1.0 &   0.538462 \\
Parissa         &   1.0 &   1.000000 \\
Parker          &   1.0 &   0.862857 \\
Parneet         &   1.0 &   0.666667 \\
Parnika         &   1.0 &   1.000000 \\
Parris          &   1.0 &   0.454545 \\
Parrish         &   1.0 &   1.000000 \\
Parveen         &   1.0 &   1.000000 \\
Pascha          &   1.0 &   1.000000 \\
Passion         &   1.0 &   0.416667 \\
Pat             &   1.0 &   0.018657 \\
Pati            &   1.0 &   1.000000 \\
Patience        &   1.0 &   0.166667 \\
Patrcia         &   1.0 &   1.000000 \\
Patrica         &   1.0 &   0.200000 \\
Patrice         &   1.0 &   0.044643 \\
Patricia        &   1.0 &   0.007614 \\
Patrick         &   1.0 &   0.636364 \\
Patrina         &   1.0 &   0.625000 \\
Patrisha        &   1.0 &   0.625000 \\
Patrisia        &   1.0 &   0.227273 \\
Patsie          &   1.0 &   1.000000 \\
Patsy           &   1.0 &   0.029940 \\
Patt            &   1.0 &   0.714286 \\
Patti           &   1.0 &   0.040179 \\
Pattie          &   1.0 &   0.227273 \\
Patty           &   1.0 &   0.019608 \\
Patzy           &   1.0 &   1.000000 \\
Paul            &   1.0 &   0.352941 \\
Paula           &   1.0 &   0.065051 \\
Paulene         &   1.0 &   1.000000 \\
Pauleth         &   1.0 &   1.000000 \\
Pauletta        &   1.0 &   0.555556 \\
Paulette        &   1.0 &   0.213793 \\
Paulina         &   1.0 &   0.318519 \\
Pauline         &   1.0 &   0.041667 \\
Paxton          &   1.0 &   0.833333 \\
Payal           &   1.0 &   0.625000 \\
Payden          &   1.0 &   1.000000 \\
Payge           &   1.0 &   1.000000 \\
Payson          &   1.0 &   0.833333 \\
Payten          &   1.0 &   0.214286 \\
Payton          &   1.0 &   0.266667 \\
Paytyn          &   1.0 &   1.000000 \\
Paz             &   1.0 &   1.000000 \\
Peace           &   1.0 &   0.833333 \\
Peaches         &   1.0 &   0.857143 \\
Pearl           &   1.0 &   0.564706 \\
Pearla          &   1.0 &   0.416667 \\
Pebbles         &   1.0 &   0.500000 \\
Pedro           &   1.0 &   0.500000 \\
Peg             &   1.0 &   1.000000 \\
Pegah           &   1.0 &   1.000000 \\
Peggi           &   1.0 &   1.000000 \\
Peggie          &   1.0 &   0.352941 \\
Peggy           &   1.0 &   0.015674 \\
Peighton        &   1.0 &   0.454545 \\
Pema            &   1.0 &   1.000000 \\
Penelope        &   1.0 &   0.897390 \\
Penina          &   1.0 &   1.000000 \\
Pennelope       &   1.0 &   1.000000 \\
Penney          &   1.0 &   0.545455 \\
Penni           &   1.0 &   0.357143 \\
Pennie          &   1.0 &   0.233333 \\
Penny           &   1.0 &   0.151420 \\
Pepper          &   1.0 &   0.482759 \\
Percilla        &   1.0 &   1.000000 \\
Peri            &   1.0 &   0.400000 \\
Perla           &   1.0 &   0.128931 \\
Perlita         &   1.0 &   0.545455 \\
Perri           &   1.0 &   0.300000 \\
Perris          &   1.0 &   1.000000 \\
Perry           &   1.0 &   1.000000 \\
Persephone      &   1.0 &   1.000000 \\
Persia          &   1.0 &   0.333333 \\
Peter           &   1.0 &   0.500000 \\
Petra           &   1.0 &   0.371429 \\
Petrina         &   1.0 &   0.555556 \\
Peyten          &   1.0 &   1.000000 \\
Peytin          &   1.0 &   1.000000 \\
Peyton          &   1.0 &   0.459016 \\
Phaedra         &   1.0 &   0.384615 \\
Phebe           &   1.0 &   1.000000 \\
Philicia        &   1.0 &   0.833333 \\
Philip          &   1.0 &   0.714286 \\
Philippa        &   1.0 &   0.727273 \\
Phillip         &   1.0 &   0.625000 \\
Phillis         &   1.0 &   0.600000 \\
Philomena       &   1.0 &   0.777778 \\
Phoebe          &   1.0 &   0.855491 \\
Phoenix         &   1.0 &   0.721311 \\
Phoenyx         &   1.0 &   0.700000 \\
Phoua           &   1.0 &   0.714286 \\
Phuc            &   1.0 &   1.000000 \\
Phung           &   1.0 &   0.555556 \\
Phuong          &   1.0 &   0.146341 \\
Phylicia        &   1.0 &   0.128205 \\
Phylis          &   1.0 &   0.800000 \\
Phyliss         &   1.0 &   1.000000 \\
Phyllis         &   1.0 &   0.011442 \\
Pia             &   1.0 &   1.000000 \\
Piedad          &   1.0 &   1.000000 \\
Pier            &   1.0 &   1.000000 \\
Pilar           &   1.0 &   0.268293 \\
Piper           &   1.0 &   0.502146 \\
Pippa           &   1.0 &   0.461538 \\
Pixie           &   1.0 &   0.714286 \\
Poet            &   1.0 &   1.000000 \\
Pola            &   1.0 &   1.000000 \\
Polett          &   1.0 &   1.000000 \\
Polette         &   1.0 &   0.833333 \\
Polina          &   1.0 &   0.625000 \\
Polly           &   1.0 &   0.096154 \\
Pollyanna       &   1.0 &   1.000000 \\
Pooja           &   1.0 &   0.280000 \\
Poonam          &   1.0 &   0.416667 \\
Poppy           &   1.0 &   1.000000 \\
Porcha          &   1.0 &   0.416667 \\
Porche          &   1.0 &   0.416667 \\
Porchea         &   1.0 &   1.000000 \\
Porscha         &   1.0 &   0.315789 \\
Porsche         &   1.0 &   0.192308 \\
Porschea        &   1.0 &   1.000000 \\
Porsha          &   1.0 &   0.135135 \\
Porshay         &   1.0 &   1.000000 \\
Porshia         &   1.0 &   0.833333 \\
Porter          &   1.0 &   1.000000 \\
Portia          &   1.0 &   0.172414 \\
Portland        &   1.0 &   1.000000 \\
Posey           &   1.0 &   1.000000 \\
Posie           &   1.0 &   1.000000 \\
Prabhleen       &   1.0 &   1.000000 \\
Prachi          &   1.0 &   0.700000 \\
Pragya          &   1.0 &   1.000000 \\
Prairie         &   1.0 &   1.000000 \\
Praise          &   1.0 &   0.833333 \\
Prajna          &   1.0 &   1.000000 \\
Pranathi        &   1.0 &   1.000000 \\
Pranavi         &   1.0 &   0.750000 \\
Precilla        &   1.0 &   0.875000 \\
Preciosa        &   1.0 &   0.625000 \\
Precious        &   1.0 &   0.061404 \\
Preet           &   1.0 &   1.000000 \\
Preethi         &   1.0 &   1.000000 \\
Preeti          &   1.0 &   1.000000 \\
Prerana         &   1.0 &   1.000000 \\
Prescilla       &   1.0 &   0.454545 \\
Preslee         &   1.0 &   0.294118 \\
Presleigh       &   1.0 &   0.714286 \\
Presley         &   1.0 &   0.572052 \\
Preslie         &   1.0 &   0.384615 \\
Pressley        &   1.0 &   1.000000 \\
Pricila         &   1.0 &   0.185185 \\
Pricilla        &   1.0 &   0.113636 \\
Primrose        &   1.0 &   0.733333 \\
Princesa        &   1.0 &   1.000000 \\
Princess        &   1.0 &   0.229508 \\
Princessa       &   1.0 &   0.600000 \\
Priscella       &   1.0 &   0.555556 \\
Priscila        &   1.0 &   0.197368 \\
Priscilla       &   1.0 &   0.132759 \\
Priseis         &   1.0 &   1.000000 \\
Prisha          &   1.0 &   0.733333 \\
Prisila         &   1.0 &   0.333333 \\
Prisilla        &   1.0 &   0.428571 \\
Prisma          &   1.0 &   0.384615 \\
Priya           &   1.0 &   0.277778 \\
Priyanka        &   1.0 &   0.227273 \\
Promise         &   1.0 &   0.740741 \\
Prudence        &   1.0 &   0.375000 \\
Puja            &   1.0 &   0.900000 \\
Puneet          &   1.0 &   0.714286 \\
Purity          &   1.0 &   1.000000 \\
Pyper           &   1.0 &   0.600000 \\
Qamar           &   1.0 &   1.000000 \\
Qiana           &   1.0 &   0.147059 \\
Quanisha        &   1.0 &   1.000000 \\
Queen           &   1.0 &   0.809524 \\
Queena          &   1.0 &   0.360000 \\
Queenie         &   1.0 &   0.250000 \\
Quetzal         &   1.0 &   1.000000 \\
Quetzali        &   1.0 &   0.357143 \\
Quetzalli       &   1.0 &   1.000000 \\
Quetzally       &   1.0 &   1.000000 \\
Quetzaly        &   1.0 &   1.000000 \\
Quiana          &   1.0 &   0.156250 \\
Quianna         &   1.0 &   1.000000 \\
Quin            &   1.0 &   1.000000 \\
Quincey         &   1.0 &   0.714286 \\
Quinci          &   1.0 &   1.000000 \\
Quincy          &   1.0 &   0.625000 \\
Quinlan         &   1.0 &   0.857143 \\
Quinley         &   1.0 &   1.000000 \\
Quinn           &   1.0 &   0.846154 \\
Quinnley        &   1.0 &   1.000000 \\
Quorra          &   1.0 &   0.625000 \\
Quyen           &   1.0 &   0.500000 \\
Quynh           &   1.0 &   0.857143 \\
Raahi           &   1.0 &   1.000000 \\
Rabecca         &   1.0 &   0.750000 \\
Rabia           &   1.0 &   1.000000 \\
Rachael         &   1.0 &   0.018797 \\
Rachal          &   1.0 &   0.857143 \\
Rachana         &   1.0 &   1.000000 \\
Racheal         &   1.0 &   0.151515 \\
Rachel          &   1.0 &   0.052885 \\
Rachelann       &   1.0 &   0.625000 \\
Rachelanne      &   1.0 &   1.000000 \\
Rachele         &   1.0 &   0.333333 \\
Rachell         &   1.0 &   0.411765 \\
Rachelle        &   1.0 &   0.028571 \\
Rachyl          &   1.0 &   1.000000 \\
Racine          &   1.0 &   0.454545 \\
Racquel         &   1.0 &   0.156250 \\
Radha           &   1.0 &   0.875000 \\
Radhika         &   1.0 &   0.384615 \\
Radiah          &   1.0 &   1.000000 \\
Rae             &   1.0 &   0.609756 \\
Raeana          &   1.0 &   1.000000 \\
Raeann          &   1.0 &   0.277778 \\
Raeanna         &   1.0 &   0.583333 \\
Raeanne         &   1.0 &   0.416667 \\
Raechel         &   1.0 &   0.294118 \\
Raegan          &   1.0 &   0.637931 \\
Rael            &   1.0 &   1.000000 \\
Raeleen         &   1.0 &   0.555556 \\
Raeleigh        &   1.0 &   1.000000 \\
Raelene         &   1.0 &   0.333333 \\
Raelin          &   1.0 &   1.000000 \\
Raelle          &   1.0 &   1.000000 \\
Raelyn          &   1.0 &   1.000000 \\
Raelynn         &   1.0 &   1.000000 \\
Raelynne        &   1.0 &   1.000000 \\
Raena           &   1.0 &   0.500000 \\
Raengel         &   1.0 &   1.000000 \\
Raeven          &   1.0 &   1.000000 \\
Raevyn          &   1.0 &   0.625000 \\
Raeya           &   1.0 &   0.700000 \\
Rafael          &   1.0 &   0.454545 \\
Rafaela         &   1.0 &   0.421053 \\
Rafaella        &   1.0 &   0.583333 \\
Raffaella       &   1.0 &   1.000000 \\
Ragan           &   1.0 &   0.833333 \\
Raghad          &   1.0 &   1.000000 \\
Raguel          &   1.0 &   0.833333 \\
Raha            &   1.0 &   1.000000 \\
Rahaf           &   1.0 &   0.875000 \\
Rahcel          &   1.0 &   1.000000 \\
Rahel           &   1.0 &   0.833333 \\
Rahil           &   1.0 &   1.000000 \\
Rahma           &   1.0 &   0.714286 \\
Rahwa           &   1.0 &   1.000000 \\
Rai             &   1.0 &   1.000000 \\
Raiden          &   1.0 &   0.857143 \\
Railey          &   1.0 &   1.000000 \\
Rain            &   1.0 &   0.823529 \\
Raina           &   1.0 &   0.555556 \\
Rainbow         &   1.0 &   0.454545 \\
Raine           &   1.0 &   0.666667 \\
Rainee          &   1.0 &   1.000000 \\
Rainey          &   1.0 &   0.875000 \\
Rainie          &   1.0 &   1.000000 \\
Raisa           &   1.0 &   0.333333 \\
Raissa          &   1.0 &   0.833333 \\
Raiya           &   1.0 &   0.857143 \\
Raiza           &   1.0 &   1.000000 \\
Raizel          &   1.0 &   0.625000 \\
Rajanee         &   1.0 &   1.000000 \\
Rajdeep         &   1.0 &   1.000000 \\
Rakel           &   1.0 &   1.000000 \\
Rakia           &   1.0 &   1.000000 \\
Raleigh         &   1.0 &   1.000000 \\
Rama            &   1.0 &   0.833333 \\
Ramandeep       &   1.0 &   1.000000 \\
Ramie           &   1.0 &   1.000000 \\
Ramiyah         &   1.0 &   1.000000 \\
Ramon           &   1.0 &   0.625000 \\
Ramona          &   1.0 &   0.200913 \\
Ramsey          &   1.0 &   0.888889 \\
Ramya           &   1.0 &   1.000000 \\
Rana            &   1.0 &   0.500000 \\
Ranae           &   1.0 &   0.666667 \\
Randa           &   1.0 &   0.454545 \\
Randal          &   1.0 &   1.000000 \\
Randall         &   1.0 &   0.600000 \\
Randee          &   1.0 &   0.357143 \\
Randi           &   1.0 &   0.065574 \\
Randie          &   1.0 &   0.416667 \\
Randy           &   1.0 &   0.185185 \\
Ranee           &   1.0 &   0.625000 \\
Raneem          &   1.0 &   1.000000 \\
Ranesha         &   1.0 &   1.000000 \\
Rani            &   1.0 &   0.777778 \\
Rania           &   1.0 &   0.555556 \\
Ranisha         &   1.0 &   0.750000 \\
Raniya          &   1.0 &   1.000000 \\
Raniyah         &   1.0 &   0.555556 \\
Ranya           &   1.0 &   0.750000 \\
Raquel          &   1.0 &   0.102094 \\
Raquelin        &   1.0 &   1.000000 \\
Raquell         &   1.0 &   1.000000 \\
Raquelle        &   1.0 &   0.875000 \\
Rashada         &   1.0 &   1.000000 \\
Rashanda        &   1.0 &   1.000000 \\
Rashaun         &   1.0 &   1.000000 \\
Rashawn         &   1.0 &   1.000000 \\
Rasheda         &   1.0 &   0.454545 \\
Rasheeda        &   1.0 &   0.454545 \\
Rasheedah       &   1.0 &   0.454545 \\
Rashel          &   1.0 &   0.466667 \\
Rashell         &   1.0 &   0.555556 \\
Rashelle        &   1.0 &   0.466667 \\
Rashida         &   1.0 &   0.200000 \\
Rashonda        &   1.0 &   0.714286 \\
Rathana         &   1.0 &   1.000000 \\
Raul            &   1.0 &   0.625000 \\
Raveena         &   1.0 &   0.384615 \\
Raven           &   1.0 &   0.462585 \\
Ravenna         &   1.0 &   1.000000 \\
Ravin           &   1.0 &   1.000000 \\
Ravina          &   1.0 &   1.000000 \\
Ravleen         &   1.0 &   1.000000 \\
Ravneet         &   1.0 &   0.714286 \\
Ravyn           &   1.0 &   0.384615 \\
Rawan           &   1.0 &   0.750000 \\
Ray             &   1.0 &   0.888889 \\
Raya            &   1.0 &   1.000000 \\
Rayah           &   1.0 &   0.909091 \\
Rayan           &   1.0 &   0.833333 \\
Rayana          &   1.0 &   0.555556 \\
Rayann          &   1.0 &   0.500000 \\
Rayanna         &   1.0 &   0.416667 \\
Rayanne         &   1.0 &   0.500000 \\
Raychel         &   1.0 &   0.461538 \\
Raychelle       &   1.0 &   1.000000 \\
Raye            &   1.0 &   0.714286 \\
Raygan          &   1.0 &   1.000000 \\
Rayla           &   1.0 &   1.000000 \\
Raylee          &   1.0 &   0.700000 \\
Rayleen         &   1.0 &   0.652174 \\
Rayleigh        &   1.0 &   0.857143 \\
Raylena         &   1.0 &   0.833333 \\
Raylene         &   1.0 &   0.280000 \\
Raylin          &   1.0 &   1.000000 \\
Raylina         &   1.0 &   1.000000 \\
Raylynn         &   1.0 &   0.571429 \\
Rayma           &   1.0 &   1.000000 \\
Raymond         &   1.0 &   0.500000 \\
Rayna           &   1.0 &   0.488372 \\
Rayne           &   1.0 &   0.758621 \\
Raynelle        &   1.0 &   1.000000 \\
Raynesha        &   1.0 &   0.833333 \\
Raynette        &   1.0 &   1.000000 \\
Raynisha        &   1.0 &   1.000000 \\
Rayonna         &   1.0 &   1.000000 \\
Rayshawn        &   1.0 &   1.000000 \\
Rayven          &   1.0 &   0.388889 \\
Razan           &   1.0 &   0.888889 \\
Rea             &   1.0 &   1.000000 \\
Reagan          &   1.0 &   0.600000 \\
Reana           &   1.0 &   1.000000 \\
Reann           &   1.0 &   0.857143 \\
Reanna          &   1.0 &   0.100000 \\
Reanne          &   1.0 &   0.500000 \\
Reannon         &   1.0 &   1.000000 \\
Reba            &   1.0 &   0.333333 \\
Rebbeca         &   1.0 &   0.833333 \\
Rebbecca        &   1.0 &   0.500000 \\
Rebeca          &   1.0 &   0.256410 \\
Rebecah         &   1.0 &   1.000000 \\
Rebecca         &   1.0 &   0.076534 \\
Rebeccah        &   1.0 &   0.272727 \\
Rebecka         &   1.0 &   0.208333 \\
Rebeckah        &   1.0 &   0.750000 \\
Rebeka          &   1.0 &   0.333333 \\
Rebekah         &   1.0 &   0.108225 \\
Rebekka         &   1.0 &   0.357143 \\
Rebekkah        &   1.0 &   0.416667 \\
Rebel           &   1.0 &   0.437500 \\
Rechelle        &   1.0 &   1.000000 \\
Reece           &   1.0 &   0.391304 \\
Reed            &   1.0 &   1.000000 \\
Reegan          &   1.0 &   0.857143 \\
Reem            &   1.0 &   1.000000 \\
Reema           &   1.0 &   0.666667 \\
Reena           &   1.0 &   0.250000 \\
Reese           &   1.0 &   0.593137 \\
Reet            &   1.0 &   0.454545 \\
Reeva           &   1.0 &   1.000000 \\
Reeya           &   1.0 &   1.000000 \\
Refugio         &   1.0 &   0.500000 \\
Regan           &   1.0 &   0.409091 \\
Regena          &   1.0 &   1.000000 \\
Regenia         &   1.0 &   1.000000 \\
Reggie          &   1.0 &   1.000000 \\
Reghan          &   1.0 &   1.000000 \\
Regina          &   1.0 &   0.429844 \\
Reginae         &   1.0 &   1.000000 \\
Reginald        &   1.0 &   1.000000 \\
Regine          &   1.0 &   0.400000 \\
Reginia         &   1.0 &   1.000000 \\
Rehanna         &   1.0 &   1.000000 \\
Rehmat          &   1.0 &   1.000000 \\
Rei             &   1.0 &   0.294118 \\
Reia            &   1.0 &   0.833333 \\
Reid            &   1.0 &   1.000000 \\
Reign           &   1.0 &   0.958904 \\
Reiko           &   1.0 &   0.750000 \\
Reiley          &   1.0 &   1.000000 \\
Reilly          &   1.0 &   0.125000 \\
Reina           &   1.0 &   0.827160 \\
Rekha           &   1.0 &   1.000000 \\
Relina          &   1.0 &   1.000000 \\
Rema            &   1.0 &   1.000000 \\
Remedy          &   1.0 &   0.555556 \\
Remi            &   1.0 &   1.000000 \\
Remie           &   1.0 &   1.000000 \\
Remington       &   1.0 &   0.826087 \\
Remmi           &   1.0 &   0.625000 \\
Remmie          &   1.0 &   1.000000 \\
Remmy           &   1.0 &   1.000000 \\
Remy            &   1.0 &   0.912500 \\
Ren             &   1.0 &   0.588235 \\
Rena            &   1.0 &   0.210526 \\
Renae           &   1.0 &   0.193548 \\
Renata          &   1.0 &   0.835227 \\
Renate          &   1.0 &   0.500000 \\
Renatta         &   1.0 &   0.666667 \\
Renay           &   1.0 &   0.454545 \\
Renda           &   1.0 &   1.000000 \\
Rene            &   1.0 &   0.043165 \\
Renea           &   1.0 &   0.500000 \\
Renee           &   1.0 &   0.038737 \\
Renell          &   1.0 &   1.000000 \\
Renesmae        &   1.0 &   1.000000 \\
Renesme         &   1.0 &   1.000000 \\
Renesmee        &   1.0 &   0.703704 \\
Renetta         &   1.0 &   1.000000 \\
Renette         &   1.0 &   1.000000 \\
Renie           &   1.0 &   1.000000 \\
Renika          &   1.0 &   1.000000 \\
Renisha         &   1.0 &   0.750000 \\
Renita          &   1.0 &   0.200000 \\
Renley          &   1.0 &   1.000000 \\
Renne           &   1.0 &   1.000000 \\
Rennie          &   1.0 &   1.000000 \\
Reonna          &   1.0 &   1.000000 \\
Resa            &   1.0 &   1.000000 \\
Reshma          &   1.0 &   1.000000 \\
Reta            &   1.0 &   0.500000 \\
Retal           &   1.0 &   1.000000 \\
Retha           &   1.0 &   0.545455 \\
Retta           &   1.0 &   1.000000 \\
Reva            &   1.0 &   1.000000 \\
Revel           &   1.0 &   1.000000 \\
Reverie         &   1.0 &   1.000000 \\
Rey             &   1.0 &   0.571429 \\
Reya            &   1.0 &   1.000000 \\
Reyanna         &   1.0 &   0.714286 \\
Reyes           &   1.0 &   1.000000 \\
Reylene         &   1.0 &   0.714286 \\
Reyna           &   1.0 &   0.457516 \\
Reynalda        &   1.0 &   1.000000 \\
Reyyan          &   1.0 &   1.000000 \\
Rhea            &   1.0 &   1.000000 \\
Rheana          &   1.0 &   1.000000 \\
Rheanna         &   1.0 &   0.411765 \\
Rheba           &   1.0 &   1.000000 \\
Rhema           &   1.0 &   1.000000 \\
Rheya           &   1.0 &   0.500000 \\
Rhian           &   1.0 &   0.714286 \\
Rhiana          &   1.0 &   0.357143 \\
Rhianna         &   1.0 &   0.087719 \\
Rhiannon        &   1.0 &   0.202247 \\
Rhianon         &   1.0 &   1.000000 \\
Rhiley          &   1.0 &   0.666667 \\
Rhina           &   1.0 &   1.000000 \\
Rhoda           &   1.0 &   0.217391 \\
Rhodora         &   1.0 &   1.000000 \\
Rhona           &   1.0 &   1.000000 \\
Rhonda          &   1.0 &   0.005903 \\
Rhondalyn       &   1.0 &   1.000000 \\
Rhonna          &   1.0 &   1.000000 \\
Rhyan           &   1.0 &   0.312500 \\
Rhylee          &   1.0 &   0.500000 \\
Rhylie          &   1.0 &   1.000000 \\
Rhylin          &   1.0 &   1.000000 \\
Rhys            &   1.0 &   1.000000 \\
Rhythm          &   1.0 &   1.000000 \\
Ria             &   1.0 &   0.390244 \\
Riah            &   1.0 &   0.714286 \\
Rian            &   1.0 &   0.350000 \\
Riana           &   1.0 &   0.269231 \\
Riane           &   1.0 &   0.833333 \\
Riann           &   1.0 &   0.625000 \\
Rianna          &   1.0 &   0.162162 \\
Riannah         &   1.0 &   1.000000 \\
Rianne          &   1.0 &   0.750000 \\
Rica            &   1.0 &   0.750000 \\
Ricarda         &   1.0 &   1.000000 \\
Ricardo         &   1.0 &   0.500000 \\
Ricci           &   1.0 &   0.636364 \\
Richa           &   1.0 &   0.750000 \\
Richard         &   1.0 &   0.151515 \\
Richele         &   1.0 &   0.416667 \\
Richelle        &   1.0 &   0.139535 \\
Rickey          &   1.0 &   1.000000 \\
Ricki           &   1.0 &   0.296296 \\
Rickie          &   1.0 &   0.800000 \\
Ricky           &   1.0 &   1.000000 \\
Rida            &   1.0 &   0.555556 \\
Riddhi          &   1.0 &   1.000000 \\
Riddhima        &   1.0 &   1.000000 \\
Ridhi           &   1.0 &   0.555556 \\
Ridhima         &   1.0 &   1.000000 \\
Ridley          &   1.0 &   0.777778 \\
Rielle          &   1.0 &   0.625000 \\
Rigoberto       &   1.0 &   1.000000 \\
Rihana          &   1.0 &   0.615385 \\
Rihanna         &   1.0 &   0.096774 \\
Rika            &   1.0 &   1.000000 \\
Riki            &   1.0 &   0.875000 \\
Rikki           &   1.0 &   0.092593 \\
Rilee           &   1.0 &   0.437500 \\
Rileigh         &   1.0 &   0.727273 \\
Riley           &   1.0 &   0.674603 \\
Rilla           &   1.0 &   1.000000 \\
Rilyn           &   1.0 &   0.636364 \\
Rilynn          &   1.0 &   0.782609 \\
Rima            &   1.0 &   0.500000 \\
Rin             &   1.0 &   0.833333 \\
Rina            &   1.0 &   0.217391 \\
Rinda           &   1.0 &   1.000000 \\
Rinoa           &   1.0 &   1.000000 \\
Rio             &   1.0 &   0.750000 \\
Riona           &   1.0 &   0.750000 \\
Riot            &   1.0 &   1.000000 \\
Ripley          &   1.0 &   1.000000 \\
Risa            &   1.0 &   0.333333 \\
Rise            &   1.0 &   0.714286 \\
Risha           &   1.0 &   1.000000 \\
Rishika         &   1.0 &   0.545455 \\
Rita            &   1.0 &   0.061069 \\
Rithika         &   1.0 &   0.714286 \\
Ritika          &   1.0 &   0.416667 \\
Riva            &   1.0 &   0.714286 \\
River           &   1.0 &   0.992857 \\
Riverlynn       &   1.0 &   1.000000 \\
Rivers          &   1.0 &   1.000000 \\
Rivka           &   1.0 &   0.888889 \\
Rivkah          &   1.0 &   1.000000 \\
Riya            &   1.0 &   0.732143 \\
Riyah           &   1.0 &   0.714286 \\
Riyan           &   1.0 &   1.000000 \\
Riyana          &   1.0 &   0.714286 \\
Roanne          &   1.0 &   1.000000 \\
Robbi           &   1.0 &   0.555556 \\
Robbie          &   1.0 &   0.116279 \\
Robbin          &   1.0 &   0.134615 \\
Robbyn          &   1.0 &   0.714286 \\
Roben           &   1.0 &   1.000000 \\
Robert          &   1.0 &   0.125000 \\
Roberta         &   1.0 &   0.014799 \\
Roberto         &   1.0 &   0.416667 \\
Robin           &   1.0 &   0.035740 \\
Robyn           &   1.0 &   0.166667 \\
Robyne          &   1.0 &   1.000000 \\
Robynn          &   1.0 &   0.714286 \\
Robynne         &   1.0 &   0.833333 \\
Rochel          &   1.0 &   0.714286 \\
Rochele         &   1.0 &   1.000000 \\
Rochell         &   1.0 &   0.666667 \\
Rochelle        &   1.0 &   0.025126 \\
Rocio           &   1.0 &   0.069388 \\
Rocky           &   1.0 &   1.000000 \\
Rocquel         &   1.0 &   1.000000 \\
Rodney          &   1.0 &   1.000000 \\
Rodolfo         &   1.0 &   1.000000 \\
Roger           &   1.0 &   1.000000 \\
Rogue           &   1.0 &   0.615385 \\
Rohini          &   1.0 &   1.000000 \\
Roisin          &   1.0 &   0.875000 \\
Rolanda         &   1.0 &   0.416667 \\
Rolinda         &   1.0 &   1.000000 \\
Roma            &   1.0 &   0.600000 \\
Roman           &   1.0 &   1.000000 \\
Romana          &   1.0 &   0.857143 \\
Rome            &   1.0 &   1.000000 \\
Romee           &   1.0 &   0.857143 \\
Romelia         &   1.0 &   0.454545 \\
Romi            &   1.0 &   1.000000 \\
Romie           &   1.0 &   1.000000 \\
Romina          &   1.0 &   1.000000 \\
Romona          &   1.0 &   0.750000 \\
Romy            &   1.0 &   0.974359 \\
Rona            &   1.0 &   0.217391 \\
Ronald          &   1.0 &   0.357143 \\
Ronda           &   1.0 &   0.025510 \\
Rondi           &   1.0 &   0.500000 \\
Ronee           &   1.0 &   1.000000 \\
Roneisha        &   1.0 &   1.000000 \\
Ronelle         &   1.0 &   1.000000 \\
Ronesha         &   1.0 &   0.857143 \\
Ronette         &   1.0 &   0.454545 \\
Roni            &   1.0 &   0.347826 \\
Ronica          &   1.0 &   0.625000 \\
Ronika          &   1.0 &   0.857143 \\
Ronin           &   1.0 &   1.000000 \\
Ronique         &   1.0 &   1.000000 \\
Ronisha         &   1.0 &   0.411765 \\
Ronit           &   1.0 &   0.833333 \\
Ronna           &   1.0 &   0.217391 \\
Ronnah          &   1.0 &   1.000000 \\
Ronnesha        &   1.0 &   1.000000 \\
Ronnette        &   1.0 &   0.416667 \\
Ronni           &   1.0 &   0.428571 \\
Ronnie          &   1.0 &   0.312500 \\
Ronniesha       &   1.0 &   1.000000 \\
Ronnisha        &   1.0 &   0.857143 \\
Roohi           &   1.0 &   1.000000 \\
Rooney          &   1.0 &   1.000000 \\
Rori            &   1.0 &   0.823529 \\
Rory            &   1.0 &   1.000000 \\
Rosa            &   1.0 &   0.145873 \\
Rosabella       &   1.0 &   0.500000 \\
Rosabelle       &   1.0 &   0.833333 \\
Rosaelena       &   1.0 &   1.000000 \\
Rosaicela       &   1.0 &   1.000000 \\
Rosaisela       &   1.0 &   0.320000 \\
Rosalba         &   1.0 &   0.109091 \\
Rosalee         &   1.0 &   0.791667 \\
Rosaleen        &   1.0 &   0.500000 \\
Rosaleigh       &   1.0 &   1.000000 \\
Rosali          &   1.0 &   1.000000 \\
Rosalia         &   1.0 &   1.000000 \\
Rosalie         &   1.0 &   0.743478 \\
Rosalin         &   1.0 &   1.000000 \\
Rosalina        &   1.0 &   1.000000 \\
Rosalind        &   1.0 &   0.154930 \\
Rosalinda       &   1.0 &   0.272727 \\
Rosaline        &   1.0 &   1.000000 \\
Rosalva         &   1.0 &   0.227273 \\
Rosaly          &   1.0 &   0.833333 \\
Rosalyn         &   1.0 &   0.755556 \\
Rosalynn        &   1.0 &   0.555556 \\
Rosamaria       &   1.0 &   0.172414 \\
Rosamond        &   1.0 &   0.500000 \\
Rosamund        &   1.0 &   1.000000 \\
Rosana          &   1.0 &   0.142857 \\
Rosangela       &   1.0 &   0.900000 \\
Rosangelica     &   1.0 &   0.379310 \\
Rosann          &   1.0 &   0.500000 \\
Rosanna         &   1.0 &   0.043860 \\
Rosanne         &   1.0 &   0.092593 \\
Rosario         &   1.0 &   0.277778 \\
Rosaura         &   1.0 &   0.185185 \\
Rosaysela       &   1.0 &   1.000000 \\
Rose            &   1.0 &   0.456053 \\
Roseana         &   1.0 &   1.000000 \\
Roseann         &   1.0 &   0.206897 \\
Roseanna        &   1.0 &   0.131579 \\
Roseanne        &   1.0 &   0.300000 \\
Roselani        &   1.0 &   1.000000 \\
Roselee         &   1.0 &   1.000000 \\
Roseleigh       &   1.0 &   1.000000 \\
Roselia         &   1.0 &   0.500000 \\
Roselie         &   1.0 &   0.833333 \\
Roselin         &   1.0 &   0.428571 \\
Roselinda       &   1.0 &   1.000000 \\
Roseline        &   1.0 &   1.000000 \\
Rosella         &   1.0 &   0.416667 \\
Roselle         &   1.0 &   0.833333 \\
Rosely          &   1.0 &   0.833333 \\
Roselyn         &   1.0 &   0.441860 \\
Roselyne        &   1.0 &   0.352941 \\
Roselynn        &   1.0 &   0.523810 \\
Rosemarie       &   1.0 &   0.147059 \\
Rosemary        &   1.0 &   0.295203 \\
Rosenda         &   1.0 &   0.714286 \\
Rosetta         &   1.0 &   0.312500 \\
Rosette         &   1.0 &   1.000000 \\
Rosey           &   1.0 &   1.000000 \\
Rosha           &   1.0 &   1.000000 \\
Roshanda        &   1.0 &   0.500000 \\
Roshawn         &   1.0 &   1.000000 \\
Roshell         &   1.0 &   1.000000 \\
Roshelle        &   1.0 &   1.000000 \\
Roshni          &   1.0 &   0.545455 \\
Roshon          &   1.0 &   1.000000 \\
Rosi            &   1.0 &   0.625000 \\
Rosibel         &   1.0 &   1.000000 \\
Rosicela        &   1.0 &   1.000000 \\
Rosie           &   1.0 &   0.649573 \\
Rosina          &   1.0 &   0.545455 \\
Rosio           &   1.0 &   0.092105 \\
Rosisela        &   1.0 &   1.000000 \\
Rosita          &   1.0 &   0.250000 \\
Roslyn          &   1.0 &   0.681818 \\
Roslynn         &   1.0 &   1.000000 \\
Rosmery         &   1.0 &   1.000000 \\
Ross            &   1.0 &   1.000000 \\
Rossana         &   1.0 &   0.277778 \\
Rosselyn        &   1.0 &   0.454545 \\
Rosslyn         &   1.0 &   0.714286 \\
Rossy           &   1.0 &   0.500000 \\
Rosy            &   1.0 &   0.529412 \\
Roux            &   1.0 &   1.000000 \\
Rowan           &   1.0 &   0.848739 \\
Rowen           &   1.0 &   0.722222 \\
Rowena          &   1.0 &   0.285714 \\
Rowyn           &   1.0 &   0.818182 \\
Rowynn          &   1.0 &   1.000000 \\
Roxan           &   1.0 &   0.625000 \\
Roxana          &   1.0 &   0.139535 \\
Roxane          &   1.0 &   0.172414 \\
Roxann          &   1.0 &   0.151515 \\
Roxanna         &   1.0 &   0.107143 \\
Roxanne         &   1.0 &   0.130435 \\
Roxi            &   1.0 &   1.000000 \\
Roxie           &   1.0 &   0.200000 \\
Roxsana         &   1.0 &   0.833333 \\
Roxy            &   1.0 &   0.235294 \\
Roy             &   1.0 &   1.000000 \\
Roya            &   1.0 &   0.388889 \\
Royal           &   1.0 &   0.533333 \\
Royale          &   1.0 &   1.000000 \\
Royalty         &   1.0 &   0.660000 \\
Royce           &   1.0 &   0.900000 \\
Rozalyn         &   1.0 &   0.714286 \\
Rozalynn        &   1.0 &   1.000000 \\
Rozanna         &   1.0 &   0.750000 \\
Rozanne         &   1.0 &   0.625000 \\
Rozlyn          &   1.0 &   0.875000 \\
Rozlynn         &   1.0 &   0.555556 \\
Rubani          &   1.0 &   1.000000 \\
Rubee           &   1.0 &   1.000000 \\
Ruben           &   1.0 &   0.583333 \\
Rubi            &   1.0 &   0.191781 \\
Rubicela        &   1.0 &   1.000000 \\
Rubie           &   1.0 &   0.375000 \\
Rubina          &   1.0 &   1.000000 \\
Rubit           &   1.0 &   1.000000 \\
Ruby            &   1.0 &   0.523553 \\
Rubyrose        &   1.0 &   1.000000 \\
Ruchi           &   1.0 &   1.000000 \\
Rudi            &   1.0 &   0.714286 \\
Rudy            &   1.0 &   0.666667 \\
Rue             &   1.0 &   0.941176 \\
Rufina          &   1.0 &   0.625000 \\
Ruhani          &   1.0 &   0.714286 \\
Ruhi            &   1.0 &   0.714286 \\
Rumaysa         &   1.0 &   1.000000 \\
Rumi            &   1.0 &   1.000000 \\
Runa            &   1.0 &   1.000000 \\
Rupa            &   1.0 &   1.000000 \\
Rupinder        &   1.0 &   1.000000 \\
Rusty           &   1.0 &   0.833333 \\
Rut             &   1.0 &   0.500000 \\
Ruth            &   1.0 &   0.194489 \\
Ruthann         &   1.0 &   0.555556 \\
Ruthanne        &   1.0 &   1.000000 \\
Ruthe           &   1.0 &   0.555556 \\
Ruthie          &   1.0 &   0.360000 \\
Ruthy           &   1.0 &   1.000000 \\
Ruvi            &   1.0 &   1.000000 \\
Rya             &   1.0 &   1.000000 \\
Ryah            &   1.0 &   0.545455 \\
Ryan            &   1.0 &   0.509804 \\
Ryane           &   1.0 &   0.833333 \\
Ryann           &   1.0 &   0.351852 \\
Ryanna          &   1.0 &   0.583333 \\
Ryanne          &   1.0 &   0.294118 \\
Ryatt           &   1.0 &   1.000000 \\
Ryder           &   1.0 &   0.517241 \\
Ryen            &   1.0 &   0.583333 \\
Ryenn           &   1.0 &   1.000000 \\
Ryhanna         &   1.0 &   1.000000 \\
Ryla            &   1.0 &   1.000000 \\
Rylan           &   1.0 &   0.875000 \\
Ryland          &   1.0 &   0.857143 \\
Rylann          &   1.0 &   1.000000 \\
Rylee           &   1.0 &   0.719101 \\
Ryleeann        &   1.0 &   1.000000 \\
Rylei           &   1.0 &   0.714286 \\
Ryleigh         &   1.0 &   0.666667 \\
Rylen           &   1.0 &   1.000000 \\
Ryley           &   1.0 &   0.437500 \\
Ryli            &   1.0 &   1.000000 \\
Rylie           &   1.0 &   0.675676 \\
Rylin           &   1.0 &   0.333333 \\
Rylinn          &   1.0 &   0.714286 \\
Rylyn           &   1.0 &   1.000000 \\
Rylynn          &   1.0 &   0.350000 \\
Rynn            &   1.0 &   1.000000 \\
Rysa            &   1.0 &   1.000000 \\
Saachi          &   1.0 &   0.500000 \\
Saadia          &   1.0 &   1.000000 \\
Saanjh          &   1.0 &   1.000000 \\
Saanvi          &   1.0 &   0.738462 \\
Saanvika        &   1.0 &   0.750000 \\
Saara           &   1.0 &   1.000000 \\
Saarah          &   1.0 &   1.000000 \\
Saavi           &   1.0 &   0.714286 \\
Saba            &   1.0 &   0.357143 \\
Sabah           &   1.0 &   0.833333 \\
Sabella         &   1.0 &   0.384615 \\
Sabina          &   1.0 &   0.566667 \\
Sabine          &   1.0 &   0.538462 \\
Sable           &   1.0 &   0.800000 \\
Sabra           &   1.0 &   0.400000 \\
Sabree          &   1.0 &   1.000000 \\
Sabreen         &   1.0 &   0.625000 \\
Sabreena        &   1.0 &   0.545455 \\
Sabrena         &   1.0 &   0.600000 \\
Sabrian         &   1.0 &   1.000000 \\
Sabrina         &   1.0 &   0.083123 \\
Sabrinna        &   1.0 &   0.714286 \\
Sabryna         &   1.0 &   0.642857 \\
Sacha           &   1.0 &   0.312500 \\
Sacheen         &   1.0 &   1.000000 \\
Sachi           &   1.0 &   0.384615 \\
Sachiko         &   1.0 &   0.384615 \\
Sada            &   1.0 &   1.000000 \\
Sadaf           &   1.0 &   0.800000 \\
Sadako          &   1.0 &   0.666667 \\
Sade            &   1.0 &   0.100000 \\
Sadee           &   1.0 &   0.750000 \\
Sadey           &   1.0 &   1.000000 \\
Sadhana         &   1.0 &   1.000000 \\
Sadia           &   1.0 &   1.000000 \\
Sadie           &   1.0 &   0.568966 \\
Sadiee          &   1.0 &   1.000000 \\
Sadira          &   1.0 &   1.000000 \\
Sady            &   1.0 &   1.000000 \\
Sadye           &   1.0 &   1.000000 \\
Safa            &   1.0 &   0.687500 \\
Safaa           &   1.0 &   1.000000 \\
Saffron         &   1.0 &   0.555556 \\
Safia           &   1.0 &   1.000000 \\
Safina          &   1.0 &   1.000000 \\
Safira          &   1.0 &   1.000000 \\
Safiya          &   1.0 &   0.692308 \\
Safiyah         &   1.0 &   0.833333 \\
Safiyyah        &   1.0 &   1.000000 \\
Sagan           &   1.0 &   1.000000 \\
Sage            &   1.0 &   1.000000 \\
Sagrario        &   1.0 &   1.000000 \\
Sahana          &   1.0 &   0.583333 \\
Sahar           &   1.0 &   0.520000 \\
Sahara          &   1.0 &   0.513514 \\
Sahasra         &   1.0 &   0.521739 \\
Sahian          &   1.0 &   1.000000 \\
Sahily          &   1.0 &   0.692308 \\
Sahira          &   1.0 &   0.777778 \\
Sahiti          &   1.0 &   1.000000 \\
Sahori          &   1.0 &   1.000000 \\
Sahra           &   1.0 &   0.625000 \\
Saida           &   1.0 &   0.333333 \\
Saidee          &   1.0 &   1.000000 \\
Saidy           &   1.0 &   1.000000 \\
Saige           &   1.0 &   1.000000 \\
Sailor          &   1.0 &   0.555556 \\
Saima           &   1.0 &   1.000000 \\
Saina           &   1.0 &   1.000000 \\
Saira           &   1.0 &   0.444444 \\
Saisha          &   1.0 &   0.857143 \\
Saiya           &   1.0 &   0.875000 \\
Saja            &   1.0 &   0.750000 \\
Sakari          &   1.0 &   1.000000 \\
Sakaye          &   1.0 &   1.000000 \\
Saki            &   1.0 &   0.833333 \\
Sakina          &   1.0 &   0.625000 \\
Sakinah         &   1.0 &   1.000000 \\
Sakshi          &   1.0 &   1.000000 \\
Sakura          &   1.0 &   0.285714 \\
Saleen          &   1.0 &   0.315789 \\
Saleena         &   1.0 &   0.666667 \\
Salem           &   1.0 &   0.754386 \\
Salena          &   1.0 &   0.125000 \\
Saliha          &   1.0 &   1.000000 \\
Salina          &   1.0 &   0.050000 \\
Salli           &   1.0 &   1.000000 \\
Sallie          &   1.0 &   0.161290 \\
Sally           &   1.0 &   0.037895 \\
Salma           &   1.0 &   0.376923 \\
Salome          &   1.0 &   1.000000 \\
Saloni          &   1.0 &   0.600000 \\
Salote          &   1.0 &   1.000000 \\
Salvador        &   1.0 &   0.714286 \\
Salwa           &   1.0 &   1.000000 \\
Sam             &   1.0 &   1.000000 \\
Sama            &   1.0 &   0.894737 \\
Samadhi         &   1.0 &   1.000000 \\
Samah           &   1.0 &   1.000000 \\
Samai           &   1.0 &   1.000000 \\
Samaira         &   1.0 &   0.640000 \\
Samaiya         &   1.0 &   0.666667 \\
Samanatha       &   1.0 &   0.833333 \\
Samanta         &   1.0 &   0.156250 \\
Samanth         &   1.0 &   1.000000 \\
Samantha        &   1.0 &   0.187298 \\
Samanthamarie   &   1.0 &   1.000000 \\
Samanthia       &   1.0 &   1.000000 \\
Samanvi         &   1.0 &   0.666667 \\
Samar           &   1.0 &   0.384615 \\
Samara          &   1.0 &   0.857143 \\
Samarah         &   1.0 &   0.636364 \\
Samari          &   1.0 &   0.454545 \\
Samaria         &   1.0 &   0.227273 \\
Samarra         &   1.0 &   1.000000 \\
Samatha         &   1.0 &   0.135135 \\
Samaya          &   1.0 &   0.413793 \\
Samayah         &   1.0 &   1.000000 \\
Samayra         &   1.0 &   1.000000 \\
Sameera         &   1.0 &   1.000000 \\
Samehesha       &   1.0 &   1.000000 \\
Samera          &   1.0 &   1.000000 \\
Samhita         &   1.0 &   0.500000 \\
Samhitha        &   1.0 &   1.000000 \\
Sami            &   1.0 &   0.555556 \\
Samia           &   1.0 &   0.437500 \\
Samiah          &   1.0 &   0.384615 \\
Samika          &   1.0 &   0.625000 \\
Samina          &   1.0 &   1.000000 \\
Samira          &   1.0 &   0.854167 \\
Samirah         &   1.0 &   0.600000 \\
Samiya          &   1.0 &   0.277778 \\
Samiyah         &   1.0 &   0.214286 \\
Sammantha       &   1.0 &   0.269231 \\
Sammi           &   1.0 &   1.000000 \\
Sammie          &   1.0 &   0.666667 \\
Sammy           &   1.0 &   0.625000 \\
Samone          &   1.0 &   0.454545 \\
Samra           &   1.0 &   0.833333 \\
Samreen         &   1.0 &   1.000000 \\
Samuel          &   1.0 &   0.375000 \\
Samya           &   1.0 &   0.333333 \\
Samyah          &   1.0 &   1.000000 \\
Samyra          &   1.0 &   0.636364 \\
Samyuktha       &   1.0 &   1.000000 \\
San             &   1.0 &   0.833333 \\
Sana            &   1.0 &   1.000000 \\
Sanaa           &   1.0 &   0.266667 \\
Sanae           &   1.0 &   1.000000 \\
Sanah           &   1.0 &   0.545455 \\
Sanai           &   1.0 &   0.236842 \\
Sanaia          &   1.0 &   1.000000 \\
Sanaii          &   1.0 &   1.000000 \\
Sanaiya         &   1.0 &   0.555556 \\
Sanam           &   1.0 &   0.454545 \\
Sanaya          &   1.0 &   0.304348 \\
Sanaz           &   1.0 &   1.000000 \\
Sandee          &   1.0 &   0.400000 \\
Sandeep         &   1.0 &   0.714286 \\
Sandhya         &   1.0 &   1.000000 \\
Sandi           &   1.0 &   0.089552 \\
Sandie          &   1.0 &   0.227273 \\
Sandra          &   1.0 &   0.016026 \\
Sandrine        &   1.0 &   1.000000 \\
Sandy           &   1.0 &   0.034063 \\
Sanem           &   1.0 &   0.666667 \\
Sang            &   1.0 &   0.833333 \\
Sania           &   1.0 &   0.238095 \\
Saniah          &   1.0 &   0.384615 \\
Sanika          &   1.0 &   0.777778 \\
Saniya          &   1.0 &   0.135135 \\
Saniyah         &   1.0 &   0.260870 \\
Sanjana         &   1.0 &   0.378378 \\
Sanjuana        &   1.0 &   0.777778 \\
Sanjuanita      &   1.0 &   0.857143 \\
Sanna           &   1.0 &   1.000000 \\
Sansa           &   1.0 &   1.000000 \\
Santa           &   1.0 &   0.416667 \\
Santana         &   1.0 &   0.851852 \\
Santiago        &   1.0 &   1.000000 \\
Santina         &   1.0 &   0.714286 \\
Santos          &   1.0 &   0.416667 \\
Sanvi           &   1.0 &   0.480000 \\
Sanvika         &   1.0 &   1.000000 \\
Sanvitha        &   1.0 &   1.000000 \\
Sanya           &   1.0 &   0.384615 \\
Sanyah          &   1.0 &   0.625000 \\
Saoirse         &   1.0 &   0.666667 \\
Saori           &   1.0 &   1.000000 \\
Saory           &   1.0 &   1.000000 \\
Saphira         &   1.0 &   0.470588 \\
Saphire         &   1.0 &   0.857143 \\
Sapna           &   1.0 &   0.625000 \\
Sapphira        &   1.0 &   1.000000 \\
Sapphire        &   1.0 &   1.000000 \\
Sara            &   1.0 &   0.154209 \\
Sarabeth        &   1.0 &   1.000000 \\
Sarafina        &   1.0 &   0.555556 \\
Sarah           &   1.0 &   0.087315 \\
Sarahann        &   1.0 &   1.000000 \\
Sarahgrace      &   1.0 &   1.000000 \\
Sarahi          &   1.0 &   0.658120 \\
Sarahjane       &   1.0 &   0.625000 \\
Sarahy          &   1.0 &   0.350000 \\
Sarai           &   1.0 &   0.547368 \\
Saraiah         &   1.0 &   1.000000 \\
Saraih          &   1.0 &   1.000000 \\
Saralee         &   1.0 &   1.000000 \\
Saralyn         &   1.0 &   0.833333 \\
Saran           &   1.0 &   1.000000 \\
Saranya         &   1.0 &   0.750000 \\
Saray           &   1.0 &   0.133333 \\
Saraya          &   1.0 &   0.909091 \\
Sarayah         &   1.0 &   0.555556 \\
Sarayu          &   1.0 &   0.538462 \\
Sareen          &   1.0 &   0.750000 \\
Sareena         &   1.0 &   0.714286 \\
Sarena          &   1.0 &   0.277778 \\
Sarenity        &   1.0 &   1.000000 \\
Sargun          &   1.0 &   0.750000 \\
Sarha           &   1.0 &   1.000000 \\
Sari            &   1.0 &   0.545455 \\
Saria           &   1.0 &   1.000000 \\
Sariah          &   1.0 &   0.318841 \\
Sarika          &   1.0 &   1.000000 \\
Sarin           &   1.0 &   0.857143 \\
Sarina          &   1.0 &   0.250000 \\
Sarine          &   1.0 &   1.000000 \\
Sarita          &   1.0 &   0.263158 \\
Sariya          &   1.0 &   0.583333 \\
Sariyah         &   1.0 &   0.722222 \\
Saron           &   1.0 &   0.625000 \\
Sarra           &   1.0 &   0.714286 \\
Sarrah          &   1.0 &   0.416667 \\
Saryah          &   1.0 &   1.000000 \\
Sascha          &   1.0 &   0.333333 \\
Sasha           &   1.0 &   0.352381 \\
Sasheen         &   1.0 &   1.000000 \\
Saskia          &   1.0 &   0.777778 \\
Sativa          &   1.0 &   0.625000 \\
Satori          &   1.0 &   1.000000 \\
Satsuki         &   1.0 &   1.000000 \\
Satya           &   1.0 &   0.750000 \\
Saul            &   1.0 &   1.000000 \\
Saumya          &   1.0 &   1.000000 \\
Saundra         &   1.0 &   0.095238 \\
Savana          &   1.0 &   0.135135 \\
Savanah         &   1.0 &   0.112500 \\
Savanna         &   1.0 &   0.235632 \\
Savannah        &   1.0 &   0.330549 \\
Savi            &   1.0 &   1.000000 \\
Savina          &   1.0 &   0.357143 \\
Savonna         &   1.0 &   1.000000 \\
Savreen         &   1.0 &   1.000000 \\
Savvy           &   1.0 &   0.666667 \\
Sawyer          &   1.0 &   0.731183 \\
Saya            &   1.0 &   0.277778 \\
Sayaka          &   1.0 &   1.000000 \\
Sayana          &   1.0 &   1.000000 \\
Sayda           &   1.0 &   0.833333 \\
Sayde           &   1.0 &   0.833333 \\
Saydee          &   1.0 &   0.333333 \\
Saydie          &   1.0 &   0.750000 \\
Sayla           &   1.0 &   0.857143 \\
Saylah          &   1.0 &   1.000000 \\
Sayler          &   1.0 &   0.750000 \\
Saylor          &   1.0 &   0.922078 \\
Sayoko          &   1.0 &   1.000000 \\
Sayra           &   1.0 &   0.155556 \\
Sayuri          &   1.0 &   0.242424 \\
Scarlet         &   1.0 &   0.471591 \\
Scarleth        &   1.0 &   0.600000 \\
Scarlett        &   1.0 &   0.778872 \\
Scarlette       &   1.0 &   0.586667 \\
Scarlettrose    &   1.0 &   0.750000 \\
Scherrie        &   1.0 &   1.000000 \\
Scheryl         &   1.0 &   1.000000 \\
Schuyler        &   1.0 &   0.833333 \\
Schyler         &   1.0 &   1.000000 \\
Scotland        &   1.0 &   0.714286 \\
Scotlyn         &   1.0 &   0.714286 \\
Scott           &   1.0 &   0.400000 \\
Scottie         &   1.0 &   1.000000 \\
Scottlyn        &   1.0 &   1.000000 \\
Scout           &   1.0 &   0.685714 \\
Seaira          &   1.0 &   1.000000 \\
Sean            &   1.0 &   0.227273 \\
Seana           &   1.0 &   0.238095 \\
Seaneen         &   1.0 &   1.000000 \\
Seanna          &   1.0 &   0.312500 \\
Season          &   1.0 &   0.400000 \\
Sebastian       &   1.0 &   0.833333 \\
Sebrina         &   1.0 &   0.416667 \\
Secilia         &   1.0 &   0.625000 \\
Secret          &   1.0 &   1.000000 \\
Sedona          &   1.0 &   0.666667 \\
Sedra           &   1.0 &   1.000000 \\
See             &   1.0 &   0.421053 \\
Seema           &   1.0 &   0.636364 \\
Seerat          &   1.0 &   0.600000 \\
Sefora          &   1.0 &   1.000000 \\
Sehaj           &   1.0 &   0.529412 \\
Seher           &   1.0 &   1.000000 \\
Seidy           &   1.0 &   0.666667 \\
Sejal           &   1.0 &   0.777778 \\
Sekani          &   1.0 &   1.000000 \\
Sela            &   1.0 &   0.277778 \\
Selah           &   1.0 &   0.938776 \\
Seleen          &   1.0 &   1.000000 \\
Seleena         &   1.0 &   1.000000 \\
Selena          &   1.0 &   0.256809 \\
Selene          &   1.0 &   0.908257 \\
Seleni          &   1.0 &   1.000000 \\
Selenia         &   1.0 &   0.555556 \\
Selenne         &   1.0 &   0.636364 \\
Seleste         &   1.0 &   0.583333 \\
Selia           &   1.0 &   0.833333 \\
Selin           &   1.0 &   0.500000 \\
Selina          &   1.0 &   0.144186 \\
Selinda         &   1.0 &   1.000000 \\
Seline          &   1.0 &   0.500000 \\
Selma           &   1.0 &   0.470588 \\
Semaj           &   1.0 &   0.666667 \\
Semira          &   1.0 &   0.714286 \\
Sena            &   1.0 &   0.692308 \\
Senaida         &   1.0 &   0.625000 \\
Sendi           &   1.0 &   1.000000 \\
Sendy           &   1.0 &   0.416667 \\
Seneca          &   1.0 &   0.500000 \\
Seng            &   1.0 &   1.000000 \\
Senia           &   1.0 &   1.000000 \\
Seniyah         &   1.0 &   1.000000 \\
Senna           &   1.0 &   0.700000 \\
Sensi           &   1.0 &   1.000000 \\
Senta           &   1.0 &   1.000000 \\
Sephora         &   1.0 &   0.555556 \\
September       &   1.0 &   0.888889 \\
Sequoia         &   1.0 &   0.703704 \\
Sequoyah        &   1.0 &   1.000000 \\
Sera            &   1.0 &   0.294118 \\
Serafina        &   1.0 &   0.560000 \\
Serah           &   1.0 &   0.857143 \\
Seraiah         &   1.0 &   1.000000 \\
Seraphina       &   1.0 &   0.862069 \\
Seraphine       &   1.0 &   0.714286 \\
Serayah         &   1.0 &   1.000000 \\
Sereen          &   1.0 &   1.000000 \\
Sereena         &   1.0 &   1.000000 \\
Seren           &   1.0 &   0.384615 \\
Serena          &   1.0 &   0.343662 \\
Serenah         &   1.0 &   1.000000 \\
Serene          &   1.0 &   0.518519 \\
Sereniti        &   1.0 &   1.000000 \\
Serenity        &   1.0 &   0.636842 \\
Sergio          &   1.0 &   0.428571 \\
Seri            &   1.0 &   1.000000 \\
Seriah          &   1.0 &   1.000000 \\
Serina          &   1.0 &   0.081967 \\
Serinity        &   1.0 &   0.500000 \\
Seriyah         &   1.0 &   1.000000 \\
Serly           &   1.0 &   1.000000 \\
Serra           &   1.0 &   0.625000 \\
Serrena         &   1.0 &   1.000000 \\
Serrina         &   1.0 &   1.000000 \\
Seryna          &   1.0 &   1.000000 \\
Setareh         &   1.0 &   0.600000 \\
Setayesh        &   1.0 &   1.000000 \\
Seth            &   1.0 &   1.000000 \\
Setsuko         &   1.0 &   0.545455 \\
Sevana          &   1.0 &   0.500000 \\
Sevanna         &   1.0 &   1.000000 \\
Seven           &   1.0 &   0.333333 \\
Sevilla         &   1.0 &   1.000000 \\
Sevyn           &   1.0 &   1.000000 \\
Seylah          &   1.0 &   1.000000 \\
Sha             &   1.0 &   1.000000 \\
Shaaron         &   1.0 &   1.000000 \\
Shabana         &   1.0 &   1.000000 \\
Shabnam         &   1.0 &   0.500000 \\
Shada           &   1.0 &   1.000000 \\
Shadae          &   1.0 &   0.714286 \\
Shade           &   1.0 &   0.714286 \\
Shaden          &   1.0 &   1.000000 \\
Shadi           &   1.0 &   0.700000 \\
Shadia          &   1.0 &   1.000000 \\
Shadonna        &   1.0 &   1.000000 \\
Shady           &   1.0 &   0.750000 \\
Shae            &   1.0 &   0.550000 \\
Shaela          &   1.0 &   0.600000 \\
Shaelyn         &   1.0 &   0.277778 \\
Shaelynn        &   1.0 &   0.857143 \\
Shaena          &   1.0 &   1.000000 \\
Shahd           &   1.0 &   0.714286 \\
Shaheen         &   1.0 &   1.000000 \\
Shahrzad        &   1.0 &   1.000000 \\
Shai            &   1.0 &   0.625000 \\
Shaiann         &   1.0 &   0.833333 \\
Shaianne        &   1.0 &   0.777778 \\
Shaila          &   1.0 &   0.080000 \\
Shailee         &   1.0 &   1.000000 \\
Shailene        &   1.0 &   1.000000 \\
Shaily          &   1.0 &   1.000000 \\
Shailyn         &   1.0 &   1.000000 \\
Shaina          &   1.0 &   0.080000 \\
Shaira          &   1.0 &   0.857143 \\
Shakia          &   1.0 &   1.000000 \\
Shakila         &   1.0 &   1.000000 \\
Shakira         &   1.0 &   0.166667 \\
Shakiyla        &   1.0 &   0.800000 \\
Shakthi         &   1.0 &   1.000000 \\
Shakti          &   1.0 &   0.714286 \\
Shala           &   1.0 &   0.416667 \\
Shalamar        &   1.0 &   1.000000 \\
Shalana         &   1.0 &   1.000000 \\
Shalanda        &   1.0 &   0.833333 \\
Shalane         &   1.0 &   1.000000 \\
Shalee          &   1.0 &   1.000000 \\
Shaleen         &   1.0 &   0.714286 \\
Shaleena        &   1.0 &   1.000000 \\
Shalena         &   1.0 &   1.000000 \\
Shalene         &   1.0 &   0.500000 \\
Shalia          &   1.0 &   1.000000 \\
Shalimar        &   1.0 &   0.857143 \\
Shalina         &   1.0 &   0.625000 \\
Shalini         &   1.0 &   0.625000 \\
Shalisa         &   1.0 &   0.750000 \\
Shalise         &   1.0 &   1.000000 \\
Shalita         &   1.0 &   1.000000 \\
Shaliyah        &   1.0 &   1.000000 \\
Shalom          &   1.0 &   1.000000 \\
Shalon          &   1.0 &   0.375000 \\
Shalonda        &   1.0 &   0.500000 \\
Shalyn          &   1.0 &   0.555556 \\
Shalynn         &   1.0 &   0.500000 \\
Shamara         &   1.0 &   0.714286 \\
Shamari         &   1.0 &   1.000000 \\
Shameika        &   1.0 &   1.000000 \\
Shameka         &   1.0 &   0.200000 \\
Shamia          &   1.0 &   1.000000 \\
Shamica         &   1.0 &   1.000000 \\
Shamika         &   1.0 &   0.214286 \\
Shamila         &   1.0 &   1.000000 \\
Shamira         &   1.0 &   1.000000 \\
Shamra          &   1.0 &   1.000000 \\
Shams           &   1.0 &   1.000000 \\
Shamya          &   1.0 &   1.000000 \\
Shan            &   1.0 &   0.714286 \\
Shana           &   1.0 &   0.044118 \\
Shanae          &   1.0 &   0.131579 \\
Shanah          &   1.0 &   1.000000 \\
Shanan          &   1.0 &   0.750000 \\
Shanay          &   1.0 &   0.500000 \\
Shanaya         &   1.0 &   0.377358 \\
Shanda          &   1.0 &   0.200000 \\
Shandale        &   1.0 &   1.000000 \\
Shandell        &   1.0 &   1.000000 \\
Shandi          &   1.0 &   0.384615 \\
Shandra         &   1.0 &   0.333333 \\
Shandy          &   1.0 &   1.000000 \\
Shane           &   1.0 &   0.200000 \\
Shanea          &   1.0 &   1.000000 \\
Shanece         &   1.0 &   1.000000 \\
Shanee          &   1.0 &   0.238095 \\
Shaneen         &   1.0 &   0.833333 \\
Shaneice        &   1.0 &   1.000000 \\
Shaneka         &   1.0 &   0.888889 \\
Shanel          &   1.0 &   0.294118 \\
Shanell         &   1.0 &   0.320000 \\
Shanelle        &   1.0 &   0.241379 \\
Shanequa        &   1.0 &   1.000000 \\
Shanese         &   1.0 &   1.000000 \\
Shanesha        &   1.0 &   1.000000 \\
Shanetta        &   1.0 &   1.000000 \\
Shanette        &   1.0 &   0.833333 \\
Shaney          &   1.0 &   1.000000 \\
Shani           &   1.0 &   0.166667 \\
Shania          &   1.0 &   0.093750 \\
Shaniah         &   1.0 &   0.555556 \\
Shanice         &   1.0 &   0.055556 \\
Shanie          &   1.0 &   0.833333 \\
Shaniece        &   1.0 &   0.285714 \\
Shanik          &   1.0 &   1.000000 \\
Shanika         &   1.0 &   0.217391 \\
Shanin          &   1.0 &   1.000000 \\
Shaniqua        &   1.0 &   0.260870 \\
Shanique        &   1.0 &   0.500000 \\
Shanise         &   1.0 &   0.666667 \\
Shanisha        &   1.0 &   1.000000 \\
Shanita         &   1.0 &   0.625000 \\
Shaniya         &   1.0 &   0.384615 \\
Shaniyah        &   1.0 &   0.857143 \\
Shanley         &   1.0 &   1.000000 \\
Shanna          &   1.0 &   0.035714 \\
Shannah         &   1.0 &   0.714286 \\
Shannan         &   1.0 &   0.111111 \\
Shannel         &   1.0 &   0.312500 \\
Shannell        &   1.0 &   1.000000 \\
Shannen         &   1.0 &   0.185185 \\
Shannon         &   1.0 &   0.004242 \\
Shannyn         &   1.0 &   1.000000 \\
Shanon          &   1.0 &   0.146341 \\
Shanta          &   1.0 &   0.315789 \\
Shantae         &   1.0 &   0.875000 \\
Shantal         &   1.0 &   0.333333 \\
Shantall        &   1.0 &   1.000000 \\
Shantay         &   1.0 &   0.857143 \\
Shante          &   1.0 &   0.151515 \\
Shantee         &   1.0 &   1.000000 \\
Shantel         &   1.0 &   0.104167 \\
Shantell        &   1.0 &   0.333333 \\
Shantelle       &   1.0 &   0.428571 \\
Shanti          &   1.0 &   0.277778 \\
Shanty          &   1.0 &   1.000000 \\
Shanvi          &   1.0 &   0.888889 \\
Shanya          &   1.0 &   0.777778 \\
Shanyn          &   1.0 &   1.000000 \\
Shanzay         &   1.0 &   1.000000 \\
Shaquille       &   1.0 &   1.000000 \\
Shaquita        &   1.0 &   0.875000 \\
Shara           &   1.0 &   0.240000 \\
Sharae          &   1.0 &   0.500000 \\
Sharah          &   1.0 &   1.000000 \\
Sharai          &   1.0 &   0.875000 \\
Sharan          &   1.0 &   0.714286 \\
Sharanya        &   1.0 &   0.714286 \\
Sharay          &   1.0 &   0.833333 \\
Sharaya         &   1.0 &   1.000000 \\
Sharayah        &   1.0 &   0.900000 \\
Sharda          &   1.0 &   1.000000 \\
Shardae         &   1.0 &   0.300000 \\
Shardai         &   1.0 &   1.000000 \\
Sharday         &   1.0 &   0.500000 \\
Sharde          &   1.0 &   0.333333 \\
Sharee          &   1.0 &   0.384615 \\
Shareen         &   1.0 &   0.625000 \\
Sharell         &   1.0 &   0.750000 \\
Sharen          &   1.0 &   0.300000 \\
Sharese         &   1.0 &   1.000000 \\
Sharhonda       &   1.0 &   0.857143 \\
Shari           &   1.0 &   0.014663 \\
Shariah         &   1.0 &   1.000000 \\
Sharice         &   1.0 &   1.000000 \\
Sharie          &   1.0 &   0.454545 \\
Sharika         &   1.0 &   0.833333 \\
Sharilyn        &   1.0 &   0.416667 \\
Sharina         &   1.0 &   0.454545 \\
Sharise         &   1.0 &   0.625000 \\
Sharissa        &   1.0 &   1.000000 \\
Sharisse        &   1.0 &   0.857143 \\
Sharita         &   1.0 &   0.818182 \\
Sharla          &   1.0 &   0.523810 \\
Sharlee         &   1.0 &   1.000000 \\
Sharleen        &   1.0 &   0.428571 \\
Sharlene        &   1.0 &   0.147059 \\
Sharlet         &   1.0 &   1.000000 \\
Sharlize        &   1.0 &   1.000000 \\
Sharlotte       &   1.0 &   1.000000 \\
Sharlyn         &   1.0 &   0.846154 \\
Sharmaine       &   1.0 &   0.857143 \\
Sharman         &   1.0 &   0.500000 \\
Sharnae         &   1.0 &   1.000000 \\
Sharnell        &   1.0 &   1.000000 \\
Sharol          &   1.0 &   0.714286 \\
Sharolyn        &   1.0 &   0.666667 \\
Sharon          &   1.0 &   0.010739 \\
Sharona         &   1.0 &   1.000000 \\
Sharonda        &   1.0 &   0.388889 \\
Sharone         &   1.0 &   1.000000 \\
Sharra          &   1.0 &   1.000000 \\
Sharrell        &   1.0 &   1.000000 \\
Sharri          &   1.0 &   0.500000 \\
Sharrie         &   1.0 &   0.625000 \\
Sharron         &   1.0 &   0.060606 \\
Sharry          &   1.0 &   1.000000 \\
Sharvi          &   1.0 &   1.000000 \\
Sharyl          &   1.0 &   0.277778 \\
Sharyn          &   1.0 &   0.100000 \\
Shasta          &   1.0 &   0.200000 \\
Shastelyn       &   1.0 &   1.000000 \\
Shastina        &   1.0 &   1.000000 \\
Shatara         &   1.0 &   0.500000 \\
Shaterra        &   1.0 &   1.000000 \\
Shatoya         &   1.0 &   0.857143 \\
Shaun           &   1.0 &   0.263158 \\
Shauna          &   1.0 &   0.030675 \\
Shaunda         &   1.0 &   0.800000 \\
Shaundra        &   1.0 &   0.777778 \\
Shauni          &   1.0 &   1.000000 \\
Shaunice        &   1.0 &   0.500000 \\
Shaunna         &   1.0 &   0.217391 \\
Shaunta         &   1.0 &   0.714286 \\
Shauntae        &   1.0 &   0.714286 \\
Shauntay        &   1.0 &   1.000000 \\
Shaunte         &   1.0 &   0.352941 \\
Shauntel        &   1.0 &   0.714286 \\
Shavaun         &   1.0 &   1.000000 \\
Shavon          &   1.0 &   0.125000 \\
Shavona         &   1.0 &   1.000000 \\
Shavonda        &   1.0 &   0.875000 \\
Shavone         &   1.0 &   1.000000 \\
Shavonn         &   1.0 &   1.000000 \\
Shavonna        &   1.0 &   1.000000 \\
Shavonne        &   1.0 &   0.217391 \\
Shawana         &   1.0 &   0.454545 \\
Shawanna        &   1.0 &   1.000000 \\
Shawn           &   1.0 &   0.025381 \\
Shawna          &   1.0 &   0.016287 \\
Shawnda         &   1.0 &   0.263158 \\
Shawndee        &   1.0 &   1.000000 \\
Shawndra        &   1.0 &   1.000000 \\
Shawne          &   1.0 &   0.714286 \\
Shawnee         &   1.0 &   0.185185 \\
Shawneen        &   1.0 &   1.000000 \\
Shawnette       &   1.0 &   0.750000 \\
Shawnice        &   1.0 &   1.000000 \\
Shawnie         &   1.0 &   0.875000 \\
Shawniece       &   1.0 &   1.000000 \\
Shawnna         &   1.0 &   0.185185 \\
Shawnta         &   1.0 &   0.238095 \\
Shawntae        &   1.0 &   0.583333 \\
Shawntay        &   1.0 &   1.000000 \\
Shawnte         &   1.0 &   0.193548 \\
Shawntee        &   1.0 &   0.714286 \\
Shawntel        &   1.0 &   0.500000 \\
Shawntell       &   1.0 &   1.000000 \\
Shay            &   1.0 &   0.893617 \\
Shaya           &   1.0 &   1.000000 \\
Shayal          &   1.0 &   1.000000 \\
Shayanne        &   1.0 &   1.000000 \\
Shayda          &   1.0 &   0.625000 \\
Shaye           &   1.0 &   0.642857 \\
Shayla          &   1.0 &   0.166667 \\
Shaylah         &   1.0 &   0.625000 \\
Shaylee         &   1.0 &   0.285714 \\
Shayleen        &   1.0 &   0.833333 \\
Shaylene        &   1.0 &   0.600000 \\
Shaylin         &   1.0 &   0.555556 \\
Shaylyn         &   1.0 &   0.555556 \\
Shaylynn        &   1.0 &   0.454545 \\
Shayna          &   1.0 &   0.062500 \\
Shayne          &   1.0 &   0.312500 \\
Shazia          &   1.0 &   1.000000 \\
Shea            &   1.0 &   0.431818 \\
Shealyn         &   1.0 &   0.714286 \\
Shealynn        &   1.0 &   1.000000 \\
Sheana          &   1.0 &   1.000000 \\
Sheba           &   1.0 &   0.833333 \\
Sheccid         &   1.0 &   0.466667 \\
Sheela          &   1.0 &   0.625000 \\
Sheena          &   1.0 &   0.019608 \\
Sheetal         &   1.0 &   1.000000 \\
Sheeva          &   1.0 &   1.000000 \\
Shefali         &   1.0 &   1.000000 \\
Sheida          &   1.0 &   1.000000 \\
Sheila          &   1.0 &   0.030035 \\
Sheilah         &   1.0 &   0.666667 \\
Sheilla         &   1.0 &   1.000000 \\
Sheily          &   1.0 &   0.666667 \\
Shekinah        &   1.0 &   0.833333 \\
Shela           &   1.0 &   0.666667 \\
Shelbee         &   1.0 &   1.000000 \\
Shelbey         &   1.0 &   1.000000 \\
Shelbi          &   1.0 &   0.135593 \\
Shelbie         &   1.0 &   0.116279 \\
Shelby          &   1.0 &   0.046316 \\
Shelena         &   1.0 &   1.000000 \\
Shelene         &   1.0 &   1.000000 \\
Sheli           &   1.0 &   0.636364 \\
Shelia          &   1.0 &   0.131148 \\
Shelina         &   1.0 &   1.000000 \\
Shella          &   1.0 &   1.000000 \\
Shellby         &   1.0 &   1.000000 \\
Shelle          &   1.0 &   0.714286 \\
Shellee         &   1.0 &   0.625000 \\
Shelley         &   1.0 &   0.018041 \\
Shelli          &   1.0 &   0.115942 \\
Shellie         &   1.0 &   0.107692 \\
Shellsea        &   1.0 &   1.000000 \\
Shelly          &   1.0 &   0.013183 \\
Shelsea         &   1.0 &   0.818182 \\
Shelsey         &   1.0 &   1.000000 \\
Shelsy          &   1.0 &   0.428571 \\
Shemeka         &   1.0 &   0.857143 \\
Shemika         &   1.0 &   1.000000 \\
Shena           &   1.0 &   0.588235 \\
Shenae          &   1.0 &   1.000000 \\
Shenandoah      &   1.0 &   1.000000 \\
Shenee          &   1.0 &   1.000000 \\
Shenelle        &   1.0 &   1.000000 \\
Sheng           &   1.0 &   0.444444 \\
Shenika         &   1.0 &   1.000000 \\
Shenise         &   1.0 &   1.000000 \\
Shenna          &   1.0 &   1.000000 \\
Shera           &   1.0 &   0.428571 \\
Sherah          &   1.0 &   1.000000 \\
Sheralyn        &   1.0 &   1.000000 \\
Sheran          &   1.0 &   1.000000 \\
Sheree          &   1.0 &   0.038462 \\
Shereen         &   1.0 &   0.294118 \\
Sherell         &   1.0 &   1.000000 \\
Sherelle        &   1.0 &   1.000000 \\
Sherene         &   1.0 &   0.625000 \\
Sherese         &   1.0 &   1.000000 \\
Sheri           &   1.0 &   0.011299 \\
Sherian         &   1.0 &   0.625000 \\
Sherice         &   1.0 &   0.545455 \\
Sherida         &   1.0 &   0.500000 \\
Sheridan        &   1.0 &   0.156250 \\
Sherie          &   1.0 &   0.147059 \\
Sherika         &   1.0 &   1.000000 \\
Sheril          &   1.0 &   0.454545 \\
Sherill         &   1.0 &   0.625000 \\
Sherilyn        &   1.0 &   0.350000 \\
Sherina         &   1.0 &   1.000000 \\
Sherine         &   1.0 &   1.000000 \\
Sherise         &   1.0 &   0.625000 \\
Sherisse        &   1.0 &   1.000000 \\
Sherita         &   1.0 &   0.625000 \\
Sherlene        &   1.0 &   1.000000 \\
Sherlin         &   1.0 &   0.208333 \\
Sherline        &   1.0 &   1.000000 \\
Sherly          &   1.0 &   0.416667 \\
Sherlyn         &   1.0 &   0.125581 \\
Sherlyne        &   1.0 &   0.833333 \\
Sherlynn        &   1.0 &   1.000000 \\
Sheron          &   1.0 &   0.333333 \\
Sherre          &   1.0 &   1.000000 \\
Sherree         &   1.0 &   0.312500 \\
Sherrel         &   1.0 &   1.000000 \\
Sherrell        &   1.0 &   0.500000 \\
Sherri          &   1.0 &   0.014553 \\
Sherrie         &   1.0 &   0.036810 \\
Sherril         &   1.0 &   0.500000 \\
Sherrill        &   1.0 &   0.142857 \\
Sherrilyn       &   1.0 &   1.000000 \\
Sherrin         &   1.0 &   1.000000 \\
Sherron         &   1.0 &   0.312500 \\
Sherry          &   1.0 &   0.009404 \\
Sherryl         &   1.0 &   0.250000 \\
Sheryl          &   1.0 &   0.018041 \\
Sheryle         &   1.0 &   0.500000 \\
Sheryll         &   1.0 &   0.500000 \\
Sheryn          &   1.0 &   0.833333 \\
Shevonne        &   1.0 &   1.000000 \\
Sheyanne        &   1.0 &   0.833333 \\
Sheyenne        &   1.0 &   0.500000 \\
Sheyla          &   1.0 &   0.257576 \\
Sheylin         &   1.0 &   1.000000 \\
Sheyna          &   1.0 &   1.000000 \\
Shia            &   1.0 &   0.555556 \\
Shian           &   1.0 &   0.833333 \\
Shiane          &   1.0 &   1.000000 \\
Shiann          &   1.0 &   0.375000 \\
Shianne         &   1.0 &   0.208333 \\
Shiela          &   1.0 &   0.304348 \\
Shigeko         &   1.0 &   0.375000 \\
Shila           &   1.0 &   0.600000 \\
Shilah          &   1.0 &   0.416667 \\
Shilo           &   1.0 &   0.583333 \\
Shiloh          &   1.0 &   1.000000 \\
Shilpa          &   1.0 &   1.000000 \\
Shina           &   1.0 &   1.000000 \\
Shindana        &   1.0 &   1.000000 \\
Shine           &   1.0 &   1.000000 \\
Shiori          &   1.0 &   1.000000 \\
Shir            &   1.0 &   1.000000 \\
Shira           &   1.0 &   0.272727 \\
Shiree          &   1.0 &   0.833333 \\
Shireen         &   1.0 &   0.357143 \\
Shirel          &   1.0 &   0.857143 \\
Shirelle        &   1.0 &   1.000000 \\
Shiri           &   1.0 &   1.000000 \\
Shirin          &   1.0 &   0.384615 \\
Shirl           &   1.0 &   0.533333 \\
Shirlee         &   1.0 &   0.200000 \\
Shirleen        &   1.0 &   0.400000 \\
Shirlene        &   1.0 &   0.416667 \\
Shirley         &   1.0 &   0.015424 \\
Shirlie         &   1.0 &   0.625000 \\
Shirly          &   1.0 &   1.000000 \\
Shiva           &   1.0 &   1.000000 \\
Shivali         &   1.0 &   1.000000 \\
Shivani         &   1.0 &   0.178571 \\
Shizu           &   1.0 &   0.833333 \\
Shizue          &   1.0 &   0.500000 \\
Shizuko         &   1.0 &   0.400000 \\
Shizuye         &   1.0 &   0.384615 \\
Shley           &   1.0 &   1.000000 \\
Shloka          &   1.0 &   0.625000 \\
Shona           &   1.0 &   0.294118 \\
Shonda          &   1.0 &   0.214286 \\
Shondra         &   1.0 &   0.461538 \\
Shonna          &   1.0 &   0.318182 \\
Shonta          &   1.0 &   0.416667 \\
Shontae         &   1.0 &   1.000000 \\
Shontay         &   1.0 &   1.000000 \\
Shonte          &   1.0 &   0.777778 \\
Shontel         &   1.0 &   0.833333 \\
Shontell        &   1.0 &   1.000000 \\
Shoshana        &   1.0 &   0.333333 \\
Shoshanna       &   1.0 &   1.000000 \\
Shoshannah      &   1.0 &   1.000000 \\
Shoua           &   1.0 &   0.615385 \\
Shraddha        &   1.0 &   1.000000 \\
Shravya         &   1.0 &   1.000000 \\
Shree           &   1.0 &   1.000000 \\
Shreeya         &   1.0 &   0.600000 \\
Shresta         &   1.0 &   1.000000 \\
Shreya          &   1.0 &   0.369231 \\
Shrinika        &   1.0 &   1.000000 \\
Shriya          &   1.0 &   0.333333 \\
Shruthi         &   1.0 &   0.500000 \\
Shruti          &   1.0 &   0.384615 \\
Shya            &   1.0 &   1.000000 \\
Shyan           &   1.0 &   0.714286 \\
Shyann          &   1.0 &   0.131579 \\
Shyanna         &   1.0 &   0.625000 \\
Shyanne         &   1.0 &   0.122807 \\
Shyenne         &   1.0 &   0.714286 \\
Shyla           &   1.0 &   0.403509 \\
Shylah          &   1.0 &   0.411765 \\
Shylee          &   1.0 &   0.625000 \\
Shyloh          &   1.0 &   1.000000 \\
Shyra           &   1.0 &   1.000000 \\
Sia             &   1.0 &   1.000000 \\
Siah            &   1.0 &   1.000000 \\
Siahna          &   1.0 &   1.000000 \\
Sian            &   1.0 &   0.833333 \\
Siana           &   1.0 &   0.600000 \\
Sianna          &   1.0 &   0.333333 \\
Siara           &   1.0 &   0.428571 \\
Siarah          &   1.0 &   1.000000 \\
Siarra          &   1.0 &   1.000000 \\
Sibyl           &   1.0 &   1.000000 \\
Sibylla         &   1.0 &   1.000000 \\
Sicily          &   1.0 &   0.666667 \\
Siclali         &   1.0 &   1.000000 \\
Siddhi          &   1.0 &   1.000000 \\
Sidnee          &   1.0 &   0.833333 \\
Sidney          &   1.0 &   0.141732 \\
Sidnie          &   1.0 &   1.000000 \\
Sidra           &   1.0 &   1.000000 \\
Sieanna         &   1.0 &   1.000000 \\
Siedah          &   1.0 &   1.000000 \\
Siena           &   1.0 &   0.528986 \\
Sienna          &   1.0 &   0.674944 \\
Siennah         &   1.0 &   1.000000 \\
Siera           &   1.0 &   0.400000 \\
Sierah          &   1.0 &   1.000000 \\
Sierra          &   1.0 &   0.187590 \\
Sierrah         &   1.0 &   0.384615 \\
Signe           &   1.0 &   0.545455 \\
Sigourney       &   1.0 &   1.000000 \\
Sigrid          &   1.0 &   1.000000 \\
Sila            &   1.0 &   1.000000 \\
Silbia          &   1.0 &   0.833333 \\
Silva           &   1.0 &   0.500000 \\
Silvana         &   1.0 &   0.615385 \\
Silver          &   1.0 &   0.727273 \\
Silvia          &   1.0 &   0.113861 \\
Silvina         &   1.0 &   1.000000 \\
Sima            &   1.0 &   1.000000 \\
Simar           &   1.0 &   0.857143 \\
Simarpreet      &   1.0 &   1.000000 \\
Simona          &   1.0 &   0.583333 \\
Simone          &   1.0 &   0.523810 \\
Simonne         &   1.0 &   1.000000 \\
Simra           &   1.0 &   1.000000 \\
Simran          &   1.0 &   0.133333 \\
Simranjit       &   1.0 &   0.714286 \\
Simrat          &   1.0 &   0.714286 \\
Simrit          &   1.0 &   1.000000 \\
Sina            &   1.0 &   0.714286 \\
Sinahi          &   1.0 &   0.833333 \\
Sinai           &   1.0 &   0.750000 \\
Sincere         &   1.0 &   0.454545 \\
Sinclaire       &   1.0 &   1.000000 \\
Sindhu          &   1.0 &   1.000000 \\
Sindi           &   1.0 &   0.714286 \\
Sindia          &   1.0 &   0.666667 \\
Sindy           &   1.0 &   0.238095 \\
Sinead          &   1.0 &   0.380952 \\
Sinthia         &   1.0 &   0.857143 \\
Sintia          &   1.0 &   0.500000 \\
Siobhan         &   1.0 &   0.250000 \\
Siomara         &   1.0 &   0.466667 \\
Siona           &   1.0 &   0.714286 \\
Sirat           &   1.0 &   1.000000 \\
Siren           &   1.0 &   1.000000 \\
Sirena          &   1.0 &   0.216216 \\
Sirenia         &   1.0 &   1.000000 \\
Siri            &   1.0 &   0.312500 \\
Siria           &   1.0 &   1.000000 \\
Sissi           &   1.0 &   1.000000 \\
Sissy           &   1.0 &   1.000000 \\
Sita            &   1.0 &   1.000000 \\
Sitara          &   1.0 &   1.000000 \\
Sitlali         &   1.0 &   0.714286 \\
Sitlaly         &   1.0 &   1.000000 \\
Sivan           &   1.0 &   0.555556 \\
Siya            &   1.0 &   0.914286 \\
Siyona          &   1.0 &   0.500000 \\
Skai            &   1.0 &   0.857143 \\
Skarlet         &   1.0 &   0.555556 \\
Skarlett        &   1.0 &   0.380952 \\
Skarlette       &   1.0 &   1.000000 \\
Skie            &   1.0 &   1.000000 \\
Sky             &   1.0 &   0.805556 \\
Skye            &   1.0 &   0.787037 \\
Skyelar         &   1.0 &   1.000000 \\
Skyla           &   1.0 &   0.555556 \\
Skylah          &   1.0 &   0.692308 \\
Skylar          &   1.0 &   0.668750 \\
Skylee          &   1.0 &   0.687500 \\
Skyler          &   1.0 &   0.409449 \\
Skylie          &   1.0 &   1.000000 \\
Skylin          &   1.0 &   1.000000 \\
Skyllar         &   1.0 &   1.000000 \\
Skylyn          &   1.0 &   1.000000 \\
Skylynn         &   1.0 &   0.583333 \\
Skyrah          &   1.0 &   1.000000 \\
Skyy            &   1.0 &   0.461538 \\
Slater          &   1.0 &   1.000000 \\
Sloan           &   1.0 &   0.684211 \\
Sloane          &   1.0 &   1.000000 \\
Sloka           &   1.0 &   1.000000 \\
Smantha         &   1.0 &   1.000000 \\
Smaya           &   1.0 &   1.000000 \\
Smita           &   1.0 &   1.000000 \\
Smrithi         &   1.0 &   1.000000 \\
Smriti          &   1.0 &   1.000000 \\
Sneha           &   1.0 &   0.300000 \\
Snoh            &   1.0 &   1.000000 \\
Snow            &   1.0 &   0.888889 \\
Sobeida         &   1.0 &   1.000000 \\
Socorro         &   1.0 &   0.113208 \\
Sofi            &   1.0 &   0.454545 \\
Sofia           &   1.0 &   0.619431 \\
Sofiah          &   1.0 &   1.000000 \\
Sofie           &   1.0 &   0.411765 \\
Sofija          &   1.0 &   1.000000 \\
Sofiya          &   1.0 &   0.500000 \\
Sofya           &   1.0 &   1.000000 \\
Soha            &   1.0 &   1.000000 \\
Soila           &   1.0 &   0.625000 \\
Sokha           &   1.0 &   1.000000 \\
Sokhom          &   1.0 &   1.000000 \\
Sol             &   1.0 &   1.000000 \\
Solana          &   1.0 &   1.000000 \\
Solange         &   1.0 &   0.600000 \\
Solara          &   1.0 &   0.333333 \\
Solash          &   1.0 &   0.555556 \\
Solay           &   1.0 &   1.000000 \\
Sole            &   1.0 &   0.500000 \\
Soledad         &   1.0 &   0.256410 \\
Soleen          &   1.0 &   1.000000 \\
Solei           &   1.0 &   1.000000 \\
Soleia          &   1.0 &   0.714286 \\
Soleil          &   1.0 &   1.000000 \\
Solene          &   1.0 &   0.625000 \\
Solenn          &   1.0 &   1.000000 \\
Solenne         &   1.0 &   1.000000 \\
Soliana         &   1.0 &   1.000000 \\
Solimar         &   1.0 &   1.000000 \\
Solina          &   1.0 &   1.000000 \\
Solmayra        &   1.0 &   1.000000 \\
Solstice        &   1.0 &   1.000000 \\
Somalia         &   1.0 &   1.000000 \\
Somaly          &   1.0 &   1.000000 \\
Somaya          &   1.0 &   0.300000 \\
Somer           &   1.0 &   0.227273 \\
Sommer          &   1.0 &   0.094340 \\
Sona            &   1.0 &   0.571429 \\
Sonakshi        &   1.0 &   1.000000 \\
Sonal           &   1.0 &   0.714286 \\
Sonali          &   1.0 &   0.352941 \\
Sonam           &   1.0 &   0.538462 \\
Sonda           &   1.0 &   1.000000 \\
Sondi           &   1.0 &   1.000000 \\
Sondra          &   1.0 &   0.083333 \\
Song            &   1.0 &   0.500000 \\
Sonia           &   1.0 &   0.099558 \\
Sonja           &   1.0 &   0.034483 \\
Sonji           &   1.0 &   0.600000 \\
Sonjia          &   1.0 &   1.000000 \\
Sonna           &   1.0 &   1.000000 \\
Sonnet          &   1.0 &   1.000000 \\
Sonny           &   1.0 &   1.000000 \\
Sonoma          &   1.0 &   1.000000 \\
Sonora          &   1.0 &   0.352941 \\
Sonya           &   1.0 &   0.056769 \\
Soo             &   1.0 &   1.000000 \\
Sophana         &   1.0 &   1.000000 \\
Sophea          &   1.0 &   1.000000 \\
Sopheap         &   1.0 &   1.000000 \\
Sophear         &   1.0 &   1.000000 \\
Sophi           &   1.0 &   1.000000 \\
Sophia          &   1.0 &   0.483535 \\
Sophiagrace     &   1.0 &   1.000000 \\
Sophiah         &   1.0 &   1.000000 \\
Sophiamarie     &   1.0 &   1.000000 \\
Sophiarose      &   1.0 &   1.000000 \\
Sophie          &   1.0 &   0.735043 \\
Sophy           &   1.0 &   1.000000 \\
Sophya          &   1.0 &   0.384615 \\
Sora            &   1.0 &   0.928571 \\
Soraida         &   1.0 &   0.416667 \\
Sorangel        &   1.0 &   1.000000 \\
Soraya          &   1.0 &   0.672414 \\
Sorayah         &   1.0 &   0.833333 \\
Soren           &   1.0 &   1.000000 \\
Soriah          &   1.0 &   1.000000 \\
Soriya          &   1.0 &   0.500000 \\
Sotheary        &   1.0 &   1.000000 \\
Soua            &   1.0 &   0.700000 \\
Soul            &   1.0 &   0.666667 \\
Souline         &   1.0 &   1.000000 \\
Soumya          &   1.0 &   0.833333 \\
Soyla           &   1.0 &   1.000000 \\
Sparkle         &   1.0 &   0.400000 \\
Sparrow         &   1.0 &   0.466667 \\
Special         &   1.0 &   1.000000 \\
Spencer         &   1.0 &   0.600000 \\
Spenser         &   1.0 &   0.714286 \\
Spirit          &   1.0 &   1.000000 \\
Spring          &   1.0 &   0.200000 \\
Spruha          &   1.0 &   1.000000 \\
Sravya          &   1.0 &   1.000000 \\
Sreya           &   1.0 &   1.000000 \\
Srija           &   1.0 &   1.000000 \\
Srinidhi        &   1.0 &   0.625000 \\
Srinika         &   1.0 &   0.625000 \\
Srishti         &   1.0 &   0.833333 \\
Sriya           &   1.0 &   0.416667 \\
Sruthi          &   1.0 &   0.888889 \\
Stacee          &   1.0 &   0.538462 \\
Stacey          &   1.0 &   0.010401 \\
Staci           &   1.0 &   0.048611 \\
Stacia          &   1.0 &   0.151515 \\
Stacie          &   1.0 &   0.031915 \\
Stacy           &   1.0 &   0.021992 \\
Stacye          &   1.0 &   1.000000 \\
Stanley         &   1.0 &   1.000000 \\
Staphanie       &   1.0 &   1.000000 \\
Staphany        &   1.0 &   0.428571 \\
Star            &   1.0 &   0.309524 \\
Starkisha       &   1.0 &   1.000000 \\
Starla          &   1.0 &   0.240000 \\
Starlene        &   1.0 &   0.333333 \\
Starlet         &   1.0 &   1.000000 \\
Starlett        &   1.0 &   1.000000 \\
Starlette       &   1.0 &   1.000000 \\
Starlyn         &   1.0 &   1.000000 \\
Starr           &   1.0 &   0.131579 \\
Stasha          &   1.0 &   1.000000 \\
Stasia          &   1.0 &   1.000000 \\
Stassi          &   1.0 &   0.500000 \\
Stefani         &   1.0 &   0.086207 \\
Stefania        &   1.0 &   0.357143 \\
Stefanie        &   1.0 &   0.022222 \\
Stefanny        &   1.0 &   1.000000 \\
Stefany         &   1.0 &   0.256410 \\
Steffani        &   1.0 &   0.500000 \\
Steffanie       &   1.0 &   0.238095 \\
Steffany        &   1.0 &   0.400000 \\
Steffi          &   1.0 &   0.750000 \\
Stehanie        &   1.0 &   1.000000 \\
Stella          &   1.0 &   0.805369 \\
Stellarose      &   1.0 &   0.833333 \\
Stepahnie       &   1.0 &   1.000000 \\
Stepanie        &   1.0 &   0.545455 \\
Stephaine       &   1.0 &   0.320000 \\
Stephane        &   1.0 &   0.545455 \\
Stephanee       &   1.0 &   0.833333 \\
Stephaney       &   1.0 &   1.000000 \\
Stephani        &   1.0 &   0.147059 \\
Stephania       &   1.0 &   0.350000 \\
Stephanie       &   1.0 &   0.024950 \\
Stephanieann    &   1.0 &   1.000000 \\
Stephannie      &   1.0 &   0.357143 \\
Stephanny       &   1.0 &   1.000000 \\
Stephany        &   1.0 &   0.049180 \\
Stephen         &   1.0 &   0.466667 \\
Stephenie       &   1.0 &   0.200000 \\
Stepheny        &   1.0 &   1.000000 \\
Stephine        &   1.0 &   0.384615 \\
Stephnie        &   1.0 &   1.000000 \\
Sterling        &   1.0 &   0.923077 \\
Steve           &   1.0 &   1.000000 \\
Stevee          &   1.0 &   1.000000 \\
Steven          &   1.0 &   0.238095 \\
Stevi           &   1.0 &   0.294118 \\
Stevie          &   1.0 &   0.896104 \\
Stori           &   1.0 &   0.750000 \\
Storm           &   1.0 &   0.923077 \\
Stormi          &   1.0 &   0.727273 \\
Stormie         &   1.0 &   0.416667 \\
Stormy          &   1.0 &   0.391304 \\
Story           &   1.0 &   0.937500 \\
Stphanie        &   1.0 &   1.000000 \\
Stuti           &   1.0 &   1.000000 \\
Sua             &   1.0 &   1.000000 \\
Suann           &   1.0 &   1.000000 \\
Suanne          &   1.0 &   0.625000 \\
Subrina         &   1.0 &   0.750000 \\
Success         &   1.0 &   1.000000 \\
Sucely          &   1.0 &   1.000000 \\
Sue             &   1.0 &   0.020942 \\
Sueann          &   1.0 &   1.000000 \\
Sueellen        &   1.0 &   1.000000 \\
Sueling         &   1.0 &   1.000000 \\
Suellen         &   1.0 &   0.357143 \\
Sugey           &   1.0 &   0.294118 \\
Suha            &   1.0 &   1.000000 \\
Suhana          &   1.0 &   0.666667 \\
Suhani          &   1.0 &   0.571429 \\
Suhavi          &   1.0 &   1.000000 \\
Suheidy         &   1.0 &   1.000000 \\
Suhey           &   1.0 &   1.000000 \\
Sujey           &   1.0 &   0.240000 \\
Sukari          &   1.0 &   1.000000 \\
Sukhman         &   1.0 &   1.000000 \\
Sukhmani        &   1.0 &   0.416667 \\
Sukhpreet       &   1.0 &   1.000000 \\
Suki            &   1.0 &   0.833333 \\
Suleima         &   1.0 &   0.666667 \\
Sulema          &   1.0 &   0.176471 \\
Suley           &   1.0 &   1.000000 \\
Suleyma         &   1.0 &   0.312500 \\
Sullivan        &   1.0 &   1.000000 \\
Sully           &   1.0 &   0.600000 \\
Sulma           &   1.0 &   0.800000 \\
Sumaiya         &   1.0 &   1.000000 \\
Sumaya          &   1.0 &   0.416667 \\
Sumayah         &   1.0 &   0.833333 \\
Sumayya         &   1.0 &   1.000000 \\
Sumayyah        &   1.0 &   1.000000 \\
Sumedha         &   1.0 &   1.000000 \\
Sumer           &   1.0 &   0.250000 \\
Sumi            &   1.0 &   1.000000 \\
Sumiko          &   1.0 &   0.277778 \\
Sumiye          &   1.0 &   0.714286 \\
Summer          &   1.0 &   0.651685 \\
Sun             &   1.0 &   1.000000 \\
Sundae          &   1.0 &   1.000000 \\
Sunday          &   1.0 &   0.695652 \\
Sundra          &   1.0 &   1.000000 \\
Sundus          &   1.0 &   1.000000 \\
Sunita          &   1.0 &   1.000000 \\
Sunni           &   1.0 &   0.888889 \\
Sunnie          &   1.0 &   0.947368 \\
Sunny           &   1.0 &   1.000000 \\
Sunshine        &   1.0 &   0.203390 \\
Suong           &   1.0 &   1.000000 \\
Supriya         &   1.0 &   1.000000 \\
Surabhi         &   1.0 &   0.833333 \\
Surah           &   1.0 &   1.000000 \\
Suraya          &   1.0 &   0.583333 \\
Surena          &   1.0 &   1.000000 \\
Suri            &   1.0 &   0.372549 \\
Suriah          &   1.0 &   1.000000 \\
Surina          &   1.0 &   0.416667 \\
Suriya          &   1.0 &   1.000000 \\
Surveen         &   1.0 &   1.000000 \\
Sury            &   1.0 &   0.555556 \\
Surya           &   1.0 &   0.833333 \\
Susan           &   1.0 &   0.003305 \\
Susana          &   1.0 &   0.069597 \\
Susann          &   1.0 &   0.533333 \\
Susanna         &   1.0 &   0.133333 \\
Susannah        &   1.0 &   0.263158 \\
Susanne         &   1.0 &   0.080460 \\
Susette         &   1.0 &   0.857143 \\
Susi            &   1.0 &   0.833333 \\
Susie           &   1.0 &   0.036145 \\
Susy            &   1.0 &   0.333333 \\
Sutton          &   1.0 &   1.000000 \\
Suvi            &   1.0 &   1.000000 \\
Suzan           &   1.0 &   0.100000 \\
Suzana          &   1.0 &   0.352941 \\
Suzann          &   1.0 &   0.333333 \\
Suzanna         &   1.0 &   0.151515 \\
Suzannah        &   1.0 &   1.000000 \\
Suzanne         &   1.0 &   0.006289 \\
Suzette         &   1.0 &   0.080357 \\
Suzi            &   1.0 &   0.461538 \\
Suzie           &   1.0 &   0.125000 \\
Suzy            &   1.0 &   0.092593 \\
Suzzane         &   1.0 &   1.000000 \\
Suzzanne        &   1.0 &   0.833333 \\
Suzzette        &   1.0 &   0.714286 \\
Svea            &   1.0 &   1.000000 \\
Svetlana        &   1.0 &   0.555556 \\
Swara           &   1.0 &   0.478261 \\
Swati           &   1.0 &   1.000000 \\
Swayze          &   1.0 &   1.000000 \\
Swetha          &   1.0 &   1.000000 \\
Syann           &   1.0 &   1.000000 \\
Syanna          &   1.0 &   1.000000 \\
Sybella         &   1.0 &   1.000000 \\
Sybil           &   1.0 &   0.933333 \\
Sydne           &   1.0 &   1.000000 \\
Sydnee          &   1.0 &   0.120000 \\
Sydney          &   1.0 &   0.132701 \\
Sydni           &   1.0 &   0.178571 \\
Sydnie          &   1.0 &   0.146341 \\
Syeda           &   1.0 &   0.714286 \\
Syliva          &   1.0 &   1.000000 \\
Sylvana         &   1.0 &   0.714286 \\
Sylvia          &   1.0 &   0.068519 \\
Sylvie          &   1.0 &   1.000000 \\
Symantha        &   1.0 &   0.555556 \\
Symone          &   1.0 &   0.156250 \\
Symphony        &   1.0 &   0.545455 \\
Synai           &   1.0 &   1.000000 \\
Syncere         &   1.0 &   1.000000 \\
Syndey          &   1.0 &   1.000000 \\
Syniah          &   1.0 &   1.000000 \\
Synthia         &   1.0 &   0.400000 \\
Syra            &   1.0 &   0.857143 \\
Syrah           &   1.0 &   1.000000 \\
Syreeta         &   1.0 &   0.347826 \\
Syrena          &   1.0 &   0.833333 \\
Syria           &   1.0 &   0.833333 \\
Syriah          &   1.0 &   1.000000 \\
Syriana         &   1.0 &   0.666667 \\
Tabatha         &   1.0 &   0.103448 \\
Tabbatha        &   1.0 &   1.000000 \\
Tabbitha        &   1.0 &   1.000000 \\
Tabetha         &   1.0 &   0.312500 \\
Tabita          &   1.0 &   1.000000 \\
Tabitha         &   1.0 &   0.105634 \\
Tabytha         &   1.0 &   1.000000 \\
Tacara          &   1.0 &   1.000000 \\
Tacy            &   1.0 &   1.000000 \\
Taegan          &   1.0 &   1.000000 \\
Taeko           &   1.0 &   1.000000 \\
Taeler          &   1.0 &   1.000000 \\
Taelor          &   1.0 &   0.277778 \\
Taelyn          &   1.0 &   0.384615 \\
Taelynn         &   1.0 &   0.545455 \\
Taeya           &   1.0 &   1.000000 \\
Taffy           &   1.0 &   0.416667 \\
Tahani          &   1.0 &   1.000000 \\
Tahira          &   1.0 &   1.000000 \\
Tahirah         &   1.0 &   0.833333 \\
Tahiry          &   1.0 &   1.000000 \\
Tahlia          &   1.0 &   0.700000 \\
Tahnee          &   1.0 &   0.208333 \\
Tai             &   1.0 &   0.357143 \\
Taia            &   1.0 &   1.000000 \\
Taija           &   1.0 &   1.000000 \\
Tailor          &   1.0 &   0.666667 \\
Taina           &   1.0 &   0.352941 \\
Taira           &   1.0 &   0.750000 \\
Tais            &   1.0 &   0.147059 \\
Taisha          &   1.0 &   0.545455 \\
Taishmara       &   1.0 &   1.000000 \\
Taitum          &   1.0 &   1.000000 \\
Taiya           &   1.0 &   0.625000 \\
Taiz            &   1.0 &   0.625000 \\
Taj             &   1.0 &   1.000000 \\
Taja            &   1.0 &   0.294118 \\
Tajah           &   1.0 &   0.857143 \\
Tajanae         &   1.0 &   0.500000 \\
Tajuana         &   1.0 &   1.000000 \\
Takako          &   1.0 &   1.000000 \\
Takara          &   1.0 &   0.714286 \\
Takesha         &   1.0 &   0.833333 \\
Takisha         &   1.0 &   0.900000 \\
Takiyah         &   1.0 &   1.000000 \\
Tal             &   1.0 &   1.000000 \\
Tala            &   1.0 &   0.962963 \\
Talar           &   1.0 &   0.500000 \\
Talaya          &   1.0 &   1.000000 \\
Talayah         &   1.0 &   1.000000 \\
Talea           &   1.0 &   0.833333 \\
Taleah          &   1.0 &   0.583333 \\
Taleen          &   1.0 &   1.000000 \\
Tali            &   1.0 &   0.416667 \\
Talia           &   1.0 &   0.879433 \\
Taliah          &   1.0 &   0.578947 \\
Talin           &   1.0 &   0.461538 \\
Talina          &   1.0 &   0.833333 \\
Taline          &   1.0 &   0.750000 \\
Talisa          &   1.0 &   0.333333 \\
Talise          &   1.0 &   1.000000 \\
Talisha         &   1.0 &   0.888889 \\
Talissa         &   1.0 &   1.000000 \\
Talitha         &   1.0 &   0.461538 \\
Taliya          &   1.0 &   0.777778 \\
Taliyah         &   1.0 &   0.317073 \\
Tallula         &   1.0 &   1.000000 \\
Tallulah        &   1.0 &   1.000000 \\
Talor           &   1.0 &   1.000000 \\
Talula          &   1.0 &   0.750000 \\
Talulah         &   1.0 &   1.000000 \\
Talya           &   1.0 &   0.600000 \\
Talyah          &   1.0 &   1.000000 \\
Talyn           &   1.0 &   1.000000 \\
Tam             &   1.0 &   0.500000 \\
Tama            &   1.0 &   1.000000 \\
Tamala          &   1.0 &   0.600000 \\
Tamana          &   1.0 &   1.000000 \\
Tamanika        &   1.0 &   0.555556 \\
Tamanna         &   1.0 &   1.000000 \\
Tamar           &   1.0 &   0.206897 \\
Tamara          &   1.0 &   0.019048 \\
Tamarah         &   1.0 &   0.714286 \\
Tamarie         &   1.0 &   1.000000 \\
Tamarra         &   1.0 &   1.000000 \\
Tamasha         &   1.0 &   1.000000 \\
Tamatha         &   1.0 &   0.171429 \\
Tamaya          &   1.0 &   1.000000 \\
Tamberly        &   1.0 &   1.000000 \\
Tambra          &   1.0 &   0.368421 \\
Tameca          &   1.0 &   1.000000 \\
Tameeka         &   1.0 &   1.000000 \\
Tameika         &   1.0 &   0.857143 \\
Tameka          &   1.0 &   0.092593 \\
Tamekia         &   1.0 &   0.545455 \\
Tameko          &   1.0 &   1.000000 \\
Tamela          &   1.0 &   0.230769 \\
Tamera          &   1.0 &   0.064103 \\
Tameria         &   1.0 &   1.000000 \\
Tamesha         &   1.0 &   0.750000 \\
Tami            &   1.0 &   0.012920 \\
Tamia           &   1.0 &   0.089286 \\
Tamica          &   1.0 &   0.500000 \\
Tamie           &   1.0 &   0.181818 \\
Tamieka         &   1.0 &   1.000000 \\
Tamika          &   1.0 &   0.045045 \\
Tamiko          &   1.0 &   0.235294 \\
Tamila          &   1.0 &   1.000000 \\
Tamira          &   1.0 &   0.555556 \\
Tamisha         &   1.0 &   0.333333 \\
Tamlyn          &   1.0 &   1.000000 \\
Tammara         &   1.0 &   0.368421 \\
Tammatha        &   1.0 &   1.000000 \\
Tammee          &   1.0 &   1.000000 \\
Tammera         &   1.0 &   0.416667 \\
Tammey          &   1.0 &   0.625000 \\
Tammi           &   1.0 &   0.056180 \\
Tammie          &   1.0 &   0.058824 \\
Tammra          &   1.0 &   0.454545 \\
Tammy           &   1.0 &   0.003249 \\
Tamora          &   1.0 &   0.777778 \\
Tamra           &   1.0 &   0.055556 \\
Tamre           &   1.0 &   1.000000 \\
Tamsen          &   1.0 &   1.000000 \\
Tamu            &   1.0 &   0.416667 \\
Tamura          &   1.0 &   1.000000 \\
Tamy            &   1.0 &   0.625000 \\
Tamya           &   1.0 &   0.777778 \\
Tamyra          &   1.0 &   0.750000 \\
Tana            &   1.0 &   0.185185 \\
Tanairi         &   1.0 &   0.384615 \\
Tanairy         &   1.0 &   1.000000 \\
Tanasha         &   1.0 &   0.714286 \\
Tanay           &   1.0 &   1.000000 \\
Tanaya          &   1.0 &   0.750000 \\
Tanda           &   1.0 &   1.000000 \\
Tandi           &   1.0 &   1.000000 \\
Tandra          &   1.0 &   1.000000 \\
Tandy           &   1.0 &   1.000000 \\
Tanea           &   1.0 &   0.833333 \\
Taneil          &   1.0 &   1.000000 \\
Taneisha        &   1.0 &   0.375000 \\
Taneka          &   1.0 &   0.777778 \\
Tanesha         &   1.0 &   0.181818 \\
Taneshia        &   1.0 &   0.714286 \\
Taneya          &   1.0 &   1.000000 \\
Tangela         &   1.0 &   0.833333 \\
Tangi           &   1.0 &   1.000000 \\
Tangie          &   1.0 &   1.000000 \\
Tani            &   1.0 &   0.384615 \\
Tania           &   1.0 &   0.064198 \\
Taniah          &   1.0 &   1.000000 \\
Taniesha        &   1.0 &   0.750000 \\
Tanika          &   1.0 &   0.185185 \\
Taniqua         &   1.0 &   1.000000 \\
Tanis           &   1.0 &   0.833333 \\
Tanisha         &   1.0 &   0.045872 \\
Tanishi         &   1.0 &   1.000000 \\
Tanishia        &   1.0 &   0.833333 \\
Tanishka        &   1.0 &   0.625000 \\
Taniya          &   1.0 &   0.550000 \\
Taniyah         &   1.0 &   0.294118 \\
Tanja           &   1.0 &   0.260870 \\
Tanna           &   1.0 &   0.625000 \\
Tannaz          &   1.0 &   1.000000 \\
Tanner          &   1.0 &   0.500000 \\
Tannia          &   1.0 &   0.454545 \\
Tannya          &   1.0 &   0.714286 \\
Tanveer         &   1.0 &   1.000000 \\
Tanvi           &   1.0 &   0.250000 \\
Tanya           &   1.0 &   0.021090 \\
Tanzania        &   1.0 &   0.857143 \\
Tara            &   1.0 &   0.088975 \\
Tarah           &   1.0 &   0.150000 \\
Taraji          &   1.0 &   0.428571 \\
Taralyn         &   1.0 &   1.000000 \\
Taran           &   1.0 &   1.000000 \\
Taraneh         &   1.0 &   1.000000 \\
Taren           &   1.0 &   0.555556 \\
Tari            &   1.0 &   0.285714 \\
Tariah          &   1.0 &   1.000000 \\
Tarin           &   1.0 &   0.888889 \\
Tarina          &   1.0 &   1.000000 \\
Tarra           &   1.0 &   0.538462 \\
Tarrah          &   1.0 &   0.555556 \\
Tarryn          &   1.0 &   0.625000 \\
Tarsha          &   1.0 &   1.000000 \\
Taryn           &   1.0 &   0.039735 \\
Tarynn          &   1.0 &   1.000000 \\
Tasha           &   1.0 &   0.038760 \\
Tashana         &   1.0 &   1.000000 \\
Tashawna        &   1.0 &   0.666667 \\
Tasheena        &   1.0 &   1.000000 \\
Tashia          &   1.0 &   0.500000 \\
Tashiana        &   1.0 &   1.000000 \\
Tashima         &   1.0 &   1.000000 \\
Tashina         &   1.0 &   0.250000 \\
Tasia           &   1.0 &   0.200000 \\
Tasneem         &   1.0 &   0.888889 \\
Tasnim          &   1.0 &   1.000000 \\
Tatanisha       &   1.0 &   0.461538 \\
Tate            &   1.0 &   1.000000 \\
Tatem           &   1.0 &   1.000000 \\
Tatevik         &   1.0 &   1.000000 \\
Tatia           &   1.0 &   0.205882 \\
Tatiana         &   1.0 &   0.107383 \\
Tatianna        &   1.0 &   0.184211 \\
Tatiyana        &   1.0 &   0.357143 \\
Tatjana         &   1.0 &   0.500000 \\
Tatsuko         &   1.0 &   1.000000 \\
Tatum           &   1.0 &   0.754386 \\
Tatyana         &   1.0 &   0.048544 \\
Tatyanna        &   1.0 &   0.357143 \\
Tauna           &   1.0 &   1.000000 \\
Taunya          &   1.0 &   0.461538 \\
Taura           &   1.0 &   1.000000 \\
Tauriel         &   1.0 &   1.000000 \\
Tausha          &   1.0 &   0.714286 \\
Tavia           &   1.0 &   0.454545 \\
Tawana          &   1.0 &   0.454545 \\
Tawanna         &   1.0 &   1.000000 \\
Tawna           &   1.0 &   0.555556 \\
Tawnee          &   1.0 &   0.571429 \\
Tawney          &   1.0 &   0.357143 \\
Tawni           &   1.0 &   0.230769 \\
Tawnia          &   1.0 &   0.555556 \\
Tawnie          &   1.0 &   0.529412 \\
Tawny           &   1.0 &   0.049020 \\
Tawnya          &   1.0 &   0.116279 \\
Taya            &   1.0 &   0.171429 \\
Tayah           &   1.0 &   0.538462 \\
Tayana          &   1.0 &   1.000000 \\
Tayanna         &   1.0 &   0.777778 \\
Tayde           &   1.0 &   1.000000 \\
Tayeko          &   1.0 &   1.000000 \\
Tayla           &   1.0 &   0.294118 \\
Taylar          &   1.0 &   0.416667 \\
Taylee          &   1.0 &   0.500000 \\
Taylen          &   1.0 &   1.000000 \\
Tayler          &   1.0 &   0.077670 \\
Taylin          &   1.0 &   0.833333 \\
Taylor          &   1.0 &   0.078849 \\
Taylore         &   1.0 &   1.000000 \\
Taylour         &   1.0 &   1.000000 \\
Taylyn          &   1.0 &   1.000000 \\
Taylynn         &   1.0 &   1.000000 \\
Tayna           &   1.0 &   1.000000 \\
Taytum          &   1.0 &   1.000000 \\
Tazuko          &   1.0 &   1.000000 \\
Tea             &   1.0 &   0.121951 \\
Teagan          &   1.0 &   0.518750 \\
Teagen          &   1.0 &   1.000000 \\
Teaghan         &   1.0 &   1.000000 \\
Teah            &   1.0 &   0.500000 \\
Teaira          &   1.0 &   1.000000 \\
Teairra         &   1.0 &   1.000000 \\
Teal            &   1.0 &   0.538462 \\
Teala           &   1.0 &   1.000000 \\
Teana           &   1.0 &   0.833333 \\
Teanna          &   1.0 &   0.500000 \\
Teara           &   1.0 &   0.625000 \\
Tearra          &   1.0 &   1.000000 \\
Teasha          &   1.0 &   1.000000 \\
Teddi           &   1.0 &   0.500000 \\
Teddie          &   1.0 &   1.000000 \\
Teddy           &   1.0 &   0.714286 \\
Tedi            &   1.0 &   1.000000 \\
Tedra           &   1.0 &   0.600000 \\
Teegan          &   1.0 &   0.615385 \\
Teela           &   1.0 &   0.294118 \\
Teena           &   1.0 &   0.217391 \\
Teera           &   1.0 &   1.000000 \\
Tegan           &   1.0 &   0.300000 \\
Tehani          &   1.0 &   1.000000 \\
Tehila          &   1.0 &   1.000000 \\
Tehya           &   1.0 &   0.217391 \\
Teia            &   1.0 &   1.000000 \\
Teigan          &   1.0 &   1.000000 \\
Teigen          &   1.0 &   0.833333 \\
Teila           &   1.0 &   0.714286 \\
Teisha          &   1.0 &   1.000000 \\
Teja            &   1.0 &   1.000000 \\
Telisha         &   1.0 &   1.000000 \\
Temeka          &   1.0 &   0.555556 \\
Temika          &   1.0 &   1.000000 \\
Temperance      &   1.0 &   0.222222 \\
Tempest         &   1.0 &   0.555556 \\
Tempestt        &   1.0 &   1.000000 \\
Temple          &   1.0 &   1.000000 \\
Temre           &   1.0 &   1.000000 \\
Tena            &   1.0 &   0.285714 \\
Tenaya          &   1.0 &   0.333333 \\
Tenea           &   1.0 &   1.000000 \\
Teneka          &   1.0 &   0.833333 \\
Tenesha         &   1.0 &   0.777778 \\
Tenia           &   1.0 &   1.000000 \\
Tenika          &   1.0 &   0.500000 \\
Tenille         &   1.0 &   0.636364 \\
Tenise          &   1.0 &   1.000000 \\
Tenisha         &   1.0 &   0.333333 \\
Tenley          &   1.0 &   0.200000 \\
Tennessee       &   1.0 &   1.000000 \\
Tennille        &   1.0 &   0.140351 \\
Tenzin          &   1.0 &   0.714286 \\
Teodora         &   1.0 &   1.000000 \\
Tequila         &   1.0 &   0.833333 \\
Tera            &   1.0 &   0.173913 \\
Terah           &   1.0 &   0.777778 \\
Terasa          &   1.0 &   1.000000 \\
Tere            &   1.0 &   1.000000 \\
Tereasa         &   1.0 &   0.625000 \\
Teresa          &   1.0 &   0.031915 \\
Terese          &   1.0 &   0.250000 \\
Teresia         &   1.0 &   1.000000 \\
Teresita        &   1.0 &   0.250000 \\
Teressa         &   1.0 &   0.538462 \\
Tereza          &   1.0 &   0.500000 \\
Teri            &   1.0 &   0.009881 \\
Terie           &   1.0 &   1.000000 \\
Terilyn         &   1.0 &   0.600000 \\
Terin           &   1.0 &   1.000000 \\
Terina          &   1.0 &   0.454545 \\
Terisa          &   1.0 &   0.666667 \\
Terra           &   1.0 &   0.140625 \\
Terre           &   1.0 &   0.666667 \\
Terrea          &   1.0 &   1.000000 \\
Terrell         &   1.0 &   0.666667 \\
Terren          &   1.0 &   1.000000 \\
Terresa         &   1.0 &   0.785714 \\
Terri           &   1.0 &   0.004753 \\
Terrie          &   1.0 &   0.040000 \\
Terrill         &   1.0 &   0.714286 \\
Terrilyn        &   1.0 &   1.000000 \\
Terrisa         &   1.0 &   1.000000 \\
Terrisha        &   1.0 &   1.000000 \\
Terry           &   1.0 &   0.008696 \\
Terryl          &   1.0 &   0.625000 \\
Terryn          &   1.0 &   1.000000 \\
Teruko          &   1.0 &   0.272727 \\
Teryl           &   1.0 &   0.555556 \\
Teryn           &   1.0 &   0.666667 \\
Tesa            &   1.0 &   1.000000 \\
Tesha           &   1.0 &   0.461538 \\
Teshia          &   1.0 &   1.000000 \\
Tesia           &   1.0 &   0.454545 \\
Tesla           &   1.0 &   0.666667 \\
Tess            &   1.0 &   0.092105 \\
Tessa           &   1.0 &   0.409574 \\
Tessie          &   1.0 &   0.555556 \\
Teya            &   1.0 &   0.461538 \\
Teyanna         &   1.0 &   1.000000 \\
Teyla           &   1.0 &   1.000000 \\
Teylor          &   1.0 &   1.000000 \\
Thaily          &   1.0 &   0.476190 \\
Thais           &   1.0 &   0.333333 \\
Thalia          &   1.0 &   0.418269 \\
Thamara         &   1.0 &   1.000000 \\
Thanh           &   1.0 &   0.294118 \\
Thania          &   1.0 &   0.111111 \\
Thanya          &   1.0 &   0.384615 \\
Thao            &   1.0 &   0.280000 \\
Thavy           &   1.0 &   1.000000 \\
Thea            &   1.0 &   1.000000 \\
Theadora        &   1.0 &   0.750000 \\
Theda           &   1.0 &   0.384615 \\
Theia           &   1.0 &   0.538462 \\
Thelma          &   1.0 &   0.028902 \\
Theo            &   1.0 &   1.000000 \\
Theodora        &   1.0 &   0.800000 \\
Theresa         &   1.0 &   0.012833 \\
Therese         &   1.0 &   0.043478 \\
Theresia        &   1.0 &   0.833333 \\
Theressa        &   1.0 &   0.833333 \\
Thersa          &   1.0 &   1.000000 \\
Thi             &   1.0 &   0.625000 \\
Thien           &   1.0 &   1.000000 \\
Thomas          &   1.0 &   1.000000 \\
Thomasina       &   1.0 &   1.000000 \\
Thora           &   1.0 &   1.000000 \\
Thresa          &   1.0 &   1.000000 \\
Thu             &   1.0 &   0.238095 \\
Thuan           &   1.0 &   1.000000 \\
Thuy            &   1.0 &   0.151515 \\
Thuytien        &   1.0 &   1.000000 \\
Thy             &   1.0 &   0.714286 \\
Tia             &   1.0 &   0.138462 \\
Tiahna          &   1.0 &   1.000000 \\
Tiaira          &   1.0 &   1.000000 \\
Tiana           &   1.0 &   0.319149 \\
Tianah          &   1.0 &   1.000000 \\
Tiani           &   1.0 &   0.625000 \\
Tianna          &   1.0 &   0.086957 \\
Tianne          &   1.0 &   1.000000 \\
Tiara           &   1.0 &   0.101449 \\
Tiarah          &   1.0 &   1.000000 \\
Tiare           &   1.0 &   0.312500 \\
Tiarra          &   1.0 &   0.437500 \\
Tiauna          &   1.0 &   1.000000 \\
Tichina         &   1.0 &   1.000000 \\
Tien            &   1.0 &   0.416667 \\
Tiera           &   1.0 &   0.227273 \\
Tierney         &   1.0 &   0.333333 \\
Tierra          &   1.0 &   0.090909 \\
Tiesha          &   1.0 &   0.600000 \\
Tifani          &   1.0 &   1.000000 \\
Tifanie         &   1.0 &   1.000000 \\
Tifanny         &   1.0 &   1.000000 \\
Tifany          &   1.0 &   0.857143 \\
Tiffanee        &   1.0 &   0.833333 \\
Tiffaney        &   1.0 &   0.294118 \\
Tiffani         &   1.0 &   0.093333 \\
Tiffanie        &   1.0 &   0.107143 \\
Tiffany         &   1.0 &   0.029663 \\
Tiffanyann      &   1.0 &   1.000000 \\
Tiffeny         &   1.0 &   0.625000 \\
Tiffiany        &   1.0 &   1.000000 \\
Tiffiney        &   1.0 &   0.833333 \\
Tiffini         &   1.0 &   0.454545 \\
Tiffinie        &   1.0 &   1.000000 \\
Tiffiny         &   1.0 &   0.250000 \\
Tiffney         &   1.0 &   0.714286 \\
Tigerlily       &   1.0 &   0.777778 \\
Tijera          &   1.0 &   0.857143 \\
Tika            &   1.0 &   1.000000 \\
Tikisha         &   1.0 &   1.000000 \\
Tila            &   1.0 &   1.000000 \\
Tilda           &   1.0 &   1.000000 \\
Tillie          &   1.0 &   0.370370 \\
Tilly           &   1.0 &   1.000000 \\
Timberly        &   1.0 &   1.000000 \\
Timi            &   1.0 &   0.421053 \\
Timia           &   1.0 &   0.833333 \\
Timika          &   1.0 &   1.000000 \\
Timisha         &   1.0 &   1.000000 \\
Timmie          &   1.0 &   1.000000 \\
Timothy         &   1.0 &   0.375000 \\
Tina            &   1.0 &   0.004280 \\
Tinamarie       &   1.0 &   0.666667 \\
Tinesha         &   1.0 &   0.714286 \\
Tinisha         &   1.0 &   0.384615 \\
Tinley          &   1.0 &   0.428571 \\
Tinna           &   1.0 &   1.000000 \\
Tinsley         &   1.0 &   0.454545 \\
Tiny            &   1.0 &   1.000000 \\
Tiona           &   1.0 &   0.875000 \\
Tionna          &   1.0 &   1.000000 \\
Tionne          &   1.0 &   0.714286 \\
Tiphani         &   1.0 &   0.833333 \\
Tiphanie        &   1.0 &   1.000000 \\
Tippi           &   1.0 &   1.000000 \\
Tira            &   1.0 &   0.714286 \\
Tirzah          &   1.0 &   0.625000 \\
Tisa            &   1.0 &   0.400000 \\
Tish            &   1.0 &   0.750000 \\
Tisha           &   1.0 &   0.056075 \\
Titiana         &   1.0 &   0.857143 \\
Titianna        &   1.0 &   1.000000 \\
Tiya            &   1.0 &   1.000000 \\
Tiyana          &   1.0 &   0.714286 \\
Tkeyah          &   1.0 &   1.000000 \\
Tobey           &   1.0 &   1.000000 \\
Tobi            &   1.0 &   0.300000 \\
Tobie           &   1.0 &   0.555556 \\
Toby            &   1.0 &   0.384615 \\
Toccara         &   1.0 &   0.411765 \\
Todd            &   1.0 &   0.857143 \\
Toi             &   1.0 &   0.500000 \\
Tokie           &   1.0 &   1.000000 \\
Tomasa          &   1.0 &   0.312500 \\
Tomasita        &   1.0 &   1.000000 \\
Tomeka          &   1.0 &   0.545455 \\
Tomi            &   1.0 &   0.416667 \\
Tomika          &   1.0 &   0.416667 \\
Tomiko          &   1.0 &   0.352941 \\
Tomisha         &   1.0 &   1.000000 \\
Tommi           &   1.0 &   1.000000 \\
Tommie          &   1.0 &   0.333333 \\
Tommy           &   1.0 &   1.000000 \\
Tomoko          &   1.0 &   1.000000 \\
Tona            &   1.0 &   0.555556 \\
Tonantzin       &   1.0 &   0.500000 \\
Tonda           &   1.0 &   0.500000 \\
Tonette         &   1.0 &   0.538462 \\
Toni            &   1.0 &   0.042735 \\
Tonia           &   1.0 &   0.056818 \\
Tonie           &   1.0 &   0.428571 \\
Tonika          &   1.0 &   1.000000 \\
Tonisha         &   1.0 &   0.454545 \\
Tonja           &   1.0 &   0.151515 \\
Tonna           &   1.0 &   1.000000 \\
Tonnette        &   1.0 &   1.000000 \\
Tonnie          &   1.0 &   1.000000 \\
Tony            &   1.0 &   1.000000 \\
Tonya           &   1.0 &   0.013369 \\
Topacio         &   1.0 &   0.625000 \\
Topanga         &   1.0 &   0.777778 \\
Tora            &   1.0 &   0.833333 \\
Toree           &   1.0 &   1.000000 \\
Torey           &   1.0 &   0.416667 \\
Tori            &   1.0 &   0.127551 \\
Toriana         &   1.0 &   1.000000 \\
Torie           &   1.0 &   0.357143 \\
Torrance        &   1.0 &   1.000000 \\
Torrey          &   1.0 &   0.238095 \\
Torri           &   1.0 &   0.333333 \\
Torrie          &   1.0 &   0.428571 \\
Torvi           &   1.0 &   1.000000 \\
Tory            &   1.0 &   0.294118 \\
Tosca           &   1.0 &   0.714286 \\
Tosha           &   1.0 &   0.272727 \\
Toshi           &   1.0 &   1.000000 \\
Toshia          &   1.0 &   0.857143 \\
Toshiba         &   1.0 &   1.000000 \\
Toshie          &   1.0 &   1.000000 \\
Toshiko         &   1.0 &   0.230769 \\
Toshiye         &   1.0 &   0.666667 \\
Tova            &   1.0 &   0.454545 \\
Tovah           &   1.0 &   1.000000 \\
Toy             &   1.0 &   1.000000 \\
Toya            &   1.0 &   0.416667 \\
Toyoko          &   1.0 &   0.714286 \\
Trace           &   1.0 &   1.000000 \\
Tracee          &   1.0 &   0.200000 \\
Tracey          &   1.0 &   0.015674 \\
Traci           &   1.0 &   0.014620 \\
Tracia          &   1.0 &   1.000000 \\
Tracie          &   1.0 &   0.028409 \\
Tracy           &   1.0 &   0.007386 \\
Tracye          &   1.0 &   0.714286 \\
Tram            &   1.0 &   0.454545 \\
Tramaine        &   1.0 &   0.888889 \\
Tran            &   1.0 &   0.384615 \\
Trang           &   1.0 &   0.178571 \\
Tranisha        &   1.0 &   1.000000 \\
Travis          &   1.0 &   0.583333 \\
Treanna         &   1.0 &   1.000000 \\
Treasa          &   1.0 &   0.857143 \\
Treasure        &   1.0 &   0.608696 \\
Treena          &   1.0 &   0.294118 \\
Trena           &   1.0 &   0.208333 \\
Trenisha        &   1.0 &   1.000000 \\
Trenity         &   1.0 &   1.000000 \\
Trenna          &   1.0 &   1.000000 \\
Tresa           &   1.0 &   0.333333 \\
Trese           &   1.0 &   1.000000 \\
Tresha          &   1.0 &   0.833333 \\
Tressa          &   1.0 &   0.333333 \\
Tressie         &   1.0 &   1.000000 \\
Treva           &   1.0 &   0.615385 \\
Trevor          &   1.0 &   0.714286 \\
Tria            &   1.0 &   1.000000 \\
Triana          &   1.0 &   0.555556 \\
Trianna         &   1.0 &   1.000000 \\
Tricia          &   1.0 &   0.017606 \\
Trina           &   1.0 &   0.024038 \\
Trinette        &   1.0 &   0.461538 \\
Trinh           &   1.0 &   0.545455 \\
Trini           &   1.0 &   0.750000 \\
Trinidad        &   1.0 &   0.272727 \\
Trinidy         &   1.0 &   0.875000 \\
Trinitee        &   1.0 &   0.714286 \\
Triniti         &   1.0 &   0.312500 \\
Trinity         &   1.0 &   0.134653 \\
Trish           &   1.0 &   0.227273 \\
Trisha          &   1.0 &   0.025773 \\
Trishia         &   1.0 &   0.833333 \\
Trista          &   1.0 &   0.102564 \\
Tristan         &   1.0 &   0.206897 \\
Tristen         &   1.0 &   0.272727 \\
Tristin         &   1.0 &   0.347826 \\
Tristina        &   1.0 &   1.000000 \\
Tristine        &   1.0 &   1.000000 \\
Tristyn         &   1.0 &   0.416667 \\
Tritia          &   1.0 &   0.375000 \\
Trixie          &   1.0 &   0.750000 \\
Troi            &   1.0 &   0.833333 \\
Troy            &   1.0 &   0.428571 \\
Troylene        &   1.0 &   1.000000 \\
Tru             &   1.0 &   1.000000 \\
Truc            &   1.0 &   1.000000 \\
Trude           &   1.0 &   1.000000 \\
Trudi           &   1.0 &   0.315789 \\
Trudie          &   1.0 &   0.500000 \\
Trudy           &   1.0 &   0.075949 \\
True            &   1.0 &   0.826087 \\
Trulee          &   1.0 &   1.000000 \\
Truly           &   1.0 &   0.750000 \\
Truth           &   1.0 &   1.000000 \\
Trynity         &   1.0 &   1.000000 \\
Tsuyako         &   1.0 &   1.000000 \\
Tu              &   1.0 &   1.000000 \\
Tuesday         &   1.0 &   0.416667 \\
Tula            &   1.0 &   0.857143 \\
Tulsi           &   1.0 &   1.000000 \\
Tunisia         &   1.0 &   0.714286 \\
Tuongvi         &   1.0 &   1.000000 \\
Turquoise       &   1.0 &   1.000000 \\
Tuyen           &   1.0 &   0.625000 \\
Tuyet           &   1.0 &   0.555556 \\
Tvisha          &   1.0 &   1.000000 \\
Twana           &   1.0 &   1.000000 \\
Twanna          &   1.0 &   1.000000 \\
Twila           &   1.0 &   0.185185 \\
Twisha          &   1.0 &   1.000000 \\
Twyla           &   1.0 &   0.411765 \\
Tya             &   1.0 &   1.000000 \\
Tyana           &   1.0 &   0.333333 \\
Tyanna          &   1.0 &   0.500000 \\
Tyara           &   1.0 &   1.000000 \\
Tyeisha         &   1.0 &   0.625000 \\
Tyesha          &   1.0 &   0.411765 \\
Tyeshia         &   1.0 &   1.000000 \\
Tyiesha         &   1.0 &   1.000000 \\
Tyisha          &   1.0 &   0.384615 \\
Tyla            &   1.0 &   0.454545 \\
Tylar           &   1.0 &   0.555556 \\
Tyleah          &   1.0 &   1.000000 \\
Tylee           &   1.0 &   0.833333 \\
Tylene          &   1.0 &   1.000000 \\
Tyler           &   1.0 &   0.063694 \\
Tylie           &   1.0 &   1.000000 \\
Tylor           &   1.0 &   1.000000 \\
Tylyn           &   1.0 &   1.000000 \\
Tyna            &   1.0 &   1.000000 \\
Tyneisha        &   1.0 &   1.000000 \\
Tynesha         &   1.0 &   1.000000 \\
Tynisa          &   1.0 &   1.000000 \\
Tynisha         &   1.0 &   0.615385 \\
Tyra            &   1.0 &   0.074627 \\
Tyrah           &   1.0 &   1.000000 \\
Tyree           &   1.0 &   0.833333 \\
Tyresha         &   1.0 &   0.555556 \\
Tyrisha         &   1.0 &   0.833333 \\
Tyronda         &   1.0 &   1.000000 \\
Tysha           &   1.0 &   1.000000 \\
Tytiana         &   1.0 &   1.000000 \\
Ulani           &   1.0 &   0.555556 \\
Ulanni          &   1.0 &   1.000000 \\
Uliana          &   1.0 &   1.000000 \\
Ulyana          &   1.0 &   1.000000 \\
Ulyssa          &   1.0 &   1.000000 \\
Uma             &   1.0 &   0.700000 \\
Umaiza          &   1.0 &   1.000000 \\
Umi             &   1.0 &   0.714286 \\
Una             &   1.0 &   0.466667 \\
Unique          &   1.0 &   0.140000 \\
Unity           &   1.0 &   0.833333 \\
Unknown         &   1.0 &   0.476190 \\
Uriah           &   1.0 &   0.625000 \\
Uriel           &   1.0 &   1.000000 \\
Uriyah          &   1.0 &   1.000000 \\
Ursula          &   1.0 &   0.142857 \\
Urvi            &   1.0 &   0.857143 \\
Uyen            &   1.0 &   0.500000 \\
Vaani           &   1.0 &   0.769231 \\
Vaanya          &   1.0 &   0.625000 \\
Vada            &   1.0 &   0.875000 \\
Vaeda           &   1.0 &   0.818182 \\
Vaida           &   1.0 &   0.714286 \\
Vail            &   1.0 &   1.000000 \\
Vaishnavi       &   1.0 &   0.666667 \\
Val             &   1.0 &   0.285714 \\
Valarie         &   1.0 &   0.109091 \\
Valda           &   1.0 &   0.714286 \\
Vale            &   1.0 &   1.000000 \\
Valen           &   1.0 &   1.000000 \\
Valencia        &   1.0 &   0.520000 \\
Valency         &   1.0 &   1.000000 \\
Valene          &   1.0 &   0.700000 \\
Valentina       &   1.0 &   0.956577 \\
Valentine       &   1.0 &   0.444444 \\
Valeri          &   1.0 &   0.500000 \\
Valeria         &   1.0 &   0.333333 \\
Valerie         &   1.0 &   0.398558 \\
Valery          &   1.0 &   0.365854 \\
Valerya         &   1.0 &   1.000000 \\
Valerye         &   1.0 &   1.000000 \\
Valeska         &   1.0 &   0.714286 \\
Valinda         &   1.0 &   0.600000 \\
Valisa          &   1.0 &   0.833333 \\
Valkyrie        &   1.0 &   0.695652 \\
Vallerie        &   1.0 &   0.545455 \\
Vallery         &   1.0 &   0.875000 \\
Valli           &   1.0 &   1.000000 \\
Vallorie        &   1.0 &   1.000000 \\
Valorie         &   1.0 &   0.161290 \\
Vamika          &   1.0 &   1.000000 \\
Van             &   1.0 &   0.263158 \\
Vana            &   1.0 &   0.833333 \\
Vanda           &   1.0 &   1.000000 \\
Vandy           &   1.0 &   1.000000 \\
Vanellope       &   1.0 &   0.655172 \\
Vanesa          &   1.0 &   0.077778 \\
Vanesha         &   1.0 &   1.000000 \\
Vaness          &   1.0 &   0.538462 \\
Vanessa         &   1.0 &   0.073485 \\
Vanessamarie    &   1.0 &   1.000000 \\
Vanetta         &   1.0 &   1.000000 \\
Vaneza          &   1.0 &   0.555556 \\
Vanezza         &   1.0 &   1.000000 \\
Vangie          &   1.0 &   1.000000 \\
Vani            &   1.0 &   1.000000 \\
Vania           &   1.0 &   0.222222 \\
Vanisha         &   1.0 &   0.833333 \\
Vanita          &   1.0 &   1.000000 \\
Vanity          &   1.0 &   0.615385 \\
Vanna           &   1.0 &   0.562500 \\
Vannary         &   1.0 &   1.000000 \\
Vannesa         &   1.0 &   0.156250 \\
Vannessa        &   1.0 &   0.102041 \\
Vannia          &   1.0 &   0.750000 \\
Vanshika        &   1.0 &   1.000000 \\
Vanya           &   1.0 &   1.000000 \\
Varnika         &   1.0 &   0.555556 \\
Varsha          &   1.0 &   0.461538 \\
Varshini        &   1.0 &   1.000000 \\
Vashti          &   1.0 &   1.000000 \\
Vasilisa        &   1.0 &   0.600000 \\
Vasthi          &   1.0 &   1.000000 \\
Vasti           &   1.0 &   1.000000 \\
Vaughn          &   1.0 &   1.000000 \\
Vayda           &   1.0 &   0.700000 \\
Veda            &   1.0 &   1.000000 \\
Vedha           &   1.0 &   1.000000 \\
Vedika          &   1.0 &   0.714286 \\
Veeksha         &   1.0 &   1.000000 \\
Veena           &   1.0 &   1.000000 \\
Veera           &   1.0 &   0.562500 \\
Vega            &   1.0 &   0.555556 \\
Vela            &   1.0 &   0.625000 \\
Velda           &   1.0 &   0.333333 \\
Velen           &   1.0 &   1.000000 \\
Velia           &   1.0 &   0.250000 \\
Velina          &   1.0 &   1.000000 \\
Velinda         &   1.0 &   1.000000 \\
Vella           &   1.0 &   1.000000 \\
Velma           &   1.0 &   0.074627 \\
Velta           &   1.0 &   1.000000 \\
Velva           &   1.0 &   1.000000 \\
Velvet          &   1.0 &   0.315789 \\
Vena            &   1.0 &   1.000000 \\
Venba           &   1.0 &   0.833333 \\
Venecia         &   1.0 &   0.416667 \\
Venesa          &   1.0 &   0.833333 \\
Venessa         &   1.0 &   0.145455 \\
Venetia         &   1.0 &   0.466667 \\
Venezia         &   1.0 &   0.555556 \\
Venice          &   1.0 &   0.650000 \\
Venicia         &   1.0 &   1.000000 \\
Venise          &   1.0 &   1.000000 \\
Venita          &   1.0 &   0.217391 \\
Venus           &   1.0 &   0.894737 \\
Vera            &   1.0 &   0.953488 \\
Verania         &   1.0 &   0.150000 \\
Verda           &   1.0 &   0.500000 \\
Verena          &   1.0 &   0.777778 \\
Verenice        &   1.0 &   0.129630 \\
Verenis         &   1.0 &   1.000000 \\
Verenise        &   1.0 &   0.600000 \\
Veridiana       &   1.0 &   1.000000 \\
Verity          &   1.0 &   0.416667 \\
Verla           &   1.0 &   0.625000 \\
Verna           &   1.0 &   0.086207 \\
Vernell         &   1.0 &   1.000000 \\
Vernetta        &   1.0 &   0.625000 \\
Vernice         &   1.0 &   0.625000 \\
Vernita         &   1.0 &   0.857143 \\
Verona          &   1.0 &   0.625000 \\
Veroncia        &   1.0 &   1.000000 \\
Veronica        &   1.0 &   0.065140 \\
Veronika        &   1.0 &   0.526316 \\
Veronique       &   1.0 &   0.600000 \\
Vesper          &   1.0 &   0.833333 \\
Vesta           &   1.0 &   0.555556 \\
Veta            &   1.0 &   1.000000 \\
Veyda           &   1.0 &   1.000000 \\
Vi              &   1.0 &   0.400000 \\
Via             &   1.0 &   0.714286 \\
Viana           &   1.0 &   1.000000 \\
Vianca          &   1.0 &   0.151515 \\
Vianet          &   1.0 &   1.000000 \\
Vianey          &   1.0 &   0.290909 \\
Vianka          &   1.0 &   0.777778 \\
Vianna          &   1.0 &   0.620690 \\
Vianne          &   1.0 &   1.000000 \\
Vianney         &   1.0 &   0.341463 \\
Vibha           &   1.0 &   0.454545 \\
Vibiana         &   1.0 &   1.000000 \\
Vicci           &   1.0 &   1.000000 \\
Vicenta         &   1.0 &   0.625000 \\
Vichelle        &   1.0 &   1.000000 \\
Vickey          &   1.0 &   0.156250 \\
Vicki           &   1.0 &   0.008596 \\
Vickie          &   1.0 &   0.010753 \\
Vicktoria       &   1.0 &   1.000000 \\
Vicky           &   1.0 &   0.028037 \\
Victor          &   1.0 &   0.294118 \\
Victoria        &   1.0 &   0.512309 \\
Victory         &   1.0 &   0.500000 \\
Victorya        &   1.0 &   0.833333 \\
Vida            &   1.0 &   1.000000 \\
Vidalia         &   1.0 &   1.000000 \\
Videl           &   1.0 &   1.000000 \\
Vidhi           &   1.0 &   0.833333 \\
Vidushi         &   1.0 &   1.000000 \\
Vielka          &   1.0 &   1.000000 \\
Vienna          &   1.0 &   1.000000 \\
Viha            &   1.0 &   1.000000 \\
Vihaana         &   1.0 &   1.000000 \\
Vihana          &   1.0 &   1.000000 \\
Vika            &   1.0 &   1.000000 \\
Viki            &   1.0 &   0.411765 \\
Vikki           &   1.0 &   0.121951 \\
Viktoria        &   1.0 &   0.421053 \\
Vilma           &   1.0 &   0.222222 \\
Vina            &   1.0 &   0.538462 \\
Vincent         &   1.0 &   0.454545 \\
Viola           &   1.0 &   0.309278 \\
Violet          &   1.0 &   1.000000 \\
Violeta         &   1.0 &   0.956989 \\
Violett         &   1.0 &   1.000000 \\
Violetta        &   1.0 &   0.705882 \\
Violette        &   1.0 &   0.886792 \\
Virdiana        &   1.0 &   1.000000 \\
Virgie          &   1.0 &   0.500000 \\
Virgina         &   1.0 &   1.000000 \\
Virginia        &   1.0 &   0.033121 \\
Viridiana       &   1.0 &   0.085470 \\
Virjinia        &   1.0 &   1.000000 \\
Virna           &   1.0 &   1.000000 \\
Vita            &   1.0 &   1.000000 \\
Vitalia         &   1.0 &   1.000000 \\
Vittoria        &   1.0 &   0.454545 \\
Viva            &   1.0 &   1.000000 \\
Viveca          &   1.0 &   1.000000 \\
Vivi            &   1.0 &   0.500000 \\
Vivia           &   1.0 &   1.000000 \\
Vivian          &   1.0 &   0.448878 \\
Viviana         &   1.0 &   0.520635 \\
Viviane         &   1.0 &   0.500000 \\
Viviann         &   1.0 &   0.750000 \\
Vivianna        &   1.0 &   0.592593 \\
Vivianne        &   1.0 &   0.523810 \\
Vivica          &   1.0 &   0.533333 \\
Vivien          &   1.0 &   0.394737 \\
Vivienne        &   1.0 &   0.552036 \\
Vivika          &   1.0 &   1.000000 \\
Vivyana         &   1.0 &   1.000000 \\
Viyona          &   1.0 &   1.000000 \\
Vonda           &   1.0 &   0.136364 \\
Vonetta         &   1.0 &   0.714286 \\
Vonna           &   1.0 &   1.000000 \\
Vonnie          &   1.0 &   0.500000 \\
Vrinda          &   1.0 &   1.000000 \\
Vy              &   1.0 &   0.294118 \\
Walker          &   1.0 &   1.000000 \\
Wanda           &   1.0 &   0.017301 \\
Wannetta        &   1.0 &   1.000000 \\
Waverly         &   1.0 &   0.840000 \\
Waynisha        &   1.0 &   1.000000 \\
Wednesday       &   1.0 &   0.933333 \\
Wenda           &   1.0 &   0.833333 \\
Wende           &   1.0 &   0.857143 \\
Wendee          &   1.0 &   0.555556 \\
Wendi           &   1.0 &   0.077670 \\
Wendie          &   1.0 &   0.333333 \\
Wendolyn        &   1.0 &   1.000000 \\
Wendy           &   1.0 &   0.035013 \\
Wenona          &   1.0 &   0.875000 \\
Wenonah         &   1.0 &   1.000000 \\
Wesley          &   1.0 &   0.937500 \\
Weslie          &   1.0 &   1.000000 \\
Weslyn          &   1.0 &   0.833333 \\
Weslynn         &   1.0 &   1.000000 \\
Westlyn         &   1.0 &   1.000000 \\
Whisper         &   1.0 &   1.000000 \\
Whitley         &   1.0 &   0.363636 \\
Whitnee         &   1.0 &   0.833333 \\
Whitney         &   1.0 &   0.031944 \\
Whitnie         &   1.0 &   1.000000 \\
Whittney        &   1.0 &   0.545455 \\
Wilda           &   1.0 &   0.600000 \\
Wilder          &   1.0 &   0.555556 \\
Wiley           &   1.0 &   1.000000 \\
Wilhelmina      &   1.0 &   0.882353 \\
Willa           &   1.0 &   0.690141 \\
Willamina       &   1.0 &   0.833333 \\
Willette        &   1.0 &   0.833333 \\
William         &   1.0 &   0.300000 \\
Willie          &   1.0 &   0.260870 \\
Willow          &   1.0 &   0.905660 \\
Wilma           &   1.0 &   0.062500 \\
Windi           &   1.0 &   0.777778 \\
Windie          &   1.0 &   1.000000 \\
Windsor         &   1.0 &   1.000000 \\
Windy           &   1.0 &   0.152174 \\
Winifred        &   1.0 &   0.187500 \\
Winnie          &   1.0 &   1.000000 \\
Winnifred       &   1.0 &   0.625000 \\
Winona          &   1.0 &   0.826087 \\
Winry           &   1.0 &   1.000000 \\
Winslow         &   1.0 &   1.000000 \\
Winter          &   1.0 &   1.000000 \\
Wisdom          &   1.0 &   0.714286 \\
Witney          &   1.0 &   1.000000 \\
Wren            &   1.0 &   1.000000 \\
Wrenlee         &   1.0 &   1.000000 \\
Wrenley         &   1.0 &   1.000000 \\
Wrenly          &   1.0 &   1.000000 \\
Wrenn           &   1.0 &   0.800000 \\
Wyatt           &   1.0 &   0.428571 \\
Wylie           &   1.0 &   1.000000 \\
Wynn            &   1.0 &   0.714286 \\
Wynne           &   1.0 &   0.857143 \\
Wynona          &   1.0 &   0.625000 \\
Wynonna         &   1.0 &   1.000000 \\
Wynter          &   1.0 &   1.000000 \\
Xaila           &   1.0 &   1.000000 \\
Xan             &   1.0 &   1.000000 \\
Xandria         &   1.0 &   0.625000 \\
Xara            &   1.0 &   1.000000 \\
Xareni          &   1.0 &   1.000000 \\
Xaria           &   1.0 &   0.454545 \\
Xavier          &   1.0 &   1.000000 \\
Xaviera         &   1.0 &   1.000000 \\
Xayah           &   1.0 &   1.000000 \\
Xcaret          &   1.0 &   0.833333 \\
Xee             &   1.0 &   1.000000 \\
Xela            &   1.0 &   1.000000 \\
Xena            &   1.0 &   0.860000 \\
Xenia           &   1.0 &   0.233333 \\
Xenovia         &   1.0 &   1.000000 \\
Xia             &   1.0 &   0.583333 \\
Xiadani         &   1.0 &   0.833333 \\
Xiamara         &   1.0 &   1.000000 \\
Xiana           &   1.0 &   0.375000 \\
Xianna          &   1.0 &   1.000000 \\
Xiclaly         &   1.0 &   1.000000 \\
Xilenia         &   1.0 &   1.000000 \\
Ximena          &   1.0 &   0.768407 \\
Ximenna         &   1.0 &   0.636364 \\
Xinyi           &   1.0 &   1.000000 \\
Xiomara         &   1.0 &   0.991935 \\
Xitlali         &   1.0 &   0.447761 \\
Xitlalic        &   1.0 &   0.400000 \\
Xitlalli        &   1.0 &   0.461538 \\
Xitlally        &   1.0 &   0.636364 \\
Xitlaly         &   1.0 &   0.274194 \\
Xitllali        &   1.0 &   1.000000 \\
Xochi           &   1.0 &   1.000000 \\
Xochil          &   1.0 &   0.357143 \\
Xochilt         &   1.0 &   0.200000 \\
Xochilth        &   1.0 &   0.833333 \\
Xochitl         &   1.0 &   0.837209 \\
Xoe             &   1.0 &   1.000000 \\
Xoey            &   1.0 &   1.000000 \\
Xolani          &   1.0 &   1.000000 \\
Xotchil         &   1.0 &   1.000000 \\
Xuan            &   1.0 &   0.714286 \\
Xyla            &   1.0 &   1.000000 \\
Xylah           &   1.0 &   1.000000 \\
Xylia           &   1.0 &   1.000000 \\
Xylina          &   1.0 &   1.000000 \\
Xymena          &   1.0 &   1.000000 \\
Yadhira         &   1.0 &   0.184615 \\
Yadira          &   1.0 &   0.053659 \\
Yadria          &   1.0 &   1.000000 \\
Yaeko           &   1.0 &   0.500000 \\
Yael            &   1.0 &   0.863636 \\
Yahaira         &   1.0 &   0.061224 \\
Yahayra         &   1.0 &   1.000000 \\
Yahir           &   1.0 &   1.000000 \\
Yahira          &   1.0 &   0.454545 \\
Yahritza        &   1.0 &   1.000000 \\
Yailin          &   1.0 &   1.000000 \\
Yailyn          &   1.0 &   1.000000 \\
Yaindhi         &   1.0 &   1.000000 \\
Yaira           &   1.0 &   0.625000 \\
Yaire           &   1.0 &   0.184211 \\
Yairis          &   1.0 &   1.000000 \\
Yaiza           &   1.0 &   0.500000 \\
Yajahira        &   1.0 &   1.000000 \\
Yajaira         &   1.0 &   0.071429 \\
Yajayra         &   1.0 &   0.466667 \\
Yakelin         &   1.0 &   0.454545 \\
Yakeline        &   1.0 &   1.000000 \\
Yakira          &   1.0 &   1.000000 \\
Yalda           &   1.0 &   1.000000 \\
Yalitza         &   1.0 &   0.179487 \\
Yalonda         &   1.0 &   1.000000 \\
Yameli          &   1.0 &   0.727273 \\
Yamila          &   1.0 &   0.620690 \\
Yamile          &   1.0 &   0.108696 \\
Yamilet         &   1.0 &   0.450704 \\
Yamileth        &   1.0 &   0.962025 \\
Yamilett        &   1.0 &   0.714286 \\
Yamilette       &   1.0 &   0.857143 \\
Yamilex         &   1.0 &   0.068493 \\
Yaminah         &   1.0 &   1.000000 \\
Yamna           &   1.0 &   0.714286 \\
Yana            &   1.0 &   1.000000 \\
Yanai           &   1.0 &   1.000000 \\
Yanay           &   1.0 &   1.000000 \\
Yancy           &   1.0 &   0.714286 \\
Yanel           &   1.0 &   1.000000 \\
Yaneli          &   1.0 &   0.422222 \\
Yanell          &   1.0 &   1.000000 \\
Yanelli         &   1.0 &   0.600000 \\
Yanelly         &   1.0 &   0.500000 \\
Yanely          &   1.0 &   0.217391 \\
Yanessa         &   1.0 &   1.000000 \\
Yanet           &   1.0 &   0.222222 \\
Yaneth          &   1.0 &   0.857143 \\
Yanett          &   1.0 &   1.000000 \\
Yanette         &   1.0 &   0.555556 \\
Yang            &   1.0 &   1.000000 \\
Yani            &   1.0 &   1.000000 \\
Yanilen         &   1.0 &   0.583333 \\
Yanin           &   1.0 &   0.714286 \\
Yanina          &   1.0 &   0.454545 \\
Yanine          &   1.0 &   1.000000 \\
Yanira          &   1.0 &   0.115385 \\
Yanitza         &   1.0 &   1.000000 \\
Yanna           &   1.0 &   1.000000 \\
Yannely         &   1.0 &   1.000000 \\
Yannis          &   1.0 &   1.000000 \\
Yaquelin        &   1.0 &   0.208333 \\
Yaqueline       &   1.0 &   0.666667 \\
Yaquelyn        &   1.0 &   1.000000 \\
Yara            &   1.0 &   1.000000 \\
Yared           &   1.0 &   1.000000 \\
Yarel           &   1.0 &   0.684211 \\
Yareli          &   1.0 &   0.550898 \\
Yarelie         &   1.0 &   1.000000 \\
Yarelin         &   1.0 &   1.000000 \\
Yarelli         &   1.0 &   0.750000 \\
Yarelly         &   1.0 &   0.555556 \\
Yarely          &   1.0 &   0.540984 \\
Yarelyn         &   1.0 &   1.000000 \\
Yaremi          &   1.0 &   1.000000 \\
Yareni          &   1.0 &   0.466667 \\
Yareth          &   1.0 &   0.714286 \\
Yarethzi        &   1.0 &   1.000000 \\
Yarethzy        &   1.0 &   0.555556 \\
Yaretsi         &   1.0 &   0.384615 \\
Yaretsy         &   1.0 &   1.000000 \\
Yaretzi         &   1.0 &   0.521008 \\
Yaretzie        &   1.0 &   1.000000 \\
Yaretzy         &   1.0 &   0.250000 \\
Yarexi          &   1.0 &   1.000000 \\
Yari            &   1.0 &   1.000000 \\
Yaribeth        &   1.0 &   1.000000 \\
Yaricza         &   1.0 &   1.000000 \\
Yarima          &   1.0 &   1.000000 \\
Yaris           &   1.0 &   0.833333 \\
Yarisbel        &   1.0 &   0.555556 \\
Yarisbeth       &   1.0 &   1.000000 \\
Yarishna        &   1.0 &   1.000000 \\
Yarissa         &   1.0 &   1.000000 \\
Yarithza        &   1.0 &   1.000000 \\
Yaritza         &   1.0 &   0.244186 \\
Yaritzel        &   1.0 &   1.000000 \\
Yaritzi         &   1.0 &   1.000000 \\
Yaritzy         &   1.0 &   1.000000 \\
Yarixa          &   1.0 &   1.000000 \\
Yasaman         &   1.0 &   0.750000 \\
Yasamin         &   1.0 &   0.857143 \\
Yasemin         &   1.0 &   1.000000 \\
Yashica         &   1.0 &   1.000000 \\
Yashika         &   1.0 &   0.833333 \\
Yashvi          &   1.0 &   1.000000 \\
Yaslin          &   1.0 &   0.857143 \\
Yasmeen         &   1.0 &   0.174603 \\
Yasmen          &   1.0 &   1.000000 \\
Yasmin          &   1.0 &   0.133333 \\
Yasmina         &   1.0 &   0.555556 \\
Yasmine         &   1.0 &   0.086957 \\
Yasna           &   1.0 &   1.000000 \\
Yasuko          &   1.0 &   0.583333 \\
Yatana          &   1.0 &   1.000000 \\
Yatzari         &   1.0 &   0.500000 \\
Yatzil          &   1.0 &   0.428571 \\
Yatziri         &   1.0 &   0.714286 \\
Yatziry         &   1.0 &   0.312500 \\
Yaxeni          &   1.0 &   1.000000 \\
Yayeko          &   1.0 &   1.000000 \\
Yayoi           &   1.0 &   1.000000 \\
Yazaira         &   1.0 &   1.000000 \\
Yazleen         &   1.0 &   1.000000 \\
Yazlin          &   1.0 &   0.750000 \\
Yazlyn          &   1.0 &   0.750000 \\
Yazmeen         &   1.0 &   1.000000 \\
Yazmin          &   1.0 &   0.146341 \\
Yazmine         &   1.0 &   0.151515 \\
Yecenia         &   1.0 &   0.179487 \\
Yee             &   1.0 &   1.000000 \\
Yehudis         &   1.0 &   1.000000 \\
Yeila           &   1.0 &   1.000000 \\
Yeilin          &   1.0 &   1.000000 \\
Yeimi           &   1.0 &   0.357143 \\
Yeimy           &   1.0 &   0.666667 \\
Yeira           &   1.0 &   1.000000 \\
Yelena          &   1.0 &   1.000000 \\
Yelitza         &   1.0 &   0.470588 \\
Yemaya          &   1.0 &   1.000000 \\
Yen             &   1.0 &   0.357143 \\
Yena            &   1.0 &   0.888889 \\
Yeneisy         &   1.0 &   1.000000 \\
Yeng            &   1.0 &   1.000000 \\
Yeni            &   1.0 &   0.500000 \\
Yenia           &   1.0 &   1.000000 \\
Yenifer         &   1.0 &   0.263158 \\
Yenna           &   1.0 &   1.000000 \\
Yennifer        &   1.0 &   0.625000 \\
Yenny           &   1.0 &   0.750000 \\
Yer             &   1.0 &   0.230769 \\
Yeraldin        &   1.0 &   0.357143 \\
Yerania         &   1.0 &   1.000000 \\
Yesenia         &   1.0 &   0.021413 \\
Yesica          &   1.0 &   0.133333 \\
Yeslin          &   1.0 &   1.000000 \\
Yesmin          &   1.0 &   1.000000 \\
Yessenia        &   1.0 &   0.076923 \\
Yessica         &   1.0 &   0.090909 \\
Yetzali         &   1.0 &   1.000000 \\
Yeva            &   1.0 &   1.000000 \\
Yevette         &   1.0 &   0.277778 \\
Yexalen         &   1.0 &   1.000000 \\
Yezenia         &   1.0 &   0.384615 \\
Yicel           &   1.0 &   0.833333 \\
Yihan           &   1.0 &   1.000000 \\
Yilda           &   1.0 &   1.000000 \\
Yilia           &   1.0 &   0.800000 \\
Yilin           &   1.0 &   1.000000 \\
Ying            &   1.0 &   0.750000 \\
Yinuo           &   1.0 &   1.000000 \\
Yisel           &   1.0 &   0.285714 \\
Yissel          &   1.0 &   0.545455 \\
Yiyi            &   1.0 &   0.625000 \\
Yizel           &   1.0 &   1.000000 \\
Yizza           &   1.0 &   1.000000 \\
Yliana          &   1.0 &   1.000000 \\
Ymani           &   1.0 &   1.000000 \\
Ynez            &   1.0 &   0.714286 \\
Yoali           &   1.0 &   1.000000 \\
Yoana           &   1.0 &   0.222222 \\
Yoandra         &   1.0 &   1.000000 \\
Yoanna          &   1.0 &   0.454545 \\
Yoatzi          &   1.0 &   0.714286 \\
Yobana          &   1.0 &   1.000000 \\
Yocelin         &   1.0 &   0.161290 \\
Yoceline        &   1.0 &   1.000000 \\
Yocelyn         &   1.0 &   0.391304 \\
Yocelyne        &   1.0 &   1.000000 \\
Yocheved        &   1.0 &   1.000000 \\
Yohana          &   1.0 &   0.882353 \\
Yohanna         &   1.0 &   0.666667 \\
Yoko            &   1.0 &   0.625000 \\
Yolanda         &   1.0 &   0.011429 \\
Yolando         &   1.0 &   1.000000 \\
Yolette         &   1.0 &   1.000000 \\
Yolonda         &   1.0 &   0.300000 \\
Yoltzin         &   1.0 &   1.000000 \\
Yomaira         &   1.0 &   0.172414 \\
Yomara          &   1.0 &   1.000000 \\
Yomayra         &   1.0 &   1.000000 \\
Yomira          &   1.0 &   0.272727 \\
Yoneko          &   1.0 &   0.750000 \\
Yorley          &   1.0 &   0.928571 \\
Yoselin         &   1.0 &   0.182353 \\
Yoseline        &   1.0 &   0.193548 \\
Yoselyn         &   1.0 &   0.204545 \\
Yoshi           &   1.0 &   1.000000 \\
Yoshie          &   1.0 &   1.000000 \\
Yoshiko         &   1.0 &   0.147059 \\
Yoshimi         &   1.0 &   1.000000 \\
Yoshiye         &   1.0 &   0.416667 \\
Yosra           &   1.0 &   1.000000 \\
Yosselin        &   1.0 &   0.454545 \\
Yosselyn        &   1.0 &   1.000000 \\
Youa            &   1.0 &   0.777778 \\
Youlanda        &   1.0 &   1.000000 \\
Young           &   1.0 &   0.833333 \\
Yousra          &   1.0 &   1.000000 \\
Yovana          &   1.0 &   0.375000 \\
Yovanna         &   1.0 &   0.500000 \\
Yoyo            &   1.0 &   0.833333 \\
Ysabel          &   1.0 &   0.400000 \\
Ysabella        &   1.0 &   0.333333 \\
Ysabelle        &   1.0 &   0.615385 \\
Ysamar          &   1.0 &   1.000000 \\
Ysenia          &   1.0 &   0.666667 \\
Ytzel           &   1.0 &   0.555556 \\
Yu              &   1.0 &   1.000000 \\
Yudany          &   1.0 &   1.000000 \\
Yudith          &   1.0 &   0.454545 \\
Yue             &   1.0 &   1.000000 \\
Yuhan           &   1.0 &   1.000000 \\
Yui             &   1.0 &   0.454545 \\
Yuka            &   1.0 &   0.500000 \\
Yukari          &   1.0 &   1.000000 \\
Yuki            &   1.0 &   0.545455 \\
Yukiko          &   1.0 &   0.500000 \\
Yukiye          &   1.0 &   0.625000 \\
Yuko            &   1.0 &   1.000000 \\
Yulanda         &   1.0 &   1.000000 \\
Yuleimy         &   1.0 &   1.000000 \\
Yuli            &   1.0 &   0.857143 \\
Yulia           &   1.0 &   0.625000 \\
Yuliana         &   1.0 &   0.264000 \\
Yulianna        &   1.0 &   0.406250 \\
Yulie           &   1.0 &   1.000000 \\
Yuliet          &   1.0 &   1.000000 \\
Yulieth         &   1.0 &   1.000000 \\
Yulisa          &   1.0 &   0.065789 \\
Yulissa         &   1.0 &   0.109890 \\
Yulitza         &   1.0 &   1.000000 \\
Yuliza          &   1.0 &   0.400000 \\
Yuma            &   1.0 &   1.000000 \\
Yumalai         &   1.0 &   1.000000 \\
Yumalay         &   1.0 &   0.545455 \\
Yumi            &   1.0 &   0.909091 \\
Yumiko          &   1.0 &   1.000000 \\
Yumna           &   1.0 &   1.000000 \\
Yuna            &   1.0 &   1.000000 \\
Yunuen          &   1.0 &   0.454545 \\
Yurani          &   1.0 &   0.320000 \\
Yurany          &   1.0 &   1.000000 \\
Yuri            &   1.0 &   0.266667 \\
Yuriana         &   1.0 &   0.666667 \\
Yurico          &   1.0 &   0.833333 \\
Yuridia         &   1.0 &   0.121212 \\
Yuridiana       &   1.0 &   1.000000 \\
Yuriko          &   1.0 &   0.263158 \\
Yuritza         &   1.0 &   0.875000 \\
Yuritzi         &   1.0 &   0.294118 \\
Yuritzy         &   1.0 &   1.000000 \\
Yury            &   1.0 &   0.500000 \\
Yusra           &   1.0 &   1.000000 \\
Yutong          &   1.0 &   1.000000 \\
Yuval           &   1.0 &   1.000000 \\
Yuvia           &   1.0 &   0.571429 \\
Yuxi            &   1.0 &   1.000000 \\
Yvanna          &   1.0 &   0.625000 \\
Yvett           &   1.0 &   0.555556 \\
Yvette          &   1.0 &   0.067873 \\
Yvonna          &   1.0 &   0.857143 \\
Yvonne          &   1.0 &   0.040230 \\
Yvonnie         &   1.0 &   1.000000 \\
Yzabel          &   1.0 &   1.000000 \\
Yzabella        &   1.0 &   0.384615 \\
Yzabelle        &   1.0 &   1.000000 \\
Zaara           &   1.0 &   0.750000 \\
Zabdi           &   1.0 &   1.000000 \\
Zabrina         &   1.0 &   0.545455 \\
Zachary         &   1.0 &   0.388889 \\
Zada            &   1.0 &   0.750000 \\
Zadie           &   1.0 &   1.000000 \\
Zaela           &   1.0 &   1.000000 \\
Zahara          &   1.0 &   0.520000 \\
Zahava          &   1.0 &   1.000000 \\
Zahira          &   1.0 &   0.625000 \\
Zahra           &   1.0 &   0.735294 \\
Zahraa          &   1.0 &   1.000000 \\
Zahrah          &   1.0 &   1.000000 \\
Zaia            &   1.0 &   0.800000 \\
Zaiah           &   1.0 &   1.000000 \\
Zaida           &   1.0 &   0.315789 \\
Zaidee          &   1.0 &   0.833333 \\
Zaidy           &   1.0 &   1.000000 \\
Zaila           &   1.0 &   0.923077 \\
Zailey          &   1.0 &   0.800000 \\
Zailyn          &   1.0 &   1.000000 \\
Zaina           &   1.0 &   0.789474 \\
Zainab          &   1.0 &   0.864865 \\
Zainah          &   1.0 &   1.000000 \\
Zaira           &   1.0 &   0.728814 \\
Zairah          &   1.0 &   1.000000 \\
Zaire           &   1.0 &   0.454545 \\
Zaiya           &   1.0 &   0.714286 \\
Zaiyah          &   1.0 &   1.000000 \\
Zakia           &   1.0 &   1.000000 \\
Zakiah          &   1.0 &   1.000000 \\
Zakiya          &   1.0 &   0.416667 \\
Zakiyah         &   1.0 &   1.000000 \\
Zakiyyah        &   1.0 &   0.500000 \\
Zala            &   1.0 &   1.000000 \\
Zalaya          &   1.0 &   1.000000 \\
Zalayah         &   1.0 &   1.000000 \\
Zalina          &   1.0 &   1.000000 \\
Zaliyah         &   1.0 &   1.000000 \\
Zalma           &   1.0 &   1.000000 \\
Zamantha        &   1.0 &   0.500000 \\
Zamara          &   1.0 &   1.000000 \\
Zamari          &   1.0 &   1.000000 \\
Zamaya          &   1.0 &   0.714286 \\
Zamiah          &   1.0 &   1.000000 \\
Zamira          &   1.0 &   0.555556 \\
Zamirah         &   1.0 &   1.000000 \\
Zamiyah         &   1.0 &   0.555556 \\
Zamora          &   1.0 &   1.000000 \\
Zamya           &   1.0 &   1.000000 \\
Zamyra          &   1.0 &   1.000000 \\
Zana            &   1.0 &   0.368421 \\
Zanaya          &   1.0 &   1.000000 \\
Zanayah         &   1.0 &   1.000000 \\
Zandra          &   1.0 &   0.416667 \\
Zaneta          &   1.0 &   0.666667 \\
Zanetta         &   1.0 &   1.000000 \\
Zania           &   1.0 &   1.000000 \\
Zaniah          &   1.0 &   0.818182 \\
Zaniya          &   1.0 &   0.500000 \\
Zaniyah         &   1.0 &   0.902439 \\
Zanna           &   1.0 &   1.000000 \\
Zara            &   1.0 &   0.817734 \\
Zarah           &   1.0 &   0.888889 \\
Zarahi          &   1.0 &   0.833333 \\
Zarai           &   1.0 &   1.000000 \\
Zaraya          &   1.0 &   1.000000 \\
Zarayah         &   1.0 &   1.000000 \\
Zareen          &   1.0 &   1.000000 \\
Zareena         &   1.0 &   1.000000 \\
Zarela          &   1.0 &   1.000000 \\
Zari            &   1.0 &   1.000000 \\
Zaria           &   1.0 &   1.000000 \\
Zariah          &   1.0 &   0.925373 \\
Zarina          &   1.0 &   0.529412 \\
Zariya          &   1.0 &   0.454545 \\
Zariyah         &   1.0 &   0.944444 \\
Zarria          &   1.0 &   1.000000 \\
Zarya           &   1.0 &   1.000000 \\
Zaryah          &   1.0 &   0.750000 \\
Zavannah        &   1.0 &   1.000000 \\
Zaya            &   1.0 &   0.558824 \\
Zayah           &   1.0 &   0.750000 \\
Zayana          &   1.0 &   1.000000 \\
Zayda           &   1.0 &   0.521739 \\
Zaydee          &   1.0 &   1.000000 \\
Zayla           &   1.0 &   1.000000 \\
Zaylah          &   1.0 &   0.571429 \\
Zaylani         &   1.0 &   1.000000 \\
Zaylee          &   1.0 &   0.656250 \\
Zaylie          &   1.0 &   1.000000 \\
Zaylynn         &   1.0 &   1.000000 \\
Zayna           &   1.0 &   0.785714 \\
Zaynab          &   1.0 &   0.684211 \\
Zaynah          &   1.0 &   0.714286 \\
Zayra           &   1.0 &   0.461538 \\
Zazie           &   1.0 &   1.000000 \\
Zeba            &   1.0 &   1.000000 \\
Zeena           &   1.0 &   1.000000 \\
Zeenat          &   1.0 &   1.000000 \\
Zehra           &   1.0 &   1.000000 \\
Zeina           &   1.0 &   0.466667 \\
Zeinab          &   1.0 &   0.555556 \\
Zelda           &   1.0 &   0.884058 \\
Zelena          &   1.0 &   1.000000 \\
Zelene          &   1.0 &   1.000000 \\
Zelia           &   1.0 &   1.000000 \\
Zelie           &   1.0 &   0.500000 \\
Zella           &   1.0 &   0.368421 \\
Zelma           &   1.0 &   0.454545 \\
Zeltzin         &   1.0 &   1.000000 \\
Zemira          &   1.0 &   0.625000 \\
Zemirah         &   1.0 &   0.750000 \\
Zen             &   1.0 &   1.000000 \\
Zena            &   1.0 &   0.285714 \\
Zenab           &   1.0 &   1.000000 \\
Zenaida         &   1.0 &   0.476190 \\
Zenaya          &   1.0 &   1.000000 \\
Zendaya         &   1.0 &   1.000000 \\
Zenia           &   1.0 &   0.437500 \\
Zeniah          &   1.0 &   1.000000 \\
Zeniyah         &   1.0 &   1.000000 \\
Zenobia         &   1.0 &   0.833333 \\
Zephaniah       &   1.0 &   1.000000 \\
Zephyr          &   1.0 &   0.666667 \\
Zeppelin        &   1.0 &   1.000000 \\
Zeriah          &   1.0 &   1.000000 \\
Zeya            &   1.0 &   1.000000 \\
Zeynep          &   1.0 &   0.923077 \\
Zhane           &   1.0 &   0.208333 \\
Zharia          &   1.0 &   1.000000 \\
Zhavia          &   1.0 &   0.517241 \\
Zhuri           &   1.0 &   1.000000 \\
Zia             &   1.0 &   0.864865 \\
Ziah            &   1.0 &   0.500000 \\
Ziana           &   1.0 &   0.833333 \\
Zianna          &   1.0 &   1.000000 \\
Zienna          &   1.0 &   1.000000 \\
Ziggy           &   1.0 &   0.454545 \\
Zimal           &   1.0 &   0.625000 \\
Zina            &   1.0 &   0.138889 \\
Zinnia          &   1.0 &   0.714286 \\
Zion            &   1.0 &   0.666667 \\
Ziona           &   1.0 &   0.833333 \\
Zipporah        &   1.0 &   0.666667 \\
Zita            &   1.0 &   1.000000 \\
Zitlali         &   1.0 &   0.294118 \\
Zitlalic        &   1.0 &   1.000000 \\
Zitlalli        &   1.0 &   1.000000 \\
Zitlaly         &   1.0 &   0.227273 \\
Ziva            &   1.0 &   1.000000 \\
Zixuan          &   1.0 &   1.000000 \\
Ziya            &   1.0 &   1.000000 \\
Ziyah           &   1.0 &   0.500000 \\
Zlata           &   1.0 &   1.000000 \\
Zoe             &   1.0 &   0.799799 \\
Zoee            &   1.0 &   0.428571 \\
Zoei            &   1.0 &   1.000000 \\
Zoejane         &   1.0 &   1.000000 \\
Zoella          &   1.0 &   1.000000 \\
Zoelle          &   1.0 &   0.750000 \\
Zoey            &   1.0 &   0.693510 \\
Zofia           &   1.0 &   0.538462 \\
Zoha            &   1.0 &   0.818182 \\
Zohal           &   1.0 &   1.000000 \\
Zohar           &   1.0 &   1.000000 \\
Zohemy          &   1.0 &   1.000000 \\
Zohra           &   1.0 &   1.000000 \\
Zoie            &   1.0 &   0.400000 \\
Zoila           &   1.0 &   0.315789 \\
Zola            &   1.0 &   0.708333 \\
Zona            &   1.0 &   0.857143 \\
Zonia           &   1.0 &   1.000000 \\
Zooey           &   1.0 &   0.210526 \\
Zophia          &   1.0 &   0.750000 \\
Zora            &   1.0 &   1.000000 \\
Zoraida         &   1.0 &   0.545455 \\
Zoraya          &   1.0 &   1.000000 \\
Zoriah          &   1.0 &   1.000000 \\
Zosia           &   1.0 &   0.555556 \\
Zoua            &   1.0 &   0.777778 \\
Zowie           &   1.0 &   0.500000 \\
Zoya            &   1.0 &   1.000000 \\
Zula            &   1.0 &   1.000000 \\
Zulay           &   1.0 &   0.416667 \\
Zuleica         &   1.0 &   0.500000 \\
Zuleidy         &   1.0 &   1.000000 \\
Zuleika         &   1.0 &   0.416667 \\
Zuleima         &   1.0 &   0.241379 \\
Zulema          &   1.0 &   0.562500 \\
Zuley           &   1.0 &   1.000000 \\
Zuleyka         &   1.0 &   0.205882 \\
Zuleyma         &   1.0 &   0.222222 \\
Zully           &   1.0 &   0.428571 \\
Zulma           &   1.0 &   0.368421 \\
Zuly            &   1.0 &   0.750000 \\
Zunaira         &   1.0 &   1.000000 \\
Zuni            &   1.0 &   1.000000 \\
Zuri            &   1.0 &   1.000000 \\
Zuria           &   1.0 &   0.294118 \\
Zuriel          &   1.0 &   1.000000 \\
Zurisadai       &   1.0 &   0.750000 \\
Zury            &   1.0 &   0.533333 \\
Zuzu            &   1.0 &   1.000000 \\
Zya             &   1.0 &   1.000000 \\
Zyah            &   1.0 &   1.000000 \\
Zyana           &   1.0 &   0.388889 \\
Zyanna          &   1.0 &   0.625000 \\
Zyanya          &   1.0 &   0.466667 \\
Zyla            &   1.0 &   1.000000 \\
Zylah           &   1.0 &   1.000000 \\
Zyra            &   1.0 &   1.000000 \\
Zyrah           &   1.0 &   0.833333 \\
\bottomrule
\end{tabular}

\hypertarget{some-data-science-payoff}{%
\subsection{Some Data Science Payoff}\label{some-data-science-payoff}}

By sorting \texttt{rtp\_table}, we can see the names whose popularity
has decreased the most.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rtp\_table }\OperatorTok{=}\NormalTok{ rtp\_table.rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{"Count"}\NormalTok{: }\StringTok{"Count RTP"}\NormalTok{\})}
\NormalTok{rtp\_table.sort\_values(}\StringTok{"Count RTP"}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &  Year &  Count RTP \\
Name   &       &            \\
\midrule
Debra  &   1.0 &   0.001260 \\
Debbie &   1.0 &   0.002815 \\
Carol  &   1.0 &   0.003180 \\
Tammy  &   1.0 &   0.003249 \\
Susan  &   1.0 &   0.003305 \\
\bottomrule
\end{tabular}

To visualize the above \texttt{DataFrame}, let's look at the line plot
below:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.express }\ImportTok{as}\NormalTok{ px}
\NormalTok{px.line(f\_babynames[f\_babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Debra"}\NormalTok{], x }\OperatorTok{=} \StringTok{"Year"}\NormalTok{, y }\OperatorTok{=} \StringTok{"Count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

We can get the list of the top 10 names and then plot popularity with
the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top10 }\OperatorTok{=}\NormalTok{ rtp\_table.sort\_values(}\StringTok{"Count RTP"}\NormalTok{).head(}\DecValTok{10}\NormalTok{).index}
\NormalTok{px.line(}
\NormalTok{    f\_babynames[f\_babynames[}\StringTok{"Name"}\NormalTok{].isin(top10)], }
\NormalTok{    x }\OperatorTok{=} \StringTok{"Year"}\NormalTok{, }
\NormalTok{    y }\OperatorTok{=} \StringTok{"Count"}\NormalTok{, }
\NormalTok{    color }\OperatorTok{=} \StringTok{"Name"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

As a quick exercise, consider what code would compute the total number
of babies with each name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.groupby(}\StringTok{"Name"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head()}
\CommentTok{\# alternative solution: }
\CommentTok{\# babynames.groupby("Name")[["Count"]].sum()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Name    &        \\
\midrule
Aadan   &     18 \\
Aadarsh &      6 \\
Aaden   &    647 \\
Aadhav  &     27 \\
Aadhini &      6 \\
\bottomrule
\end{tabular}

\hypertarget{groupby-continued}{%
\section{\texorpdfstring{\texttt{.groupby()},
Continued}{.groupby(), Continued}}\label{groupby-continued}}

We'll work with the \texttt{elections} \texttt{DataFrame} again.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{)}
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

\hypertarget{raw-groupby-objects}{%
\subsection{\texorpdfstring{Raw \texttt{GroupBy}
Objects}{Raw GroupBy Objects}}\label{raw-groupby-objects}}

The result of \texttt{groupby} applied to a \texttt{DataFrame} is a
\texttt{DataFrameGroupBy} object, \textbf{not} a \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_by\_year }\OperatorTok{=}\NormalTok{ elections.groupby(}\StringTok{"Year"}\NormalTok{)}
\BuiltInTok{type}\NormalTok{(grouped\_by\_year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
pandas.core.groupby.generic.DataFrameGroupBy
\end{verbatim}

There are several ways to look into \texttt{DataFrameGroupBy} objects:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_by\_party }\OperatorTok{=}\NormalTok{ elections.groupby(}\StringTok{"Party"}\NormalTok{)}
\NormalTok{grouped\_by\_party.groups}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{'American': [22, 126], 'American Independent': [115, 119, 124], 'Anti-Masonic': [6], 'Anti-Monopoly': [38], 'Citizens': [127], 'Communist': [89], 'Constitution': [160, 164, 172], 'Constitutional Union': [24], 'Democratic': [2, 4, 8, 10, 13, 14, 17, 20, 28, 29, 34, 37, 39, 45, 47, 52, 55, 57, 64, 70, 74, 77, 81, 83, 86, 91, 94, 97, 100, 105, 108, 111, 114, 116, 118, 123, 129, 134, 137, 140, 144, 151, 158, 162, 168, 176, 178], 'Democratic-Republican': [0, 1], 'Dixiecrat': [103], 'Farmer–Labor': [78], 'Free Soil': [15, 18], 'Green': [149, 155, 156, 165, 170, 177, 181], 'Greenback': [35], 'Independent': [121, 130, 143, 161, 167, 174], 'Liberal Republican': [31], 'Libertarian': [125, 128, 132, 138, 139, 146, 153, 159, 163, 169, 175, 180], 'National Democratic': [50], 'National Republican': [3, 5], 'National Union': [27], 'Natural Law': [148], 'New Alliance': [136], 'Northern Democratic': [26], 'Populist': [48, 61, 141], 'Progressive': [68, 82, 101, 107], 'Prohibition': [41, 44, 49, 51, 54, 59, 63, 67, 73, 75, 99], 'Reform': [150, 154], 'Republican': [21, 23, 30, 32, 33, 36, 40, 43, 46, 53, 56, 60, 65, 69, 72, 79, 80, 84, 87, 90, 96, 98, 104, 106, 109, 112, 113, 117, 120, 122, 131, 133, 135, 142, 145, 152, 157, 166, 171, 173, 179], 'Socialist': [58, 62, 66, 71, 76, 85, 88, 92, 95, 102], 'Southern Democratic': [25], 'States' Rights': [110], 'Taxpayers': [147], 'Union': [93], 'Union Labor': [42], 'Whig': [7, 9, 11, 12, 16, 19]}
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_by\_party.get\_group(}\StringTok{"Socialist"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &        Candidate &      Party &  Popular vote & Result &         \% \\
\midrule
58  &  1904 &   Eugene V. Debs &  Socialist &        402810 &   loss &  2.985897 \\
62  &  1908 &   Eugene V. Debs &  Socialist &        420852 &   loss &  2.850866 \\
66  &  1912 &   Eugene V. Debs &  Socialist &        901551 &   loss &  6.004354 \\
71  &  1916 &  Allan L. Benson &  Socialist &        590524 &   loss &  3.194193 \\
76  &  1920 &   Eugene V. Debs &  Socialist &        913693 &   loss &  3.428282 \\
85  &  1928 &    Norman Thomas &  Socialist &        267478 &   loss &  0.728623 \\
88  &  1932 &    Norman Thomas &  Socialist &        884885 &   loss &  2.236211 \\
92  &  1936 &    Norman Thomas &  Socialist &        187910 &   loss &  0.412876 \\
95  &  1940 &    Norman Thomas &  Socialist &        116599 &   loss &  0.234237 \\
102 &  1948 &    Norman Thomas &  Socialist &        139569 &   loss &  0.286312 \\
\bottomrule
\end{tabular}

\hypertarget{other-groupby-methods}{%
\subsection{\texorpdfstring{Other \texttt{GroupBy}
Methods}{Other GroupBy Methods}}\label{other-groupby-methods}}

There are many aggregation methods we can use with \texttt{.agg}. Some
useful options are:

\begin{itemize}
\tightlist
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.mean.html\#pandas.core.groupby.DataFrameGroupBy.mean}{\texttt{.mean}}:
  creates a new \texttt{DataFrame} with the mean value of each group
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.sum.html\#pandas.core.groupby.DataFrameGroupBy.sum}{\texttt{.sum}}:
  creates a new \texttt{DataFrame} with the sum of each group
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.max.html\#pandas.core.groupby.DataFrameGroupBy.max}{\texttt{.max}}
  and
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.min.html\#pandas.core.groupby.DataFrameGroupBy.min}{\texttt{.min}}:
  creates a new \texttt{DataFrame} with the maximum/minimum value of
  each group
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.first.html\#pandas.core.groupby.DataFrameGroupBy.first}{\texttt{.first}}
  and
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.last.html\#pandas.core.groupby.DataFrameGroupBy.last}{\texttt{.last}}:
  creates a new \texttt{DataFrame} with the first/last row in each group
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.size.html\#pandas.core.groupby.DataFrameGroupBy.size}{\texttt{.size}}:
  creates a new \textbf{\texttt{Series}} with the number of entries in
  each group
\item
  \href{https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.count.html\#pandas.core.groupby.DataFrameGroupBy.count}{\texttt{.count}}:
  creates a new \textbf{\texttt{DataFrame}} with the number of entries,
  excluding missing values.
\end{itemize}

Let's illustrate some examples by creating a \texttt{DataFrame} called
\texttt{df}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{\textquotesingle{}letter\textquotesingle{}}\NormalTok{:[}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{], }
                   \StringTok{\textquotesingle{}num\textquotesingle{}}\NormalTok{:[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,np.NaN,}\DecValTok{4}\NormalTok{], }
                   \StringTok{\textquotesingle{}state\textquotesingle{}}\NormalTok{:[np.NaN, }\StringTok{\textquotesingle{}tx\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}fl\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}hi\textquotesingle{}}\NormalTok{, np.NaN, }\StringTok{\textquotesingle{}ak\textquotesingle{}}\NormalTok{]\})}
\NormalTok{df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llrl}
\toprule
{} & letter &  num & state \\
\midrule
0 &      A &  1.0 &   NaN \\
1 &      A &  2.0 &    tx \\
2 &      B &  3.0 &    fl \\
3 &      C &  4.0 &    hi \\
4 &      C &  NaN &   NaN \\
5 &      C &  4.0 &    ak \\
\bottomrule
\end{tabular}

Note the slight difference between \texttt{.size()} and
\texttt{.count()}: while \texttt{.size()} returns a \texttt{Series} and
counts the number of entries including the missing values,
\texttt{.count()} returns a \texttt{DataFrame} and counts the number of
entries in each column \emph{excluding missing values}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.groupby(}\StringTok{"letter"}\NormalTok{).size()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  0 \\
letter &    \\
\midrule
A      &  2 \\
B      &  1 \\
C      &  3 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df.groupby(}\StringTok{"letter"}\NormalTok{).count()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &  num &  state \\
letter &      &        \\
\midrule
A      &    2 &      1 \\
B      &    1 &      1 \\
C      &    2 &      2 \\
\bottomrule
\end{tabular}

You might recall that the \texttt{value\_counts()} function in the
previous note does something similar. It turns out
\texttt{value\_counts()} and \texttt{groupby.size()} are the same,
except \texttt{value\_counts()} sorts the resulting \texttt{Series} in
descending order automatically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{"letter"}\NormalTok{].value\_counts()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  letter \\
\midrule
C &       3 \\
A &       2 \\
B &       1 \\
\bottomrule
\end{tabular}

These (and other) aggregation functions are so common that
\texttt{pandas} allows for writing shorthand. Instead of explicitly
stating the use of \texttt{.agg}, we can call the function directly on
the \texttt{GroupBy} object.

For example, the following are equivalent:

\begin{itemize}
\tightlist
\item
  \texttt{elections.groupby("Candidate").agg(mean)}
\item
  \texttt{elections.groupby("Candidate").mean()}
\end{itemize}

There are many other methods that \texttt{pandas} supports. You can
check them out on the
\href{https://pandas.pydata.org/docs/reference/groupby.html}{\texttt{pandas}
documentation}.

\hypertarget{filtering-by-group}{%
\subsection{Filtering by Group}\label{filtering-by-group}}

Another common use for \texttt{GroupBy} objects is to filter data by
group.

\texttt{groupby.filter} takes an argument \texttt{func}, where
\texttt{func} is a function that:

\begin{itemize}
\tightlist
\item
  Takes a \texttt{DataFrame} object as input
\item
  Returns a single \texttt{True} or \texttt{False}.
\end{itemize}

\texttt{groupby.filter} applies \texttt{func} to each
group/sub-\texttt{DataFrame}:

\begin{itemize}
\tightlist
\item
  If \texttt{func} returns \texttt{True} for a group, then all rows
  belonging to the group are preserved.
\item
  If \texttt{func} returns \texttt{False} for a group, then all rows
  belonging to that group are filtered out.
\end{itemize}

In other words, sub-\texttt{DataFrame}s that correspond to \texttt{True}
are returned in the final result, whereas those with a \texttt{False}
value are not. Importantly, \texttt{groupby.filter} is different from
\texttt{groupby.agg} in that an \emph{entire} sub-\texttt{DataFrame} is
returned in the final \texttt{DataFrame}, not just a single row. As a
result, \texttt{groupby.filter} preserves the original indices and the
column we grouped on does \textbf{NOT} become the index!

To illustrate how this happens, let's go back to the \texttt{elections}
dataset. Say we want to identify ``tight'' election years -- that is, we
want to find all rows that correspond to election years where all
candidates in that year won a similar portion of the total vote.
Specifically, let's find all rows corresponding to a year where no
candidate won more than 45\% of the total vote.

In other words, we want to:

\begin{itemize}
\tightlist
\item
  Find the years where the maximum \texttt{\%} in that year is less than
  45\%
\item
  Return all \texttt{DataFrame} rows that correspond to these years
\end{itemize}

For each year, we need to find the maximum \texttt{\%} among \emph{all}
rows for that year. If this maximum \texttt{\%} is lower than 45\%, we
will tell \texttt{pandas} to keep all rows corresponding to that year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.groupby(}\StringTok{"Year"}\NormalTok{).}\BuiltInTok{filter}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ sf: sf[}\StringTok{"\%"}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{\textless{}} \DecValTok{45}\NormalTok{).head(}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &             Candidate &                 Party &  Popular vote & Result &          \% \\
\midrule
23 &  1860 &       Abraham Lincoln &            Republican &       1855993 &    win &  39.699408 \\
24 &  1860 &             John Bell &  Constitutional Union &        590901 &   loss &  12.639283 \\
25 &  1860 &  John C. Breckinridge &   Southern Democratic &        848019 &   loss &  18.138998 \\
26 &  1860 &    Stephen A. Douglas &   Northern Democratic &       1380202 &   loss &  29.522311 \\
66 &  1912 &        Eugene V. Debs &             Socialist &        901551 &   loss &   6.004354 \\
67 &  1912 &      Eugene W. Chafin &           Prohibition &        208156 &   loss &   1.386325 \\
68 &  1912 &    Theodore Roosevelt &           Progressive &       4122721 &   loss &  27.457433 \\
69 &  1912 &          William Taft &            Republican &       3486242 &   loss &  23.218466 \\
70 &  1912 &        Woodrow Wilson &            Democratic &       6296284 &    win &  41.933422 \\
\bottomrule
\end{tabular}

What's going on here? In this example, we've defined our filtering
function, \texttt{func}, to be
\texttt{lambda\ sf:\ sf{[}"\%"{]}.max()\ \textless{}\ 45}. This
filtering function will find the maximum \texttt{"\%"} value among all
entries in the grouped sub-\texttt{DataFrame}, which we call
\texttt{sf}. If the maximum value is less than 45, then the filter
function will return \texttt{True} and all rows in that grouped
sub-\texttt{DataFrame} will appear in the final output
\texttt{DataFrame}.

Examine the \texttt{DataFrame} above. Notice how, in this preview of the
first 9 rows, all entries from the years 1860 and 1912 appear. This
means that in 1860 and 1912, no candidate in that year won more than
45\% of the total vote.

You may ask: how is the \texttt{groupby.filter} procedure different to
the boolean filtering we've seen previously? Boolean filtering considers
\emph{individual} rows when applying a boolean condition. For example,
the code \texttt{elections{[}elections{[}"\%"{]}\ \textless{}\ 45{]}}
will check the \texttt{"\%"} value of every single row in
\texttt{elections}; if it is less than 45, then that row will be kept in
the output. \texttt{groupby.filter}, in contrast, applies a boolean
condition \emph{across} all rows in a group. If not all rows in that
group satisfy the condition specified by the filter, the entire group
will be discarded in the output.

\hypertarget{aggregation-with-lambda-functions}{%
\subsection{\texorpdfstring{Aggregation with \texttt{lambda}
Functions}{Aggregation with lambda Functions}}\label{aggregation-with-lambda-functions}}

What if we wish to aggregate our \texttt{DataFrame} using a non-standard
function -- for example, a function of our own design? We can do so by
combining \texttt{.agg} with \texttt{lambda} expressions.

Let's first consider a puzzle to jog our memory. We will attempt to find
the \texttt{Candidate} from each \texttt{Party} with the highest
\texttt{\%} of votes.

A naive approach may be to group by the \texttt{Party} column and
aggregate by the maximum.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.groupby(}\StringTok{"Party"}\NormalTok{).agg(}\BuiltInTok{max}\NormalTok{).head(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlrlr}
\toprule
{} &  Year &           Candidate &  Popular vote & Result &          \% \\
Party                 &       &                     &               &        &            \\
\midrule
American              &  1976 &  Thomas J. Anderson &        873053 &   loss &  21.554001 \\
American Independent  &  1976 &       Lester Maddox &       9901118 &   loss &  13.571218 \\
Anti-Masonic          &  1832 &        William Wirt &        100715 &   loss &   7.821583 \\
Anti-Monopoly         &  1884 &     Benjamin Butler &        134294 &   loss &   1.335838 \\
Citizens              &  1980 &      Barry Commoner &        233052 &   loss &   0.270182 \\
Communist             &  1932 &   William Z. Foster &        103307 &   loss &   0.261069 \\
Constitution          &  2016 &    Michael Peroutka &        203091 &   loss &   0.152398 \\
Constitutional Union  &  1860 &           John Bell &        590901 &   loss &  12.639283 \\
Democratic            &  2020 &      Woodrow Wilson &      81268924 &    win &  61.344703 \\
Democratic-Republican &  1824 &   John Quincy Adams &        151271 &    win &  57.210122 \\
\bottomrule
\end{tabular}

This approach is clearly wrong -- the \texttt{DataFrame} claims that
Woodrow Wilson won the presidency in 2020.

Why is this happening? Here, the \texttt{max} aggregation function is
taken over every column \emph{independently}. Among Democrats,
\texttt{max} is computing:

\begin{itemize}
\tightlist
\item
  The most recent \texttt{Year} a Democratic candidate ran for president
  (2020)
\item
  The \texttt{Candidate} with the alphabetically ``largest'' name
  (``Woodrow Wilson'')
\item
  The \texttt{Result} with the alphabetically ``largest'' outcome
  (``win'')
\end{itemize}

Instead, let's try a different approach. We will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort the \texttt{DataFrame} so that rows are in descending order of
  \texttt{\%}
\item
  Group by \texttt{Party} and select the first row of each
  sub-\texttt{DataFrame}
\end{enumerate}

While it may seem unintuitive, sorting \texttt{elections} by descending
order of \texttt{\%} is extremely helpful. If we then group by
\texttt{Party}, the first row of each \texttt{GroupBy} object will
contain information about the \texttt{Candidate} with the highest voter
\texttt{\%}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections\_sorted\_by\_percent }\OperatorTok{=}\NormalTok{ elections.sort\_values(}\StringTok{"\%"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{elections\_sorted\_by\_percent.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &           Candidate &       Party &  Popular vote & Result &          \% \\
\midrule
114 &  1964 &      Lyndon Johnson &  Democratic &      43127041 &    win &  61.344703 \\
91  &  1936 &  Franklin Roosevelt &  Democratic &      27752648 &    win &  60.978107 \\
120 &  1972 &       Richard Nixon &  Republican &      47168710 &    win &  60.907806 \\
79  &  1920 &      Warren Harding &  Republican &      16144093 &    win &  60.574501 \\
133 &  1984 &       Ronald Reagan &  Republican &      54455472 &    win &  59.023326 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections\_sorted\_by\_percent.groupby(}\StringTok{"Party"}\NormalTok{).agg(}\KeywordTok{lambda}\NormalTok{ x : x.iloc[}\DecValTok{0}\NormalTok{]).head(}\DecValTok{10}\NormalTok{)}

\CommentTok{\# Equivalent to the below code}
\CommentTok{\# elections\_sorted\_by\_percent.groupby("Party").agg(\textquotesingle{}first\textquotesingle{}).head(10)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlrlr}
\toprule
{} &  Year &          Candidate &  Popular vote & Result &          \% \\
Party                 &       &                    &               &        &            \\
\midrule
American              &  1856 &   Millard Fillmore &        873053 &   loss &  21.554001 \\
American Independent  &  1968 &     George Wallace &       9901118 &   loss &  13.571218 \\
Anti-Masonic          &  1832 &       William Wirt &        100715 &   loss &   7.821583 \\
Anti-Monopoly         &  1884 &    Benjamin Butler &        134294 &   loss &   1.335838 \\
Citizens              &  1980 &     Barry Commoner &        233052 &   loss &   0.270182 \\
Communist             &  1932 &  William Z. Foster &        103307 &   loss &   0.261069 \\
Constitution          &  2008 &      Chuck Baldwin &        199750 &   loss &   0.152398 \\
Constitutional Union  &  1860 &          John Bell &        590901 &   loss &  12.639283 \\
Democratic            &  1964 &     Lyndon Johnson &      43127041 &    win &  61.344703 \\
Democratic-Republican &  1824 &     Andrew Jackson &        151271 &   loss &  57.210122 \\
\bottomrule
\end{tabular}

Here's an illustration of the process:

Notice how our code correctly determines that Lyndon Johnson from the
Democratic Party has the highest voter \texttt{\%}.

More generally, \texttt{lambda} functions are used to design custom
aggregation functions that aren't pre-defined by Python. The input
parameter \texttt{x} to the \texttt{lambda} function is a
\texttt{GroupBy} object. Therefore, it should make sense why
\texttt{lambda\ x\ :\ x.iloc{[}0{]}} selects the first row in each
groupby object.

In fact, there's a few different ways to approach this problem. Each
approach has different tradeoffs in terms of readability, performance,
memory consumption, complexity, etc. We've given a few examples below.

\textbf{Note}: Understanding these alternative solutions is not
required. They are given to demonstrate the vast number of
problem-solving approaches in \texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using the idxmax function}
\NormalTok{best\_per\_party }\OperatorTok{=}\NormalTok{ elections.loc[elections.groupby(}\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{].idxmax()]}
\NormalTok{best\_per\_party.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &         Candidate &                 Party &  Popular vote & Result &          \% \\
\midrule
22  &  1856 &  Millard Fillmore &              American &        873053 &   loss &  21.554001 \\
115 &  1968 &    George Wallace &  American Independent &       9901118 &   loss &  13.571218 \\
6   &  1832 &      William Wirt &          Anti-Masonic &        100715 &   loss &   7.821583 \\
38  &  1884 &   Benjamin Butler &         Anti-Monopoly &        134294 &   loss &   1.335838 \\
127 &  1980 &    Barry Commoner &              Citizens &        233052 &   loss &   0.270182 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using the .drop\_duplicates function}
\NormalTok{best\_per\_party2 }\OperatorTok{=}\NormalTok{ elections.sort\_values(}\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{).drop\_duplicates([}\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{], keep}\OperatorTok{=}\StringTok{\textquotesingle{}last\textquotesingle{}}\NormalTok{)}
\NormalTok{best\_per\_party2.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &           Candidate &           Party &  Popular vote & Result &         \% \\
\midrule
148 &  1996 &        John Hagelin &     Natural Law &        113670 &   loss &  0.118219 \\
164 &  2008 &       Chuck Baldwin &    Constitution &        199750 &   loss &  0.152398 \\
110 &  1956 &  T. Coleman Andrews &  States' Rights &        107929 &   loss &  0.174883 \\
147 &  1996 &     Howard Phillips &       Taxpayers &        184656 &   loss &  0.192045 \\
136 &  1988 &       Lenora Fulani &    New Alliance &        217221 &   loss &  0.237804 \\
\bottomrule
\end{tabular}

\hypertarget{aggregating-data-with-pivot-tables}{%
\section{Aggregating Data with Pivot
Tables}\label{aggregating-data-with-pivot-tables}}

We know now that \texttt{.groupby} gives us the ability to group and
aggregate data across our \texttt{DataFrame}. The examples above formed
groups using just one column in the \texttt{DataFrame}. It's possible to
group by multiple columns at once by passing in a list of column names
to \texttt{.groupby}.

Let's consider the \texttt{babynames} dataset again. In this problem, we
will find the total number of baby names associated with each sex for
each year. To do this, we'll group by \emph{both} the \texttt{"Year"}
and \texttt{"Sex"} columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrl}
\toprule
{} & State & Sex &  Year &      Name &  Count & First Letter \\
\midrule
115957 &    CA &   F &  1990 &  Deandrea &      5 &            D \\
101976 &    CA &   F &  1986 &  Deandrea &      6 &            D \\
131029 &    CA &   F &  1994 &  Leandrea &      5 &            L \\
108731 &    CA &   F &  1988 &  Deandrea &      5 &            D \\
308131 &    CA &   M &  1985 &  Deandrea &      6 &            D \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Find the total number of baby names associated with each sex for each }
\CommentTok{\# year in the data}
\NormalTok{babynames.groupby([}\StringTok{"Year"}\NormalTok{, }\StringTok{"Sex"}\NormalTok{])[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llr}
\toprule
     &   &  Count \\
Year & Sex &        \\
\midrule
1910 & F &   5950 \\
     & M &   3213 \\
1911 & F &   6602 \\
     & M &   3381 \\
1912 & F &   9804 \\
     & M &   8142 \\
\bottomrule
\end{tabular}

Notice that both \texttt{"Year"} and \texttt{"Sex"} serve as the index
of the \texttt{DataFrame} (they are both rendered in bold). We've
created a \emph{multi-index} \texttt{DataFrame} where two different
index values, the year and sex, are used to uniquely identify each row.

This isn't the most intuitive way of representing this data -- and,
because multi-indexed DataFrames have multiple dimensions in their
index, they can often be difficult to use.

Another strategy to aggregate across two columns is to create a pivot
table. You saw these back in
\href{https://inferentialthinking.com/chapters/08/3/Cross-Classifying_by_More_than_One_Variable.html\#pivot-tables-rearranging-the-output-of-group}{Data
8}. One set of values is used to create the index of the pivot table;
another set is used to define the column names. The values contained in
each cell of the table correspond to the aggregated data for each
index-column pair.

Here's an illustration of the process:

The best way to understand pivot tables is to see one in action. Let's
return to our original goal of summing the total number of names
associated with each combination of year and sex. We'll call the
\texttt{pandas}
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html}{\texttt{.pivot\_table}}
method to create a new table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textasciigrave{}pivot\_table\textasciigrave{} method is used to generate a Pandas pivot table}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{babynames.pivot\_table(}
\NormalTok{    index }\OperatorTok{=} \StringTok{"Year"}\NormalTok{,}
\NormalTok{    columns }\OperatorTok{=} \StringTok{"Sex"}\NormalTok{,    }
\NormalTok{    values }\OperatorTok{=} \StringTok{"Count"}\NormalTok{, }
\NormalTok{    aggfunc }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{, }
\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
Sex &      F &      M \\
Year &        &        \\
\midrule
1910 &   5950 &   3213 \\
1911 &   6602 &   3381 \\
1912 &   9804 &   8142 \\
1913 &  11860 &  10234 \\
1914 &  13815 &  13111 \\
\bottomrule
\end{tabular}

Looks a lot better! Now, our \texttt{DataFrame} is structured with clear
index-column combinations. Each entry in the pivot table represents the
summed count of names for a given combination of \texttt{"Year"} and
\texttt{"Sex"}.

Let's take a closer look at the code implemented above.

\begin{itemize}
\tightlist
\item
  \texttt{index\ =\ "Year"} specifies the column name in the original
  \texttt{DataFrame} that should be used as the index of the pivot table
\item
  \texttt{columns\ =\ "Sex"} specifies the column name in the original
  \texttt{DataFrame} that should be used to generate the columns of the
  pivot table
\item
  \texttt{values\ =\ "Count"} indicates what values from the original
  \texttt{DataFrame} should be used to populate the entry for each
  index-column combination
\item
  \texttt{aggfunc\ =\ np.sum} tells \texttt{pandas} what function to use
  when aggregating the data specified by \texttt{values}. Here, we are
  summing the name counts for each pair of \texttt{"Year"} and
  \texttt{"Sex"}
\end{itemize}

We can even include multiple values in the index or columns of our pivot
tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames\_pivot }\OperatorTok{=}\NormalTok{ babynames.pivot\_table(}
\NormalTok{    index}\OperatorTok{=}\StringTok{"Year"}\NormalTok{,     }\CommentTok{\# the rows (turned into index)}
\NormalTok{    columns}\OperatorTok{=}\StringTok{"Sex"}\NormalTok{,    }\CommentTok{\# the column values}
\NormalTok{    values}\OperatorTok{=}\NormalTok{[}\StringTok{"Count"}\NormalTok{, }\StringTok{"Name"}\NormalTok{], }
\NormalTok{    aggfunc}\OperatorTok{=}\BuiltInTok{max}\NormalTok{,      }\CommentTok{\# group operation}
\NormalTok{)}
\NormalTok{babynames\_pivot.head(}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrrll}
\toprule
{} & \multicolumn{2}{l}{Count} & \multicolumn{2}{l}{Name} \\
Sex &     F &     M &       F &        M \\
Year &       &       &         &          \\
\midrule
1910 &   295 &   237 &  Yvonne &  William \\
1911 &   390 &   214 &   Zelma &   Willis \\
1912 &   534 &   501 &  Yvonne &  Woodrow \\
1913 &   584 &   614 &   Zelma &   Yoshio \\
1914 &   773 &   769 &   Zelma &   Yoshio \\
1915 &   998 &  1033 &    Zita &    Yukio \\
\bottomrule
\end{tabular}

Note that each row provides the number of girls and number of boys
having that year's most common name, and also lists the alphabetically
largest girl name and boy name. The counts for number of girls/boys in
the resulting \texttt{DataFrame} do not correspond to the names listed.
For example, in 1910, the most popular girl name is given to 295 girls,
but that name was likely not Yvonne.

\hypertarget{joining-tables}{%
\section{Joining Tables}\label{joining-tables}}

When working on data science projects, we're unlikely to have absolutely
all the data we want contained in a single \texttt{DataFrame} -- a
real-world data scientist needs to grapple with data coming from
multiple sources. If we have access to multiple datasets with related
information, we can join two or more tables into a single
\texttt{DataFrame}.

To put this into practice, we'll revisit the \texttt{elections} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

Say we want to understand the popularity of the names of each
presidential candidate in 2022. To do this, we'll need the combined data
of \texttt{babynames} \emph{and} \texttt{elections}.

We'll start by creating a new column containing the first name of each
presidential candidate. This will help us join each name in
\texttt{elections} to the corresponding name data in \texttt{babynames}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This \textasciigrave{}str\textasciigrave{} operation splits each candidate\textquotesingle{}s full name at each }
\CommentTok{\# blank space, then takes just the candidate\textquotesingle{}s first name}
\NormalTok{elections[}\StringTok{"First Name"}\NormalTok{] }\OperatorTok{=}\NormalTok{ elections[}\StringTok{"Candidate"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.split().}\BuiltInTok{str}\NormalTok{[}\DecValTok{0}\NormalTok{]}
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlrl}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% & First Name \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 &     Andrew \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 &       John \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 &     Andrew \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 &       John \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 &     Andrew \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here, we\textquotesingle{}ll only consider \textasciigrave{}babynames\textasciigrave{} data from 2022}
\NormalTok{babynames\_2022 }\OperatorTok{=}\NormalTok{ babynames[babynames[}\StringTok{"Year"}\NormalTok{]}\OperatorTok{==}\DecValTok{2022}\NormalTok{]}
\NormalTok{babynames\_2022.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrl}
\toprule
{} & State & Sex &  Year &     Name &  Count & First Letter \\
\midrule
237964 &    CA &   F &  2022 &  Leandra &     10 &            L \\
404916 &    CA &   M &  2022 &  Leandro &     99 &            L \\
405892 &    CA &   M &  2022 &  Andreas &     14 &            A \\
235927 &    CA &   F &  2022 &   Andrea &    322 &            A \\
405695 &    CA &   M &  2022 &  Deandre &     18 &            D \\
\bottomrule
\end{tabular}

Now, we're ready to join the two tables.
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{\texttt{pd.merge}}
is the \texttt{pandas} method used to join \texttt{DataFrame}s together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged }\OperatorTok{=}\NormalTok{ pd.merge(left }\OperatorTok{=}\NormalTok{ elections, right }\OperatorTok{=}\NormalTok{ babynames\_2022, }\OperatorTok{\textbackslash{}}
\NormalTok{                  left\_on }\OperatorTok{=} \StringTok{"First Name"}\NormalTok{, right\_on }\OperatorTok{=} \StringTok{"Name"}\NormalTok{)}
\NormalTok{merged.head()}
\CommentTok{\# Notice that pandas automatically specifies \textasciigrave{}Year\_x\textasciigrave{} and \textasciigrave{}Year\_y\textasciigrave{} }
\CommentTok{\# when both merged DataFrames have the same column name to avoid confusion}

\CommentTok{\# Second option}
\CommentTok{\# merged = elections.merge(right = babynames\_2022, \textbackslash{}}
    \CommentTok{\# left\_on = "First Name", right\_on = "Name")}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllrlrlllrlrl}
\toprule
{} &  Year\_x &          Candidate &                  Party &  Popular vote & Result &          \% & First Name & State & Sex &  Year\_y &    Name &  Count & First Letter \\
\midrule
0 &    1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 &     Andrew &    CA &   M &    2022 &  Andrew &    741 &            A \\
1 &    1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 &     Andrew &    CA &   M &    2022 &  Andrew &    741 &            A \\
2 &    1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 &     Andrew &    CA &   M &    2022 &  Andrew &    741 &            A \\
3 &    1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 &       John &    CA &   M &    2022 &    John &    490 &            J \\
4 &    1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 &       John &    CA &   M &    2022 &    John &    490 &            J \\
\bottomrule
\end{tabular}

Let's take a closer look at the parameters:

\begin{itemize}
\tightlist
\item
  \texttt{left} and \texttt{right} parameters are used to specify the
  \texttt{DataFrame}s to be joined.
\item
  \texttt{left\_on} and \texttt{right\_on} parameters are assigned to
  the string names of the columns to be used when performing the join.
  These two \texttt{on} parameters tell \texttt{pandas} what values
  should act as pairing keys to determine which rows to merge across the
  \texttt{DataFrame}s. We'll talk more about this idea of a pairing key
  next lecture.
\end{itemize}

\hypertarget{parting-note-2}{%
\section{Parting Note}\label{parting-note-2}}

Congratulations! We finally tackled \texttt{pandas}. Don't worry if you
are still not feeling very comfortable with it---you will have plenty of
chances to practice over the next few weeks.

Next, we will get our hands dirty with some real-world datasets and use
our \texttt{pandas} knowledge to conduct some exploratory data analysis.

\bookmarksetup{startatroot}

\hypertarget{data-cleaning-and-eda}{%
\chapter{Data Cleaning and EDA}\label{data-cleaning-and-eda}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\CommentTok{\#\%matplotlib inline}
\NormalTok{plt.rcParams[}\StringTok{\textquotesingle{}figure.figsize\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ (}\DecValTok{12}\NormalTok{, }\DecValTok{9}\NormalTok{)}

\NormalTok{sns.}\BuiltInTok{set}\NormalTok{()}
\NormalTok{sns.set\_context(}\StringTok{\textquotesingle{}talk\textquotesingle{}}\NormalTok{)}
\NormalTok{np.set\_printoptions(threshold}\OperatorTok{=}\DecValTok{20}\NormalTok{, precision}\OperatorTok{=}\DecValTok{2}\NormalTok{, suppress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{pd.set\_option(}\StringTok{\textquotesingle{}display.max\_rows\textquotesingle{}}\NormalTok{, }\DecValTok{30}\NormalTok{)}
\NormalTok{pd.set\_option(}\StringTok{\textquotesingle{}display.max\_columns\textquotesingle{}}\NormalTok{, }\VariableTok{None}\NormalTok{)}
\NormalTok{pd.set\_option(}\StringTok{\textquotesingle{}display.precision\textquotesingle{}}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\CommentTok{\# This option stops scientific notation for pandas}
\NormalTok{pd.set\_option(}\StringTok{\textquotesingle{}display.float\_format\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}}\SpecialCharTok{\{:.2f\}}\StringTok{\textquotesingle{}}\NormalTok{.}\BuiltInTok{format}\NormalTok{)}

\CommentTok{\# Silence some spurious seaborn warnings}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{"ignore"}\NormalTok{, category}\OperatorTok{=}\PreprocessorTok{FutureWarning}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Recognize common file formats
\item
  Categorize data by its variable type
\item
  Build awareness of issues with data faithfulness and develop targeted
  solutions
\end{itemize}

\end{tcolorbox}

In the past few lectures, we've learned that \texttt{pandas} is a
toolkit to restructure, modify, and explore a dataset. What we haven't
yet touched on is \emph{how} to make these data transformation
decisions. When we receive a new set of data from the ``real world,''
how do we know what processing we should do to convert this data into a
usable form?

\textbf{Data cleaning}, also called \textbf{data wrangling}, is the
process of transforming raw data to facilitate subsequent analysis. It
is often used to address issues like:

\begin{itemize}
\tightlist
\item
  Unclear structure or formatting
\item
  Missing or corrupted values
\item
  Unit conversions
\item
  \ldots and so on
\end{itemize}

\textbf{Exploratory Data Analysis (EDA)} is the process of understanding
a new dataset. It is an open-ended, informal analysis that involves
familiarizing ourselves with the variables present in the data,
discovering potential hypotheses, and identifying possible issues with
the data. This last point can often motivate further data cleaning to
address any problems with the dataset's format; because of this, EDA and
data cleaning are often thought of as an ``infinite loop,'' with each
process driving the other.

In this lecture, we will consider the key properties of data to consider
when performing data cleaning and EDA. In doing so, we'll develop a
``checklist'' of sorts for you to consider when approaching a new
dataset. Throughout this process, we'll build a deeper understanding of
this early (but very important!) stage of the data science lifecycle.

\hypertarget{structure}{%
\section{Structure}\label{structure}}

We often prefer rectangular data for data analysis. Rectangular
structures are easy to manipulate and analyze. A key element of data
cleaning is about transforming data to be more rectangular.

There are two kinds of rectangular data: tables and matrices. Tables
have named columns with different data types and are manipulated using
data transformation languages. Matrices contain numeric data of the same
type and are manipulated using linear algebra.

\hypertarget{file-formats}{%
\subsection{File Formats}\label{file-formats}}

There are many file types for storing structured data: TSV, JSON, XML,
ASCII, SAS, etc. We'll only cover CSV, TSV, and JSON in lecture, but
you'll likely encounter other formats as you work with different
datasets. Reading documentation is your best bet for understanding how
to process the multitude of different file types.

\hypertarget{csv}{%
\subsubsection{CSV}\label{csv}}

CSVs, which stand for \textbf{Comma-Separated Values}, are a common
tabular data format. In the past two \texttt{pandas} lectures, we
briefly touched on the idea of file format: the way data is encoded in a
file for storage. Specifically, our \texttt{elections} and
\texttt{babynames} datasets were stored and loaded as CSVs:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &     \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss & 57.21 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win & 42.79 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win & 56.20 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss & 43.80 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win & 54.57 \\
\bottomrule
\end{tabular}

To better understand the properties of a CSV, let's take a look at the
first few rows of the raw data file to see what it looks like before
being loaded into a \texttt{DataFrame}. We'll use the \texttt{repr()}
function to return the raw string with its special characters:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/elections.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ table:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ table:}
        \BuiltInTok{print}\NormalTok{(}\BuiltInTok{repr}\NormalTok{(row))}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Year,Candidate,Party,Popular vote,Result,%\n'
'1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n'
'1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n'
'1828,Andrew Jackson,Democratic,642806,win,56.20392707\n'
\end{verbatim}

Each row, or \textbf{record}, in the data is delimited by a newline
\texttt{\textbackslash{}n}. Each column, or \textbf{field}, in the data
is delimited by a comma \texttt{,} (hence, comma-separated!).

\hypertarget{tsv}{%
\subsubsection{TSV}\label{tsv}}

Another common file type is \textbf{TSV (Tab-Separated Values)}. In a
TSV, records are still delimited by a newline
\texttt{\textbackslash{}n}, while fields are delimited by
\texttt{\textbackslash{}t} tab character.

Let's check out the first few rows of the raw TSV file. Again, we'll use
the \texttt{repr()} function so that \texttt{print} shows the special
characters.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/elections.txt"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ table:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ table:}
        \BuiltInTok{print}\NormalTok{(}\BuiltInTok{repr}\NormalTok{(row))}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'\ufeffYear\tCandidate\tParty\tPopular vote\tResult\t%\n'
'1824\tAndrew Jackson\tDemocratic-Republican\t151271\tloss\t57.21012204\n'
'1824\tJohn Quincy Adams\tDemocratic-Republican\t113142\twin\t42.78987796\n'
'1828\tAndrew Jackson\tDemocratic\t642806\twin\t56.20392707\n'
\end{verbatim}

TSVs can be loaded into \texttt{pandas} using \texttt{pd.read\_csv}.
We'll need to specify the \textbf{delimiter} with
parameter\texttt{sep=\textquotesingle{}\textbackslash{}t\textquotesingle{}}
\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{(documentation)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.read\_csv(}\StringTok{"data/elections.txt"}\NormalTok{, sep}\OperatorTok{=}\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}t}\StringTok{\textquotesingle{}}\NormalTok{).head(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &     \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss & 57.21 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win & 42.79 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win & 56.20 \\
\bottomrule
\end{tabular}

An issue with CSVs and TSVs comes up whenever there are commas or tabs
within the records. How does \texttt{pandas} differentiate between a
comma delimiter vs.~a comma within the field itself, for example
\texttt{8,900}? To remedy this, check out the
\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{\texttt{quotechar}
parameter}.

\hypertarget{json}{%
\subsubsection{JSON}\label{json}}

\textbf{JSON (JavaScript Object Notation)} files behave similarly to
Python dictionaries. A raw JSON is shown below.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/elections.json"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ table:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ table:}
        \BuiltInTok{print}\NormalTok{(row)}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{8}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[

 {

   "Year": 1824,

   "Candidate": "Andrew Jackson",

   "Party": "Democratic-Republican",

   "Popular vote": 151271,

   "Result": "loss",

   "%": 57.21012204

 },
\end{verbatim}

JSON files can be loaded into \texttt{pandas} using
\texttt{pd.read\_json}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.read\_json(}\StringTok{\textquotesingle{}data/elections.json\textquotesingle{}}\NormalTok{).head(}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &     \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss & 57.21 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win & 42.79 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win & 56.20 \\
\bottomrule
\end{tabular}

\hypertarget{eda-with-json-berkeley-covid-19-data}{%
\paragraph{EDA with JSON: Berkeley COVID-19
Data}\label{eda-with-json-berkeley-covid-19-data}}

The City of Berkeley Open Data
\href{https://data.cityofberkeley.info/Health/COVID-19-Confirmed-Cases/xn6j-b766}{website}
has a dataset with COVID-19 Confirmed Cases among Berkeley residents by
date. Let's download the file and save it as a JSON (note the source URL
file type is also a JSON). In the interest of reproducible data science,
we will download the data programatically. We have defined some helper
functions in the
\href{https://ds100.org/fa23/resources/assets/lectures/lec05/lec05-eda.html}{\texttt{ds100\_utils.py}}
file that we can reuse these helper functions in many different
notebooks.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ ds100\_utils }\ImportTok{import}\NormalTok{ fetch\_and\_cache}

\NormalTok{covid\_file }\OperatorTok{=}\NormalTok{ fetch\_and\_cache(}
    \StringTok{"https://data.cityofberkeley.info/api/views/xn6j{-}b766/rows.json?accessType=DOWNLOAD"}\NormalTok{,}
    \StringTok{"confirmed{-}cases.json"}\NormalTok{,}
\NormalTok{    force}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{covid\_file          }\CommentTok{\# a file path wrapper object}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Using cached version that was downloaded (UTC): Mon Mar 18 21:13:08 2024
\end{verbatim}

\begin{verbatim}
PosixPath('data/confirmed-cases.json')
\end{verbatim}

\hypertarget{file-size}{%
\subparagraph{File Size}\label{file-size}}

Let's start our analysis by getting a rough estimate of the size of the
dataset to inform the tools we use to view the data. For relatively
small datasets, we can use a text editor or spreadsheet. For larger
datasets, more programmatic exploration or distributed computing tools
may be more fitting. Here we will use \texttt{Python} tools to probe the
file.

Since there seem to be text files, let's investigate the number of
lines, which often corresponds to the number of records

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ os}

\BuiltInTok{print}\NormalTok{(covid\_file, }\StringTok{"is"}\NormalTok{, os.path.getsize(covid\_file) }\OperatorTok{/} \FloatTok{1e6}\NormalTok{, }\StringTok{"MB"}\NormalTok{)}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(covid\_file, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
    \BuiltInTok{print}\NormalTok{(covid\_file, }\StringTok{"is"}\NormalTok{, }\BuiltInTok{sum}\NormalTok{(}\DecValTok{1} \ControlFlowTok{for}\NormalTok{ l }\KeywordTok{in}\NormalTok{ f), }\StringTok{"lines."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
data/confirmed-cases.json is 0.116367 MB
data/confirmed-cases.json is 1110 lines.
\end{verbatim}

\hypertarget{unix-commands}{%
\subparagraph{Unix Commands}\label{unix-commands}}

As part of the EDA workflow, Unix commands can come in very handy. In
fact, there's an entire book called
\href{https://datascienceatthecommandline.com/}{``Data Science at the
Command Line''} that explores this idea in depth! In Jupyter/IPython,
you can prefix lines with \texttt{!} to execute arbitrary Unix commands,
and within those lines, you can refer to Python variables and
expressions with the syntax \texttt{\{expr\}}.

Here, we use the \texttt{ls} command to list files, using the
\texttt{-lh} flags, which request ``long format with information in
human-readable form.'' We also use the \texttt{wc} command for ``word
count,'' but with the \texttt{-l} flag, which asks for line counts
instead of words.

These two give us the same information as the code above, albeit in a
slightly different form:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{ls }\OperatorTok{{-}}\NormalTok{lh \{covid\_file\}}
\OperatorTok{!}\NormalTok{wc }\OperatorTok{{-}}\NormalTok{l \{covid\_file\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-rw-r--r--  1 Ishani  staff   114K Mar 18 21:13 data/confirmed-cases.json
\end{verbatim}

\begin{verbatim}
    1109 data/confirmed-cases.json
\end{verbatim}

\hypertarget{file-contents}{%
\subparagraph{File Contents}\label{file-contents}}

Let's explore the data format using \texttt{Python}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(covid\_file, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
    \ControlFlowTok{for}\NormalTok{ i, row }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(f):}
        \BuiltInTok{print}\NormalTok{(}\BuiltInTok{repr}\NormalTok{(row)) }\CommentTok{\# print raw strings}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}=} \DecValTok{4}\NormalTok{: }\ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'{\n'
'  "meta" : {\n'
'    "view" : {\n'
'      "id" : "xn6j-b766",\n'
'      "name" : "COVID-19 Confirmed Cases",\n'
\end{verbatim}

We can use the \texttt{head} Unix command (which is where
\texttt{pandas}' \texttt{head} method comes from!) to see the first few
lines of the file:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\NormalTok{head }\OperatorTok{{-}}\DecValTok{5}\NormalTok{ \{covid\_file\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{
  "meta" : {
    "view" : {
      "id" : "xn6j-b766",
      "name" : "COVID-19 Confirmed Cases",
\end{verbatim}

In order to load the JSON file into \texttt{pandas}, Let's first do some
EDA with Oython's \texttt{json} package to understand the particular
structure of this JSON file so that we can decide what (if anything) to
load into \texttt{pandas}. Python has relatively good support for JSON
data since it closely matches the internal python object model. In the
following cell we import the entire JSON datafile into a python
dictionary using the \texttt{json} package.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ json}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(covid\_file, }\StringTok{"rb"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    covid\_json }\OperatorTok{=}\NormalTok{ json.load(f)}
\end{Highlighting}
\end{Shaded}

The \texttt{covid\_json} variable is now a dictionary encoding the data
in the file:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{type}\NormalTok{(covid\_json)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict
\end{verbatim}

We can examine what keys are in the top level JSON object by listing out
the keys.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covid\_json.keys()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict_keys(['meta', 'data'])
\end{verbatim}

\textbf{Observation}: The JSON dictionary contains a \texttt{meta} key
which likely refers to metadata (data about the data). Metadata is often
maintained with the data and can be a good source of additional
information.

We can investigate the metadata further by examining the keys associated
with the metadata.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covid\_json[}\StringTok{\textquotesingle{}meta\textquotesingle{}}\NormalTok{].keys()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict_keys(['view'])
\end{verbatim}

The \texttt{meta} key contains another dictionary called \texttt{view}.
This likely refers to metadata about a particular ``view'' of some
underlying database. We will learn more about views when we study SQL
later in the class.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{covid\_json[}\StringTok{\textquotesingle{}meta\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}view\textquotesingle{}}\NormalTok{].keys()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
dict_keys(['id', 'name', 'assetType', 'attribution', 'averageRating', 'category', 'createdAt', 'description', 'displayType', 'downloadCount', 'hideFromCatalog', 'hideFromDataJson', 'newBackend', 'numberOfComments', 'oid', 'provenance', 'publicationAppendEnabled', 'publicationDate', 'publicationGroup', 'publicationStage', 'rowsUpdatedAt', 'rowsUpdatedBy', 'tableId', 'totalTimesRated', 'viewCount', 'viewLastModified', 'viewType', 'approvals', 'columns', 'grants', 'metadata', 'owner', 'query', 'rights', 'tableAuthor', 'tags', 'flags'])
\end{verbatim}

Notice that this a nested/recursive data structure. As we dig deeper we
reveal more and more keys and the corresponding data:

\begin{verbatim}
meta
|-> data
    | ... (haven't explored yet)
|-> view
    | -> id
    | -> name
    | -> attribution 
    ...
    | -> description
    ...
    | -> columns
    ...
\end{verbatim}

There is a key called description in the view sub dictionary. This
likely contains a description of the data:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(covid\_json[}\StringTok{\textquotesingle{}meta\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}view\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}description\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Counts of confirmed COVID-19 cases among Berkeley residents by date.
\end{verbatim}

\hypertarget{examining-the-data-field-for-records}{%
\subparagraph{Examining the Data Field for
Records}\label{examining-the-data-field-for-records}}

We can look at a few entries in the \texttt{data} field. This is what
we'll load into \texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{i}\SpecialCharTok{:03\}}\SpecialStringTok{ | }\SpecialCharTok{\{}\NormalTok{covid\_json[}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{][i]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
000 | ['row-kzbg.v7my-c3y2', '00000000-0000-0000-0405-CB14DE51DAA7', 0, 1643733903, None, 1643733903, None, '{ }', '2020-02-28T00:00:00', '1', '1']
001 | ['row-jkyx_9u4r-h2yw', '00000000-0000-0000-F806-86D0DBE0E17F', 0, 1643733903, None, 1643733903, None, '{ }', '2020-02-29T00:00:00', '0', '1']
002 | ['row-qifg_4aug-y3ym', '00000000-0000-0000-2DCE-4D1872F9B216', 0, 1643733903, None, 1643733903, None, '{ }', '2020-03-01T00:00:00', '0', '1']
\end{verbatim}

Observations: * These look like equal-length records, so maybe
\texttt{data} is a table! * But what do each of values in the record
mean? Where can we find column headers?

For that, we'll need the \texttt{columns} key in the metadata
dictionary. This returns a list:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{type}\NormalTok{(covid\_json[}\StringTok{\textquotesingle{}meta\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}view\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}columns\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
list
\end{verbatim}

\hypertarget{summary-of-exploring-the-json-file}{%
\subparagraph{Summary of exploring the JSON
file}\label{summary-of-exploring-the-json-file}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The above \textbf{metadata} tells us a lot about the columns in the
  data including column names, potential data anomalies, and a basic
  statistic.
\item
  Because of its non-tabular structure, JSON makes it easier (than CSV)
  to create \textbf{self-documenting data}, meaning that information
  about the data is stored in the same file as the data.
\item
  Self-documenting data can be helpful since it maintains its own
  description and these descriptions are more likely to be updated as
  data changes.
\end{enumerate}

\hypertarget{loading-covid-data-into-pandas}{%
\subparagraph{\texorpdfstring{Loading COVID Data into
\texttt{pandas}}{Loading COVID Data into pandas}}\label{loading-covid-data-into-pandas}}

Finally, let's load the data (not the metadata) into a \texttt{pandas}
\texttt{DataFrame}. In the following block of code we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Translate the JSON records into a \texttt{DataFrame}:

  \begin{itemize}
  \tightlist
  \item
    fields:
    \texttt{covid\_json{[}\textquotesingle{}meta\textquotesingle{}{]}{[}\textquotesingle{}view\textquotesingle{}{]}{[}\textquotesingle{}columns\textquotesingle{}{]}}
  \item
    records:
    \texttt{covid\_json{[}\textquotesingle{}data\textquotesingle{}{]}}
  \end{itemize}
\item
  Remove columns that have no metadata description. This would be a bad
  idea in general, but here we remove these columns since the above
  analysis suggests they are unlikely to contain useful information.
\item
  Examine the \texttt{tail} of the table.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data from JSON and assign column titles}
\NormalTok{covid }\OperatorTok{=}\NormalTok{ pd.DataFrame(}
\NormalTok{    covid\_json[}\StringTok{\textquotesingle{}data\textquotesingle{}}\NormalTok{],}
\NormalTok{    columns}\OperatorTok{=}\NormalTok{[c[}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ c }\KeywordTok{in}\NormalTok{ covid\_json[}\StringTok{\textquotesingle{}meta\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}view\textquotesingle{}}\NormalTok{][}\StringTok{\textquotesingle{}columns\textquotesingle{}}\NormalTok{]])}

\NormalTok{covid.tail()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrrlrlllll}
\toprule
{} &                 sid &                                    id &  position &  created\_at & created\_meta &  updated\_at & updated\_meta & meta &                 Date & New Cases & Cumulative Cases \\
\midrule
699 &  row-49b6\_x8zv.gyum &  00000000-0000-0000-A18C-9174A6D05774 &         0 &  1643733903 &         None &  1643733903 &         None &  \{ \} &  2022-01-27T00:00:00 &       106 &            10694 \\
700 &  row-gs55-p5em.y4v9 &  00000000-0000-0000-F41D-5724AEABB4D6 &         0 &  1643733903 &         None &  1643733903 &         None &  \{ \} &  2022-01-28T00:00:00 &       223 &            10917 \\
701 &  row-3pyj.tf95-qu67 &  00000000-0000-0000-BEE3-B0188D2518BD &         0 &  1643733903 &         None &  1643733903 &         None &  \{ \} &  2022-01-29T00:00:00 &       139 &            11056 \\
702 &  row-cgnd.8syv.jvjn &  00000000-0000-0000-C318-63CF75F7F740 &         0 &  1643733903 &         None &  1643733903 &         None &  \{ \} &  2022-01-30T00:00:00 &        33 &            11089 \\
703 &  row-qywv\_24x6-237y &  00000000-0000-0000-FE92-9789FED3AA20 &         0 &  1643733903 &         None &  1643733903 &         None &  \{ \} &  2022-01-31T00:00:00 &        42 &            11131 \\
\bottomrule
\end{tabular}

\hypertarget{primary-and-foreign-keys}{%
\subsection{Primary and Foreign Keys}\label{primary-and-foreign-keys}}

Last time, we introduced \texttt{.merge} as the \texttt{pandas} method
for joining multiple \texttt{DataFrame}s together. In our discussion of
joins, we touched on the idea of using a ``key'' to determine what rows
should be merged from each table. Let's take a moment to examine this
idea more closely.

The \textbf{primary key} is the column or set of columns in a table that
\emph{uniquely} determine the values of the remaining columns. It can be
thought of as the unique identifier for each individual row in the
table. For example, a table of Data 100 students might use each
student's Cal ID as the primary key.

\begin{tabular}{lrll}
\toprule
{} &      Cal ID &   Name &             Major \\
\midrule
0 &  3034619471 &   Oski &      Data Science \\
1 &  3035619472 &  Ollie &  Computer Science \\
2 &  3025619473 &  Orrie &      Data Science \\
3 &  3046789372 &  Ollie &         Economics \\
\bottomrule
\end{tabular}

The \textbf{foreign key} is the column or set of columns in a table that
reference primary keys in other tables. Knowing a dataset's foreign keys
can be useful when assigning the \texttt{left\_on} and
\texttt{right\_on} parameters of \texttt{.merge}. In the table of office
hour tickets below, \texttt{"Cal\ ID"} is a foreign key referencing the
previous table.

\begin{tabular}{lrrl}
\toprule
{} &  OH Request &      Cal ID &  Question \\
\midrule
0 &           1 &  3034619471 &   HW 2 Q1 \\
1 &           2 &  3035619472 &   HW 2 Q3 \\
2 &           3 &  3025619473 &  Lab 3 Q4 \\
3 &           4 &  3035619472 &   HW 2 Q7 \\
\bottomrule
\end{tabular}

\hypertarget{variable-types}{%
\subsection{Variable Types}\label{variable-types}}

Variables are columns. A variable is a measurement of a particular
concept. Variables have two common properties: data type/storage type
and variable type/feature type. The data type of a variable indicates
how each variable value is stored in memory (integer, floating point,
boolean, etc.) and affects which \texttt{pandas} functions are used. The
variable type is a conceptualized measurement of information (and
therefore indicates what values a variable can take on). Variable type
is identified through expert knowledge, exploring the data itself, or
consulting the data codebook. The variable type affects how one
visualizes and inteprets the data. In this class, ``variable types'' are
conceptual.

After loading data into a file, it's a good idea to take the time to
understand what pieces of information are encoded in the dataset. In
particular, we want to identify what variable types are present in our
data. Broadly speaking, we can categorize variables into one of two
overarching types.

\textbf{Quantitative variables} describe some numeric quantity or
amount. We can divide quantitative data further into:

\begin{itemize}
\tightlist
\item
  \textbf{Continuous quantitative variables}: numeric data that can be
  measured on a continuous scale to arbitrary precision. Continuous
  variables do not have a strict set of possible values -- they can be
  recorded to any number of decimal places. For example, weights, GPA,
  or CO2 concentrations.
\item
  \textbf{Discrete quantitative variables}: numeric data that can only
  take on a finite set of possible values. For example, someone's age or
  the number of siblings they have.
\end{itemize}

\textbf{Qualitative variables}, also known as \textbf{categorical
variables}, describe data that isn't measuring some quantity or amount.
The sub-categories of categorical data are:

\begin{itemize}
\tightlist
\item
  \textbf{Ordinal qualitative variables}: categories with ordered
  levels. Specifically, ordinal variables are those where the difference
  between levels has no consistent, quantifiable meaning. Some examples
  include levels of education (high school, undergrad, grad, etc.),
  income bracket (low, medium, high), or Yelp rating.
\item
  \textbf{Nominal qualitative variables}: categories with no specific
  order. For example, someone's political affiliation or Cal ID number.
\end{itemize}

\begin{figure}

{\centering \includegraphics{eda/images/variable.png}

}

\caption{Classification of variable types}

\end{figure}

Note that many variables don't sit neatly in just one of these
categories. Qualitative variables could have numeric levels, and
conversely, quantitative variables could be stored as strings.

\hypertarget{granularity-scope-and-temporality}{%
\section{Granularity, Scope, and
Temporality}\label{granularity-scope-and-temporality}}

After understanding the structure of the dataset, the next task is to
determine what exactly the data represents. We'll do so by considering
the data's granularity, scope, and temporality.

\hypertarget{granularity}{%
\subsection{Granularity}\label{granularity}}

The \textbf{granularity} of a dataset is what a single row represents.
You can also think of it as the level of detail included in the data. To
determine the data's granularity, ask: what does each row in the dataset
represent? Fine-grained data contains a high level of detail, with a
single row representing a small individual unit. For example, each
record may represent one person. Coarse-grained data is encoded such
that a single row represents a large individual unit -- for example,
each record may represent a group of people.

\hypertarget{scope}{%
\subsection{Scope}\label{scope}}

The \textbf{scope} of a dataset is the subset of the population covered
by the data. If we were investigating student performance in Data
Science courses, a dataset with a narrow scope might encompass all
students enrolled in Data 100 whereas a dataset with an expansive scope
might encompass all students in California.

\hypertarget{temporality}{%
\subsection{Temporality}\label{temporality}}

The \textbf{temporality} of a dataset describes the periodicity over
which the data was collected as well as when the data was most recently
collected or updated.

Time and date fields of a dataset could represent a few things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  when the ``event'' happened
\item
  when the data was collected, or when it was entered into the system
\item
  when the data was copied into the database
\end{enumerate}

To fully understand the temporality of the data, it also may be
necessary to standardize time zones or inspect recurring time-based
trends in the data (do patterns recur in 24-hour periods? Over the
course of a month? Seasonally?). The convention for standardizing time
is the Coordinated Universal Time (UTC), an international time standard
measured at 0 degrees latitude that stays consistent throughout the year
(no daylight savings). We can represent Berkeley's time zone, Pacific
Standard Time (PST), as UTC-7 (with daylight savings).

\hypertarget{temporality-with-pandas-dt-accessors}{%
\subsubsection{\texorpdfstring{Temporality with \texttt{pandas}'
\texttt{dt}
accessors}{Temporality with pandas' dt accessors}}\label{temporality-with-pandas-dt-accessors}}

Let's briefly look at how we can use \texttt{pandas}' \texttt{dt}
accessors to work with dates/times in a dataset using the dataset you'll
see in Lab 3: the Berkeley PD Calls for Service dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calls }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/Berkeley\_PD\_{-}\_Calls\_for\_Service.csv"}\NormalTok{)}
\NormalTok{calls.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllllrlllll}
\toprule
{} &    CASENO &                   OFFENSE &                 EVENTDT & EVENTTM &            CVLEGEND &  CVDOW &                InDbDate &                                     Block\_Location &                BLKADDR &      City & State \\
\midrule
0 &  21014296 &  THEFT MISD. (UNDER \$950) &  04/01/2021 12:00:00 AM &   10:58 &             LARCENY &      4 &  06/15/2021 12:00:00 AM &             Berkeley, CA\textbackslash n(37.869058, -122.270455) &                    NaN &  Berkeley &    CA \\
1 &  21014391 &  THEFT MISD. (UNDER \$950) &  04/01/2021 12:00:00 AM &   10:38 &             LARCENY &      4 &  06/15/2021 12:00:00 AM &             Berkeley, CA\textbackslash n(37.869058, -122.270455) &                    NaN &  Berkeley &    CA \\
2 &  21090494 &  THEFT MISD. (UNDER \$950) &  04/19/2021 12:00:00 AM &   12:15 &             LARCENY &      1 &  06/15/2021 12:00:00 AM &  2100 BLOCK HASTE ST\textbackslash nBerkeley, CA\textbackslash n(37.864908,... &    2100 BLOCK HASTE ST &  Berkeley &    CA \\
3 &  21090204 &  THEFT FELONY (OVER \$950) &  02/13/2021 12:00:00 AM &   17:00 &             LARCENY &      6 &  06/15/2021 12:00:00 AM &  2600 BLOCK WARRING ST\textbackslash nBerkeley, CA\textbackslash n(37.86393... &  2600 BLOCK WARRING ST &  Berkeley &    CA \\
4 &  21090179 &             BURGLARY AUTO &  02/08/2021 12:00:00 AM &    6:20 &  BURGLARY - VEHICLE &      1 &  06/15/2021 12:00:00 AM &  2700 BLOCK GARBER ST\textbackslash nBerkeley, CA\textbackslash n(37.86066,... &   2700 BLOCK GARBER ST &  Berkeley &    CA \\
\bottomrule
\end{tabular}

Looks like there are three columns with dates/times: \texttt{EVENTDT},
\texttt{EVENTTM}, and \texttt{InDbDate}.

Most likely, \texttt{EVENTDT} stands for the date when the event took
place, \texttt{EVENTTM} stands for the time of day the event took place
(in 24-hr format), and \texttt{InDbDate} is the date this call is
recorded onto the database.

If we check the data type of these columns, we will see they are stored
as strings. We can convert them to \texttt{datetime} objects using
pandas \texttt{to\_datetime} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calls[}\StringTok{"EVENTDT"}\NormalTok{] }\OperatorTok{=}\NormalTok{ pd.to\_datetime(calls[}\StringTok{"EVENTDT"}\NormalTok{])}
\NormalTok{calls.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllllrlllll}
\toprule
{} &    CASENO &                   OFFENSE &    EVENTDT & EVENTTM &            CVLEGEND &  CVDOW &                InDbDate &                                     Block\_Location &                BLKADDR &      City & State \\
\midrule
0 &  21014296 &  THEFT MISD. (UNDER \$950) & 2021-04-01 &   10:58 &             LARCENY &      4 &  06/15/2021 12:00:00 AM &             Berkeley, CA\textbackslash n(37.869058, -122.270455) &                    NaN &  Berkeley &    CA \\
1 &  21014391 &  THEFT MISD. (UNDER \$950) & 2021-04-01 &   10:38 &             LARCENY &      4 &  06/15/2021 12:00:00 AM &             Berkeley, CA\textbackslash n(37.869058, -122.270455) &                    NaN &  Berkeley &    CA \\
2 &  21090494 &  THEFT MISD. (UNDER \$950) & 2021-04-19 &   12:15 &             LARCENY &      1 &  06/15/2021 12:00:00 AM &  2100 BLOCK HASTE ST\textbackslash nBerkeley, CA\textbackslash n(37.864908,... &    2100 BLOCK HASTE ST &  Berkeley &    CA \\
3 &  21090204 &  THEFT FELONY (OVER \$950) & 2021-02-13 &   17:00 &             LARCENY &      6 &  06/15/2021 12:00:00 AM &  2600 BLOCK WARRING ST\textbackslash nBerkeley, CA\textbackslash n(37.86393... &  2600 BLOCK WARRING ST &  Berkeley &    CA \\
4 &  21090179 &             BURGLARY AUTO & 2021-02-08 &    6:20 &  BURGLARY - VEHICLE &      1 &  06/15/2021 12:00:00 AM &  2700 BLOCK GARBER ST\textbackslash nBerkeley, CA\textbackslash n(37.86066,... &   2700 BLOCK GARBER ST &  Berkeley &    CA \\
\bottomrule
\end{tabular}

Now, we can use the \texttt{dt} accessor on this column.

We can get the month:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calls[}\StringTok{"EVENTDT"}\NormalTok{].dt.month.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  EVENTDT \\
\midrule
0 &        4 \\
1 &        4 \\
2 &        4 \\
3 &        2 \\
4 &        2 \\
\bottomrule
\end{tabular}

Which day of the week the date is on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calls[}\StringTok{"EVENTDT"}\NormalTok{].dt.dayofweek.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  EVENTDT \\
\midrule
0 &        3 \\
1 &        3 \\
2 &        0 \\
3 &        5 \\
4 &        0 \\
\bottomrule
\end{tabular}

Check the mimimum values to see if there are any suspicious-looking, 70s
dates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{calls.sort\_values(}\StringTok{"EVENTDT"}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllllrlllll}
\toprule
{} &    CASENO &                   OFFENSE &    EVENTDT & EVENTTM &                CVLEGEND &  CVDOW &                InDbDate &                                     Block\_Location &                  BLKADDR &      City & State \\
\midrule
2513 &  20057398 &       BURGLARY COMMERCIAL & 2020-12-17 &   16:05 &   BURGLARY - COMMERCIAL &      4 &  06/15/2021 12:00:00 AM &  600 BLOCK GILMAN ST\textbackslash nBerkeley, CA\textbackslash n(37.878405,... &      600 BLOCK GILMAN ST &  Berkeley &    CA \\
624  &  20057207 &     ASSAULT/BATTERY MISD. & 2020-12-17 &   16:50 &                 ASSAULT &      4 &  06/15/2021 12:00:00 AM &  2100 BLOCK SHATTUCK AVE\textbackslash nBerkeley, CA\textbackslash n(37.871... &  2100 BLOCK SHATTUCK AVE &  Berkeley &    CA \\
154  &  20092214 &           THEFT FROM AUTO & 2020-12-17 &   18:30 &  LARCENY - FROM VEHICLE &      4 &  06/15/2021 12:00:00 AM &  800 BLOCK SHATTUCK AVE\textbackslash nBerkeley, CA\textbackslash n(37.8918... &   800 BLOCK SHATTUCK AVE &  Berkeley &    CA \\
659  &  20057324 &  THEFT MISD. (UNDER \$950) & 2020-12-17 &   15:44 &                 LARCENY &      4 &  06/15/2021 12:00:00 AM &  1800 BLOCK 4TH ST\textbackslash nBerkeley, CA\textbackslash n(37.869888, -... &        1800 BLOCK 4TH ST &  Berkeley &    CA \\
993  &  20057573 &      BURGLARY RESIDENTIAL & 2020-12-17 &   22:15 &  BURGLARY - RESIDENTIAL &      4 &  06/15/2021 12:00:00 AM &  1700 BLOCK STUART ST\textbackslash nBerkeley, CA\textbackslash n(37.857495... &     1700 BLOCK STUART ST &  Berkeley &    CA \\
\bottomrule
\end{tabular}

Doesn't look like it! We are good!

We can also do many things with the \texttt{dt} accessor like switching
time zones and converting time back to UNIX/POSIX time. Check out the
documentation on
\href{https://pandas.pydata.org/docs/user_guide/basics.html\#basics-dt-accessors}{\texttt{.dt}
accessor} and
\href{https://pandas.pydata.org/docs/user_guide/timeseries.html\#}{time
series/date functionality}.

\hypertarget{faithfulness}{%
\section{Faithfulness}\label{faithfulness}}

At this stage in our data cleaning and EDA workflow, we've achieved
quite a lot: we've identified how our data is structured, come to terms
with what information it encodes, and gained insight as to how it was
generated. Throughout this process, we should always recall the original
intent of our work in Data Science -- to use data to better understand
and model the real world. To achieve this goal, we need to ensure that
the data we use is faithful to reality; that is, that our data
accurately captures the ``real world.''

Data used in research or industry is often ``messy'' -- there may be
errors or inaccuracies that impact the faithfulness of the dataset.
Signs that data may not be faithful include:

\begin{itemize}
\tightlist
\item
  Unrealistic or ``incorrect'' values, such as negative counts,
  locations that don't exist, or dates set in the future
\item
  Violations of obvious dependencies, like an age that does not match a
  birthday
\item
  Clear signs that data was entered by hand, which can lead to spelling
  errors or fields that are incorrectly shifted
\item
  Signs of data falsification, such as fake email addresses or repeated
  use of the same names
\item
  Duplicated records or fields containing the same information
\item
  Truncated data, e.g.~Microsoft Excel would limit the number of rows to
  655536 and the number of columns to 255
\end{itemize}

We often solve some of these more common issues in the following ways:

\begin{itemize}
\tightlist
\item
  Spelling errors: apply corrections or drop records that aren't in a
  dictionary
\item
  Time zone inconsistencies: convert to a common time zone (e.g.~UTC)
\item
  Duplicated records or fields: identify and eliminate duplicates (using
  primary keys)
\item
  Unspecified or inconsistent units: infer the units and check that
  values are in reasonable ranges in the data
\end{itemize}

\hypertarget{missing-values}{%
\subsection{Missing Values}\label{missing-values}}

Another common issue encountered with real-world datasets is that of
missing data. One strategy to resolve this is to simply drop any records
with missing values from the dataset. This does, however, introduce the
risk of inducing biases -- it is possible that the missing or corrupt
records may be systemically related to some feature of interest in the
data. Another solution is to keep the data as \texttt{NaN} values.

A third method to address missing data is to perform
\textbf{imputation}: infer the missing values using other data available
in the dataset. There is a wide variety of imputation techniques that
can be implemented; some of the most common are listed below.

\begin{itemize}
\tightlist
\item
  Average imputation: replace missing values with the average value for
  that field
\item
  Hot deck imputation: replace missing values with some random value
\item
  Regression imputation: develop a model to predict missing values and
  replace with the predicted value from the model.
\item
  Multiple imputation: replace missing values with multiple random
  values
\end{itemize}

Regardless of the strategy used to deal with missing data, we should
think carefully about \emph{why} particular records or fields may be
missing -- this can help inform whether or not the absence of these
values is significant or meaningful.

\hypertarget{eda-demo-1-tuberculosis-in-the-united-states}{%
\section{EDA Demo 1: Tuberculosis in the United
States}\label{eda-demo-1-tuberculosis-in-the-united-states}}

Now, let's walk through the data-cleaning and EDA workflow to see what
can we learn about the presence of Tuberculosis in the United States!

We will examine the data included in the
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w\#T1_down}{original
CDC article} published in 2021.

\hypertarget{csvs-and-field-names}{%
\subsection{CSVs and Field Names}\label{csvs-and-field-names}}

Suppose Table 1 was saved as a CSV file located in
\texttt{data/cdc\_tuberculosis.csv}.

We can then explore the CSV (which is a text file, and does not contain
binary-encoded data) in many ways: 1. Using a text editor like emacs,
vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only),
Excel, Google Sheets, etc. 3. The \texttt{Python} file object 4.
\texttt{pandas}, using \texttt{pd.read\_csv()}

To try out options 1 and 2, you can view or download the Tuberculosis
from the
\href{https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https\%3A\%2F\%2Fgithub.com\%2FDS-100\%2Ffa23-student\&urlpath=lab\%2Ftree\%2Ffa23-student\%2Flecture\%2Flec05\%2Flec04-eda.ipynb\&branch=main}{lecture
demo notebook} under the \texttt{data} folder in the left hand menu.
Notice how the CSV file is a type of \textbf{rectangular data (i.e.,
tabular data) stored as comma-separated values}.

Next, let's try out option 3 using the \texttt{Python} file object.
We'll look at the first four lines:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ f:}
        \BuiltInTok{print}\NormalTok{(row)}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
,No. of TB cases,,,TB incidence,,

U.S. jurisdiction,2019,2020,2021,2019,2020,2021

Total,"8,900","7,173","7,860",2.71,2.16,2.37

Alabama,87,72,92,1.77,1.43,1.83
\end{verbatim}

Whoa, why are there blank lines interspaced between the lines of the
CSV?

You may recall that all line breaks in text files are encoded as the
special newline character \texttt{\textbackslash{}n}. Python's
\texttt{print()} prints each string (including the newline), and an
additional newline on top of that.

If you're curious, we can use the \texttt{repr()} function to return the
raw string with all special characters:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ f:}
        \BuiltInTok{print}\NormalTok{(}\BuiltInTok{repr}\NormalTok{(row)) }\CommentTok{\# print raw strings}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
',No. of TB cases,,,TB incidence,,\n'
'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\n'
'Total,"8,900","7,173","7,860",2.71,2.16,2.37\n'
'Alabama,87,72,92,1.77,1.43,1.83\n'
\end{verbatim}

Finally, let's try option 4 and use the tried-and-true Data 100
approach: \texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{)}
\NormalTok{tb\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} &         Unnamed: 0 & No. of TB cases & Unnamed: 2 & Unnamed: 3 &  TB incidence &  Unnamed: 5 &  Unnamed: 6 \\
\midrule
0 &  U.S. jurisdiction &            2019 &       2020 &       2021 &       2019.00 &     2020.00 &     2021.00 \\
1 &              Total &           8,900 &      7,173 &      7,860 &          2.71 &        2.16 &        2.37 \\
2 &            Alabama &              87 &         72 &         92 &          1.77 &        1.43 &        1.83 \\
3 &             Alaska &              58 &         58 &         58 &          7.91 &        7.92 &        7.92 \\
4 &            Arizona &             183 &        136 &        129 &          2.51 &        1.89 &        1.77 \\
\bottomrule
\end{tabular}

You may notice some strange things about this table: what's up with the
``Unnamed'' column names and the first row?

Congratulations --- you're ready to wrangle your data! Because of how
things are stored, we'll need to clean the data a bit to name our
columns better.

A reasonable first step is to identify the row with the right header.
The \texttt{pd.read\_csv()} function
(\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{documentation})
has the convenient \texttt{header} parameter that we can set to use the
elements in row 1 as the appropriate columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\CommentTok{\# row index}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} & U.S. jurisdiction &   2019 &   2020 &   2021 &  2019.1 &  2020.1 &  2021.1 \\
\midrule
0 &             Total &  8,900 &  7,173 &  7,860 &    2.71 &    2.16 &    2.37 \\
1 &           Alabama &     87 &     72 &     92 &    1.77 &    1.43 &    1.83 \\
2 &            Alaska &     58 &     58 &     58 &    7.91 &    7.92 &    7.92 \\
3 &           Arizona &    183 &    136 &    129 &    2.51 &    1.89 &    1.77 \\
4 &          Arkansas &     64 &     59 &     69 &    2.12 &    1.96 &    2.28 \\
\bottomrule
\end{tabular}

Wait\ldots but now we can't differentiate betwen the ``Number of TB
cases'' and ``TB incidence'' year columns. \texttt{pandas} has tried to
make our lives easier by automatically adding ``.1'' to the latter
columns, but this doesn't help us, as humans, understand the data.

We can do this manually with \texttt{df.rename()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html?highlight=rename\#pandas.DataFrame.rename}{documentation}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rename\_dict }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}2019\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2019\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2020\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2020\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2021\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2021\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2019.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2019\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2020.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2020\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2021.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2021\textquotesingle{}}\NormalTok{\}}
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ tb\_df.rename(columns}\OperatorTok{=}\NormalTok{rename\_dict)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} & U.S. jurisdiction & TB cases 2019 & TB cases 2020 & TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
0 &             Total &         8,900 &         7,173 &         7,860 &               2.71 &               2.16 &               2.37 \\
1 &           Alabama &            87 &            72 &            92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &            58 &            58 &            58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &           183 &           136 &           129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &            64 &            59 &            69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\hypertarget{record-granularity}{%
\subsection{Record Granularity}\label{record-granularity}}

You might already be wondering: what's up with that first record?

Row 0 is what we call a \textbf{rollup record}, or summary record. It's
often useful when displaying tables to humans. The \textbf{granularity}
of record 0 (Totals) vs the rest of the records (States) is different.

Okay, EDA step two. How was the rollup record aggregated?

Let's check if Total TB cases is the sum of all state TB cases. If we
sum over all rows, we should get \textbf{2x} the total cases in each of
our TB cases by year (why do you think this is?).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                                                  0 \\
\midrule
U.S. jurisdiction &  TotalAlabamaAlaskaArizonaArkansasCaliforniaCol... \\
TB cases 2019     &  8,9008758183642,111666718245583029973261085237... \\
TB cases 2020     &  7,1737258136591,706525417194122219282169239376... \\
TB cases 2021     &  7,8609258129691,750585443194992281064255127494... \\
TB incidence 2019 &                                             109.94 \\
TB incidence 2020 &                                              93.09 \\
TB incidence 2021 &                                             102.94 \\
\bottomrule
\end{tabular}

Whoa, what's going on with the TB cases in 2019, 2020, and 2021? Check
out the column types:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.dtypes}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &        0 \\
\midrule
U.S. jurisdiction &   object \\
TB cases 2019     &   object \\
TB cases 2020     &   object \\
TB cases 2021     &   object \\
TB incidence 2019 &  float64 \\
TB incidence 2020 &  float64 \\
TB incidence 2021 &  float64 \\
\bottomrule
\end{tabular}

Since there are commas in the values for TB cases, the numbers are read
as the \texttt{object} datatype, or \textbf{storage type} (close to the
\texttt{Python} string datatype), so \texttt{pandas} is concatenating
strings instead of adding integers (recall that Python can ``sum'', or
concatenate, strings together: \texttt{"data"\ +\ "100"} evaluates to
\texttt{"data100"}).

Fortunately \texttt{read\_csv} also has a \texttt{thousands} parameter
(\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{documentation}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# improve readability: chaining method calls with outer parentheses/line breaks}
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{1}\NormalTok{, thousands}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{rename\_dict)}
\NormalTok{)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
0 &             Total &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
1 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                                                  0 \\
\midrule
U.S. jurisdiction &  TotalAlabamaAlaskaArizonaArkansasCaliforniaCol... \\
TB cases 2019     &                                              17800 \\
TB cases 2020     &                                              14346 \\
TB cases 2021     &                                              15720 \\
TB incidence 2019 &                                             109.94 \\
TB incidence 2020 &                                              93.09 \\
TB incidence 2021 &                                             102.94 \\
\bottomrule
\end{tabular}

The total TB cases look right. Phew!

Let's just look at the records with \textbf{state-level granularity}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_tb\_df }\OperatorTok{=}\NormalTok{ tb\_df[}\DecValTok{1}\NormalTok{:]}
\NormalTok{state\_tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
1 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
5 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 \\
\bottomrule
\end{tabular}

\hypertarget{gather-census-data}{%
\subsection{Gather Census Data}\label{gather-census-data}}

U.S. Census population estimates
\href{https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html}{source}
(2019),
\href{https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html}{source}
(2020-2021).

Running the below cells cleans the data. There are a few new methods
here: * \texttt{df.convert\_dtypes()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html}{documentation})
conveniently converts all float dtypes into ints and is out of scope for
the class. * \texttt{df.drop\_na()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html}{documentation})
will be explained in more detail next time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2010s census data}
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/nst{-}est2019{-}01.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{3}\NormalTok{, thousands}\OperatorTok{=}\StringTok{","}\NormalTok{)}
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    census\_2010s\_df}
\NormalTok{    .reset\_index()}
\NormalTok{    .drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"index"}\NormalTok{, }\StringTok{"Census"}\NormalTok{, }\StringTok{"Estimates Base"}\NormalTok{])}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"Unnamed: 0"}\NormalTok{: }\StringTok{"Geographic Area"}\NormalTok{\})}
\NormalTok{    .convert\_dtypes()                 }\CommentTok{\# "smart" converting of columns, use at your own risk}
\NormalTok{    .dropna()                         }\CommentTok{\# we\textquotesingle{}ll introduce this next time}
\NormalTok{)}
\NormalTok{census\_2010s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ census\_2010s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.strip(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{)}

\CommentTok{\# with pd.option\_context(\textquotesingle{}display.min\_rows\textquotesingle{}, 30): \# shows more rows}
\CommentTok{\#     display(census\_2010s\_df)}
    
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrr}
\toprule
{} & Geographic Area &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
\midrule
0 &   United States &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
1 &       Northeast &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
2 &         Midwest &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
3 &           South &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
4 &            West &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

Occasionally, you will want to modify code that you have imported. To
reimport those modifications you can either use \texttt{python}'s
\texttt{importlib} library:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ importlib }\ImportTok{import} \BuiltInTok{reload}
\BuiltInTok{reload}\NormalTok{(utils)}
\end{Highlighting}
\end{Shaded}

or use \texttt{iPython} magic which will intelligently import code when
files change:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{load\_ext autoreload}
\OperatorTok{\%}\NormalTok{autoreload }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# census 2020s data}
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/NST{-}EST2022{-}POP.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{3}\NormalTok{, thousands}\OperatorTok{=}\StringTok{","}\NormalTok{)}
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    census\_2020s\_df}
\NormalTok{    .reset\_index()}
\NormalTok{    .drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"index"}\NormalTok{, }\StringTok{"Unnamed: 1"}\NormalTok{])}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"Unnamed: 0"}\NormalTok{: }\StringTok{"Geographic Area"}\NormalTok{\})}
\NormalTok{    .convert\_dtypes()                 }\CommentTok{\# "smart" converting of columns, use at your own risk}
\NormalTok{    .dropna()                         }\CommentTok{\# we\textquotesingle{}ll introduce this next time}
\NormalTok{)}
\NormalTok{census\_2020s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ census\_2020s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.strip(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{)}

\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrr}
\toprule
{} & Geographic Area &       2020 &       2021 &       2022 \\
\midrule
0 &   United States &  331511512 &  332031554 &  333287557 \\
1 &       Northeast &   57448898 &   57259257 &   57040406 \\
2 &         Midwest &   68961043 &   68836505 &   68787595 \\
3 &           South &  126450613 &  127346029 &  128716192 \\
4 &            West &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

\hypertarget{joining-data-merging-dataframes}{%
\subsection{\texorpdfstring{Joining Data (Merging
\texttt{DataFrame}s)}{Joining Data (Merging DataFrames)}}\label{joining-data-merging-dataframes}}

Time to \texttt{merge}! Here we use the \texttt{DataFrame} method
\texttt{df1.merge(right=df2,\ ...)} on \texttt{DataFrame} \texttt{df1}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{documentation}).
Contrast this with the function
\texttt{pd.merge(left=df1,\ right=df2,\ ...)}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.merge.html?highlight=pandas\%20merge\#pandas.merge}{documentation}).
Feel free to use either.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# merge TB DataFrame with two US census DataFrames}
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df,}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df,}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrlrrrrrrrrrrlrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 & Geographic Area\_x &      2010 &      2011 &      2012 &      2013 &      2014 &      2015 &      2016 &      2017 &      2018 &      2019 & Geographic Area\_y &      2020 &      2021 &      2022 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &           Alabama &   4785437 &   4799069 &   4815588 &   4830081 &   4841799 &   4852347 &   4863525 &   4874486 &   4887681 &   4903185 &           Alabama &   5031362 &   5049846 &   5074296 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &            Alaska &    713910 &    722128 &    730443 &    737068 &    736283 &    737498 &    741456 &    739700 &    735139 &    731545 &            Alaska &    732923 &    734182 &    733583 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &           Arizona &   6407172 &   6472643 &   6554978 &   6632764 &   6730413 &   6829676 &   6941072 &   7044008 &   7158024 &   7278717 &           Arizona &   7179943 &   7264877 &   7359197 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &          Arkansas &   2921964 &   2940667 &   2952164 &   2959400 &   2967392 &   2978048 &   2989918 &   3001345 &   3009733 &   3017804 &          Arkansas &   3014195 &   3028122 &   3045637 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &        California &  37319502 &  37638369 &  37948800 &  38260787 &  38596972 &  38918045 &  39167117 &  39358497 &  39461588 &  39512223 &        California &  39501653 &  39142991 &  39029342 \\
\bottomrule
\end{tabular}

Having all of these columns is a little unwieldy. We could either drop
the unneeded columns now, or just merge on smaller census
\texttt{DataFrame}s. Let's do the latter.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# try merging again, but cleaner this time}
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df[[}\StringTok{"Geographic Area"}\NormalTok{, }\StringTok{"2019"}\NormalTok{]],}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .drop(columns}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df[[}\StringTok{"Geographic Area"}\NormalTok{, }\StringTok{"2020"}\NormalTok{, }\StringTok{"2021"}\NormalTok{]],}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .drop(columns}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 \\
\bottomrule
\end{tabular}

\hypertarget{reproducing-data-compute-incidence}{%
\subsection{Reproducing Data: Compute
Incidence}\label{reproducing-data-compute-incidence}}

Let's recompute incidence to make sure we know where the original CDC
numbers came from.

From the
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w\#T1_down}{CDC
report}: TB incidence is computed as ``Cases per 100,000 persons using
mid-year population estimates from the U.S. Census Bureau.''

If we define a group as 100,000 people, then we can compute the TB
incidence for a given state population as

\[\text{TB incidence} = \frac{\text{TB cases in population}}{\text{groups in population}} = \frac{\text{TB cases in population}}{\text{population}/100000} \]

\[= \frac{\text{TB cases in population}}{\text{population}} \times 100000\]

Let's try this for 2019:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df[}\StringTok{"recompute incidence 2019"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\StringTok{"TB cases 2019"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\StringTok{"2019"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 &  recompute incidence 2019 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 &                      1.77 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 &                      7.93 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 &                      2.51 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 &                      2.12 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 &                      5.34 \\
\bottomrule
\end{tabular}

Awesome!!!

Let's use a for-loop and Python format strings to compute TB incidence
for all years. Python f-strings are just used for the purposes of this
demo, but they're handy to know when you explore data beyond this course
(\href{https://docs.python.org/3/tutorial/inputoutput.html}{documentation}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# recompute incidence for all years}
\ControlFlowTok{for}\NormalTok{ year }\KeywordTok{in}\NormalTok{ [}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{]:}
\NormalTok{    tb\_census\_df[}\SpecialStringTok{f"recompute incidence }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\SpecialStringTok{f"TB cases }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 &                      1.77 &                      1.43 &                      1.82 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 &                      7.93 &                      7.91 &                      7.90 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 &                      2.51 &                      1.89 &                      1.78 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 &                      2.12 &                      1.96 &                      2.28 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 &                      5.34 &                      4.32 &                      4.47 \\
\bottomrule
\end{tabular}

These numbers look pretty close!!! There are a few errors in the
hundredths place, particularly in 2021. It may be useful to further
explore reasons behind this discrepancy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df.describe()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &        2019 &        2020 &        2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
count &          51.00 &          51.00 &          51.00 &              51.00 &              51.00 &              51.00 &       51.00 &       51.00 &       51.00 &                     51.00 &                     51.00 &                     51.00 \\
mean  &         174.51 &         140.65 &         154.12 &               2.10 &               1.78 &               1.97 &  6436069.08 &  6500225.73 &  6510422.63 &                      2.10 &                      1.78 &                      1.97 \\
std   &         341.74 &         271.06 &         286.78 &               1.50 &               1.34 &               1.48 &  7360660.47 &  7408168.46 &  7394300.08 &                      1.50 &                      1.34 &                      1.47 \\
min   &           1.00 &           0.00 &           2.00 &               0.17 &               0.00 &               0.21 &   578759.00 &   577605.00 &   579483.00 &                      0.17 &                      0.00 &                      0.21 \\
25\%   &          25.50 &          29.00 &          23.00 &               1.29 &               1.21 &               1.23 &  1789606.00 &  1820311.00 &  1844920.00 &                      1.30 &                      1.21 &                      1.23 \\
50\%   &          70.00 &          67.00 &          69.00 &               1.80 &               1.52 &               1.70 &  4467673.00 &  4507445.00 &  4506589.00 &                      1.81 &                      1.52 &                      1.69 \\
75\%   &         180.50 &         139.00 &         150.00 &               2.58 &               1.99 &               2.22 &  7446805.00 &  7451987.00 &  7502811.00 &                      2.58 &                      1.99 &                      2.22 \\
max   &        2111.00 &        1706.00 &        1750.00 &               7.91 &               7.92 &               7.92 & 39512223.00 & 39501653.00 & 39142991.00 &                      7.93 &                      7.91 &                      7.90 \\
\bottomrule
\end{tabular}

\hypertarget{bonus-eda-reproducing-the-reported-statistic}{%
\subsection{Bonus EDA: Reproducing the Reported
Statistic}\label{bonus-eda-reproducing-the-reported-statistic}}

\textbf{How do we reproduce that reported statistic in the original
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w}{CDC
report}?}

\begin{quote}
Reported TB incidence (cases per 100,000 persons) increased
\textbf{9.4\%}, from \textbf{2.2} during 2020 to \textbf{2.4} during
2021 but was lower than incidence during 2019 (2.7). Increases occurred
among both U.S.-born and non--U.S.-born persons.
\end{quote}

This is TB incidence computed across the entire U.S. population! How do
we reproduce this? * We need to reproduce the ``Total'' TB incidences in
our rolled record. * But our current \texttt{tb\_census\_df} only has 51
entries (50 states plus Washington, D.C.). There is no rolled record. *
What happened\ldots?

Let's get exploring!

Before we keep exploring, we'll set all indexes to more meaningful
values, instead of just numbers that pertain to some row at some point.
This will make our cleaning slightly easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ tb\_df.set\_index(}\StringTok{"U.S. jurisdiction"}\NormalTok{)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
U.S. jurisdiction &                &                &                &                    &                    &                    \\
\midrule
Total             &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
Alabama           &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
Alaska            &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
Arizona           &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
Arkansas          &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ census\_2010s\_df.set\_index(}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
United States   &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ census\_2020s\_df.set\_index(}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrr}
\toprule
{} &       2020 &       2021 &       2022 \\
Geographic Area &            &            &            \\
\midrule
United States   &  331511512 &  332031554 &  333287557 \\
Northeast       &   57448898 &   57259257 &   57040406 \\
Midwest         &   68961043 &   68836505 &   68787595 \\
South           &  126450613 &  127346029 &  128716192 \\
West            &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

It turns out that our merge above only kept state records, even though
our original \texttt{tb\_df} had the ``Total'' rolled record:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
U.S. jurisdiction &                &                &                &                    &                    &                    \\
\midrule
Total             &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
Alabama           &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
Alaska            &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
Arizona           &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
Arkansas          &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

Recall that \texttt{merge} by default does an \textbf{inner} merge by
default, meaning that it only preserves keys that are present in
\textbf{both} \texttt{DataFrame}s.

The rolled records in our census \texttt{DataFrame} have different
\texttt{Geographic\ Area} fields, which was the key we merged on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
United States   &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

The Census \texttt{DataFrame} has several rolled records. The aggregate
record we are looking for actually has the Geographic Area named
``United States''.

One straightforward way to get the right merge is to rename the value
itself. Because we now have the Geographic Area index, we'll use
\texttt{df.rename()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html}{documentation}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rename rolled record for 2010s}
\NormalTok{census\_2010s\_df.rename(index}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}United States\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{\}, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
Total           &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# same, but for 2020s rename rolled record}
\NormalTok{census\_2020s\_df.rename(index}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}United States\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{\}, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrr}
\toprule
{} &       2020 &       2021 &       2022 \\
Geographic Area &            &            &            \\
\midrule
Total           &  331511512 &  332031554 &  333287557 \\
Northeast       &   57448898 &   57259257 &   57040406 \\
Midwest         &   68961043 &   68836505 &   68787595 \\
South           &  126450613 &  127346029 &  128716192 \\
West            &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

Next let's rerun our merge. Note the different chaining, because we are
now merging on indexes (\texttt{df.merge()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{documentation}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df[[}\StringTok{"2019"}\NormalTok{]],}
\NormalTok{           left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df[[}\StringTok{"2020"}\NormalTok{, }\StringTok{"2021"}\NormalTok{]],}
\NormalTok{           left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &       2019 &       2020 &       2021 \\
\midrule
Total    &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 &  328239523 &  331511512 &  332031554 \\
Alabama  &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &    4903185 &    5031362 &    5049846 \\
Alaska   &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &     731545 &     732923 &     734182 \\
Arizona  &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &    7278717 &    7179943 &    7264877 \\
Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &    3017804 &    3014195 &    3028122 \\
\bottomrule
\end{tabular}

Finally, let's recompute our incidences:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# recompute incidence for all years}
\ControlFlowTok{for}\NormalTok{ year }\KeywordTok{in}\NormalTok{ [}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{]:}
\NormalTok{    tb\_census\_df[}\SpecialStringTok{f"recompute incidence }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\SpecialStringTok{f"TB cases }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &       2019 &       2020 &       2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
Total    &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 &  328239523 &  331511512 &  332031554 &                      2.71 &                      2.16 &                      2.37 \\
Alabama  &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &    4903185 &    5031362 &    5049846 &                      1.77 &                      1.43 &                      1.82 \\
Alaska   &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &     731545 &     732923 &     734182 &                      7.93 &                      7.91 &                      7.90 \\
Arizona  &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &    7278717 &    7179943 &    7264877 &                      2.51 &                      1.89 &                      1.78 \\
Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &    3017804 &    3014195 &    3028122 &                      2.12 &                      1.96 &                      2.28 \\
\bottomrule
\end{tabular}

We reproduced the total U.S. incidences correctly!

We're almost there. Let's revisit the quote:

\begin{quote}
Reported TB incidence (cases per 100,000 persons) increased
\textbf{9.4\%}, from \textbf{2.2} during 2020 to \textbf{2.4} during
2021 but was lower than incidence during 2019 (2.7). Increases occurred
among both U.S.-born and non--U.S.-born persons.
\end{quote}

Recall that percent change from \(A\) to \(B\) is computed as
\(\text{percent change} = \frac{B - A}{A} \times 100\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{incidence\_2020 }\OperatorTok{=}\NormalTok{ tb\_census\_df.loc[}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}recompute incidence 2020\textquotesingle{}}\NormalTok{]}
\NormalTok{incidence\_2020}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.1637257652759883
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{incidence\_2021 }\OperatorTok{=}\NormalTok{ tb\_census\_df.loc[}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}recompute incidence 2021\textquotesingle{}}\NormalTok{]}
\NormalTok{incidence\_2021}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.3672448914298068
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference }\OperatorTok{=}\NormalTok{ (incidence\_2021 }\OperatorTok{{-}}\NormalTok{ incidence\_2020)}\OperatorTok{/}\NormalTok{incidence\_2020 }\OperatorTok{*} \DecValTok{100}
\NormalTok{difference}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
9.405957511804143
\end{verbatim}

\hypertarget{eda-demo-2-mauna-loa-co2-data-a-lesson-in-data-faithfulness}{%
\section{EDA Demo 2: Mauna Loa CO2 Data -- A Lesson in Data
Faithfulness}\label{eda-demo-2-mauna-loa-co2-data-a-lesson-in-data-faithfulness}}

\href{https://gml.noaa.gov/ccgg/trends/data.html}{Mauna Loa Observatory}
has been monitoring CO2 concentrations since 1958.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_file }\OperatorTok{=} \StringTok{"data/co2\_mm\_mlo.txt"}
\end{Highlighting}
\end{Shaded}

Let's do some \textbf{EDA}!!

\hypertarget{reading-this-file-into-pandas}{%
\subsection{\texorpdfstring{Reading this file into
\texttt{Pandas}?}{Reading this file into Pandas?}}\label{reading-this-file-into-pandas}}

Let's instead check out this \texttt{.txt} file. Some questions to keep
in mind: Do we trust this file extension? What structure is it?

Lines 71-78 (inclusive) are shown below:

\begin{verbatim}
line number |                            file contents

71          |   #            decimal     average   interpolated    trend    #days
72          |   #             date                             (season corr)
73          |   1958   3    1958.208      315.71      315.71      314.62     -1
74          |   1958   4    1958.292      317.45      317.45      315.29     -1
75          |   1958   5    1958.375      317.50      317.50      314.71     -1
76          |   1958   6    1958.458      -99.99      317.10      314.85     -1
77          |   1958   7    1958.542      315.86      315.86      314.98     -1
78          |   1958   8    1958.625      314.93      314.93      315.94     -1
\end{verbatim}

Notice how:

\begin{itemize}
\tightlist
\item
  The values are separated by white space, possibly tabs.
\item
  The data line up down the rows. For example, the month appears in 7th
  to 8th position of each line.
\item
  The 71st and 72nd lines in the file contain column headings split over
  two lines.
\end{itemize}

We can use~\texttt{read\_csv}~to read the data into a \texttt{pandas}
\texttt{DataFrame}, and we provide several arguments to specify that the
separators are white space, there is no header (\textbf{we will set our
own column names}), and to skip the first 72 rows of the file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2 }\OperatorTok{=}\NormalTok{ pd.read\_csv(}
\NormalTok{    co2\_file, header }\OperatorTok{=} \VariableTok{None}\NormalTok{, skiprows }\OperatorTok{=} \DecValTok{72}\NormalTok{,}
\NormalTok{    sep }\OperatorTok{=} \VerbatimStringTok{r\textquotesingle{}\textbackslash{}s+\textquotesingle{}}       \CommentTok{\#delimiter for continuous whitespace (stay tuned for regex next lecture))}
\NormalTok{)}
\NormalTok{co2.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &     0 &  1 &       2 &      3 &      4 &      5 &  6 \\
\midrule
0 &  1958 &  3 & 1958.21 & 315.71 & 315.71 & 314.62 & -1 \\
1 &  1958 &  4 & 1958.29 & 317.45 & 317.45 & 315.29 & -1 \\
2 &  1958 &  5 & 1958.38 & 317.50 & 317.50 & 314.71 & -1 \\
3 &  1958 &  6 & 1958.46 & -99.99 & 317.10 & 314.85 & -1 \\
4 &  1958 &  7 & 1958.54 & 315.86 & 315.86 & 314.98 & -1 \\
\bottomrule
\end{tabular}

Congratulations! You've wrangled the data!

\ldots But our columns aren't named. \textbf{We need to do more EDA.}

\hypertarget{exploring-variable-feature-types}{%
\subsection{Exploring Variable Feature
Types}\label{exploring-variable-feature-types}}

The NOAA \href{https://gml.noaa.gov/ccgg/trends/}{webpage} might have
some useful tidbits (in this case it doesn't).

Using this information, we'll rerun \texttt{pd.read\_csv}, but this time
with some \textbf{custom column names.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2 }\OperatorTok{=}\NormalTok{ pd.read\_csv(}
\NormalTok{    co2\_file, header }\OperatorTok{=} \VariableTok{None}\NormalTok{, skiprows }\OperatorTok{=} \DecValTok{72}\NormalTok{,}
\NormalTok{    sep }\OperatorTok{=} \StringTok{\textquotesingle{}\textbackslash{}s+\textquotesingle{}}\NormalTok{, }\CommentTok{\#regex for continuous whitespace (next lecture)}
\NormalTok{    names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}Yr\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Mo\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}DecDate\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Int\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Trend\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Days\textquotesingle{}}\NormalTok{]}
\NormalTok{)}
\NormalTok{co2.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
0 &  1958 &   3 &  1958.21 & 315.71 & 315.71 & 314.62 &    -1 \\
1 &  1958 &   4 &  1958.29 & 317.45 & 317.45 & 315.29 &    -1 \\
2 &  1958 &   5 &  1958.38 & 317.50 & 317.50 & 314.71 &    -1 \\
3 &  1958 &   6 &  1958.46 & -99.99 & 317.10 & 314.85 &    -1 \\
4 &  1958 &   7 &  1958.54 & 315.86 & 315.86 & 314.98 &    -1 \\
\bottomrule
\end{tabular}

\hypertarget{visualizing-co2}{%
\subsection{Visualizing CO2}\label{visualizing-co2}}

Scientific studies tend to have very clean data, right\ldots? Let's jump
right in and make a time series plot of CO2 monthly averages.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.lineplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}DecDate\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{co2)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-62-output-1.pdf}

}

\end{figure}

The code above uses the \texttt{seaborn} plotting library (abbreviated
\texttt{sns}). We will cover this in the Visualization lecture, but now
you don't need to worry about how it works!

Yikes! Plotting the data uncovered a problem. The sharp vertical lines
suggest that we have some \textbf{missing values}. What happened here?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
0 &  1958 &   3 &  1958.21 & 315.71 & 315.71 & 314.62 &    -1 \\
1 &  1958 &   4 &  1958.29 & 317.45 & 317.45 & 315.29 &    -1 \\
2 &  1958 &   5 &  1958.38 & 317.50 & 317.50 & 314.71 &    -1 \\
3 &  1958 &   6 &  1958.46 & -99.99 & 317.10 & 314.85 &    -1 \\
4 &  1958 &   7 &  1958.54 & 315.86 & 315.86 & 314.98 &    -1 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2.tail()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
733 &  2019 &   4 &  2019.29 & 413.32 & 413.32 & 410.49 &    26 \\
734 &  2019 &   5 &  2019.38 & 414.66 & 414.66 & 411.20 &    28 \\
735 &  2019 &   6 &  2019.46 & 413.92 & 413.92 & 411.58 &    27 \\
736 &  2019 &   7 &  2019.54 & 411.77 & 411.77 & 411.43 &    23 \\
737 &  2019 &   8 &  2019.62 & 409.95 & 409.95 & 411.84 &    29 \\
\bottomrule
\end{tabular}

Some data have unusual values like -1 and -99.99.

Let's check the description at the top of the file again.

\begin{itemize}
\tightlist
\item
  -1 signifies a missing value for the number of days \texttt{Days} the
  equipment was in operation that month.
\item
  -99.99 denotes a missing monthly average \texttt{Avg}
\end{itemize}

How can we fix this? First, let's explore other aspects of our data.
Understanding our data will help us decide what to do with the missing
values.

\hypertarget{sanity-checks-reasoning-about-the-data}{%
\subsection{Sanity Checks: Reasoning about the
data}\label{sanity-checks-reasoning-about-the-data}}

First, we consider the shape of the data. How many rows should we have?

\begin{itemize}
\tightlist
\item
  If chronological order, we should have one record per month.
\item
  Data from March 1958 to August 2019.
\item
  We should have \$ 12 \times (2019-1957) - 2 - 4 = 738 \$ records.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(738, 7)
\end{verbatim}

Nice!! The number of rows (i.e.~records) match our expectations.

Let's now check the quality of each feature.

\hypertarget{understanding-missing-value-1-days}{%
\subsection{\texorpdfstring{Understanding Missing Value 1:
\texttt{Days}}{Understanding Missing Value 1: Days}}\label{understanding-missing-value-1-days}}

\texttt{Days} is a time field, so let's analyze other time fields to see
if there is an explanation for missing values of days of operation.

Let's start with \textbf{months}, \texttt{Mo}.

Are we missing any records? The number of months should have 62 or 61
instances (March 1957-August 2019).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2[}\StringTok{"Mo"}\NormalTok{].value\_counts().sort\_index()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Mo \\
\midrule
1  &  61 \\
2  &  61 \\
3  &  62 \\
4  &  62 \\
5  &  62 \\
6  &  62 \\
7  &  62 \\
8  &  62 \\
9  &  61 \\
10 &  61 \\
11 &  61 \\
12 &  61 \\
\bottomrule
\end{tabular}

As expected Jan, Feb, Sep, Oct, Nov, and Dec have 61 occurrences and the
rest 62.

Next let's explore \textbf{days} \texttt{Days} itself, which is the
number of days that the measurement equipment worked.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.displot(co2[}\StringTok{\textquotesingle{}Days\textquotesingle{}}\NormalTok{])}\OperatorTok{;}
\NormalTok{plt.title(}\StringTok{"Distribution of days feature"}\NormalTok{)}\OperatorTok{;} \CommentTok{\# suppresses unneeded plotting output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-67-output-2.pdf}

}

\end{figure}

In terms of data quality, a handful of months have averages based on
measurements taken on fewer than half the days. In addition, there are
nearly 200 missing values--\textbf{that's about 27\% of the data}!

Finally, let's check the last time feature, \textbf{year} \texttt{Yr}.

Let's check to see if there is any connection between missing-ness and
the year of the recording.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.scatterplot(x}\OperatorTok{=}\StringTok{"Yr"}\NormalTok{, y}\OperatorTok{=}\StringTok{"Days"}\NormalTok{, data}\OperatorTok{=}\NormalTok{co2)}\OperatorTok{;}
\NormalTok{plt.title(}\StringTok{"Day field by Year"}\NormalTok{)}\OperatorTok{;} \CommentTok{\# the ; suppresses output}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-68-output-1.pdf}

}

\end{figure}

\textbf{Observations}:

\begin{itemize}
\tightlist
\item
  All of the missing data are in the early years of operation.
\item
  It appears there may have been problems with equipment in the mid to
  late 80s.
\end{itemize}

\textbf{Potential Next Steps}:

\begin{itemize}
\tightlist
\item
  Confirm these explanations through documentation about the historical
  readings.
\item
  Maybe drop the earliest recordings? However, we would want to delay
  such action until after we have examined the time trends and assess
  whether there are any potential problems.
\end{itemize}

\hypertarget{understanding-missing-value-2-avg}{%
\subsection{\texorpdfstring{Understanding Missing Value 2:
\texttt{Avg}}{Understanding Missing Value 2: Avg}}\label{understanding-missing-value-2-avg}}

Next, let's return to the -99.99 values in \texttt{Avg} to analyze the
overall quality of the CO2 measurements. We'll plot a histogram of the
average CO2 measurements

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histograms of average CO2 measurements}
\NormalTok{sns.displot(co2[}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-69-output-2.pdf}

}

\end{figure}

The non-missing values are in the 300-400 range (a regular range of CO2
levels).

We also see that there are only a few missing \texttt{Avg} values
(\textbf{\textless1\% of values}). Let's examine all of them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2[co2[}\StringTok{"Avg"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
3   &  1958 &   6 &  1958.46 & -99.99 & 317.10 & 314.85 &    -1 \\
7   &  1958 &  10 &  1958.79 & -99.99 & 312.66 & 315.61 &    -1 \\
71  &  1964 &   2 &  1964.12 & -99.99 & 320.07 & 319.61 &    -1 \\
72  &  1964 &   3 &  1964.21 & -99.99 & 320.73 & 319.55 &    -1 \\
73  &  1964 &   4 &  1964.29 & -99.99 & 321.77 & 319.48 &    -1 \\
213 &  1975 &  12 &  1975.96 & -99.99 & 330.59 & 331.60 &     0 \\
313 &  1984 &   4 &  1984.29 & -99.99 & 346.84 & 344.27 &     2 \\
\bottomrule
\end{tabular}

There doesn't seem to be a pattern to these values, other than that most
records also were missing \texttt{Days} data.

\hypertarget{drop-nan-or-impute-missing-avg-data}{%
\subsection{\texorpdfstring{Drop, \texttt{NaN}, or Impute Missing
\texttt{Avg}
Data?}{Drop, NaN, or Impute Missing Avg Data?}}\label{drop-nan-or-impute-missing-avg-data}}

How should we address the invalid \texttt{Avg} data?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Drop records
\item
  Set to NaN
\item
  Impute using some strategy
\end{enumerate}

Remember we want to fix the following plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.lineplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}DecDate\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{co2)}
\NormalTok{plt.title(}\StringTok{"CO2 Average By Month"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-71-output-1.pdf}

}

\end{figure}

Since we are plotting \texttt{Avg} vs \texttt{DecDate}, we should just
focus on dealing with missing values for \texttt{Avg}.

Let's consider a few options: 1. Drop those records 2. Replace -99.99
with NaN 3. Substitute it with a likely value for the average CO2?

What do you think are the pros and cons of each possible action?

Let's examine each of these three options.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1. Drop missing values}
\NormalTok{co2\_drop }\OperatorTok{=}\NormalTok{ co2[co2[}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{] }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{]}
\NormalTok{co2\_drop.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
0 &  1958 &   3 &  1958.21 & 315.71 & 315.71 & 314.62 &    -1 \\
1 &  1958 &   4 &  1958.29 & 317.45 & 317.45 & 315.29 &    -1 \\
2 &  1958 &   5 &  1958.38 & 317.50 & 317.50 & 314.71 &    -1 \\
4 &  1958 &   7 &  1958.54 & 315.86 & 315.86 & 314.98 &    -1 \\
5 &  1958 &   8 &  1958.62 & 314.93 & 314.93 & 315.94 &    -1 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2. Replace NaN with {-}99.99}
\NormalTok{co2\_NA }\OperatorTok{=}\NormalTok{ co2.replace(}\OperatorTok{{-}}\FloatTok{99.99}\NormalTok{, np.NaN)}
\NormalTok{co2\_NA.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
0 &  1958 &   3 &  1958.21 & 315.71 & 315.71 & 314.62 &    -1 \\
1 &  1958 &   4 &  1958.29 & 317.45 & 317.45 & 315.29 &    -1 \\
2 &  1958 &   5 &  1958.38 & 317.50 & 317.50 & 314.71 &    -1 \\
3 &  1958 &   6 &  1958.46 &    NaN & 317.10 & 314.85 &    -1 \\
4 &  1958 &   7 &  1958.54 & 315.86 & 315.86 & 314.98 &    -1 \\
\bottomrule
\end{tabular}

We'll also use a third version of the data.

First, we note that the dataset already comes with a \textbf{substitute
value} for the -99.99.

From the file description:

\begin{quote}
The \texttt{interpolated} column includes average values from the
preceding column (\texttt{average}) and \textbf{interpolated values}
where data are missing. Interpolated values are computed in two
steps\ldots{}
\end{quote}

The \texttt{Int} feature has values that exactly match those in
\texttt{Avg}, except when \texttt{Avg} is -99.99, and then a
\textbf{reasonable} estimate is used instead.

So, the third version of our data will use the \texttt{Int} feature
instead of \texttt{Avg}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 3. Use interpolated column which estimates missing Avg values}
\NormalTok{co2\_impute }\OperatorTok{=}\NormalTok{ co2.copy()}
\NormalTok{co2\_impute[}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ co2[}\StringTok{\textquotesingle{}Int\textquotesingle{}}\NormalTok{]}
\NormalTok{co2\_impute.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrr}
\toprule
{} &    Yr &  Mo &  DecDate &    Avg &    Int &  Trend &  Days \\
\midrule
0 &  1958 &   3 &  1958.21 & 315.71 & 315.71 & 314.62 &    -1 \\
1 &  1958 &   4 &  1958.29 & 317.45 & 317.45 & 315.29 &    -1 \\
2 &  1958 &   5 &  1958.38 & 317.50 & 317.50 & 314.71 &    -1 \\
3 &  1958 &   6 &  1958.46 & 317.10 & 317.10 & 314.85 &    -1 \\
4 &  1958 &   7 &  1958.54 & 315.86 & 315.86 & 314.98 &    -1 \\
\bottomrule
\end{tabular}

What's a \textbf{reasonable} estimate?

To answer this question, let's zoom in on a short time period, say the
measurements in 1958 (where we know we have two missing values).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# results of plotting data in 1958}

\KeywordTok{def}\NormalTok{ line\_and\_points(data, ax, title):}
    \CommentTok{\# assumes single year, hence Mo}
\NormalTok{    ax.plot(}\StringTok{\textquotesingle{}Mo\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{data)}
\NormalTok{    ax.scatter(}\StringTok{\textquotesingle{}Mo\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{data)}
\NormalTok{    ax.set\_xlim(}\DecValTok{2}\NormalTok{, }\DecValTok{13}\NormalTok{)}
\NormalTok{    ax.set\_title(title)}
\NormalTok{    ax.set\_xticks(np.arange(}\DecValTok{3}\NormalTok{, }\DecValTok{13}\NormalTok{))}

\KeywordTok{def}\NormalTok{ data\_year(data, year):}
    \ControlFlowTok{return}\NormalTok{ data[data[}\StringTok{"Yr"}\NormalTok{] }\OperatorTok{==} \DecValTok{1958}\NormalTok{]}
    
\CommentTok{\# uses matplotlib subplots}
\CommentTok{\# you may see more next week; focus on output for now}
\NormalTok{fig, axes }\OperatorTok{=}\NormalTok{ plt.subplots(ncols }\OperatorTok{=} \DecValTok{3}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{12}\NormalTok{, }\DecValTok{4}\NormalTok{), sharey}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{year }\OperatorTok{=} \DecValTok{1958}
\NormalTok{line\_and\_points(data\_year(co2\_drop, year), axes[}\DecValTok{0}\NormalTok{], title}\OperatorTok{=}\StringTok{"1. Drop Missing"}\NormalTok{)}
\NormalTok{line\_and\_points(data\_year(co2\_NA, year), axes[}\DecValTok{1}\NormalTok{], title}\OperatorTok{=}\StringTok{"2. Missing Set to NaN"}\NormalTok{)}
\NormalTok{line\_and\_points(data\_year(co2\_impute, year), axes[}\DecValTok{2}\NormalTok{], title}\OperatorTok{=}\StringTok{"3. Missing Interpolated"}\NormalTok{)}

\NormalTok{fig.suptitle(}\SpecialStringTok{f"Monthly Averages for }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\NormalTok{plt.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-75-output-1.pdf}

}

\end{figure}

In the big picture since there are only 7 \texttt{Avg} values missing
(\textbf{\textless1\%} of 738 months), any of these approaches would
work.

However there is some appeal to \textbf{option C, Imputing}:

\begin{itemize}
\tightlist
\item
  Shows seasonal trends for CO2
\item
  We are plotting all months in our data as a line plot
\end{itemize}

Let's replot our original figure with option 3:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.lineplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}DecDate\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{co2\_impute)}
\NormalTok{plt.title(}\StringTok{"CO2 Average By Month, Imputed"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-76-output-1.pdf}

}

\end{figure}

Looks pretty close to what we see on the NOAA
\href{https://gml.noaa.gov/ccgg/trends/}{website}!

\hypertarget{presenting-the-data-a-discussion-on-data-granularity}{%
\subsection{Presenting the Data: A Discussion on Data
Granularity}\label{presenting-the-data-a-discussion-on-data-granularity}}

From the description:

\begin{itemize}
\tightlist
\item
  Monthly measurements are averages of average day measurements.
\item
  The NOAA GML website has datasets for daily/hourly measurements too.
\end{itemize}

The data you present depends on your research question.

\textbf{How do CO2 levels vary by season?}

\begin{itemize}
\tightlist
\item
  You might want to keep average monthly data.
\end{itemize}

\textbf{Are CO2 levels rising over the past 50+ years, consistent with
global warming predictions?}

\begin{itemize}
\tightlist
\item
  You might be happier with a \textbf{coarser granularity} of average
  year data!
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_year }\OperatorTok{=}\NormalTok{ co2\_impute.groupby(}\StringTok{\textquotesingle{}Yr\textquotesingle{}}\NormalTok{).mean()}
\NormalTok{sns.lineplot(x}\OperatorTok{=}\StringTok{\textquotesingle{}Yr\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}Avg\textquotesingle{}}\NormalTok{, data}\OperatorTok{=}\NormalTok{co2\_year)}
\NormalTok{plt.title(}\StringTok{"CO2 Average By Year"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{eda/eda_files/figure-pdf/cell-77-output-1.pdf}

}

\end{figure}

Indeed, we see a rise by nearly 100 ppm of CO2 since Mauna Loa began
recording in 1958.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

We went over a lot of content this lecture; let's summarize the most
important points:

\hypertarget{dealing-with-missing-values}{%
\subsection{Dealing with Missing
Values}\label{dealing-with-missing-values}}

There are a few options we can take to deal with missing data:

\begin{itemize}
\tightlist
\item
  Drop missing records
\item
  Keep \texttt{NaN} missing values
\item
  Impute using an interpolated column
\end{itemize}

\hypertarget{eda-and-data-wrangling}{%
\subsection{EDA and Data Wrangling}\label{eda-and-data-wrangling}}

There are several ways to approach EDA and Data Wrangling:

\begin{itemize}
\tightlist
\item
  Examine the \textbf{data and metadata}: what is the date, size,
  organization, and structure of the data?
\item
  Examine each \textbf{field/attribute/dimension} individually.
\item
  Examine pairs of related dimensions (e.g.~breaking down grades by
  major).
\item
  Along the way, we can:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Visualize} or summarize the data.
  \item
    \textbf{Validate assumptions} about data and its collection process.
    Pay particular attention to when the data was collected.
  \item
    Identify and \textbf{address anomalies}.
  \item
    Apply data transformations and corrections (we'll cover this in the
    upcoming lecture).
  \item
    \textbf{Record everything you do!} Developing in Jupyter Notebook
    promotes \emph{reproducibility} of your own work!
  \end{itemize}
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{regular-expressions}{%
\chapter{Regular Expressions}\label{regular-expressions}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Understand Python string manipulation, \texttt{pandas} \texttt{Series}
  methods
\item
  Parse and create regex, with a reference table
\item
  Use vocabulary (closure, metacharacters, groups, etc.) to describe
  regex metacharacters
\end{itemize}

\end{tcolorbox}

\hypertarget{why-work-with-text}{%
\section{Why Work with Text?}\label{why-work-with-text}}

Last lecture, we learned of the difference between quantitative and
qualitative variable types. The latter includes string data --- the
primary focus of lecture 6. In this note, we'll discuss the necessary
tools to manipulate text: Python string manipulation and regular
expressions.

There are two main reasons for working with text.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Canonicalization: Convert data that has multiple formats into a
  standard form.

  \begin{itemize}
  \tightlist
  \item
    By manipulating text, we can join tables with mismatched string
    labels.
  \end{itemize}
\item
  Extract information into a new feature.

  \begin{itemize}
  \tightlist
  \item
    For example, we can extract date and time features from text.
  \end{itemize}
\end{enumerate}

\hypertarget{python-string-methods}{%
\section{Python String Methods}\label{python-string-methods}}

First, we'll introduce a few methods useful for string manipulation. The
following table includes a number of string operations supported by
Python and \texttt{pandas}. The Python functions operate on a single
string, while their equivalent in \texttt{pandas} are
\textbf{vectorized} --- they operate on a \texttt{Series} of string
data.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3889}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Python
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\texttt{Pandas} (\texttt{Series})
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Transformation & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.lower()}
\item
  \texttt{s.upper()}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.lower()}
\item
  \texttt{ser.str.upper()}
\end{itemize}
\end{minipage} \\
Replacement + Deletion & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.replace(\_)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.replace(\_)}
\end{itemize}
\end{minipage} \\
Split & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.split(\_)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.split(\_)}
\end{itemize}
\end{minipage} \\
Substring & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s{[}1:4{]}}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str{[}1:4{]}}
\end{itemize}
\end{minipage} \\
Membership & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{\textquotesingle{}\_\textquotesingle{}\ in\ s}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.contains(\_)}
\end{itemize}
\end{minipage} \\
Length & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{len(s)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.len()}
\end{itemize}
\end{minipage} \\
\end{longtable}

We'll discuss the differences between Python string functions and
\texttt{pandas} \texttt{Series} methods in the following section on
canonicalization.

\hypertarget{canonicalization}{%
\subsection{Canonicalization}\label{canonicalization}}

Assume we want to merge the given tables.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/county\_and\_state.csv\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    county\_and\_state }\OperatorTok{=}\NormalTok{ pd.read\_csv(f)}
    
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/county\_and\_population.csv\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    county\_and\_pop }\OperatorTok{=}\NormalTok{ pd.read\_csv(f)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display(county\_and\_state), display(county\_and\_pop)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lll}
\toprule
{} &                      County & State \\
\midrule
0 &              De Witt County &    IL \\
1 &        Lac qui Parle County &    MN \\
2 &      Lewis and Clark County &    MT \\
3 &  St John the Baptist Parish &    LS \\
\bottomrule
\end{tabular}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llr}
\toprule
{} &                County &  Population \\
\midrule
0 &                DeWitt &       16798 \\
1 &         Lac Qui Parle &        8067 \\
2 &         Lewis \& Clark &       55716 \\
3 &  St. John the Baptist &       43044 \\
\bottomrule
\end{tabular}

Last time, we used a \textbf{primary key} and \textbf{foreign key} to
join two tables. While neither of these keys exist in our
\texttt{DataFrame}s, the \texttt{"County"} columns look similar enough.
Can we convert these columns into one standard, canonical form to merge
the two tables?

\hypertarget{canonicalization-with-python-string-manipulation}{%
\subsubsection{Canonicalization with Python String
Manipulation}\label{canonicalization-with-python-string-manipulation}}

The following function uses Python string manipulation to convert a
single county name into canonical form. It does so by eliminating
whitespace, punctuation, and unnecessary text.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ canonicalize\_county(county\_name):}
    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        county\_name}
\NormalTok{            .lower()}
\NormalTok{            .replace(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}\&\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}and\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}county\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}parish\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}

\NormalTok{canonicalize\_county(}\StringTok{"St. John the Baptist"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'stjohnthebaptist'
\end{verbatim}

We will use the \texttt{pandas} \texttt{map} function to apply the
\texttt{canonicalize\_county} function to every row in both
\texttt{DataFrame}s. In doing so, we'll create a new column in each
called \texttt{clean\_county\_python} with the canonical form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_and\_pop[}\StringTok{\textquotesingle{}clean\_county\_python\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ county\_and\_pop[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(canonicalize\_county)}
\NormalTok{county\_and\_state[}\StringTok{\textquotesingle{}clean\_county\_python\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ county\_and\_state[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(canonicalize\_county)}
\NormalTok{display(county\_and\_state), display(county\_and\_pop)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llll}
\toprule
{} &                      County & State & clean\_county\_python \\
\midrule
0 &              De Witt County &    IL &              dewitt \\
1 &        Lac qui Parle County &    MN &         lacquiparle \\
2 &      Lewis and Clark County &    MT &       lewisandclark \\
3 &  St John the Baptist Parish &    LS &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llrl}
\toprule
{} &                County &  Population & clean\_county\_python \\
\midrule
0 &                DeWitt &       16798 &              dewitt \\
1 &         Lac Qui Parle &        8067 &         lacquiparle \\
2 &         Lewis \& Clark &       55716 &       lewisandclark \\
3 &  St. John the Baptist &       43044 &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\hypertarget{canonicalization-with-pandas-series-methods}{%
\subsubsection{Canonicalization with Pandas Series
Methods}\label{canonicalization-with-pandas-series-methods}}

Alternatively, we can use \texttt{pandas} \texttt{Series} methods to
create this standardized column. To do so, we must call the
\texttt{.str} attribute of our \texttt{Series} object prior to calling
any methods, like \texttt{.lower} and \texttt{.replace}. Notice how
these method names match their equivalent built-in Python string
functions.

Chaining multiple \texttt{Series} methods in this manner eliminates the
need to use the \texttt{map} function (as this code is vectorized).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ canonicalize\_county\_series(county\_series):}
    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        county\_series}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.lower()}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}\&\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}and\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}county\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}parish\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}

\NormalTok{county\_and\_pop[}\StringTok{\textquotesingle{}clean\_county\_pandas\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ canonicalize\_county\_series(county\_and\_pop[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{])}
\NormalTok{county\_and\_state[}\StringTok{\textquotesingle{}clean\_county\_pandas\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ canonicalize\_county\_series(county\_and\_state[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{])}
\NormalTok{display(county\_and\_pop), display(county\_and\_state)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/var/folders/7t/zbwy02ts2m7cn64fvwjqb8xw0000gp/T/ipykernel_24261/2523629438.py:3: FutureWarning:

The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.

/var/folders/7t/zbwy02ts2m7cn64fvwjqb8xw0000gp/T/ipykernel_24261/2523629438.py:3: FutureWarning:

The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.

/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llrll}
\toprule
{} &                County &  Population & clean\_county\_python & clean\_county\_pandas \\
\midrule
0 &                DeWitt &       16798 &              dewitt &              dewitt \\
1 &         Lac Qui Parle &        8067 &         lacquiparle &         lacquiparle \\
2 &         Lewis \& Clark &       55716 &       lewisandclark &       lewisandclark \\
3 &  St. John the Baptist &       43044 &    stjohnthebaptist &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllll}
\toprule
{} &                      County & State & clean\_county\_python & clean\_county\_pandas \\
\midrule
0 &              De Witt County &    IL &              dewitt &              dewitt \\
1 &        Lac qui Parle County &    MN &         lacquiparle &         lacquiparle \\
2 &      Lewis and Clark County &    MT &       lewisandclark &       lewisandclark \\
3 &  St John the Baptist Parish &    LS &    stjohnthebaptist &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\hypertarget{extraction}{%
\subsection{Extraction}\label{extraction}}

Extraction explores the idea of obtaining useful information from text
data. This will be particularily important in model building, which
we'll study in a few weeks.

Say we want to read some data from a \texttt{.txt} file.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/log.txt\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    log\_lines }\OperatorTok{=}\NormalTok{ f.readlines()}

\NormalTok{log\_lines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n',
 '193.205.203.3 - - [2/Feb/2005:17:23:6 -0800] "GET /stat141/Notes/dim.html HTTP/1.0" 404 302 "http://eeyore.ucdavis.edu/stat141/Notes/session.html"\n',
 '169.237.46.240 - "" [3/Feb/2006:10:18:37 -0800] "GET /stat141/homework/Solutions/hw1Sol.pdf HTTP/1.1"\n']
\end{verbatim}

Suppose we want to extract the day, month, year, hour, minutes, seconds,
and time zone. Unfortunately, these items are not in a fixed position
from the beginning of the string, so slicing by some fixed offset won't
work.

Instead, we can use some clever thinking. Notice how the relevant
information is contained within a set of brackets, further separated by
\texttt{/} and \texttt{:}. We can hone in on this region of text, and
split the data on these characters. Python's built-in \texttt{.split}
function makes this easy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OperatorTok{=}\NormalTok{ log\_lines[}\DecValTok{0}\NormalTok{] }\CommentTok{\# Only considering the first row of data}

\NormalTok{pertinent }\OperatorTok{=}\NormalTok{ first.split(}\StringTok{"["}\NormalTok{)[}\DecValTok{1}\NormalTok{].split(}\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{)[}\DecValTok{0}\NormalTok{]}
\NormalTok{day, month, rest }\OperatorTok{=}\NormalTok{ pertinent.split(}\StringTok{\textquotesingle{}/\textquotesingle{}}\NormalTok{)}
\NormalTok{year, hour, minute, rest }\OperatorTok{=}\NormalTok{ rest.split(}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{seconds, time\_zone }\OperatorTok{=}\NormalTok{ rest.split(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{)}
\NormalTok{day, month, year, hour, minute, seconds, time\_zone}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
('26', 'Jan', '2014', '10', '47', '58', '-0800')
\end{verbatim}

There are two problems with this code:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Python's built-in functions limit us to extract data one record at a
  time,

  \begin{itemize}
  \tightlist
  \item
    This can be resolved using the \texttt{map} function or
    \texttt{pandas} \texttt{Series} methods.
  \end{itemize}
\item
  The code is quite verbose.

  \begin{itemize}
  \tightlist
  \item
    This is a larger issue that is trickier to solve
  \end{itemize}
\end{enumerate}

In the next section, we'll introduce regular expressions - a tool that
solves problem 2.

\hypertarget{regex-basics}{%
\section{RegEx Basics}\label{regex-basics}}

A \textbf{regular expression (``RegEx'')} is a sequence of characters
that specifies a search pattern. They are written to extract specific
information from text. Regular expressions are essentially part of a
smaller programming language embedded in Python, made available through
the \texttt{re} module. As such, they have a stand-alone syntax and
methods for various capabilities.

Regular expressions are useful in many applications beyond data science.
For example, Social Security Numbers (SSNs) are often validated with
regular expressions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{r"[0{-}9]\{3\}{-}[0{-}9]\{2\}{-}[0{-}9]\{4\}"} \CommentTok{\# Regular Expression Syntax}

\CommentTok{\# 3 of any digit, then a dash,}
\CommentTok{\# then 2 of any digit, then a dash,}
\CommentTok{\# then 4 of any digit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'[0-9]{3}-[0-9]{2}-[0-9]{4}'
\end{verbatim}

There are a ton of resources to learn and experiment with regular
expressions. A few are provided below:

\begin{itemize}
\tightlist
\item
  \href{https://docs.python.org/3/howto/regex.html}{Official Regex
  Guide}
\item
  \href{https://ds100.org/sp22/resources/assets/hw/regex_reference.pdf}{Data
  100 Reference Sheet}
\item
  \href{https://regex101.com/}{Regex101.com}

  \begin{itemize}
  \tightlist
  \item
    Be sure to check Python under the category on the left.
  \end{itemize}
\end{itemize}

\hypertarget{basics-regex-syntax}{%
\subsection{Basics RegEx Syntax}\label{basics-regex-syntax}}

There are four basic operations with regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1875}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1771}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1458}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2083}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{Or}: \texttt{\textbar{}} & 4 & AA\textbar BAAB & AA BAAB & every
other string \\
\texttt{Concatenation} & 3 & AABAAB & AABAAB & every other string \\
\texttt{Closure}: \texttt{*} (zero or more) & 2 & AB*A & AA ABBBBBBA &
AB ABABA \\
\texttt{Group}: \texttt{()} (parenthesis) & 1 & A(A\textbar B)AAB (AB)*A
& AAAAB ABAAB A ABABABABA & every other string AA ABBA \\
\end{longtable}

Notice how these metacharacter operations are ordered. Rather than being
literal characters, these \textbf{metacharacters} manipulate adjacent
characters. \texttt{()} takes precedence, followed by \texttt{*}, and
finally \texttt{\textbar{}}. This allows us to differentiate between
very different regex commands like \texttt{AB*} and \texttt{(AB)*}. The
former reads ``\texttt{A} then zero or more copies of \texttt{B}'',
while the latter specifies ``zero or more copies of \texttt{AB}''.

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\textbf{Question 1}: Give a regular expression that matches
\texttt{moon}, \texttt{moooon}, etc. Your expression should match any
even number of \texttt{o}s except zero (i.e.~don't match \texttt{mn}).

\textbf{Answer 1}: \texttt{moo(oo)*n}

\begin{itemize}
\tightlist
\item
  Hardcoding \texttt{oo} before the capture group ensures that
  \texttt{mn} is not matched.
\item
  A capture group of \texttt{(oo)*} ensures the number of \texttt{o}'s
  is even.
\end{itemize}

\textbf{Question 2}: Using only basic operations, formulate a regex that
matches \texttt{muun}, \texttt{muuuun}, \texttt{moon}, \texttt{moooon},
etc. Your expression should match any even number of \texttt{u}s or
\texttt{o}s except zero (i.e.~don't match \texttt{mn}).

\textbf{Answer 2}: \texttt{m(uu(uu)*\textbar{}oo(oo)*)n}

\begin{itemize}
\tightlist
\item
  The leading \texttt{m} and trailing \texttt{n} ensures that only
  strings beginning with \texttt{m} and ending with \texttt{n} are
  matched.
\item
  Notice how the outer capture group surrounds the \texttt{\textbar{}}.

  \begin{itemize}
  \tightlist
  \item
    Consider the regex \texttt{m(uu(uu)*)\textbar{}(oo(oo)*)n}. This
    incorrectly matches \texttt{muu} and \texttt{oooon}.

    \begin{itemize}
    \tightlist
    \item
      Each OR clause is everything to the left and right of
      \texttt{\textbar{}}. The incorrect solution matches only half of
      the string, and ignores either the beginning \texttt{m} or
      trailing \texttt{n}.
    \item
      A set of parenthesis must surround \texttt{\textbar{}}. That way,
      each OR clause is everything to the left and right of
      \texttt{\textbar{}} \textbf{within} the group. This ensures both
      the beginning \texttt{m} \emph{and} trailing \texttt{n} are
      matched.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{regex-expanded}{%
\section{RegEx Expanded}\label{regex-expanded}}

Provided below are more complex regular expression functions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1619}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1810}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{Any\ Character}: \texttt{.} (except newline) & .U.U.U. & CUMULUS
JUGULUM & SUCCUBUS TUMULTUOUS \\
\texttt{Character\ Class}: \texttt{{[}{]}} (match one character in
\texttt{{[}{]}}) & {[}A-Za-z{]}{[}a-z{]}* & word Capitalized & camelCase
4illegal \\
\texttt{Repeated\ "a"\ Times}: \texttt{\{a\}} & j{[}aeiou{]}\{3\}hn &
jaoehn jooohn & jhn jaeiouhn \\
\texttt{Repeated\ "from\ a\ to\ b"\ Times}: \texttt{\{a,\ b\}} &
j{[}ou{]}\{1,2\}hn & john juohn & jhn jooohn \\
\texttt{At\ Least\ One}: \texttt{+} & jo+hn & john joooooohn & jhn
jjohn \\
\texttt{Zero\ or\ One}: \texttt{?} & joh?n & jon john & any other
string \\
\end{longtable}

A character class matches a single character in its class. These
characters can be hardcoded ------ in the case of \texttt{{[}aeiou{]}}
------ or shorthand can be specified to mean a range of characters.
Examples include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{{[}A-Z{]}}: Any capitalized letter
\item
  \texttt{{[}a-z{]}}: Any lowercase letter
\item
  \texttt{{[}0-9{]}}: Any single digit
\item
  \texttt{{[}A-Za-z{]}}: Any capitalized of lowercase letter
\item
  \texttt{{[}A-Za-z0-9{]}}: Any capitalized or lowercase letter or
  single digit
\end{enumerate}

\hypertarget{examples-1}{%
\subsubsection{Examples}\label{examples-1}}

Let's analyze a few examples of complex regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Does Not Match
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.*SPB.*}
\end{enumerate}
\end{minipage} & \\
RASPBERRY SPBOO & SUBSPACE SUBSPECIES \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{{[}0-9{]}\{3\}-{[}0-9{]}\{2\}-{[}0-9{]}\{4\}}
\end{enumerate}
\end{minipage} & \\
231-41-5121 573-57-1821 & 231415121 57-3571821 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \texttt{{[}a-z{]}+@({[}a-z{]}+\textbackslash{}.)+(edu\textbar{}com)}
\end{enumerate}
\end{minipage} & \\
horse@pizza.com horse@pizza.food.com & frank\_99@yahoo.com hug@cs \\
\end{longtable}

\textbf{Explanations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.*SPB.*} only matches strings that contain the substring
  \texttt{SPB}.

  \begin{itemize}
  \tightlist
  \item
    The \texttt{.*} metacharacter matches any amount of non-negative
    characters. Newlines do not count.\\
  \end{itemize}
\item
  This regular expression matches 3 of any digit, then a dash, then 2 of
  any digit, then a dash, then 4 of any digit.

  \begin{itemize}
  \tightlist
  \item
    You'll recognize this as the familiar Social Security Number regular
    expression.
  \end{itemize}
\item
  Matches any email with a \texttt{com} or \texttt{edu} domain, where
  all characters of the email are letters.

  \begin{itemize}
  \tightlist
  \item
    At least one \texttt{.} must precede the domain name. Including a
    backslash \texttt{\textbackslash{}} before any metacharacter (in
    this case, the \texttt{.}) tells RegEx to match that character
    exactly.
  \end{itemize}
\end{enumerate}

\hypertarget{convenient-regex}{%
\section{Convenient RegEx}\label{convenient-regex}}

Here are a few more convenient regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1619}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1810}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{built\ in\ character\ class} & \texttt{\textbackslash{}w+}
\texttt{\textbackslash{}d+} \texttt{\textbackslash{}s+} & Fawef\_03
231123 \texttt{whitespace} & this person 423 people
\texttt{non-whitespace} \\
\texttt{character\ class\ negation}: \texttt{{[}\^{}{]}} (everything
except the given characters) & {[}\^{}a-z{]}+. & PEPPERS3982 17211!↑å &
porch CLAmS \\
\texttt{escape\ character}: \texttt{\textbackslash{}} (match the literal
next character) & cow\textbackslash.com & cow.com & cowscom \\
\texttt{beginning\ of\ line}: \texttt{\^{}} & \^{}ark & ark two ark o
ark & dark \\
\texttt{end\ of\ line}: \texttt{\$} & ark\$ & dark ark o ark & ark
two \\
\texttt{lazy\ version\ of\ zero\ or\ more} : \texttt{*?} & 5.*?5 & 5005
55 & 5005005 \\
\end{longtable}

\hypertarget{greediness}{%
\subsection{Greediness}\label{greediness}}

In order to fully understand the last operation in the table, we have to
discuss greediness. RegEx is greedy -- it will look for the longest
possible match in a string. To motivate this with an example, consider
the pattern
\texttt{\textless{}div\textgreater{}.*\textless{}/div\textgreater{}}.
Given the sentence below, we would hope that the bolded portions would
be matched:

``This is a
\textbf{\textless div\textgreater example\textless/div\textgreater{}} of
greediness \textless div\textgreater in\textless/div\textgreater{}
regular expressions.'' ''

In actuality, the way RegEx processes the text given that pattern is as
follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ``Look for the exact string \textless{}\div\textgreater{}''
\item
  then, ``look for any character 0 or more times''
\item
  then, ``look for the exact string \textless/div\textgreater{}''
\end{enumerate}

The result would be all the characters starting from the leftmost
\textless div\textgreater{} and the rightmost
\textless/div\textgreater{} (inclusive). We can fix this by making our
pattern non-greedy,
\texttt{\textless{}div\textgreater{}.*?\textless{}/div\textgreater{}}.
You can read up more on the documentation
\href{https://docs.python.org/3/howto/regex.html\#greedy-versus-non-greedy}{here}.

\hypertarget{examples-2}{%
\subsection{Examples}\label{examples-2}}

Let's revisit our earlier problem of extracting date/time data from the
given \texttt{.txt} files. Here is how the data looked.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_lines[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n'
\end{verbatim}

\textbf{Question}: Give a regular expression that matches everything
contained within and including the brackets - the day, month, year,
hour, minutes, seconds, and time zone.

\textbf{Answer}: \texttt{\textbackslash{}{[}.*\textbackslash{}{]}}

\begin{itemize}
\tightlist
\item
  Notice how matching the literal \texttt{{[}} and \texttt{{]}} is
  necessary. Therefore, an escape character \texttt{\textbackslash{}} is
  required before both \texttt{{[}} and \texttt{{]}} --- otherwise these
  metacharacters will match character classes.
\item
  We need to match a particular format between \texttt{{[}} and
  \texttt{{]}}. For this example, \texttt{.*} will suffice.
\end{itemize}

\textbf{Alternative Solution}:
\texttt{\textbackslash{}{[}\textbackslash{}w+/\textbackslash{}w+/\textbackslash{}w+:\textbackslash{}w+:\textbackslash{}w+:\textbackslash{}w+\textbackslash{}s-\textbackslash{}w+\textbackslash{}{]}}

\begin{itemize}
\tightlist
\item
  This solution is much safer.

  \begin{itemize}
  \tightlist
  \item
    Imagine the data between \texttt{{[}} and \texttt{{]}} was garbage -
    \texttt{.*} will still match that.
  \item
    The alternate solution will only match data that follows the correct
    format.
  \end{itemize}
\end{itemize}

\hypertarget{regex-in-python-and-pandas-regex-groups}{%
\section{Regex in Python and Pandas (RegEx
Groups)}\label{regex-in-python-and-pandas-regex-groups}}

\hypertarget{canonicalization-1}{%
\subsection{Canonicalization}\label{canonicalization-1}}

\hypertarget{canonicalization-with-regex}{%
\subsubsection{Canonicalization with
RegEx}\label{canonicalization-with-regex}}

Earlier in this note, we examined the process of canonicalization using
\texttt{python} string manipulation and \texttt{pandas} \texttt{Series}
methods. However, we mentioned this approach had a major flaw: our code
was unnecessarily verbose. Equipped with our knowledge of regular
expressions, let's fix this.

To do so, we need to understand a few functions in the \texttt{re}
module. The first of these is the substitute function:
\texttt{re.sub(pattern,\ rep1,\ text)}. It behaves similarly to
\texttt{python}'s built-in \texttt{.replace} function, and returns text
with all instances of \texttt{pattern} replaced by \texttt{rep1}.

The regular expression here removes text surrounded by
\texttt{\textless{}\textgreater{}} (also known as HTML tags).

In order, the pattern matches \ldots{} 1. a single \texttt{\textless{}}
2. any character that is not a \texttt{\textgreater{}} : div, td
valign\ldots, /td, /div 3. a single \texttt{\textgreater{}}

Any substring in \texttt{text} that fulfills all three conditions will
be replaced by \texttt{\textquotesingle{}\textquotesingle{}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"\textless{}div\textgreater{}\textless{}td valign=\textquotesingle{}top\textquotesingle{}\textgreater{}Moo\textless{}/td\textgreater{}\textless{}/div\textgreater{}"}
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"\textless{}[\^{}\textgreater{}]+\textgreater{}"}
\NormalTok{re.sub(pattern, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, text) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Moo'
\end{verbatim}

Notice the \texttt{r} preceding the regular expression pattern; this
specifies the regular expression is a raw string. Raw strings do not
recognize escape sequences (i.e., the Python newline metacharacter
\texttt{\textbackslash{}n}). This makes them useful for regular
expressions, which often contain literal \texttt{\textbackslash{}}
characters.

In other words, don't forget to tag your RegEx with an \texttt{r}.

\hypertarget{canonicalization-with-pandas}{%
\subsubsection{\texorpdfstring{Canonicalization with
\texttt{pandas}}{Canonicalization with pandas}}\label{canonicalization-with-pandas}}

We can also use regular expressions with \texttt{pandas} \texttt{Series}
methods. This gives us the benefit of operating on an entire column of
data as opposed to a single value. The code is simple:
\texttt{ser.str.replace(pattern,\ repl,\ regex=True}).

Consider the following \texttt{DataFrame} \texttt{html\_data} with a
single column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{"HTML"}\NormalTok{: [}\StringTok{"\textless{}div\textgreater{}\textless{}td valign=\textquotesingle{}top\textquotesingle{}\textgreater{}Moo\textless{}/td\textgreater{}\textless{}/div\textgreater{}"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                 \StringTok{"\textless{}a href=\textquotesingle{}http://ds100.org\textquotesingle{}\textgreater{}Link\textless{}/a\textgreater{}"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                 \StringTok{"\textless{}b\textgreater{}Bold text\textless{}/b\textgreater{}"}\NormalTok{]\}}
\NormalTok{html\_data }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &                                  HTML \\
\midrule
0 &  <div><td valign='top'>Moo</td></div> \\
1 &   <a href='http://ds100.org'>Link</a> \\
2 &                      <b>Bold text</b> \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"\textless{}[\^{}\textgreater{}]+\textgreater{}"}
\NormalTok{html\_data[}\StringTok{\textquotesingle{}HTML\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.replace(pattern, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, regex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &       HTML \\
\midrule
0 &        Moo \\
1 &       Link \\
2 &  Bold text \\
\bottomrule
\end{tabular}

\hypertarget{extraction-1}{%
\subsection{Extraction}\label{extraction-1}}

\hypertarget{extraction-with-regex}{%
\subsubsection{Extraction with RegEx}\label{extraction-with-regex}}

Just like with canonicalization, the \texttt{re} module provides
capability to extract relevant text from a string:
\texttt{re.findall(pattern,\ text)}. This function returns a list of all
matches to \texttt{pattern}.

Using the familiar regular expression for Social Security Numbers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"My social security number is 123{-}45{-}6789 bro, or maybe it’s 321{-}45{-}6789."}
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"[0{-}9]}\SpecialCharTok{\{3\}}\VerbatimStringTok{{-}[0{-}9]}\SpecialCharTok{\{2\}}\VerbatimStringTok{{-}[0{-}9]}\SpecialCharTok{\{4\}}\VerbatimStringTok{"}
\NormalTok{re.findall(pattern, text)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['123-45-6789', '321-45-6789']
\end{verbatim}

\hypertarget{extraction-with-pandas}{%
\subsubsection{\texorpdfstring{Extraction with
\texttt{pandas}}{Extraction with pandas}}\label{extraction-with-pandas}}

\texttt{pandas} similarily provides extraction functionality on a
\texttt{Series} of data: \texttt{ser.str.findall(pattern)}

Consider the following \texttt{DataFrame} \texttt{ssn\_data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{"SSN"}\NormalTok{: [}\StringTok{"987{-}65{-}4321"}\NormalTok{, }\StringTok{"forty"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                \StringTok{"123{-}45{-}6789 bro or 321{-}45{-}6789"}\NormalTok{,}
               \StringTok{"999{-}99{-}9999"}\NormalTok{]\}}
\NormalTok{ssn\_data }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{ll}
\toprule
{} &                             SSN \\
\midrule
0 &                     987-65-4321 \\
1 &                           forty \\
2 &  123-45-6789 bro or 321-45-6789 \\
3 &                     999-99-9999 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_data[}\StringTok{"SSN"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.findall(pattern)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                         SSN \\
\midrule
0 &               [987-65-4321] \\
1 &                          [] \\
2 &  [123-45-6789, 321-45-6789] \\
3 &               [999-99-9999] \\
\bottomrule
\end{tabular}

This function returns a list for every row containing the pattern
matches in a given string.

As you may expect, there are similar \texttt{pandas} equivalents for
other \texttt{re} functions as well. \texttt{Series.str.extract} takes
in a pattern and returns a \texttt{DataFrame} of each capture group's
first match in the string. In contrast, \texttt{Series.str.extractall}
returns a multi-indexed \texttt{DataFrame} of all matches for each
capture group. You can see the difference in the outputs below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern\_cg }\OperatorTok{=} \VerbatimStringTok{r"([0{-}9]}\SpecialCharTok{\{3\}}\VerbatimStringTok{){-}([0{-}9]}\SpecialCharTok{\{2\}}\VerbatimStringTok{){-}([0{-}9]}\SpecialCharTok{\{4\}}\VerbatimStringTok{)"}
\NormalTok{ssn\_data[}\StringTok{"SSN"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.extract(pattern\_cg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{llll}
\toprule
{} &    0 &    1 &     2 \\
\midrule
0 &  987 &   65 &  4321 \\
1 &  NaN &  NaN &   NaN \\
2 &  123 &   45 &  6789 \\
3 &  999 &   99 &  9999 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_data[}\StringTok{"SSN"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.extractall(pattern\_cg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllll}
\toprule
  &   &    0 &   1 &     2 \\
{} & match &      &     &       \\
\midrule
0 & 0 &  987 &  65 &  4321 \\
2 & 0 &  123 &  45 &  6789 \\
  & 1 &  321 &  45 &  6789 \\
3 & 0 &  999 &  99 &  9999 \\
\bottomrule
\end{tabular}

\hypertarget{regular-expression-capture-groups}{%
\subsection{Regular Expression Capture
Groups}\label{regular-expression-capture-groups}}

Earlier we used parentheses \texttt{(} \texttt{)} to specify the highest
order of operation in regular expressions. However, they have another
meaning; parentheses are often used to represent \textbf{capture
groups}. Capture groups are essentially, a set of smaller regular
expressions that match multiple substrings in text data.

Let's take a look at an example.

\hypertarget{example-1}{%
\subsubsection{Example 1}\label{example-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"Observations: 03:04:53 {-} Horse awakens. }\CharTok{\textbackslash{}}
\StringTok{        03:05:14 {-} Horse goes back to sleep."}
\end{Highlighting}
\end{Shaded}

Say we want to capture all occurences of time data (hour, minute, and
second) as \emph{separate entities}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern\_1 }\OperatorTok{=} \VerbatimStringTok{r"(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d)"}
\NormalTok{re.findall(pattern\_1, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[('03', '04', '53'), ('03', '05', '14')]
\end{verbatim}

Notice how the given pattern has 3 capture groups, each specified by the
regular expression \texttt{(\textbackslash{}d\textbackslash{}d)}. We
then use \texttt{re.findall} to return these capture groups, each as
tuples containing 3 matches.

These regular expression capture groups can be different. We can use the
\texttt{(\textbackslash{}d\{2\})} shorthand to extract the same data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern\_2 }\OperatorTok{=} \VerbatimStringTok{r"(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d}\SpecialCharTok{\{2\}}\VerbatimStringTok{)"}
\NormalTok{re.findall(pattern\_2, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[('03', '04', '53'), ('03', '05', '14')]
\end{verbatim}

\hypertarget{example-2}{%
\subsubsection{Example 2}\label{example-2}}

With the notion of capture groups, convince yourself how the following
regular expression works.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OperatorTok{=}\NormalTok{ log\_lines[}\DecValTok{0}\NormalTok{]}
\NormalTok{first}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r\textquotesingle{}\textbackslash{}[(\textbackslash{}d+)\textbackslash{}/(\textbackslash{}w+)\textbackslash{}/(\textbackslash{}d+):(\textbackslash{}d+):(\textbackslash{}d+):(\textbackslash{}d+) (.+)\textbackslash{}]\textquotesingle{}}
\NormalTok{day, month, year, hour, minute, second, time\_zone }\OperatorTok{=}\NormalTok{ re.findall(pattern, first)[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(day, month, year, hour, minute, second, time\_zone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
26 Jan 2014 10 47 58 -0800
\end{verbatim}

\hypertarget{limitations-of-regular-expressions}{%
\section{Limitations of Regular
Expressions}\label{limitations-of-regular-expressions}}

Today, we explored the capabilities of regular expressions in data
wrangling with text data. However, there are a few things to be wary of.

Writing regular expressions is like writing a program.

\begin{itemize}
\tightlist
\item
  Need to know the syntax well.
\item
  Can be easier to write than to read.
\item
  Can be difficult to debug.
\end{itemize}

Regular expressions are terrible at certain types of problems:

\begin{itemize}
\tightlist
\item
  For parsing a hierarchical structure, such as JSON, use the
  \texttt{json.load()} parser, not RegEx!
\item
  Complex features (e.g.~valid email address).
\item
  Counting (same number of instances of a and b). (impossible)
\item
  Complex properties (palindromes, balanced parentheses). (impossible)
\end{itemize}

Ultimately, the goal is not to memorize all regular expressions. Rather,
the aim is to:

\begin{itemize}
\tightlist
\item
  Understand what RegEx is capable of.
\item
  Parse and create RegEx, with a reference table
\item
  Use vocabulary (metacharacter, escape character, groups, etc.) to
  describe regex metacharacters.
\item
  Differentiate between (), {[}{]}, \{\}
\item
  Design your own character classes with \d, \w, \s,
  {[}\ldots-\ldots{]}, \^{}, etc.
\item
  Use \texttt{python} and \texttt{pandas} RegEx methods.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{visualization-i}{%
\chapter{Visualization I}\label{visualization-i}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Understand the theories behind effective visualizations and start to
  generate plots of our own with \texttt{matplotlib} and
  \texttt{seaborn}.
\item
  Analyze histograms and identify the skewness, potential outliers, and
  the mode.
\item
  Use \texttt{boxplot} and \texttt{violinplot} to compare two
  distributions.
\end{itemize}

\end{tcolorbox}

In our journey of the data science lifecycle, we have begun to explore
the vast world of exploratory data analysis. More recently, we learned
how to pre-process data using various data manipulation techniques. As
we work towards understanding our data, there is one key component
missing in our arsenal --- the ability to visualize and discern
relationships in existing data.

These next two lectures will introduce you to various examples of data
visualizations and their underlying theory. In doing so, we'll motivate
their importance in real-world examples with the use of plotting
libraries.

\hypertarget{visualizations-in-data-8-and-data-100-so-far}{%
\section{Visualizations in Data 8 and Data 100 (so
far)}\label{visualizations-in-data-8-and-data-100-so-far}}

You've likely encountered several forms of data visualizations in your
studies. You may remember two such examples from Data 8: line plots,
scatter plots, and histograms. Each of these served a unique purpose.
For example, line plots displayed how numerical quantities changed over
time, while histograms were useful in understanding a variable's
distribution.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Line Chart
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Scatter Plot
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
\end{longtable}

\begin{longtable}[]{@{}l@{}}
\toprule\noalign{}
Histogram \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
 \\
\end{longtable}

\hypertarget{goals-of-visualization}{%
\section{Goals of Visualization}\label{goals-of-visualization}}

Visualizations are useful for a number of reasons. In Data 100, we
consider two areas in particular:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To broaden your understanding of the data. Summarizing trends visually
  before in-depth analysis is a key part of exploratory data analysis.
  Creating these graphs is a lightweight, iterative and flexible process
  that helps us investigate relationships between variables.
\item
  To communicate results/conclusions to others. These visualizations are
  highly editorial, selective, and fine-tuned to achieve a
  communications goal, so be thoughtful and careful about its clarity,
  accessibility, and necessary context.
\end{enumerate}

Altogether, these goals emphasize the fact that visualizations aren't a
matter of making ``pretty'' pictures; we need to do a lot of thinking
about what stylistic choices communicate ideas most effectively.

This course note will focus on the first half of visualization topics in
Data 100. The goal here is to understand how to choose the ``right''
plot depending on different variable types and, secondly, how to
generate these plots using code.

\hypertarget{an-overview-of-distributions}{%
\section{An Overview of
Distributions}\label{an-overview-of-distributions}}

A distribution describes both the set of values that a single variable
can take and the frequency of unique values in a single variable. For
example, if we're interested in the distribution of students across Data
100 discussion sections, the set of possible values is a list of
discussion sections (10-11am, 11-12pm, etc.), and the frequency that
each of those values occur is the number of students enrolled in each
section. In other words, the we're interested in how a variable is
distributed across it's possible values. Therefore, distributions must
satisfy two properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The total frequency of all categories must sum to 100\%
\item
  Total count should sum to the total number of datapoints if we're
  using raw counts.
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Not a Valid Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Valid Distribution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
This is not a valid distribution since individuals can be associated
with more than one category and the bar values demonstrate values in
minutes and not probability. & This example satisfies the two properties
of distributions, so it is a valid distribution. \\
\end{longtable}

\hypertarget{variable-types-should-inform-plot-choice}{%
\section{Variable Types Should Inform Plot
Choice}\label{variable-types-should-inform-plot-choice}}

Different plots are more or less suited for displaying particular types
of variables, laid out in the diagram below:

The first step of any visualization is to identify the type(s) of
variables we're working with. From here, we can select an appropriate
plot type:

\hypertarget{qualitative-variables-bar-plots}{%
\section{Qualitative Variables: Bar
Plots}\label{qualitative-variables-bar-plots}}

A \textbf{bar plot} is one of the most common ways of displaying the
\textbf{distribution} of a \textbf{qualitative} (categorical) variable.
The length of a bar plot encodes the frequency of a category; the width
encodes no useful information. The color \emph{could} indicate a
sub-category, but this is not necessarily the case.

Let's contextualize this in an example. We will use the World Bank
dataset (\texttt{wb}) in our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{wb }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/world\_bank.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{wb.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\toprule
{} & Continent &   Country &  Primary completion rate: Male: \% of relevant age group: 2015 &  Primary completion rate: Female: \% of relevant age group: 2015 &  Lower secondary completion rate: Male: \% of relevant age group: 2015 &  Lower secondary completion rate: Female: \% of relevant age group: 2015 &  Youth literacy rate: Male: \% of ages 15-24: 2005-14 &  Youth literacy rate: Female: \% of ages 15-24: 2005-14 &  Adult literacy rate: Male: \% ages 15 and older: 2005-14 &  Adult literacy rate: Female: \% ages 15 and older: 2005-14 &  Students at lowest proficiency on PISA: Mathematics: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Reading: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Science: \% of 15 year-olds: 2015 &  Population: millions: 2016 &  Surface area: sq. km thousands: 2016 &  Population density: people per sq. km: 2016 &  Gross national income, Atlas method: \$ billions: 2016 &  Gross national income per capita, Atlas method: \$: 2016 &  Purchasing power parity gross national income: \$ billions: 2016 &  per capita: \$: 2016 &  Gross domestic product: \% growth : 2016 &  per capita: \% growth: 2016 &  Prevalence of smoking: Male: \% of adults: 2015 &  Prevalence of smoking: Female: \% of adults: 2015 &  Incidence of tuberculosis: per 100,000 people: 2015 &  Prevalence of diabetes: \% of population ages 20 to 79: 2015 &  Incidence of HIV: Total: \% of uninfected population ages 15-49: 2015 &  Prevalence of HIV: Total: \% of population ages 15-49: 2015 &  Prevalence of HIV: Women's share of population ages 15+ living with HIV: \%: 2015 &  Prevalence of HIV: Youth, Male: \% of population ages 15-24: 2015 &  Prevalence of HIV: Youth, Female: \% of population ages 15-24: 2015 &  Antiretroviral therapy coverage: \% of people living with HIV: 2015 &  Cause of death: Communicable diseases and maternal, prenatal, and nutrition conditions: \% of population: 2015 &  Cause of death: Non-communicable diseases: \% of population: 2015 &  Cause of death: Injuries: \% of population: 2015 &  Access to an improved water source: \% of population: 1990 &  Access to an improved water source: \% of population: 2015 &  Access to improved sanitation facilities: \% of population: 1990 &  Access to improved sanitation facilities: \% of population: 2015 &  Child immunization rate: Measles: \% of children ages 12-23 months: 2015 &  Child immunization rate: DTP3: \% of children ages 12-23 months: 2015 &  Children with acute respiratory infection taken to health provider: \% of children under age 5 with ARI: 2009-2016 &  Children with diarrhea who received oral rehydration and continuous feeding: \% of children under age 5 with diarrhea: 2009-2016 &  Children sleeping under treated bed nets: \% of children under age 5: 2009-2016 &  Children with fever receiving antimalarial drugs: \% of children under age 5 with fever: 2009-2016 &  Tuberculosis: Treatment success rate: \% of new cases: 2014 &  Tuberculosis: Cases detection rate: \% of new estimated cases: 2015 \\
\midrule
0 &    Africa &   Algeria &                                              106.0 &                                              105.0 &                                               68.0 &                                               85.0 &                                               96.0 &                                               92.0 &                                               83.0 &                                               68.0 &                                               51.0 &                                               11.0 &                                                4.0 &                        40.6 &                                2381.7 &                                         17.0 &                                              173.5 &                                             4270.0 &                                              597.9 &              14720.0 &                                      3.7 &                         1.8 &                                             NaN &                                               NaN &                                               75.0 &                                                7.5 &                                               0.01 &                                                0.1 &                                               46.0 &                                                0.1 &                                                0.1 &                                               90.0 &                                               16.0 &                                               74.0 &                                             10.0 &                                               92.0 &                                               84.0 &                                               80.0 &                                               88.0 &                                               95.0 &                                               95.0 &                                               66.0 &                                               42.0 &                                                NaN &                                                NaN &                                               88.0 &                                               80.0 \\
1 &    Africa &    Angola &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               79.0 &                                               67.0 &                                               82.0 &                                               60.0 &                                                NaN &                                                NaN &                                                NaN &                        28.8 &                                1246.7 &                                         23.0 &                                               99.0 &                                             3440.0 &                                              179.3 &               6220.0 &                                      0.0 &                        -3.3 &                                             NaN &                                               NaN &                                              370.0 &                                                4.1 &                                               0.19 &                                                2.2 &                                               60.0 &                                                0.5 &                                                0.9 &                                               29.0 &                                               62.0 &                                               27.0 &                                             11.0 &                                               46.0 &                                               49.0 &                                               22.0 &                                               52.0 &                                               55.0 &                                               64.0 &                                                NaN &                                                NaN &                                               25.9 &                                               28.3 &                                               34.0 &                                               64.0 \\
2 &    Africa &     Benin &                                               83.0 &                                               73.0 &                                               50.0 &                                               37.0 &                                               55.0 &                                               31.0 &                                               41.0 &                                               18.0 &                                                NaN &                                                NaN &                                                NaN &                        10.9 &                                 114.8 &                                         96.0 &                                                8.9 &                                              820.0 &                                               23.6 &               2170.0 &                                      4.0 &                         1.1 &                                            18.0 &                                               1.0 &                                               60.0 &                                                0.8 &                                               0.07 &                                                1.1 &                                               58.0 &                                                0.2 &                                                0.4 &                                               49.0 &                                               52.0 &                                               37.0 &                                             10.0 &                                               57.0 &                                               78.0 &                                                7.0 &                                               20.0 &                                               75.0 &                                               79.0 &                                               23.0 &                                               33.0 &                                               72.7 &                                               25.9 &                                               89.0 &                                               61.0 \\
3 &    Africa &  Botswana &                                               98.0 &                                              101.0 &                                               86.0 &                                               87.0 &                                               96.0 &                                               99.0 &                                               87.0 &                                               89.0 &                                                NaN &                                                NaN &                                                NaN &                         2.3 &                                 581.7 &                                          4.0 &                                               14.9 &                                             6610.0 &                                               36.9 &              16380.0 &                                      2.9 &                         1.0 &                                             NaN &                                               NaN &                                              356.0 &                                                5.6 &                                               0.94 &                                               22.2 &                                               57.0 &                                                3.9 &                                                9.8 &                                               78.0 &                                               49.0 &                                               42.0 &                                              9.0 &                                               92.0 &                                               96.0 &                                               39.0 &                                               63.0 &                                               97.0 &                                               95.0 &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               77.0 &                                               62.0 \\
5 &    Africa &   Burundi &                                               58.0 &                                               66.0 &                                               35.0 &                                               30.0 &                                               90.0 &                                               88.0 &                                               89.0 &                                               85.0 &                                                NaN &                                                NaN &                                                NaN &                        10.5 &                                  27.8 &                                        410.0 &                                                2.9 &                                              280.0 &                                                8.1 &                770.0 &                                     -0.6 &                        -3.7 &                                             NaN &                                               NaN &                                              122.0 &                                                2.7 &                                               0.02 &                                                1.0 &                                               60.0 &                                                0.3 &                                                0.3 &                                               54.0 &                                               57.0 &                                               31.0 &                                             12.0 &                                               69.0 &                                               76.0 &                                               42.0 &                                               48.0 &                                               93.0 &                                               94.0 &                                               55.0 &                                               43.0 &                                               53.8 &                                               25.4 &                                               91.0 &                                               51.0 \\
\bottomrule
\end{tabular}

We can visualize the distribution of the \texttt{Continent} column using
a bar plot. There are a few ways to do this.

\hypertarget{plotting-in-pandas}{%
\subsection{Plotting in Pandas}\label{plotting-in-pandas}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wb[}\StringTok{\textquotesingle{}Continent\textquotesingle{}}\NormalTok{].value\_counts().plot(kind}\OperatorTok{=}\StringTok{\textquotesingle{}bar\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-3-output-1.pdf}

}

\end{figure}

Recall that \texttt{.value\_counts()} returns a \texttt{Series} with the
total count of each unique value. We call
\texttt{.plot(kind=\textquotesingle{}bar\textquotesingle{})} on this
result to visualize these counts as a bar plot.

Plotting methods in \texttt{pandas} are the least preferred and not
supported in Data 100, as their functionality is limited. Instead,
future examples will focus on other libraries built specifically for
visualizing data. The most well-known library here is
\texttt{matplotlib}.

\hypertarget{plotting-in-matplotlib}{%
\subsection{Plotting in Matplotlib}\label{plotting-in-matplotlib}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt }\CommentTok{\# matplotlib is typically given the alias plt}

\NormalTok{continent }\OperatorTok{=}\NormalTok{ wb[}\StringTok{\textquotesingle{}Continent\textquotesingle{}}\NormalTok{].value\_counts()}
\NormalTok{plt.bar(continent.index, continent)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Continent\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

While more code is required to achieve the same result,
\texttt{matplotlib} is often used over \texttt{pandas} for its ability
to plot more complex visualizations, some of which are discussed
shortly.

However, note how we needed to label the axes with \texttt{plt.xlabel}
and \texttt{plt.ylabel}, as \texttt{matplotlib} does not support
automatic axis labeling. To get around these inconveniences, we can use
a more efficient plotting library: \texttt{seaborn}.

\hypertarget{plotting-in-seaborn}{%
\subsection{\texorpdfstring{Plotting in
\texttt{Seaborn}}{Plotting in Seaborn}}\label{plotting-in-seaborn}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns }\CommentTok{\# seaborn is typically given the alias sns}
\NormalTok{sns.countplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{\textquotesingle{}Continent\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

In contrast to \texttt{matplotlib}, the general structure of a
\texttt{seaborn} call involves passing in an entire \texttt{DataFrame},
and then specifying what column(s) to plot. \texttt{seaborn.countplot}
both counts and visualizes the number of unique values in a given
column. This column is specified by the \texttt{x} argument to
\texttt{sns.countplot}, while the \texttt{DataFrame} is specified by the
\texttt{data} argument.

For the vast majority of visualizations, \texttt{seaborn} is far more
concise and aesthetically pleasing than \texttt{matplotlib}. However,
the color scheme of this particular bar plot is arbitrary - it encodes
no additional information about the categories themselves. This is not
always true; color may signify meaningful detail in other
visualizations. We'll explore this more in-depth during the next
lecture.

By now, you'll have noticed that each of these plotting libraries have a
very different syntax. As with \texttt{pandas}, we'll teach you the
important methods in \texttt{matplotlib} and \texttt{seaborn}, but
you'll learn more through documentation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://matplotlib.org/stable/index.html}{Matplotlib
  Documentation}
\item
  \href{https://seaborn.pydata.org/}{Seaborn Documentation}
\end{enumerate}

\hypertarget{distributions-of-quantitative-variables}{%
\section{Distributions of Quantitative
Variables}\label{distributions-of-quantitative-variables}}

Revisiting our example with the \texttt{wb} DataFrame, let's plot the
distribution of \texttt{Gross\ national\ income\ per\ capita}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wb.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\toprule
{} & Continent &   Country &  Primary completion rate: Male: \% of relevant age group: 2015 &  Primary completion rate: Female: \% of relevant age group: 2015 &  Lower secondary completion rate: Male: \% of relevant age group: 2015 &  Lower secondary completion rate: Female: \% of relevant age group: 2015 &  Youth literacy rate: Male: \% of ages 15-24: 2005-14 &  Youth literacy rate: Female: \% of ages 15-24: 2005-14 &  Adult literacy rate: Male: \% ages 15 and older: 2005-14 &  Adult literacy rate: Female: \% ages 15 and older: 2005-14 &  Students at lowest proficiency on PISA: Mathematics: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Reading: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Science: \% of 15 year-olds: 2015 &  Population: millions: 2016 &  Surface area: sq. km thousands: 2016 &  Population density: people per sq. km: 2016 &  Gross national income, Atlas method: \$ billions: 2016 &  Gross national income per capita, Atlas method: \$: 2016 &  Purchasing power parity gross national income: \$ billions: 2016 &  per capita: \$: 2016 &  Gross domestic product: \% growth : 2016 &  per capita: \% growth: 2016 &  Prevalence of smoking: Male: \% of adults: 2015 &  Prevalence of smoking: Female: \% of adults: 2015 &  Incidence of tuberculosis: per 100,000 people: 2015 &  Prevalence of diabetes: \% of population ages 20 to 79: 2015 &  Incidence of HIV: Total: \% of uninfected population ages 15-49: 2015 &  Prevalence of HIV: Total: \% of population ages 15-49: 2015 &  Prevalence of HIV: Women's share of population ages 15+ living with HIV: \%: 2015 &  Prevalence of HIV: Youth, Male: \% of population ages 15-24: 2015 &  Prevalence of HIV: Youth, Female: \% of population ages 15-24: 2015 &  Antiretroviral therapy coverage: \% of people living with HIV: 2015 &  Cause of death: Communicable diseases and maternal, prenatal, and nutrition conditions: \% of population: 2015 &  Cause of death: Non-communicable diseases: \% of population: 2015 &  Cause of death: Injuries: \% of population: 2015 &  Access to an improved water source: \% of population: 1990 &  Access to an improved water source: \% of population: 2015 &  Access to improved sanitation facilities: \% of population: 1990 &  Access to improved sanitation facilities: \% of population: 2015 &  Child immunization rate: Measles: \% of children ages 12-23 months: 2015 &  Child immunization rate: DTP3: \% of children ages 12-23 months: 2015 &  Children with acute respiratory infection taken to health provider: \% of children under age 5 with ARI: 2009-2016 &  Children with diarrhea who received oral rehydration and continuous feeding: \% of children under age 5 with diarrhea: 2009-2016 &  Children sleeping under treated bed nets: \% of children under age 5: 2009-2016 &  Children with fever receiving antimalarial drugs: \% of children under age 5 with fever: 2009-2016 &  Tuberculosis: Treatment success rate: \% of new cases: 2014 &  Tuberculosis: Cases detection rate: \% of new estimated cases: 2015 \\
\midrule
0 &    Africa &   Algeria &                                              106.0 &                                              105.0 &                                               68.0 &                                               85.0 &                                               96.0 &                                               92.0 &                                               83.0 &                                               68.0 &                                               51.0 &                                               11.0 &                                                4.0 &                        40.6 &                                2381.7 &                                         17.0 &                                              173.5 &                                             4270.0 &                                              597.9 &              14720.0 &                                      3.7 &                         1.8 &                                             NaN &                                               NaN &                                               75.0 &                                                7.5 &                                               0.01 &                                                0.1 &                                               46.0 &                                                0.1 &                                                0.1 &                                               90.0 &                                               16.0 &                                               74.0 &                                             10.0 &                                               92.0 &                                               84.0 &                                               80.0 &                                               88.0 &                                               95.0 &                                               95.0 &                                               66.0 &                                               42.0 &                                                NaN &                                                NaN &                                               88.0 &                                               80.0 \\
1 &    Africa &    Angola &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               79.0 &                                               67.0 &                                               82.0 &                                               60.0 &                                                NaN &                                                NaN &                                                NaN &                        28.8 &                                1246.7 &                                         23.0 &                                               99.0 &                                             3440.0 &                                              179.3 &               6220.0 &                                      0.0 &                        -3.3 &                                             NaN &                                               NaN &                                              370.0 &                                                4.1 &                                               0.19 &                                                2.2 &                                               60.0 &                                                0.5 &                                                0.9 &                                               29.0 &                                               62.0 &                                               27.0 &                                             11.0 &                                               46.0 &                                               49.0 &                                               22.0 &                                               52.0 &                                               55.0 &                                               64.0 &                                                NaN &                                                NaN &                                               25.9 &                                               28.3 &                                               34.0 &                                               64.0 \\
2 &    Africa &     Benin &                                               83.0 &                                               73.0 &                                               50.0 &                                               37.0 &                                               55.0 &                                               31.0 &                                               41.0 &                                               18.0 &                                                NaN &                                                NaN &                                                NaN &                        10.9 &                                 114.8 &                                         96.0 &                                                8.9 &                                              820.0 &                                               23.6 &               2170.0 &                                      4.0 &                         1.1 &                                            18.0 &                                               1.0 &                                               60.0 &                                                0.8 &                                               0.07 &                                                1.1 &                                               58.0 &                                                0.2 &                                                0.4 &                                               49.0 &                                               52.0 &                                               37.0 &                                             10.0 &                                               57.0 &                                               78.0 &                                                7.0 &                                               20.0 &                                               75.0 &                                               79.0 &                                               23.0 &                                               33.0 &                                               72.7 &                                               25.9 &                                               89.0 &                                               61.0 \\
3 &    Africa &  Botswana &                                               98.0 &                                              101.0 &                                               86.0 &                                               87.0 &                                               96.0 &                                               99.0 &                                               87.0 &                                               89.0 &                                                NaN &                                                NaN &                                                NaN &                         2.3 &                                 581.7 &                                          4.0 &                                               14.9 &                                             6610.0 &                                               36.9 &              16380.0 &                                      2.9 &                         1.0 &                                             NaN &                                               NaN &                                              356.0 &                                                5.6 &                                               0.94 &                                               22.2 &                                               57.0 &                                                3.9 &                                                9.8 &                                               78.0 &                                               49.0 &                                               42.0 &                                              9.0 &                                               92.0 &                                               96.0 &                                               39.0 &                                               63.0 &                                               97.0 &                                               95.0 &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               77.0 &                                               62.0 \\
5 &    Africa &   Burundi &                                               58.0 &                                               66.0 &                                               35.0 &                                               30.0 &                                               90.0 &                                               88.0 &                                               89.0 &                                               85.0 &                                                NaN &                                                NaN &                                                NaN &                        10.5 &                                  27.8 &                                        410.0 &                                                2.9 &                                              280.0 &                                                8.1 &                770.0 &                                     -0.6 &                        -3.7 &                                             NaN &                                               NaN &                                              122.0 &                                                2.7 &                                               0.02 &                                                1.0 &                                               60.0 &                                                0.3 &                                                0.3 &                                               54.0 &                                               57.0 &                                               31.0 &                                             12.0 &                                               69.0 &                                               76.0 &                                               42.0 &                                               48.0 &                                               93.0 &                                               94.0 &                                               55.0 &                                               43.0 &                                               53.8 &                                               25.4 &                                               91.0 &                                               51.0 \\
\bottomrule
\end{tabular}

How should we define our categories for this variable? In the previous
example, these were a few unique values of the \texttt{Continent}
column. If we use similar logic here, our categories are the different
numerical values contained in the
\texttt{Gross\ national\ income\ per\ capita} column.

Under this assumption, let's plot this distribution using the
\texttt{seaborn.countplot} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.countplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{\textquotesingle{}Gross national income per capita, Atlas method: $: 2016\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

What happened? A bar plot (either \texttt{plt.bar} or
\texttt{sns.countplot}) will create a separate bar for each unique value
of a variable. With a continuous variable, we may not have a finite
number of possible values, which can lead to situations like above where
we would need many, many bars to display each unique value.

Specifically, we can say this histogram suffers from
\textbf{overplotting} as we are unable to interpret the plot and gain
any meaningful insight.

Rather than bar plots, to visualize the distribution of a continuous
variable, we use one of the following types of plots:

\begin{itemize}
\tightlist
\item
  Histogram
\item
  Box plot
\item
  Violin plot
\end{itemize}

\hypertarget{box-plots-and-violin-plots}{%
\subsection{Box Plots and Violin
Plots}\label{box-plots-and-violin-plots}}

Box plots and violin plots are two very similar kinds of visualizations.
Both display the distribution of a variable using information about
\textbf{quartiles}.

In a box plot, the width of the box at any point does not encode
meaning. In a violin plot, the width of the plot indicates the density
of the distribution at each possible value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.boxplot(data}\OperatorTok{=}\NormalTok{wb, y}\OperatorTok{=}\StringTok{\textquotesingle{}Gross national income per capita, Atlas method: $: 2016\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.violinplot(data}\OperatorTok{=}\NormalTok{wb, y}\OperatorTok{=}\StringTok{"Gross national income per capita, Atlas method: $: 2016"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

A quartile represents a 25\% portion of the data. We say that:

\begin{itemize}
\tightlist
\item
  The first quartile (Q1) represents the 25th percentile -- 25\% of the
  data is smaller than or equal to the first quartile.
\item
  The second quartile (Q2) represents the 50th percentile, also known as
  the median -- 50\% of the data is smaller than or equal to the second
  quartile.
\item
  The third quartile (Q3) represents the 75th percentile -- 75\% of the
  data is smaller than or equal to the third quartile.
\end{itemize}

This means that the middle 50\% of the data lies between the first and
third quartiles. This is demonstrated in the histogram below. The three
quartiles are marked with red vertical bars.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gdp }\OperatorTok{=}\NormalTok{ wb[}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{]}
\NormalTok{gdp }\OperatorTok{=}\NormalTok{ gdp[}\OperatorTok{\textasciitilde{}}\NormalTok{gdp.isna()]}

\NormalTok{q1, q2, q3 }\OperatorTok{=}\NormalTok{ np.percentile(gdp, [}\DecValTok{25}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{75}\NormalTok{])}

\NormalTok{wb\_quartiles }\OperatorTok{=}\NormalTok{ wb.copy()}
\NormalTok{wb\_quartiles[}\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \VariableTok{None}
\NormalTok{wb\_quartiles.loc[(wb\_quartiles[}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{] }\OperatorTok{\textless{}}\NormalTok{ q1) }\OperatorTok{|}\NormalTok{ (wb\_quartiles[}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{] }\OperatorTok{\textgreater{}}\NormalTok{ q3), }\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}Outside of the middle 50\%\textquotesingle{}}
\NormalTok{wb\_quartiles.loc[(wb\_quartiles[}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{] }\OperatorTok{\textgreater{}}\NormalTok{ q1) }\OperatorTok{\&}\NormalTok{ (wb\_quartiles[}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{] }\OperatorTok{\textless{}}\NormalTok{ q3), }\StringTok{\textquotesingle{}category\textquotesingle{}}\NormalTok{] }\OperatorTok{=} \StringTok{\textquotesingle{}In the middle 50\%\textquotesingle{}}

\NormalTok{sns.histplot(wb\_quartiles, x}\OperatorTok{=}\StringTok{"Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016"}\NormalTok{, hue}\OperatorTok{=}\StringTok{"category"}\NormalTok{)}
\NormalTok{sns.rugplot([q1, q2, q3], c}\OperatorTok{=}\StringTok{"firebrick"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{6}\NormalTok{, height}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-10-output-1.pdf}

}

\end{figure}

In a box plot, the lower extent of the box lies at Q1, while the upper
extent of the box lies at Q3. The horizontal line in the middle of the
box corresponds to Q2 (equivalently, the median).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.boxplot(data}\OperatorTok{=}\NormalTok{wb, y}\OperatorTok{=}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-11-output-1.pdf}

}

\end{figure}

The \textbf{whiskers} of a box-plot are the two points that lie at the
{[}\(1^{st}\) Quartile \(-\) (\(1.5\times\) IQR){]}, and the
{[}\(3^{rd}\) Quartile \(+\) (\(1.5\times\) IQR){]}. They are the lower
and upper ranges of ``normal'' data (the points excluding outliers).

The different forms of information contained in a box plot can be
summarised as follows:

A violin plot displays quartile information, albeit a bit more subtly
through smoothed density curves. Look closely at the center vertical bar
of the violin plot below; the three quartiles and ``whiskers'' are still
present!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.violinplot(data}\OperatorTok{=}\NormalTok{wb, y}\OperatorTok{=}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-12-output-1.pdf}

}

\end{figure}

\hypertarget{side-by-side-box-and-violin-plots}{%
\subsection{Side-by-Side Box and Violin
Plots}\label{side-by-side-box-and-violin-plots}}

Plotting side-by-side box or violin plots allows us to compare
distributions across different categories. In other words, they enable
us to plot both a qualitative variable and a quantitative continuous
variable in one visualization.

With \texttt{seaborn}, we can easily create side-by-side plots by
specifying both an x and y column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.boxplot(data}\OperatorTok{=}\NormalTok{wb, x}\OperatorTok{=}\StringTok{"Continent"}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}Gross domestic product: }\SpecialCharTok{\% g}\StringTok{rowth : 2016\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-13-output-1.pdf}

}

\end{figure}

\hypertarget{histograms}{%
\subsection{Histograms}\label{histograms}}

You are likely familiar with histograms from Data 8. A histogram
collects continuous data into bins, then plots this binned data. Each
bin reflects the density of datapoints with values that lie between the
left and right ends of the bin; in other words, the \textbf{area} of
each bin is proportional to the \textbf{percentage} of datapoints it
contains.

\hypertarget{plotting-histograms}{%
\subsubsection{Plotting Histograms}\label{plotting-histograms}}

Below, we plot a histogram using matplotlib and seaborn. Which graph do
you prefer?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textasciigrave{}edgecolor\textasciigrave{} argument controls the color of the bin edges}
\NormalTok{gni }\OperatorTok{=}\NormalTok{ wb[}\StringTok{"Gross national income per capita, Atlas method: $: 2016"}\NormalTok{]}
\NormalTok{plt.hist(gni, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"white"}\NormalTok{)}

\CommentTok{\# Add labels}
\NormalTok{plt.xlabel(}\StringTok{"Gross national income per capita"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-14-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data}\OperatorTok{=}\NormalTok{wb, x}\OperatorTok{=}\StringTok{"Gross national income per capita, Atlas method: $: 2016"}\NormalTok{, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-15-output-1.pdf}

}

\end{figure}

\hypertarget{overlaid-histograms}{%
\subsubsection{Overlaid Histograms}\label{overlaid-histograms}}

We can overlay histograms (or density curves) to compare distributions
across qualitative categories.

The \texttt{hue} parameter of \texttt{sns.histplot} specifies the column
that should be used to determine the color of each category.
\texttt{hue} can be used in many \texttt{seaborn} plotting functions.

Notice that the resulting plot includes a legend describing which color
corresponds to each hemisphere -- a legend should always be included if
color is used to encode information in a visualization!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new variable to store the hemisphere in which each country is located}
\NormalTok{north }\OperatorTok{=}\NormalTok{ [}\StringTok{"Asia"}\NormalTok{, }\StringTok{"Europe"}\NormalTok{, }\StringTok{"N. America"}\NormalTok{]}
\NormalTok{south }\OperatorTok{=}\NormalTok{ [}\StringTok{"Africa"}\NormalTok{, }\StringTok{"Oceania"}\NormalTok{, }\StringTok{"S. America"}\NormalTok{]}
\NormalTok{wb.loc[wb[}\StringTok{"Continent"}\NormalTok{].isin(north), }\StringTok{"Hemisphere"}\NormalTok{] }\OperatorTok{=} \StringTok{"Northern"}
\NormalTok{wb.loc[wb[}\StringTok{"Continent"}\NormalTok{].isin(south), }\StringTok{"Hemisphere"}\NormalTok{] }\OperatorTok{=} \StringTok{"Southern"}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data}\OperatorTok{=}\NormalTok{wb, x}\OperatorTok{=}\StringTok{"Gross national income per capita, Atlas method: $: 2016"}\NormalTok{, hue}\OperatorTok{=}\StringTok{"Hemisphere"}\NormalTok{, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-17-output-1.pdf}

}

\end{figure}

Again, each bin of a histogram is scaled such that its \textbf{area} is
proportional to the \textbf{percentage} of all datapoints that it
contains.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{densities, bins, \_ }\OperatorTok{=}\NormalTok{ plt.hist(gni, density}\OperatorTok{=}\VariableTok{True}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"white"}\NormalTok{, bins}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Gross national income per capita"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"First bin has width }\SpecialCharTok{\{}\NormalTok{bins[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{bins[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ and height }\SpecialCharTok{\{}\NormalTok{densities[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"This corresponds to }\SpecialCharTok{\{}\NormalTok{bins[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{bins[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ * }\SpecialCharTok{\{}\NormalTok{densities[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ = }\SpecialCharTok{\{}\NormalTok{(bins[}\DecValTok{1}\NormalTok{]}\OperatorTok{{-}}\NormalTok{bins[}\DecValTok{0}\NormalTok{])}\OperatorTok{*}\NormalTok{densities[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\DecValTok{100}\SpecialCharTok{\}}\SpecialStringTok{\% of the data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
First bin has width 16410.0 and height 4.7741589911386953e-05
This corresponds to 16410.0 * 4.7741589911386953e-05 = 78.343949044586% of the data
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-18-output-2.pdf}

}

\end{figure}

\hypertarget{evaluating-histograms}{%
\subsubsection{Evaluating Histograms}\label{evaluating-histograms}}

Histograms allow us to assess a distribution by their shape. There are a
few properties of histograms we can analyze:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Skewness and Tails

  \begin{itemize}
  \tightlist
  \item
    Skewed left vs skewed right
  \item
    Left tail vs right tail
  \end{itemize}
\item
  Outliers

  \begin{itemize}
  \tightlist
  \item
    Using percentiles
  \end{itemize}
\item
  Modes

  \begin{itemize}
  \tightlist
  \item
    Most commonly occuring data
  \end{itemize}
\end{enumerate}

\hypertarget{skewness-and-tails}{%
\paragraph{Skewness and Tails}\label{skewness-and-tails}}

The skew of a histogram describes the direction in which its ``tail''
extends. - A distribution with a long right tail is \textbf{skewed
right} (such as \texttt{Gross\ national\ income\ per\ capita}). In a
right-skewed distribution, the few large outliers ``pull'' the mean to
the \textbf{right} of the median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{\textquotesingle{}Gross national income per capita, Atlas method: $: 2016\textquotesingle{}}\NormalTok{, stat }\OperatorTok{=} \StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Distribution with a long right tail\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Distribution with a long right tail')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-19-output-2.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  A distribution with a long left tail is \textbf{skewed left} (such as
  \texttt{Access\ to\ an\ improved\ water\ source}). In a left-skewed
  distribution, the few small outliers ``pull'' the mean to the
  \textbf{left} of the median.
\end{itemize}

In the case where a distribution has equal-sized right and left tails,
it is \textbf{symmetric}. The mean is approximately \textbf{equal} to
the median. Think of mean as the balancing point of the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{\textquotesingle{}Access to an improved water source: }\SpecialCharTok{\% o}\StringTok{f population: 2015\textquotesingle{}}\NormalTok{, stat }\OperatorTok{=} \StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{plt.title(}\StringTok{\textquotesingle{}Distribution with a long left tail\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 1.0, 'Distribution with a long left tail')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-20-output-2.pdf}

}

\end{figure}

\hypertarget{outliers}{%
\paragraph{Outliers}\label{outliers}}

Loosely speaking, an \textbf{outlier} is defined as a data point that
lies an abnormally large distance away from other values. Let's make
this more concrete. As you may have observed in the box plot infographic
earlier, we define \textbf{outliers} to be the data points that fall
beyond the whiskers. Specifically, values that are less than the
{[}\(1^{st}\) Quartile \(-\) (\(1.5\times\) IQR){]}, or greater than
{[}\(3^{rd}\) Quartile \(+\) (\(1.5\times\) IQR).{]}

\hypertarget{modes}{%
\paragraph{Modes}\label{modes}}

In Data 100, we describe a ``mode'' of a histogram as a peak in the
distribution. Often, however, it is difficult to determine what counts
as its own ``peak.'' For example, the number of peaks in the
distribution of HIV rates across different countries varies depending on
the number of histogram bins we plot.

If we set the number of bins to 5, the distribution appears unimodal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rename the very long column name for convenience}
\NormalTok{wb }\OperatorTok{=}\NormalTok{ wb.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}Antiretroviral therapy coverage: }\SpecialCharTok{\% o}\StringTok{f people living with HIV: 2015\textquotesingle{}}\NormalTok{:}\StringTok{"HIV rate"}\NormalTok{\})}
\CommentTok{\# With 5 bins, it seems that there is only one peak}
\NormalTok{sns.histplot(data}\OperatorTok{=}\NormalTok{wb, x}\OperatorTok{=}\StringTok{"HIV rate"}\NormalTok{, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{, bins}\OperatorTok{=}\DecValTok{5}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"5 histogram bins"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# With 10 bins, there seem to be two peaks}

\NormalTok{sns.histplot(data}\OperatorTok{=}\NormalTok{wb, x}\OperatorTok{=}\StringTok{"HIV rate"}\NormalTok{, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{, bins}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"10 histogram bins"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-22-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# And with 20 bins, it becomes hard to say what counts as a "peak"!}

\NormalTok{sns.histplot(data}\OperatorTok{=}\NormalTok{wb, x }\OperatorTok{=}\StringTok{"HIV rate"}\NormalTok{, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{, bins}\OperatorTok{=}\DecValTok{20}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"20 histogram bins"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

In part, it is these ambiguities that motivate us to consider using
Kernel Density Estimation (KDE), which we will explore more in the next
lecture.

\bookmarksetup{startatroot}

\hypertarget{visualization-ii}{%
\chapter{Visualization II}\label{visualization-ii}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Understanding KDE for plotting distributions and estimating density
  curves.
\item
  Using transformations to analyze the relationship between two
  variables.
\item
  Evaluating the quality of a visualization based on visualization
  theory concepts.
\end{itemize}

\end{tcolorbox}

\hypertarget{kernel-density-estimation}{%
\section{Kernel Density Estimation}\label{kernel-density-estimation}}

Often, we want to identify general trends across a distribution, rather
than focus on detail. Smoothing a distribution helps generalize the
structure of the data and eliminate noise.

\hypertarget{kde-theory}{%
\subsection{KDE Theory}\label{kde-theory}}

A \textbf{kernel density estimate (KDE)} is a smooth, continuous
function that approximates a curve. It allows us to represent general
trends in a distribution without focusing on the details, which is
useful for analyzing the broad structure of a dataset.

More formally, a KDE attempts to approximate the underlying
\textbf{probability distribution} from which our dataset was drawn. You
may have encountered the idea of a probability distribution in your
other classes; if not, we'll discuss it at length in the next lecture.
For now, you can think of a probability distribution as a description of
how likely it is for us to sample a particular value in our dataset.

A KDE curve estimates the probability density function of a random
variable. Consider the example below, where we have used
\texttt{sns.displot} to plot both a histogram (containing the data
points we actually collected) and a KDE curve (representing the
\emph{approximated} probability distribution from which this data was
drawn) using data from the World Bank dataset (\texttt{wb}).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{wb }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/world\_bank.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{wb }\OperatorTok{=}\NormalTok{ wb.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}Antiretroviral therapy coverage: }\SpecialCharTok{\% o}\StringTok{f people living with HIV: 2015\textquotesingle{}}\NormalTok{:}\StringTok{"HIV rate"}\NormalTok{,}
                       \StringTok{\textquotesingle{}Gross national income per capita, Atlas method: $: 2016\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}gni\textquotesingle{}}\NormalTok{\})}
\NormalTok{wb.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\toprule
{} & Continent &   Country &  Primary completion rate: Male: \% of relevant age group: 2015 &  Primary completion rate: Female: \% of relevant age group: 2015 &  Lower secondary completion rate: Male: \% of relevant age group: 2015 &  Lower secondary completion rate: Female: \% of relevant age group: 2015 &  Youth literacy rate: Male: \% of ages 15-24: 2005-14 &  Youth literacy rate: Female: \% of ages 15-24: 2005-14 &  Adult literacy rate: Male: \% ages 15 and older: 2005-14 &  Adult literacy rate: Female: \% ages 15 and older: 2005-14 &  Students at lowest proficiency on PISA: Mathematics: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Reading: \% of 15 year-olds: 2015 &  Students at lowest proficiency on PISA: Science: \% of 15 year-olds: 2015 &  Population: millions: 2016 &  Surface area: sq. km thousands: 2016 &  Population density: people per sq. km: 2016 &  Gross national income, Atlas method: \$ billions: 2016 &     gni &  Purchasing power parity gross national income: \$ billions: 2016 &  per capita: \$: 2016 &  Gross domestic product: \% growth : 2016 &  per capita: \% growth: 2016 &  Prevalence of smoking: Male: \% of adults: 2015 &  Prevalence of smoking: Female: \% of adults: 2015 &  Incidence of tuberculosis: per 100,000 people: 2015 &  Prevalence of diabetes: \% of population ages 20 to 79: 2015 &  Incidence of HIV: Total: \% of uninfected population ages 15-49: 2015 &  Prevalence of HIV: Total: \% of population ages 15-49: 2015 &  Prevalence of HIV: Women's share of population ages 15+ living with HIV: \%: 2015 &  Prevalence of HIV: Youth, Male: \% of population ages 15-24: 2015 &  Prevalence of HIV: Youth, Female: \% of population ages 15-24: 2015 &  HIV rate &  Cause of death: Communicable diseases and maternal, prenatal, and nutrition conditions: \% of population: 2015 &  Cause of death: Non-communicable diseases: \% of population: 2015 &  Cause of death: Injuries: \% of population: 2015 &  Access to an improved water source: \% of population: 1990 &  Access to an improved water source: \% of population: 2015 &  Access to improved sanitation facilities: \% of population: 1990 &  Access to improved sanitation facilities: \% of population: 2015 &  Child immunization rate: Measles: \% of children ages 12-23 months: 2015 &  Child immunization rate: DTP3: \% of children ages 12-23 months: 2015 &  Children with acute respiratory infection taken to health provider: \% of children under age 5 with ARI: 2009-2016 &  Children with diarrhea who received oral rehydration and continuous feeding: \% of children under age 5 with diarrhea: 2009-2016 &  Children sleeping under treated bed nets: \% of children under age 5: 2009-2016 &  Children with fever receiving antimalarial drugs: \% of children under age 5 with fever: 2009-2016 &  Tuberculosis: Treatment success rate: \% of new cases: 2014 &  Tuberculosis: Cases detection rate: \% of new estimated cases: 2015 \\
\midrule
0 &    Africa &   Algeria &                                              106.0 &                                              105.0 &                                               68.0 &                                               85.0 &                                               96.0 &                                               92.0 &                                               83.0 &                                               68.0 &                                               51.0 &                                               11.0 &                                                4.0 &                        40.6 &                                2381.7 &                                         17.0 &                                              173.5 &  4270.0 &                                              597.9 &              14720.0 &                                      3.7 &                         1.8 &                                             NaN &                                               NaN &                                               75.0 &                                                7.5 &                                               0.01 &                                                0.1 &                                               46.0 &                                                0.1 &                                                0.1 &      90.0 &                                               16.0 &                                               74.0 &                                             10.0 &                                               92.0 &                                               84.0 &                                               80.0 &                                               88.0 &                                               95.0 &                                               95.0 &                                               66.0 &                                               42.0 &                                                NaN &                                                NaN &                                               88.0 &                                               80.0 \\
1 &    Africa &    Angola &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               79.0 &                                               67.0 &                                               82.0 &                                               60.0 &                                                NaN &                                                NaN &                                                NaN &                        28.8 &                                1246.7 &                                         23.0 &                                               99.0 &  3440.0 &                                              179.3 &               6220.0 &                                      0.0 &                        -3.3 &                                             NaN &                                               NaN &                                              370.0 &                                                4.1 &                                               0.19 &                                                2.2 &                                               60.0 &                                                0.5 &                                                0.9 &      29.0 &                                               62.0 &                                               27.0 &                                             11.0 &                                               46.0 &                                               49.0 &                                               22.0 &                                               52.0 &                                               55.0 &                                               64.0 &                                                NaN &                                                NaN &                                               25.9 &                                               28.3 &                                               34.0 &                                               64.0 \\
2 &    Africa &     Benin &                                               83.0 &                                               73.0 &                                               50.0 &                                               37.0 &                                               55.0 &                                               31.0 &                                               41.0 &                                               18.0 &                                                NaN &                                                NaN &                                                NaN &                        10.9 &                                 114.8 &                                         96.0 &                                                8.9 &   820.0 &                                               23.6 &               2170.0 &                                      4.0 &                         1.1 &                                            18.0 &                                               1.0 &                                               60.0 &                                                0.8 &                                               0.07 &                                                1.1 &                                               58.0 &                                                0.2 &                                                0.4 &      49.0 &                                               52.0 &                                               37.0 &                                             10.0 &                                               57.0 &                                               78.0 &                                                7.0 &                                               20.0 &                                               75.0 &                                               79.0 &                                               23.0 &                                               33.0 &                                               72.7 &                                               25.9 &                                               89.0 &                                               61.0 \\
3 &    Africa &  Botswana &                                               98.0 &                                              101.0 &                                               86.0 &                                               87.0 &                                               96.0 &                                               99.0 &                                               87.0 &                                               89.0 &                                                NaN &                                                NaN &                                                NaN &                         2.3 &                                 581.7 &                                          4.0 &                                               14.9 &  6610.0 &                                               36.9 &              16380.0 &                                      2.9 &                         1.0 &                                             NaN &                                               NaN &                                              356.0 &                                                5.6 &                                               0.94 &                                               22.2 &                                               57.0 &                                                3.9 &                                                9.8 &      78.0 &                                               49.0 &                                               42.0 &                                              9.0 &                                               92.0 &                                               96.0 &                                               39.0 &                                               63.0 &                                               97.0 &                                               95.0 &                                                NaN &                                                NaN &                                                NaN &                                                NaN &                                               77.0 &                                               62.0 \\
5 &    Africa &   Burundi &                                               58.0 &                                               66.0 &                                               35.0 &                                               30.0 &                                               90.0 &                                               88.0 &                                               89.0 &                                               85.0 &                                                NaN &                                                NaN &                                                NaN &                        10.5 &                                  27.8 &                                        410.0 &                                                2.9 &   280.0 &                                                8.1 &                770.0 &                                     -0.6 &                        -3.7 &                                             NaN &                                               NaN &                                              122.0 &                                                2.7 &                                               0.02 &                                                1.0 &                                               60.0 &                                                0.3 &                                                0.3 &      54.0 &                                               57.0 &                                               31.0 &                                             12.0 &                                               69.0 &                                               76.0 &                                               42.0 &                                               48.0 &                                               93.0 &                                               94.0 &                                               55.0 &                                               43.0 &                                               53.8 &                                               25.4 &                                               91.0 &                                               51.0 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{sns.displot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{\textquotesingle{}HIV rate\textquotesingle{}}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{                       kde }\OperatorTok{=} \VariableTok{True}\NormalTok{, stat }\OperatorTok{=} \StringTok{"density"}\NormalTok{)}

\NormalTok{plt.title(}\StringTok{"Distribution of HIV rates"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-3-output-2.pdf}

}

\end{figure}

Notice that the smooth KDE curve is higher when the histogram bins are
taller. You can think of the height of the KDE curve as representing how
``probable'' it is that we randomly sample a datapoint with the
corresponding value. This intuitively makes sense -- if we have already
collected more datapoints with a particular value (resulting in a tall
histogram bin), it is more likely that, if we randomly sample another
datapoint, we will sample one with a similar value (resulting in a high
KDE curve).

The area under a probability density function should always integrate to
1, representing the fact that the total probability of a distribution
should always sum to 100\%. Hence, a KDE curve will always have an area
under the curve of 1.

\hypertarget{constructing-a-kde}{%
\subsection{Constructing a KDE}\label{constructing-a-kde}}

We perform kernel density estimation using three steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Place a kernel at each datapoint.
\item
  Normalize the kernels to have a total area of 1 (across all kernels).
\item
  Sum the normalized kernels.
\end{enumerate}

We'll explain what a ``kernel'' is momentarily.

To make things simpler, let's construct a KDE for a small, artificially
generated dataset of 5 datapoints: \([2.2, 2.8, 3.7, 5.3, 5.7]\). In the
plot below, each vertical bar represents one data point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ [}\FloatTok{2.2}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{3.7}\NormalTok{, }\FloatTok{5.3}\NormalTok{, }\FloatTok{5.7}\NormalTok{]}

\NormalTok{sns.rugplot(data, height}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}

\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-4-output-1.pdf}

}

\end{figure}

Our goal is to create the following KDE curve, which was generated
automatically by \texttt{sns.kdeplot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.kdeplot(data)}

\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

\hypertarget{step-1-place-a-kernel-at-each-data-point}{%
\subsubsection{Step 1: Place a Kernel at Each Data
Point}\label{step-1-place-a-kernel-at-each-data-point}}

To begin generating a density curve, we need to choose a \textbf{kernel}
and \textbf{bandwidth value (\(\alpha\))}. What are these exactly?

A \textbf{kernel} is a density curve. It is the mathematical function
that attempts to capture the randomness of each data point in our
sampled data. To explain what this means, consider just \emph{one} of
the datapoints in our dataset: \(2.2\). We obtained this datapoint by
randomly sampling some information out in the real world (you can
imagine \(2.2\) as representing a single measurement taken in an
experiment, for example). If we were to sample a new datapoint, we may
obtain a slightly different value. It could be higher than \(2.2\); it
could also be lower than \(2.2\). We make the assumption that any future
sampled datapoints will likely be similar in value to the data we've
already drawn. This means that our \emph{kernel} -- our description of
the probability of randomly sampling any new value -- will be greatest
at the datapoint we've already drawn but still have non-zero probability
above and below it. The area under any kernel should integrate to 1,
representing the total probability of drawing a new datapoint.

A \textbf{bandwidth value}, usually denoted by \(\alpha\), represents
the width of the kernel. A large value of \(\alpha\) will result in a
wide, short kernel function, while a small value with result in a
narrow, tall kernel.

Below, we place a \textbf{Gaussian kernel}, plotted in orange, over the
datapoint \(2.2\). A Gaussian kernel is simply the normal distribution,
which you may have called a bell curve in Data 8.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gaussian\_kernel(x, z, a):}
    \CommentTok{\# We\textquotesingle{}ll discuss where this mathematical formulation came from later}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{1}\OperatorTok{/}\NormalTok{np.sqrt(}\DecValTok{2}\OperatorTok{*}\NormalTok{np.pi}\OperatorTok{*}\NormalTok{a}\OperatorTok{**}\DecValTok{2}\NormalTok{)) }\OperatorTok{*}\NormalTok{ np.exp((}\OperatorTok{{-}}\NormalTok{(x }\OperatorTok{{-}}\NormalTok{ z)}\OperatorTok{**}\DecValTok{2} \OperatorTok{/}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ a}\OperatorTok{**}\DecValTok{2}\NormalTok{)))}

\CommentTok{\# Plot our datapoint}
\NormalTok{sns.rugplot([}\FloatTok{2.2}\NormalTok{], height}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}

\CommentTok{\# Plot the kernel}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{plt.plot(x, gaussian\_kernel(x, }\FloatTok{2.2}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-6-output-1.pdf}

}

\end{figure}

To begin creating our KDE, we place a kernel on \emph{each} datapoint in
our dataset. For our dataset of 5 points, we will have 5 kernels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# You will work with the functions below in Lab 4}
\KeywordTok{def}\NormalTok{ create\_kde(kernel, pts, a):}
    \CommentTok{\# Takes in a kernel, set of points, and alpha}
    \CommentTok{\# Returns the KDE as a function}
    \KeywordTok{def}\NormalTok{ f(x):}
\NormalTok{        output }\OperatorTok{=} \DecValTok{0}
        \ControlFlowTok{for}\NormalTok{ pt }\KeywordTok{in}\NormalTok{ pts:}
\NormalTok{            output }\OperatorTok{+=}\NormalTok{ kernel(x, pt, a)}
        \ControlFlowTok{return}\NormalTok{ output }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(pts) }\CommentTok{\# Normalization factor}
    \ControlFlowTok{return}\NormalTok{ f}

\KeywordTok{def}\NormalTok{ plot\_kde(kernel, pts, a):}
    \CommentTok{\# Calls create\_kde and plots the corresponding KDE}
\NormalTok{    f }\OperatorTok{=}\NormalTok{ create\_kde(kernel, pts, a)}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.linspace(}\BuiltInTok{min}\NormalTok{(pts) }\OperatorTok{{-}} \DecValTok{5}\NormalTok{, }\BuiltInTok{max}\NormalTok{(pts) }\OperatorTok{+} \DecValTok{5}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ [f(xi) }\ControlFlowTok{for}\NormalTok{ xi }\KeywordTok{in}\NormalTok{ x]}
\NormalTok{    plt.plot(x, y)}\OperatorTok{;}
    
\KeywordTok{def}\NormalTok{ plot\_separate\_kernels(kernel, pts, a, norm}\OperatorTok{=}\VariableTok{False}\NormalTok{):}
    \CommentTok{\# Plots individual kernels, which are then summed to create the KDE}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ np.linspace(}\BuiltInTok{min}\NormalTok{(pts) }\OperatorTok{{-}} \DecValTok{5}\NormalTok{, }\BuiltInTok{max}\NormalTok{(pts) }\OperatorTok{+} \DecValTok{5}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
    \ControlFlowTok{for}\NormalTok{ pt }\KeywordTok{in}\NormalTok{ pts:}
\NormalTok{        y }\OperatorTok{=}\NormalTok{ kernel(x, pt, a)}
        \ControlFlowTok{if}\NormalTok{ norm:}
\NormalTok{            y }\OperatorTok{/=} \BuiltInTok{len}\NormalTok{(pts)}
\NormalTok{        plt.plot(x, y)}
    
\NormalTok{    plt.show()}\OperatorTok{;}
    
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}

\NormalTok{plot\_separate\_kernels(gaussian\_kernel, data, a }\OperatorTok{=} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\hypertarget{step-2-normalize-kernels-to-have-a-total-area-of-1}{%
\subsubsection{Step 2: Normalize Kernels to Have a Total Area of
1}\label{step-2-normalize-kernels-to-have-a-total-area-of-1}}

Above, we said that \emph{each} kernel has an area of 1. Earlier, we
also said that our goal is to construct a KDE curve using these kernels
with a \emph{total} area of 1. If we were to directly sum the kernels as
they are, we would produce a KDE curve with an integrated area of (5
kernels) \(\times\) (area of 1 each) = 5. To avoid this, we will
\textbf{normalize} each of our kernels. This involves multiplying each
kernel by \(\frac{1}{\#\:\text{datapoints}}\).

In the cell below, we multiply each of our 5 kernels by \(\frac{1}{5}\)
to apply normalization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}

\CommentTok{\# The \textasciigrave{}norm\textasciigrave{} argument specifies whether or not to normalize the kernels}
\NormalTok{plot\_separate\_kernels(gaussian\_kernel, data, a }\OperatorTok{=} \DecValTok{1}\NormalTok{, norm }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

\hypertarget{step-3-sum-the-normalized-kernels}{%
\subsubsection{Step 3: Sum the Normalized
Kernels}\label{step-3-sum-the-normalized-kernels}}

Our KDE curve is the sum of the normalized kernels. Notice that the
final curve is identical to the plot generated by \texttt{sns.kdeplot}
we saw earlier!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{plt.ylim(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Data"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Density"}\NormalTok{)}

\NormalTok{plot\_kde(gaussian\_kernel, data, a }\OperatorTok{=} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\hypertarget{kernel-functions-and-bandwidths}{%
\subsection{Kernel Functions and
Bandwidths}\label{kernel-functions-and-bandwidths}}

A general ``KDE formula'' function is given above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(K_{\alpha}(x, x_i)\) is the kernel centered on the observation
  \texttt{i}.

  \begin{itemize}
  \tightlist
  \item
    Each kernel individually has area 1.
  \item
    x represents any number on the number line. It is the input to our
    function.
  \end{itemize}
\item
  \(n\) is the number of observed datapoints that we have.

  \begin{itemize}
  \tightlist
  \item
    We multiply by \(\frac{1}{n}\) so that the total area of the KDE is
    still 1.
  \end{itemize}
\item
  Each \(x_i \in \{x_1, x_2, \dots, x_n\}\) represents an observed
  datapoint.

  \begin{itemize}
  \tightlist
  \item
    These are what we use to create our KDE by summing multiple shifted
    kernels centered at these points.
  \end{itemize}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(\alpha\) (alpha) is the bandwidth or smoothing parameter.
\end{itemize}

A \textbf{kernel} (for our purposes) is a valid density function. This
means it:

\begin{itemize}
\tightlist
\item
  Must be non-negative for all inputs.
\item
  Must integrate to 1.
\end{itemize}

\hypertarget{gaussian-kernel}{%
\subsubsection{Gaussian Kernel}\label{gaussian-kernel}}

The most common kernel is the \textbf{Gaussian kernel}. The Gaussian
kernel is equivalent to the Gaussian probability density function (the
Normal distribution), centered at the observed value with a standard
deviation of (this is known as the \textbf{bandwidth} parameter).

\[K_a(x, x_i) = \frac{1}{\sqrt{2\pi\alpha^{2}}}e^{-\frac{(x-x_i)^{2}}{2\alpha^{2}}}\]

In this formula:

\begin{itemize}
\tightlist
\item
  \(x\) (no subscript) represents any value along the x-axis of our plot
\item
  \(x_i\) represents the \(i\) -th datapoint in our dataset. It is one
  of the values that we have actually collected in our data sampling
  process. In our example earlier, \(x_i=2.2\). Those of you who have
  taken a probability class may recognize \(x_i\) as the \textbf{mean}
  of the normal distribution.
\item
  Each kernel is \textbf{centered} on our observed values, so its
  distribution mean is \(x_i\).
\item
  \(\alpha\) is the bandwidth parameter, representing the width of our
  kernel. More formally, \(\alpha\) is the \textbf{standard deviation}
  of the Gaussian curve.

  \begin{itemize}
  \tightlist
  \item
    A large value of \(\alpha\) will produce a kernel that is wider and
    shorter -- this leads to a smoother KDE when the kernels are summed
    together.
  \item
    A small value of \(\alpha\) will produce a narrower, taller kernel,
    and, with it, a noisier KDE.
  \end{itemize}
\end{itemize}

The details of this (admittedly intimidating) formula are less important
than understanding its role in kernel density estimation -- this
equation gives us the shape of each kernel.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Gaussian Kernel, \(\alpha\) = 0.1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gaussian Kernel, \(\alpha\) = 1
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Gaussian Kernel, \(\alpha\) = 2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gaussian Kernel, \(\alpha\) = 10
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
\end{longtable}

\hypertarget{boxcar-kernel}{%
\subsubsection{Boxcar Kernel}\label{boxcar-kernel}}

Another example of a kernel is the \textbf{Boxcar kernel}. The boxcar
kernel assigns a uniform density to points within a ``window'' of the
observation, and a density of 0 elsewhere. The equation below is a
boxcar kernel with the center at \(x_i\) and the bandwidth of
\(\alpha\).

\[K_a(x, x_i) = \begin{cases}
        \frac{1}{\alpha}, & |x - x_i| \le \frac{\alpha}{2}\\
        0, & \text{else }
    \end{cases}\]

The boxcar kernel is seldom used in practice -- we include it here to
demonstrate that a kernel function can take whatever form you would
like, provided it integrates to 1 and does not output negative values.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ boxcar\_kernel(alpha, x, z):}
    \ControlFlowTok{return}\NormalTok{ (((x}\OperatorTok{{-}}\NormalTok{z)}\OperatorTok{\textgreater{}={-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{\&}\NormalTok{((x}\OperatorTok{{-}}\NormalTok{z)}\OperatorTok{\textless{}=}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}\OperatorTok{/}\NormalTok{alpha}

\NormalTok{xs }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{alpha}\OperatorTok{=}\DecValTok{1}
\NormalTok{kde\_curve }\OperatorTok{=}\NormalTok{ [boxcar\_kernel(alpha, x, }\DecValTok{0}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]}
\NormalTok{plt.plot(xs, kde\_curve)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-10-output-1.pdf}

}

\caption{The Boxcar kernel centered at 0 with bandwidth \(\alpha\) = 1.}

\end{figure}

The diagram on the right is how the density curve for our 5 point
dataset would have looked had we used the Boxcar kernel with bandwidth
\(\alpha\) = 1.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
KDE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Boxcar
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& \\
\end{longtable}

\hypertarget{diving-deeper-into-displot}{%
\section{\texorpdfstring{Diving Deeper into
\texttt{displot}}{Diving Deeper into displot}}\label{diving-deeper-into-displot}}

As we saw earlier, we can use \texttt{seaborn}'s \texttt{displot}
function to plot various distributions. In particular, \texttt{displot}
allows you to specify the \texttt{kind} of plot and is a wrapper for
\texttt{histplot}, \texttt{kdeplot}, and \texttt{ecdfplot}.

Below, we can see a couple of examples of how \texttt{sns.displot} can
be used to plot various distributions.

First, we can plot a histogram by setting \texttt{kind} to
\texttt{"hist"}. Note that here we've specified
\texttt{stat\ =\ density} to normalize the histogram such that the area
under the histogram is equal to 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.displot(data}\OperatorTok{=}\NormalTok{wb, }
\NormalTok{            x}\OperatorTok{=}\StringTok{"gni"}\NormalTok{, }
\NormalTok{            kind}\OperatorTok{=}\StringTok{"hist"}\NormalTok{, }
\NormalTok{            stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{) }\CommentTok{\# default: stat=count and density integrates to 1}
\NormalTok{plt.title(}\StringTok{"Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-11-output-2.pdf}

}

\end{figure}

Now, what if we want to generate a KDE plot? We can set \texttt{kind} =
to \texttt{"kde"}!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.displot(data}\OperatorTok{=}\NormalTok{wb, }
\NormalTok{            x}\OperatorTok{=}\StringTok{"gni"}\NormalTok{, }
\NormalTok{            kind}\OperatorTok{=}\StringTok{\textquotesingle{}kde\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-12-output-2.pdf}

}

\end{figure}

And finally, if we want to generate an Empirical Cumulative Distribution
Function (ECDF), we can specify \texttt{kind\ =\ "ecdf"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.displot(data}\OperatorTok{=}\NormalTok{wb, }
\NormalTok{            x}\OperatorTok{=}\StringTok{"gni"}\NormalTok{, }
\NormalTok{            kind}\OperatorTok{=}\StringTok{\textquotesingle{}ecdf\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Cumulative Distribution of gross national income per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-13-output-2.pdf}

}

\end{figure}

\hypertarget{relationships-between-quantitative-variables}{%
\section{Relationships Between Quantitative
Variables}\label{relationships-between-quantitative-variables}}

Up until now, we've discussed how to visualize single-variable
distributions. Going beyond this, we want to understand the relationship
between pairs of numerical variables.

\hypertarget{scatter-plots}{%
\subsubsection{Scatter Plots}\label{scatter-plots}}

\textbf{Scatter plots} are one of the most useful tools in representing
the relationship between \textbf{pairs} of quantitative variables. They
are particularly important in gauging the strength, or correlation, of
the relationship between variables. Knowledge of these relationships can
then motivate decisions in our modeling process.

In \texttt{matplotlib}, we use the function \texttt{plt.scatter} to
generate a scatter plot. Notice that, unlike our examples of plotting
single-variable distributions, now we specify sequences of values to be
plotted along the x-axis \emph{and} the y-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.scatter(wb[}\StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{], }\OperatorTok{\textbackslash{}}
\NormalTok{            wb[}\StringTok{\textquotesingle{}Adult literacy rate: Female: \% ages 15 and older: 2005{-}14\textquotesingle{}}\NormalTok{])}

\NormalTok{plt.xlabel(}\StringTok{"}\SpecialCharTok{\% g}\StringTok{rowth per capita"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Female adult literacy rate"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-14-output-1.pdf}

}

\end{figure}

In \texttt{seaborn}, we call the function \texttt{sns.scatterplot}. We
use the \texttt{x} and \texttt{y} parameters to indicate the values to
be plotted along the x and y axes, respectively. By using the
\texttt{hue} parameter, we can specify a third variable to be used for
coloring each scatter point.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.scatterplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{               y }\OperatorTok{=} \StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{, }
\NormalTok{               hue }\OperatorTok{=} \StringTok{"Continent"}\NormalTok{)}

\NormalTok{plt.title(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-15-output-1.pdf}

}

\end{figure}

\hypertarget{overplotting}{%
\paragraph{Overplotting}\label{overplotting}}

Although the plots above communicate the general relationship between
the two plotted variables, they both suffer a major limitation --
\textbf{overplotting}. Overplotting occurs when scatter points with
similar values are stacked on top of one another, making it difficult to
see the number of scatter points actually plotted in the visualization.
Notice how in the upper righthand region of the plots, we cannot easily
tell just how many points have been plotted. This makes our
visualizations difficult to interpret.

We have a few methods to help reduce overplotting:

\begin{itemize}
\tightlist
\item
  Decreasing the size of the scatter point markers can improve
  readability. We do this by setting a new value to the size parameter,
  \texttt{s}, of \texttt{plt.scatter} or \texttt{sns.scatterplot}.
\item
  \textbf{Jittering} is the process of adding a small amount of random
  noise to all x and y values to slightly shift the position of each
  datapoint. By randomly shifting all the data by some small distance,
  we can discern individual points more clearly without modifying the
  major trends of the original dataset.
\end{itemize}

In the cell below, we first jitter the data using
\texttt{np.random.uniform}, then re-plot it with smaller markers. The
resulting plot is much easier to interpret.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Setting a seed ensures that we produce the same plot each time}
\CommentTok{\# This means that the course notes will not change each time you access them}
\NormalTok{np.random.seed(}\DecValTok{150}\NormalTok{)}

\CommentTok{\# This call to np.random.uniform generates random numbers between {-}1 and 1}
\CommentTok{\# We add these random numbers to the original x data to jitter it slightly}
\NormalTok{x\_noise }\OperatorTok{=}\NormalTok{ np.random.uniform(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(wb))}
\NormalTok{jittered\_x }\OperatorTok{=}\NormalTok{ wb[}\StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{] }\OperatorTok{+}\NormalTok{ x\_noise}

\CommentTok{\# Repeat for y data}
\NormalTok{y\_noise }\OperatorTok{=}\NormalTok{ np.random.uniform(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\BuiltInTok{len}\NormalTok{(wb))}
\NormalTok{jittered\_y }\OperatorTok{=}\NormalTok{ wb[}\StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{] }\OperatorTok{+}\NormalTok{ y\_noise}

\CommentTok{\# Setting the size parameter \textasciigrave{}s\textasciigrave{} changes the size of each point}
\NormalTok{plt.scatter(jittered\_x, jittered\_y, s}\OperatorTok{=}\DecValTok{15}\NormalTok{)}

\NormalTok{plt.xlabel(}\StringTok{"}\SpecialCharTok{\% g}\StringTok{rowth per capita (jittered)"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Female adult literacy rate (jittered)"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-16-output-1.pdf}

}

\end{figure}

\hypertarget{lmplot-and-jointplot}{%
\subsubsection{\texorpdfstring{\texttt{lmplot} and
\texttt{jointplot}}{lmplot and jointplot}}\label{lmplot-and-jointplot}}

\texttt{seaborn} also includes several built-in functions for creating
more sophisticated scatter plots. Two of the most commonly used examples
are \texttt{sns.lmplot} and \texttt{sns.jointplot}.

\texttt{sns.lmplot} plots both a scatter plot \emph{and} a linear
regression line, all in one function call. We'll discuss linear
regression in a few lectures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.lmplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{           y }\OperatorTok{=} \StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{)}

\NormalTok{plt.title(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-17-output-2.pdf}

}

\end{figure}

\texttt{sns.jointplot} creates a visualization with three components: a
scatter plot, a histogram of the distribution of x values, and a
histogram of the distribution of y values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.jointplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{           y }\OperatorTok{=} \StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{)}

\CommentTok{\# plt.suptitle allows us to shift the title up so it does not overlap with the histogram}
\NormalTok{plt.suptitle(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}
\NormalTok{plt.subplots\_adjust(top}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-18-output-1.pdf}

}

\end{figure}

\hypertarget{hex-plots}{%
\subsubsection{Hex plots}\label{hex-plots}}

For datasets with a very large number of datapoints, jittering is
unlikely to fully resolve the issue of overplotting. In these cases, we
can attempt to visualize our data by its \emph{density}, rather than
displaying each individual datapoint.

\textbf{Hex plots} can be thought of as two-dimensional histograms that
show the joint distribution between two variables. This is particularly
useful when working with very dense data. In a hex plot, the x-y plane
is binned into hexagons. Hexagons that are darker in color indicate a
greater density of data -- that is, there are more data points that lie
in the region enclosed by the hexagon.

We can generate a hex plot using \texttt{sns.jointplot} modified with
the \texttt{kind} parameter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.jointplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{              y }\OperatorTok{=} \StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{              kind }\OperatorTok{=} \StringTok{"hex"}\NormalTok{)}

\CommentTok{\# plt.suptitle allows us to shift the title up so it does not overlap with the histogram}
\NormalTok{plt.suptitle(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}
\NormalTok{plt.subplots\_adjust(top}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-19-output-1.pdf}

}

\end{figure}

\hypertarget{contour-plots}{%
\subsubsection{Contour Plots}\label{contour-plots}}

\textbf{Contour plots} are an alternative way of plotting the joint
distribution of two variables. You can think of them as the
2-dimensional versions of KDE plots. A contour plot can be interpreted
in a similar way to a
\href{https://gisgeography.com/contour-lines-topographic-map/}{topographic
map}. Each contour line represents an area that has the same
\emph{density} of datapoints throughout the region. Contours marked with
darker colors contain more datapoints (a higher density) in that region.

\texttt{sns.kdeplot} will generate a contour plot if we specify both x
and y data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.kdeplot(data }\OperatorTok{=}\NormalTok{ wb, x }\OperatorTok{=} \StringTok{"per capita: }\SpecialCharTok{\% g}\StringTok{rowth: 2016"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{            y }\OperatorTok{=} \StringTok{"Adult literacy rate: Female: \% ages 15 and older: 2005{-}14"}\NormalTok{, }\OperatorTok{\textbackslash{}}
\NormalTok{            fill }\OperatorTok{=} \VariableTok{True}\NormalTok{)}

\NormalTok{plt.title(}\StringTok{"Female adult literacy against }\SpecialCharTok{\% g}\StringTok{rowth"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-20-output-1.pdf}

}

\end{figure}

\hypertarget{transformations}{%
\section{Transformations}\label{transformations}}

We have now covered visualizations in great depth, looking into various
forms of visualizations, plotting libraries, and high-level theory.

Much of this was done to uncover insights in data, which will prove
necessary when we begin building models of data later in the course. A
strong graphical correlation between two variables hints at an
underlying relationship that we may want to study in greater detail.
However, relying on visual relationships alone is limiting - not all
plots show association. The presence of outliers and other statistical
anomalies makes it hard to interpret data.

\textbf{Transformations} are the process of manipulating data to find
significant relationships between variables. These are often found by
applying mathematical functions to variables that ``transform'' their
range of possible values and highlight some previously hidden
associations between data.

To see why we may want to transform data, consider the following plot of
adult literacy rates against gross national income.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Some data cleaning to help with the next example}
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.DataFrame(index}\OperatorTok{=}\NormalTok{wb.index)}
\NormalTok{df[}\StringTok{\textquotesingle{}lit\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ wb[}\StringTok{\textquotesingle{}Adult literacy rate: Female: \% ages 15 and older: 2005{-}14\textquotesingle{}}\NormalTok{] }\OperatorTok{\textbackslash{}}
            \OperatorTok{+}\NormalTok{ wb[}\StringTok{"Adult literacy rate: Male: \% ages 15 and older: 2005{-}14"}\NormalTok{]}
\NormalTok{df[}\StringTok{\textquotesingle{}inc\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ wb[}\StringTok{\textquotesingle{}gni\textquotesingle{}}\NormalTok{]}
\NormalTok{df.dropna(inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{plt.scatter(df[}\StringTok{"inc"}\NormalTok{], df[}\StringTok{"lit"}\NormalTok{])}
\NormalTok{plt.xlabel(}\StringTok{"Gross national income per capita"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Adult literacy rate"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Adult literacy rate against GNI per capita"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-21-output-1.pdf}

}

\end{figure}

This plot is difficult to interpret for two reasons:

\begin{itemize}
\tightlist
\item
  The data shown in the visualization appears almost ``smushed'' -- it
  is heavily concentrated in the upper lefthand region of the plot. Even
  if we jittered the dataset, we likely would not be able to fully
  assess all datapoints in that area.
\item
  It is hard to generalize a clear relationship between the two plotted
  variables. While adult literacy rate appears to share some positive
  relationship with gross national income, we are not able to describe
  the specifics of this trend in much detail.
\end{itemize}

A transformation would allow us to visualize this data more clearly,
which, in turn, would enable us to describe the underlying relationship
between our variables of interest.

We will most commonly apply a transformation to \textbf{linearize a
relationship} between variables. If we find a transformation to make a
scatter plot of two variables linear, we can ``backtrack'' to find the
exact relationship between the variables. This helps us in two major
ways. Firstly, linear relationships are particularly simple to interpret
-- we have an intuitive sense of what the slope and intercept of a
linear trend represent, and how they can help us understand the
relationship between two variables. Secondly, linear relationships are
the backbone of linear models. We will begin exploring linear modeling
in great detail next week. As we'll soon see, linear models become much
more effective when we are working with linearized data.

In the remainder of this note, we will discuss how to linearize a
dataset to produce the result below. Notice that the resulting plot
displays a rough linear relationship between the values plotted on the x
and y axes.

\hypertarget{linearization-and-applying-transformations}{%
\subsection{Linearization and Applying
Transformations}\label{linearization-and-applying-transformations}}

To linearize a relationship, begin by asking yourself: what makes the
data non-linear? It is helpful to repeat this question for each variable
in your visualization.

Let's start by considering the gross national income variable in our
plot above. Looking at the y values in the scatter plot, we can see that
many large y values are all clumped together, compressing the vertical
axis. The scale of the horizontal axis is also being distorted by the
few large outlying x values on the right.

If we decreased the size of these outliers relative to the bulk of the
data, we could reduce the distortion of the horizontal axis. How can we
do this? We need a transformation that will:

\begin{itemize}
\tightlist
\item
  Decrease the magnitude of large x values by a significant amount.
\item
  Not drastically change the magnitude of small x values.
\end{itemize}

One function that produces this result is the \textbf{log
transformation}. When we take the logarithm of a large number, the
original number will decrease in magnitude dramatically. Conversely,
when we take the logarithm of a small number, the original number does
not change its value by as significant of an amount (to illustrate this,
consider the difference between \(\log{(100)} = 4.61\) and
\(\log{(10)} = 2.3\)).

In Data 100 (and most upper-division STEM classes), \(\log\) is used to
refer to the natural logarithm with base \(e\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# np.log takes the logarithm of an array or Series}
\NormalTok{plt.scatter(np.log(df[}\StringTok{"inc"}\NormalTok{]), df[}\StringTok{"lit"}\NormalTok{])}

\NormalTok{plt.xlabel(}\StringTok{"Log(gross national income per capita)"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Adult literacy rate"}\NormalTok{)}
\NormalTok{plt.title(}\StringTok{"Adult literacy rate against Log(GNI per capita)"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-22-output-1.pdf}

}

\end{figure}

After taking the logarithm of our x values, our plot appears much more
balanced in its horizontal scale. We no longer have many datapoints
clumped on one end and a few outliers out at extreme values.

Let's repeat this reasoning for the y values. Considering only the
vertical axis of the plot, notice how there are many datapoints
concentrated at large y values. Only a few datapoints lie at smaller
values of y.

If we were to ``spread out'' these large values of y more, we would no
longer see the dense concentration in one region of the y-axis. We need
a transformation that will:

\begin{itemize}
\tightlist
\item
  Increase the magnitude of large values of y so these datapoints are
  distributed more broadly on the vertical scale,
\item
  Not substantially alter the scaling of small values of y (we do not
  want to drastically modify the lower end of the y axis, which is
  already distributed evenly on the vertical scale).
\end{itemize}

In this case, it is helpful to apply a \textbf{power transformation} --
that is, raise our y values to a power. Let's try raising our adult
literacy rate values to the power of 4. Large values raised to the power
of 4 will increase in magnitude proportionally much more than small
values raised to the power of 4 (consider the difference between
\(2^4 = 16\) and \(200^4 = 1600000000\)).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Apply a log transformation to the x values and a power transformation to the y values}
\NormalTok{plt.scatter(np.log(df[}\StringTok{"inc"}\NormalTok{]), df[}\StringTok{"lit"}\NormalTok{]}\OperatorTok{**}\DecValTok{4}\NormalTok{)}

\NormalTok{plt.xlabel(}\StringTok{"Log(gross national income per capita)"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Adult literacy rate (4th power)"}\NormalTok{)}
\NormalTok{plt.suptitle(}\StringTok{"Adult literacy rate (4th power) against Log(GNI per capita)"}\NormalTok{)}
\NormalTok{plt.subplots\_adjust(top}\OperatorTok{=}\FloatTok{0.9}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-23-output-1.pdf}

}

\end{figure}

Our scatter plot is looking a lot better! Now, we are plotting the log
of our original x values on the horizontal axis, and the 4th power of
our original y values on the vertical axis. We start to see an
approximate \emph{linear} relationship between our transformed
variables.

What can we take away from this? We now know that the log of gross
national income and adult literacy to the power of 4 are roughly
linearly related. If we denote the original, untransformed gross
national income values as \(x\) and the original adult literacy rate
values as \(y\), we can use the standard form of a linear fit to express
this relationship:

\[y^4 = m(\log{x}) + b\]

Where \(m\) represents the slope of the linear fit, while \(b\)
represents the intercept.

The cell below computes \(m\) and \(b\) for our transformed data. We'll
discuss how this code was generated in a future lecture.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The code below fits a linear regression model. We\textquotesingle{}ll discuss it at length in a future lecture}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{model.fit(np.log(df[[}\StringTok{"inc"}\NormalTok{]]), df[}\StringTok{"lit"}\NormalTok{]}\OperatorTok{**}\DecValTok{4}\NormalTok{)}
\NormalTok{m, b }\OperatorTok{=}\NormalTok{ model.coef\_[}\DecValTok{0}\NormalTok{], model.intercept\_}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The slope, m, of the transformed data is: }\SpecialCharTok{\{}\NormalTok{m}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The intercept, b, of the transformed data is: }\SpecialCharTok{\{}\NormalTok{b}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{df }\OperatorTok{=}\NormalTok{ df.sort\_values(}\StringTok{"inc"}\NormalTok{)}
\NormalTok{plt.scatter(np.log(df[}\StringTok{"inc"}\NormalTok{]), df[}\StringTok{"lit"}\NormalTok{]}\OperatorTok{**}\DecValTok{4}\NormalTok{, label}\OperatorTok{=}\StringTok{"Transformed data"}\NormalTok{)}
\NormalTok{plt.plot(np.log(df[}\StringTok{"inc"}\NormalTok{]), m}\OperatorTok{*}\NormalTok{np.log(df[}\StringTok{"inc"}\NormalTok{])}\OperatorTok{+}\NormalTok{b, c}\OperatorTok{=}\StringTok{"red"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Linear regression"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Log(gross national income per capita)"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Adult literacy rate (4th power)"}\NormalTok{)}
\NormalTok{plt.legend()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The slope, m, of the transformed data is: 336400693.43172693
The intercept, b, of the transformed data is: -1802204836.0479977
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-24-output-2.pdf}

}

\end{figure}

What if we want to understand the \emph{underlying} relationship between
our original variables, before they were transformed? We can simply
rearrange our linear expression above!

Recall our linear relationship between the transformed variables
\(\log{x}\) and \(y^4\).

\[y^4 = m(\log{x}) + b\]

By rearranging the equation, we find a relationship between the
untransformed variables \(x\) and \(y\).

\[y = [m(\log{x}) + b]^{(1/4)}\]

When we plug in the values for \(m\) and \(b\) computed above, something
interesting happens.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Now, plug the values for m and b into the relationship between the untransformed x and y}
\NormalTok{plt.scatter(df[}\StringTok{"inc"}\NormalTok{], df[}\StringTok{"lit"}\NormalTok{], label}\OperatorTok{=}\StringTok{"Untransformed data"}\NormalTok{)}
\NormalTok{plt.plot(df[}\StringTok{"inc"}\NormalTok{], (m}\OperatorTok{*}\NormalTok{np.log(df[}\StringTok{"inc"}\NormalTok{])}\OperatorTok{+}\NormalTok{b)}\OperatorTok{**}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{), c}\OperatorTok{=}\StringTok{"red"}\NormalTok{, label}\OperatorTok{=}\StringTok{"Modeled relationship"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Gross national income per capita"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Adult literacy rate"}\NormalTok{)}
\NormalTok{plt.legend()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-25-output-1.pdf}

}

\end{figure}

We have found a relationship between our original variables -- gross
national income and adult literacy rate!

Transformations are powerful tools for understanding our data in greater
detail. To summarize what we just achieved:

\begin{itemize}
\tightlist
\item
  We identified appropriate transformations to \textbf{linearize} the
  original data.
\item
  We used our knowledge of linear curves to compute the slope and
  intercept of the transformed data.
\item
  We used this slope and intercept information to derive a relationship
  in the untransformed data.
\end{itemize}

Linearization will be an important tool as we begin our work on linear
modeling next week.

\hypertarget{tukey-mosteller-bulge-diagram}{%
\subsubsection{Tukey-Mosteller Bulge
Diagram}\label{tukey-mosteller-bulge-diagram}}

The \textbf{Tukey-Mosteller Bulge Diagram} is a good guide when
determining possible transformations to achieve linearity. It is a
visual summary of the reasoning we just worked through above.

How does it work? Each curved ``bulge'' represents a possible shape of
non-linear data. To use the diagram, find which of the four bulges
resembles your dataset the most closely. Then, look at the axes of the
quadrant for this bulge. The horizontal axis will list possible
transformations that could be applied to your x data for linearization.
Similarly, the vertical axis will list possible transformations that
could be applied to your y data. Note that each axis lists two possible
transformations. While \emph{either} of these transformations has the
\emph{potential} to linearize your dataset, note that this is an
iterative process. It's important to try out these transformations and
look at the results to see whether you've actually achieved linearity.
If not, you'll need to continue testing other possible transformations.

Generally:

\begin{itemize}
\tightlist
\item
  \(\sqrt{}\) and \(\log{}\) will reduce the magnitude of large values.
\item
  Powers (\(^2\) and \(^3\)) will increase the spread in magnitude of
  large values.
\end{itemize}

\textbf{Important:} You should still understand the \emph{logic} we
worked through to determine how best to transform the data. The bulge
diagram is just a summary of this same reasoning. You will be expected
to be able to explain why a given transformation is or is not
appropriate for linearization.

\hypertarget{additional-remarks}{%
\subsection{Additional Remarks}\label{additional-remarks}}

Visualization requires a lot of thought!

\begin{itemize}
\tightlist
\item
  There are many tools for visualizing distributions.

  \begin{itemize}
  \tightlist
  \item
    Distribution of a single variable:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      Rugplot
    \item
      Histogram
    \item
      Density plot
    \item
      Box plot
    \item
      Violin plot
    \end{enumerate}
  \item
    Joint distribution of two quantitative variables:

    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      Scatter plot
    \item
      Hex plot
    \item
      Contour plot
    \end{enumerate}
  \end{itemize}
\end{itemize}

This class primarily uses \texttt{seaborn} and \texttt{matplotlib}, but
\texttt{pandas} also has basic built-in plotting methods. Many other
visualization libraries exist, and \texttt{plotly} is one of them.

\begin{itemize}
\tightlist
\item
  \texttt{plotly} creates very easily creates interactive plots.
\item
  \texttt{plotly} will occasionally appear in lecture code, labs, and
  assignments!
\end{itemize}

Next, we'll go deeper into the theory behind visualization.

\hypertarget{visualization-theory}{%
\section{Visualization Theory}\label{visualization-theory}}

This section marks a pivot to the second major topic of this lecture -
visualization theory. We'll discuss the abstract nature of
visualizations and analyze how they convey information.

Remember, we had two goals for visualizing data. This section is
particularly important in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Helping us understand the data and results,
\item
  Communicating our results and conclusions with others.
\end{enumerate}

\hypertarget{information-channels}{%
\subsection{Information Channels}\label{information-channels}}

Visualizations are able to convey information through various encodings.
In the remainder of this lecture, we'll look at the use of color, scale,
and depth, to name a few.

\hypertarget{encodings-in-rugplots}{%
\subsubsection{Encodings in Rugplots}\label{encodings-in-rugplots}}

One detail that we may have overlooked in our earlier discussion of
rugplots is the importance of encodings. Rugplots are effective visuals
because they utilize line thickness to encode frequency. Consider the
following diagram:

\hypertarget{multi-dimensional-encodings}{%
\subsubsection{Multi-Dimensional
Encodings}\label{multi-dimensional-encodings}}

Encodings are also useful for representing multi-dimensional data.
Notice how the following visual highlights four distinct ``dimensions''
of data:

\begin{itemize}
\tightlist
\item
  X-axis
\item
  Y-axis
\item
  Area
\item
  Color
\end{itemize}

The human visual perception system is only capable of visualizing data
in a three-dimensional plane, but as you've seen, we can encode many
more channels of information.

\hypertarget{harnessing-the-axes}{%
\subsection{Harnessing the Axes}\label{harnessing-the-axes}}

\hypertarget{consider-the-scale-of-the-data}{%
\subsubsection{Consider the Scale of the
Data}\label{consider-the-scale-of-the-data}}

However, we should be careful to not misrepresent relationships in our
data by manipulating the scale or axes. The visualization below
improperly portrays two seemingly independent relationships on the same
plot. The authors have clearly changed the scale of the y-axis to
mislead their audience.

Notice how the downwards-facing line segment contains values in the
millions, while the upwards-trending segment only contains values near
three hundred thousand. These lines should not be intersecting.

When there is a large difference in the magnitude of the data, it's
advised to analyze percentages instead of counts. The following diagrams
correctly display the trends in cancer screening and abortion rates.

\hypertarget{reveal-the-data}{%
\subsubsection{Reveal the Data}\label{reveal-the-data}}

Great visualizations not only consider the scale of the data but also
utilize the axes in a way that best conveys information. For example,
data scientists commonly set certain axes limits to highlight parts of
the visualization they are most interested in.

The visualization on the right captures the trend in coronavirus cases
during March of 2020. From only looking at the visualization on the
left, a viewer may incorrectly believe that coronavirus began to
skyrocket on March 4\textsuperscript{th}, 2020. However, the second
illustration tells a different story - cases rose closer to March
21\textsuperscript{th}, 2020.

\hypertarget{harnessing-color}{%
\subsection{Harnessing Color}\label{harnessing-color}}

Color is another important feature in visualizations that does more than
what meets the eye.

We already explored using color to encode a categorical variable in our
scatter plot. Let's now discuss the uses of color in novel
visualizations like colormaps and heatmaps.

5-8\% of the world is red-green color blind, so we have to be very
particular about our color scheme. We want to make these as accessible
as possible. Choosing a set of colors that work together is evidently a
challenging task!

\hypertarget{colormaps}{%
\subsubsection{Colormaps}\label{colormaps}}

Colormaps are mappings from pixel data to color values, and they're
often used to highlight distinct parts of an image. Let's investigate a
few properties of colormaps.

\textbf{Jet Colormap}

\textbf{Viridis Colormap}

The jet colormap is infamous for being misleading. While it seems more
vibrant than viridis, the aggressive colors poorly encode numerical
data. To understand why, let's analyze the following images.

The diagram on the left compares how a variety of colormaps represent
pixel data that transitions from a high to low intensity. These include
the jet colormap (row a) and grayscale (row b). Notice how the grayscale
images do the best job in smoothly transitioning between pixel data. The
jet colormap is the worst at this - the four images in row (a) look like
a conglomeration of individual colors.

The difference is also evident in the images labeled (a) and (b) on the
left side. The grayscale image is better at preserving finer detail in
the vertical line strokes. Additionally, grayscale is preferred in X-ray
scans for being more neutral. The intensity of the dark red color in the
jet colormap is frightening and indicates something is wrong.

Why is the jet colormap so much worse? The answer lies in how its color
composition is perceived to the human eye.

\textbf{Jet Colormap Perception}

\textbf{Viridis Colormap Perception}

The jet colormap is largely misleading because it is not perceptually
uniform. \textbf{Perceptually uniform colormaps} have the property that
if the pixel data goes from 0.1 to 0.2, the perceptual change is the
same as when the data goes from 0.8 to 0.9.

Notice how the said uniformity is present within the linear trend
displayed in the viridis colormap. On the other hand, the jet colormap
is largely non-linear - this is precisely why it's considered a worse
colormap.

\hypertarget{harnessing-markings}{%
\subsection{Harnessing Markings}\label{harnessing-markings}}

In our earlier discussion of multi-dimensional encodings, we analyzed a
scatter plot with four pseudo-dimensions: the two axes, area, and color.
Were these appropriate to use? The following diagram analyzes how well
the human eye can distinguish between these ``markings''.

There are a few key takeaways from this diagram

\begin{itemize}
\tightlist
\item
  Lengths are easy to discern. Don't use plots with jiggled baselines -
  keep everything axis-aligned.
\item
  Avoid pie charts! Angle judgments are inaccurate.
\item
  Areas and volumes are hard to distinguish (area charts, word clouds,
  etc.).
\end{itemize}

\hypertarget{harnessing-conditioning}{%
\subsection{Harnessing Conditioning}\label{harnessing-conditioning}}

Conditioning is the process of comparing data that belong to separate
groups. We've seen this before in overlayed distributions, side-by-side
box plots, and scatter plots with categorical encodings. Here, we'll
introduce terminology that formalizes these examples.

Consider an example where we want to analyze income earnings for males
and females with varying levels of education. There are multiple ways to
compare this data.

The barplot is an example of \textbf{juxtaposition}: placing multiple
plots side by side, with the same scale. The scatter plot is an example
of \textbf{superposition}: placing multiple density curves and scatter
plots on top of each other.

Which is better depends on the problem at hand. Here, superposition
makes the precise wage difference very clear from a quick glance.
However, many sophisticated plots convey information that favors the use
of juxtaposition. Below is one example.

\hypertarget{harnessing-context}{%
\subsection{Harnessing Context}\label{harnessing-context}}

The last component of a great visualization is perhaps the most critical
- the use of context. Adding informative titles, axis labels, and
descriptive captions are all best practices that we've heard repeatedly
in Data 8.

A publication-ready plot (and every Data 100 plot) needs:

\begin{itemize}
\tightlist
\item
  Informative title (takeaway, not description),
\item
  Axis labels,
\item
  Reference lines, markers, etc,
\item
  Legends, if appropriate,
\item
  Captions that describe data,
\end{itemize}

Captions should:

\begin{itemize}
\tightlist
\item
  Be comprehensive and self-contained,
\item
  Describe what has been graphed,
\item
  Draw attention to important features,
\item
  Describe conclusions drawn from graphs.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Understand how to appropriately collect data to help answer a
  question.
\end{itemize}

\end{tcolorbox}

In data science, understanding characteristics of a population starts
with having quality data to investigate. While it is often impossible to
collect all the data describing a population, we can overcome this by
properly sampling from the population. In this note, we will discuss
appropriate techniques for sampling from populations.

\begin{figure}

{\centering \includegraphics{sampling/images/data_life_cycle_sampling.png}

}

\caption{Lifecycle diagram}

\end{figure}

\hypertarget{censuses-and-surveys}{%
\section{Censuses and Surveys}\label{censuses-and-surveys}}

In general: a \textbf{census} is ``a complete count or survey of a
\textbf{population}, typically recording various details of
\textbf{individuals}.'' An example is the U.S. Decennial Census which
was held in April 2020. It counts \emph{every person} living in all 50
states, DC, and US territories, not just citizens. Participation is
required by law (it is mandated by the U.S. Constitution). Important
uses include the allocation of Federal funds, congressional
representation, and drawing congressional and state legislative
districts. The census is composed of a \textbf{survey} mailed to
different housing addresses in the United States.

A \textbf{survey} is a set of questions. An example is workers sampling
individuals and households. What is asked and how it is asked can affect
how the respondent answers or even whether or not they answer in the
first place.

While censuses are great, it is often very difficult and expensive to
survey everyone in a population. Imagine the amount of resources, money,
time, and energy the U.S. spent on the 2020 Census. While this does give
us more accurate information about the population, it's often infeasible
to execute. Thus, we usually survey a subset of the population instead.

A \textbf{sample} is (usually) a subset of the population that is often
used to make inferences about the population. If our sample is a good
representation of our population, then we can use it to glean useful
information at a lower cost. That being said, how the sample is drawn
will affect the reliability of such inferences. Two common sources of
error in sampling are \textbf{chance error}, where random samples can
vary from what is expected in any direction, and \textbf{bias}, which is
a systematic error in one direction. Biases can be the result of many
things, for example, our sampling scheme or survey methods.

Let's define some useful vocabulary:

\begin{itemize}
\tightlist
\item
  \textbf{Population}: The group that you want to learn something about.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Individuals} in a population are not always people. Other
    populations include bacteria in your gut (sampled using DNA
    sequencing), trees of a certain species, small businesses receiving
    a microloan, or published results in an academic journal or field.
  \end{itemize}
\item
  \textbf{Sampling Frame}: The list from which the sample is drawn.

  \begin{itemize}
  \tightlist
  \item
    For example, if sampling people, then the sampling frame is the set
    of all people that could possibly end up in your sample.
  \end{itemize}
\item
  \textbf{Sample}: Who you actually end up sampling. The sample is
  therefore a subset of your \emph{sampling frame}.
\end{itemize}

While ideally, these three sets would be exactly the same, they usually
aren't in practice. For example, there may be individuals in your
sampling frame (and hence, your sample) that are not in your population.
And generally, sample sizes are much smaller than population sizes.

\begin{figure}

{\centering \includegraphics{sampling/images/samplingframe.png}

}

\caption{Sampling\_Frames}

\end{figure}

\hypertarget{bias-a-case-study}{%
\section{Bias: A Case Study}\label{bias-a-case-study}}

The following case study is adapted from \emph{Statistics} by Freedman,
Pisani, and Purves, W.W. Norton NY, 1978.

In 1936, President Franklin D. Roosevelt (Democratic) went up for
re-election against Alf Landon (Republican). As is usual, \textbf{polls}
were conducted in the months leading up to the election to try and
predict the outcome. The \emph{Literary Digest} was a magazine that had
successfully predicted the outcome of 5 general elections coming into
1936. In their polling for the 1936 election, they sent out their survey
to 10 million individuals whom they found from phone books, lists of
magazine subscribers, and lists of country club members. Of the roughly
2.4 million people who filled out the survey, only 43\% reported they
would vote for Roosevelt; thus, the \emph{Digest} predicted that Landon
would win.

On election day, Roosevelt won in a landslide, winning 61\% of the
popular vote of about 45 million voters. How could the \emph{Digest}
have been so wrong with their polling?

It turns out that the \emph{Literary Digest} sample was not
representative of the population. Their sampling frame of people found
in phone books, lists of magazine subscribers, and lists of country club
members were more affluent and tended to vote Republican. As such, their
sampling frame was inherently skewed in Landon's favor. The
\emph{Literary Digest} completely overlooked the lion's share of voters
who were still suffering through the Great Depression. Furthermore, they
had a dismal response rate (about 24\%); who knows how the other
non-respondents would have polled? The \emph{Digest} folded just 18
months after this disaster.

At the same time, George Gallup, a rising statistician, also made
predictions about the 1936 elections. Despite having a smaller sample
size of ``only'' 50,000 (this is still more than necessary; more when we
cover the Central Limit Theorem), his estimate that 56\% of voters would
choose Roosevelt was much closer to the actual result (61\%). Gallup
also predicted the \emph{Digest}'s prediction within 1\% with a sample
size of only 3000 people by anticipating the \emph{Digest}'s affluent
sampling frame and subsampling those individuals.

So what's the moral of the story? Samples, while convenient, are subject
to chance error and \textbf{bias}. Election polling, in particular, can
involve many sources of bias. To name a few:

\begin{itemize}
\tightlist
\item
  \textbf{Selection bias} systematically excludes (or favors) particular
  groups.

  \begin{itemize}
  \tightlist
  \item
    Example: the Literary Digest poll excludes people not in phone
    books.
  \item
    How to avoid: Examine the sampling frame and the method of sampling.
  \end{itemize}
\item
  \textbf{Response bias} occurs because people don't always respond
  truthfully. Survey designers pay special detail to the nature and
  wording of questions to avoid this type of bias.

  \begin{itemize}
  \tightlist
  \item
    Example: Illegal immigrants might not answer truthfully when asked
    citizenship questions on the census survey.
  \item
    How to avoid: Examine the nature of questions and the method of
    surveying. Randomized response - flip a coin and answer yes if heads
    or answer truthfully if tails.
  \end{itemize}
\item
  \textbf{Non-response bias} occurs because people don't always respond
  to survey requests, which can skew responses.

  \begin{itemize}
  \tightlist
  \item
    Example: Only 2.4m out of 10m people responded to the \emph{Literary
    Digest}'s poll.
  \item
    How to avoid: Keep surveys short, and be persistent.
  \end{itemize}
\end{itemize}

\textbf{Randomized Response}

Suppose you want to ask someone a sensitive question: ``Have you ever
cheated on an exam?'' An individual may be embarrassed or afraid to
answer truthfully and might lie or not answer the question. One solution
is to leverage a randomized response:

First, you can ask the individual to secretly flip a fair coin; you (the
surveyor) \emph{don't} know the outcome of the coin flip.

Then, you ask them to \textbf{answer ``Yes''} if the coin landed heads
and to \textbf{answer truthfully} if the coin landed tails.

The surveyor doesn't know if the \textbf{``Yes''} means that the
\textbf{person cheated} or if it means that the \textbf{coin landed
heads}. The individual's sensitive information remains secret. However,
if the response is \textbf{``No''}, then the surveyor knows the
\textbf{individual didn't cheat}. We assume the individual is
comfortable revealing this information.

Generally, we can assume that the coin lands heads 50\% of the time,
masking the remaining 50\% of the ``No'' answers. We can therefore
\textbf{double} the proportion of ``No'' answers to estimate the
\textbf{true} fraction of ``No'' answers.

\textbf{Election Polls}

Today, the \emph{Gallup Poll} is one of the leading polls for election
results. The many sources of biases -- who responds to polls? Do voters
tell the truth? How can we predict turnout? -- still remain, but the
\emph{Gallup Poll} uses several tactics to mitigate them. Within their
sampling frame of ``civilian, non-institutionalized population'' of
adults in telephone households in continental U.S., they use random
digit dialing to include both listed/unlisted phone numbers and to avoid
selection bias. Additionally, they use a within-household selection
process to randomly select households with one or more adults. If no one
answers, re-call multiple times to avoid non-response bias.

\hypertarget{probability-samples}{%
\section{Probability Samples}\label{probability-samples}}

When sampling, it is essential to focus on the quality of the sample
rather than the quantity of the sample. A huge sample size does not fix
a bad sampling method. Our main goal is to gather a sample that is
representative of the population it came from. In this section, we'll
explore the different types of sampling and their pros and cons.

A \textbf{convenience sample} is whatever you can get ahold of; this
type of sampling is \emph{non-random}. Note that haphazard sampling is
not necessarily random sampling; there are many potential sources of
bias.

In a \textbf{probability sample}, we provide the \textbf{chance} that
any specified \textbf{set} of individuals will be in the sample
(individuals in the population can have different chances of being
selected; they don't all have to be uniform), and we sample at random
based off this known chance. For this reason, probability samples are
also called \textbf{random samples}. The randomness provides a few
benefits:

\begin{itemize}
\tightlist
\item
  Because we know the source probabilities, we can \textbf{measure the
  errors}.
\item
  Sampling at random gives us a more representative sample of the
  population, which \textbf{reduces bias}. (Note: this is only the case
  when the probability distribution we're sampling from is accurate.
  Random samples using ``bad'' or inaccurate distributions can produce
  biased estimates of population quantities.)
\item
  Probability samples allow us to \textbf{estimate} the \textbf{bias}
  and \textbf{chance error}, which helps us \textbf{quantify
  uncertainty} (more in a future lecture).
\end{itemize}

The real world is usually more complicated, and we often don't know the
initial probabilities. For example, we do not generally know the
probability that a given bacterium is in a microbiome sample or whether
people will answer when Gallup calls landlines. That being said, still
we try to model probability sampling to the best of our ability even
when the sampling or measurement process is not fully under our control.

A few common random sampling schemes:

\begin{itemize}
\tightlist
\item
  A \textbf{uniform random sample with replacement} is a sample drawn
  \textbf{uniformly} at random \textbf{with} replacement.

  \begin{itemize}
  \tightlist
  \item
    Random doesn't always mean ``uniformly at random,'' but in this
    specific context, it does.
  \item
    Some individuals in the population might get picked more than once.
  \end{itemize}
\item
  A \textbf{simple random sample (SRS)} is a sample drawn
  \textbf{uniformly} at random \textbf{without} replacement.

  \begin{itemize}
  \tightlist
  \item
    Every individual (and subset of individuals) has the same chance of
    being selected from the sampling frame.
  \item
    Every pair has the same chance as every other pair.
  \item
    Every triple has the same chance as every other triple.
  \item
    And so on.
  \end{itemize}
\item
  A \textbf{stratified random sample}, where random sampling is
  performed on strata (specific groups), and the groups together compose
  a sample.
\end{itemize}

\hypertarget{example-scheme-1-probability-sample}{%
\subsection{Example Scheme 1: Probability
Sample}\label{example-scheme-1-probability-sample}}

Suppose we have 3 TA's (\textbf{A}rman, \textbf{B}oyu,
\textbf{C}harlie): I decide to sample 2 of them as follows:

\begin{itemize}
\tightlist
\item
  I choose A with probability 1.0
\item
  I choose either B or C, each with a probability of 0.5.
\end{itemize}

We can list all the possible outcomes and their respective probabilities
in a table:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Outcome & Probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\{A, B\} & 0.5 \\
\{A, C\} & 0.5 \\
\{B, C\} & 0 \\
\end{longtable}

This is a \textbf{probability sample} (though not a great one). Of the 3
people in my population, I know the chance of getting each subset.
Suppose I'm measuring the average distance TAs live from campus.

\begin{itemize}
\tightlist
\item
  This scheme does not see the entire population!
\item
  My estimate using the single sample I take has some chance error
  depending on if I see AB or AC.
\item
  This scheme is biased towards A's response.
\end{itemize}

\hypertarget{example-scheme-2-simple-random-sample}{%
\subsection{Example Scheme 2: Simple Random
Sample}\label{example-scheme-2-simple-random-sample}}

Consider the following sampling scheme:

\begin{itemize}
\tightlist
\item
  A class roster has 1100 students listed alphabetically.
\item
  Pick one of the first 10 students on the list at random (e.g.~Student
  8).
\item
  To create your sample, take that student and every 10th student listed
  after that (e.g.~Students 8, 18, 28, 38, etc.).
\end{itemize}

Is this a probability sample?

Yes. For a sample {[}n, n + 10, n + 20, \ldots, n + 1090{]}, where 1
\textless= n \textless= 10, the probability of that sample is 1/10.
Otherwise, the probability is 0.

Only 10 possible samples!

Does each student have the same probability of being selected?

Yes. Each student is chosen with a probability of 1/10.

Is this a simple random sample?

No.~The chance of selecting (8, 18) is 1/10; the chance of selecting (8,
9) is 0.

\hypertarget{demo-barbie-v.-oppenheimer}{%
\subsection{Demo: Barbie v.
Oppenheimer}\label{demo-barbie-v.-oppenheimer}}

We are trying to collect a sample from Berkeley residents to predict the
which one of Barbie and Oppenheimer would perform better on their
opening day, July 21st.

First, let's grab a dataset that has every single resident in Berkeley
(this is a fake dataset) and which movie they \textbf{actually} watched
on July 21st.

Let's load in the \texttt{movie.csv} table. We can assume that:

\begin{itemize}
\tightlist
\item
  \texttt{is\_male} is a boolean that indicates if a resident identifies
  as male.
\item
  There are only two movies they can watch on July 21st: Barbie and
  Oppenheimer.
\item
  Every resident watches a movie (either Barbie or Oppenheimer) on July
  21st.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{\textquotesingle{}darkgrid\textquotesingle{}}\NormalTok{, font\_scale }\OperatorTok{=} \FloatTok{1.5}\NormalTok{,}
\NormalTok{              rc}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}figure.figsize\textquotesingle{}}\NormalTok{:(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{)\})}

\NormalTok{rng }\OperatorTok{=}\NormalTok{ np.random.default\_rng()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movie }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/movie.csv"}\NormalTok{)}

\CommentTok{\# create a 1/0 int that indicates Barbie vote}
\NormalTok{movie[}\StringTok{\textquotesingle{}barbie\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ (movie[}\StringTok{\textquotesingle{}movie\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \StringTok{\textquotesingle{}Barbie\textquotesingle{}}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}
\NormalTok{movie.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrllr}
\toprule
{} &  age &  is\_male &        movie &  barbie \\
\midrule
0 &   35 &    False &       Barbie &       1 \\
1 &   42 &     True &  Oppenheimer &       0 \\
2 &   55 &    False &       Barbie &       1 \\
3 &   77 &     True &  Oppenheimer &       0 \\
4 &   31 &    False &       Barbie &       1 \\
\bottomrule
\end{tabular}

What fraction of Berkeley residents chose Barbie?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actual\_barbie }\OperatorTok{=}\NormalTok{ np.mean(movie[}\StringTok{"barbie"}\NormalTok{])}
\NormalTok{actual\_barbie}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.5302792307692308
\end{verbatim}

This is the \textbf{actual outcome} of the competition. Based on this
result, Barbie would win. How did our sample of retirees do?

\hypertarget{convenience-sample-retirees}{%
\subsubsection{Convenience Sample:
Retirees}\label{convenience-sample-retirees}}

Let's take a convenience sample of people who have retired
(\textgreater= 65 years old). What proportion of them went to see Barbie
instead of Oppenheimer?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{convenience\_sample }\OperatorTok{=}\NormalTok{ movie[movie[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{] }\OperatorTok{\textgreater{}=} \DecValTok{65}\NormalTok{] }\CommentTok{\# take a convenience sample of retirees}
\NormalTok{np.mean(convenience\_sample[}\StringTok{"barbie"}\NormalTok{]) }\CommentTok{\# what proportion of them saw Barbie? }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.3744755089093924
\end{verbatim}

Based on this result, we would have predicted that Oppenheimer would
win! What happened? Is it possible that our sample is too small or
noisy?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# what\textquotesingle{}s the size of our sample? }
\BuiltInTok{len}\NormalTok{(convenience\_sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
359396
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# what proportion of our data is in the convenience sample? }
\BuiltInTok{len}\NormalTok{(convenience\_sample)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(movie)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.27645846153846154
\end{verbatim}

Seems like our sample is rather large (roughly 360,000 people), so the
error is likely not due to solely to chance.

\hypertarget{check-for-bias}{%
\subsubsection{Check for Bias}\label{check-for-bias}}

Let us aggregate all choices by age and visualize the fraction of Barbie
views, split by gender.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{votes\_by\_barbie }\OperatorTok{=}\NormalTok{ movie.groupby([}\StringTok{"age"}\NormalTok{,}\StringTok{"is\_male"}\NormalTok{]).agg(}\StringTok{"mean"}\NormalTok{, numeric\_only}\OperatorTok{=}\VariableTok{True}\NormalTok{).reset\_index()}
\NormalTok{votes\_by\_barbie.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrlr}
\toprule
{} &  age &  is\_male &    barbie \\
\midrule
0 &   18 &    False &  0.819594 \\
1 &   18 &     True &  0.667001 \\
2 &   19 &    False &  0.812214 \\
3 &   19 &     True &  0.661252 \\
4 &   20 &    False &  0.805281 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A common matplotlib/seaborn pattern: create the figure and axes object, pass ax}
\CommentTok{\# to seaborn for drawing into, and later fine{-}tune the figure via ax.}
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}\OperatorTok{;}

\NormalTok{red\_blue }\OperatorTok{=}\NormalTok{ [}\StringTok{"\#bf1518"}\NormalTok{, }\StringTok{"\#397eb7"}\NormalTok{]}
\ControlFlowTok{with}\NormalTok{ sns.color\_palette(red\_blue):}
\NormalTok{    sns.pointplot(data}\OperatorTok{=}\NormalTok{votes\_by\_barbie, x }\OperatorTok{=} \StringTok{"age"}\NormalTok{, y }\OperatorTok{=} \StringTok{"barbie"}\NormalTok{, hue }\OperatorTok{=} \StringTok{"is\_male"}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax)}

\NormalTok{new\_ticks }\OperatorTok{=}\NormalTok{ [i.get\_text() }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ ax.get\_xticklabels()]}
\NormalTok{ax.set\_xticks(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(new\_ticks), }\DecValTok{10}\NormalTok{), new\_ticks[::}\DecValTok{10}\NormalTok{])}
\NormalTok{ax.set\_title(}\StringTok{"Preferences by Demographics"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sampling/sampling_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  We see that retirees (in Berkeley) tend to watch Oppenheimer.
\item
  We also see that residents who identify as non-male tend to prefer
  Barbie.
\end{itemize}

\hypertarget{simple-random-sample}{%
\subsubsection{Simple Random Sample}\label{simple-random-sample}}

Suppose we took a simple random sample (SRS) of the same size as our
retiree sample:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(convenience\_sample)}
\NormalTok{random\_sample }\OperatorTok{=}\NormalTok{ movie.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{) }\CommentTok{\#\# By default, replace = False}
\NormalTok{np.mean(random\_sample[}\StringTok{"barbie"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.5294466271188327
\end{verbatim}

This is very close to the actual vote of 0.5302792307692308!

It turns out that we can get similar results with a \textbf{much smaller
sample size}, say, 800:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{800}
\NormalTok{random\_sample }\OperatorTok{=}\NormalTok{ movie.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{)}

\CommentTok{\# Compute the sample average and the resulting relative error}
\NormalTok{sample\_barbie }\OperatorTok{=}\NormalTok{ np.mean(random\_sample[}\StringTok{"barbie"}\NormalTok{])}
\NormalTok{err }\OperatorTok{=} \BuiltInTok{abs}\NormalTok{(sample\_barbie}\OperatorTok{{-}}\NormalTok{actual\_barbie)}\OperatorTok{/}\NormalTok{actual\_barbie}

\CommentTok{\# We can print output with Markdown formatting too...}
\ImportTok{from}\NormalTok{ IPython.display }\ImportTok{import}\NormalTok{ Markdown}
\NormalTok{Markdown(}\SpecialStringTok{f"**Actual** = }\SpecialCharTok{\{}\NormalTok{actual\_barbie}\SpecialCharTok{:.4f\}}\SpecialStringTok{, **Sample** = }\SpecialCharTok{\{}\NormalTok{sample\_barbie}\SpecialCharTok{:.4f\}}\SpecialStringTok{, "}
         \SpecialStringTok{f"**Err** = }\SpecialCharTok{\{}\DecValTok{100}\OperatorTok{*}\NormalTok{err}\SpecialCharTok{:.2f\}}\SpecialStringTok{\%."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Actual} = 0.5303, \textbf{Sample} = 0.5425, \textbf{Err} =
2.30\%.

We'll learn how to choose this number when we (re)learn the Central
Limit Theorem later in the semester.

\hypertarget{quantifying-chance-error}{%
\subsubsection{Quantifying Chance
Error}\label{quantifying-chance-error}}

In our SRS of size 800, what would be our chance error?

Let's simulate 1000 versions of taking the 800-sized SRS from before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nrep }\OperatorTok{=} \DecValTok{1000}   \CommentTok{\# number of simulations}
\NormalTok{n }\OperatorTok{=} \DecValTok{800}       \CommentTok{\# size of our sample}
\NormalTok{poll\_result }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, nrep):}
\NormalTok{    random\_sample }\OperatorTok{=}\NormalTok{ movie.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{    poll\_result.append(np.mean(random\_sample[}\StringTok{"barbie"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots()}
\NormalTok{sns.histplot(poll\_result, stat}\OperatorTok{=}\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax)}
\NormalTok{ax.axvline(actual\_barbie, color}\OperatorTok{=}\StringTok{"orange"}\NormalTok{, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sampling/sampling_files/figure-pdf/cell-13-output-1.pdf}

}

\end{figure}

What fraction of these simulated samples would have predicted Barbie?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poll\_result }\OperatorTok{=}\NormalTok{ pd.Series(poll\_result)}
\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(poll\_result }\OperatorTok{\textgreater{}} \FloatTok{0.5}\NormalTok{)}\OperatorTok{/}\DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.952
\end{verbatim}

You can see the curve looks roughly Gaussian/normal. Using KDE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(poll\_result, stat}\OperatorTok{=}\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{, kde}\OperatorTok{=}\VariableTok{True}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sampling/sampling_files/figure-pdf/cell-15-output-1.pdf}

}

\end{figure}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

Understanding the sampling process is what lets us go from describing
the data to understanding the world. Without knowing / assuming
something about how the data were collected, there is no connection
between the sample and the population. Ultimately, the dataset doesn't
tell us about the world behind the data.

\bookmarksetup{startatroot}

\hypertarget{introduction-to-modeling}{%
\chapter{Introduction to Modeling}\label{introduction-to-modeling}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Understand what models are and how to carry out the four-step modeling
  process.
\item
  Define the concept of loss and gain familiarity with \(L_1\) and
  \(L_2\) loss.
\item
  Fit the Simple Linear Regression model using minimization techniques.
\end{itemize}

\end{tcolorbox}

Up until this point in the semester, we've focused on analyzing
datasets. We've looked into the early stages of the data science
lifecycle, focusing on the programming tools, visualization techniques,
and data cleaning methods needed for data analysis.

This lecture marks a shift in focus. We will move away from examining
datasets to actually \emph{using} our data to better understand the
world. Specifically, the next sequence of lectures will explore
predictive modeling: generating models to make some predictions about
the world around us. In this lecture, we'll introduce the conceptual
framework for setting up a modeling task. In the next few lectures,
we'll put this framework into practice by implementing various kinds of
models.

\hypertarget{what-is-a-model}{%
\section{What is a Model?}\label{what-is-a-model}}

A model is an \textbf{idealized representation} of a system. A system is
a set of principles or procedures according to which something
functions. We live in a world full of systems: the procedure of turning
on a light happens according to a specific set of rules dictating the
flow of electricity. The truth behind how any event occurs is usually
complex, and many times the specifics are unknown. The workings of the
world can be viewed as its own giant procedure. Models seek to simplify
the world and distill them into workable pieces.

Example: We model the fall of an object on Earth as subject to a
constant acceleration of \(9.81 m/s^2\) due to gravity.

\begin{itemize}
\tightlist
\item
  While this describes the behavior of our system, it is merely an
  approximation.
\item
  It doesn't account for the effects of air resistance, local variations
  in gravity, etc.
\item
  In practice, it's accurate enough to be useful!
\end{itemize}

\hypertarget{reasons-for-building-models}{%
\subsection{Reasons for Building
Models}\label{reasons-for-building-models}}

Why do we want to build models? As far as data scientists and
statisticians are concerned, there are three reasons, and each implies a
different focus on modeling.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To explain complex phenomena occurring in the world we live in.
  Examples of this might be:

  \begin{itemize}
  \tightlist
  \item
    How are the parents' average height related to their children's
    average height?
  \item
    How does an object's velocity and acceleration impact how far it
    travels? (Physics: \(d = d_0 + vt + \frac{1}{2}at^2\))
  \end{itemize}

  In these cases, we care about creating models that are \emph{simple
  and interpretable}, allowing us to understand what the relationships
  between our variables are.
\item
  To make accurate predictions about unseen data. Some examples include:

  \begin{itemize}
  \tightlist
  \item
    Can we predict if an email is spam or not?
  \item
    Can we generate a one-sentence summary of this 10-page long article?
  \end{itemize}

  When making predictions, we care more about making extremely accurate
  predictions, at the cost of having an uninterpretable model. These are
  sometimes called black-box models and are common in fields like deep
  learning.
\item
  To measure the causal effects of one event on some other event. For
  example,

  \begin{itemize}
  \tightlist
  \item
    Does smoking \emph{cause} lung cancer?
  \item
    Does a job training program \emph{cause} increases in employment and
    wages?
  \end{itemize}

  This is a much harder question because most statistical tools are
  designed to infer association, not causation. We will not focus on
  this task in Data 100, but you can take other advanced classes on
  causal inference (e.g., Stat 156, Data 102) if you are intrigued!
\end{enumerate}

Most of the time, we aim to strike a balance between building
\textbf{interpretable} models and building \textbf{accurate models}.

\hypertarget{common-types-of-models}{%
\subsection{Common Types of Models}\label{common-types-of-models}}

In general, models can be split into two categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Deterministic physical (mechanistic) models: Laws that govern how the
  world works.

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Kepler\%27s_laws_of_planetary_motion\#Third_law}{Kepler's
    Third Law of Planetary Motion (1619)}: The ratio of the square of an
    object's orbital period with the cube of the semi-major axis of its
    orbit is the same for all objects orbiting the same primary.

    \begin{itemize}
    \tightlist
    \item
      \(T^2 \propto R^3\)
    \end{itemize}
  \item
    \href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Newton's
    Laws: motion and gravitation (1687)}: Newton's second law of motion
    models the relationship between the mass of an object and the force
    required to accelerate it.

    \begin{itemize}
    \tightlist
    \item
      \(F = ma\)
    \item
      \(F_g = G \frac{m_1 m_2}{r^2}\)
    \end{itemize}
  \end{itemize}
\item
  Probabilistic models: Models that attempt to understand how random
  processes evolve. These are more general and can be used to describe
  many phenomena in the real world. These models commonly make
  simplifying assumptions about the nature of the world.

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Poisson_point_process}{Poisson
    Process models}: Used to model random events that happen with some
    probability at any point in time and are strictly increasing in
    count, such as the arrival of customers at a store.
  \end{itemize}
\end{enumerate}

Note: These specific models are not in the scope of Data 100 and exist
to serve as motivation.

\hypertarget{simple-linear-regression}{%
\section{Simple Linear Regression}\label{simple-linear-regression}}

The \textbf{regression line} is the unique straight line that minimizes
the \textbf{mean squared error} of estimation among all straight lines.
As with any straight line, it can be defined by a slope and a
y-intercept:

\begin{itemize}
\tightlist
\item
  \(\text{slope} = r \cdot \frac{\text{Standard Deviation of } y}{\text{Standard Deviation of }x}\)
\item
  \(y\text{-intercept} = \text{average of }y - \text{slope}\cdot\text{average of }x\)
\item
  \(\text{regression estimate} = y\text{-intercept} + \text{slope}\cdot\text{}x\)
\item
  \(\text{residual} =\text{observed }y - \text{regression estimate}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\CommentTok{\# Set random seed for consistency }
\NormalTok{np.random.seed(}\DecValTok{43}\NormalTok{)}
\NormalTok{plt.style.use(}\StringTok{\textquotesingle{}default\textquotesingle{}}\NormalTok{) }

\CommentTok{\#Generate random noise for plotting}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \FloatTok{0.5} \OperatorTok{{-}} \DecValTok{1} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}

\CommentTok{\#plot regression line}
\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{x,y}\OperatorTok{=}\NormalTok{y)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{intro_to_modeling/intro_to_modeling_files/figure-pdf/cell-2-output-1.pdf}

}

\end{figure}

\hypertarget{notations-and-definitions}{%
\subsection{Notations and Definitions}\label{notations-and-definitions}}

For a pair of variables \(x\) and \(y\) representing our data
\(\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}\), we
denote their means/averages as \(\bar x\) and \(\bar y\) and standard
deviations as \(\sigma_x\) and \(\sigma_y\).

\hypertarget{standard-units}{%
\subsubsection{Standard Units}\label{standard-units}}

A variable is represented in standard units if the following are true:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  0 in standard units is equal to the mean (\(\bar{x}\)) in the original
  variable's units.
\item
  An increase of 1 standard unit is an increase of 1 standard deviation
  (\(\sigma_x\)) in the original variable's units.
\end{enumerate}

To convert a variable \(x_i\) into standard units, we subtract its mean
from it and divide it by its standard deviation. For example, \(x_i\) in
standard units is \(\frac{x_i - \bar x}{\sigma_x}\).

\hypertarget{correlation}{%
\subsubsection{Correlation}\label{correlation}}

The correlation (\(r\)) is the average of the product of \(x\) and
\(y\), both measured in \emph{standard units}.

\[r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i - \bar{x}}{\sigma_x})(\frac{y_i - \bar{y}}{\sigma_y})\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Correlation measures the strength of a \textbf{linear association}
  between two variables.
\item
  Correlations range between -1 and 1: \(|r| \leq 1\), with \(r=1\)
  indicating perfect linear association, and \(r=-1\) indicating perfect
  negative association. The closer \(r\) is to \(0\), the weaker the
  linear association is.
\item
  Correlation says nothing about causation and non-linear association.
  Correlation does \textbf{not} imply causation. When \(r = 0\), the two
  variables are uncorrelated. However, they could still be related
  through some non-linear relationship.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_and\_get\_corr(ax, x, y, title):}
\NormalTok{    ax.set\_xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{    ax.set\_ylim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{    ax.set\_xticks([])}
\NormalTok{    ax.set\_yticks([])}
\NormalTok{    ax.scatter(x, y, alpha }\OperatorTok{=} \FloatTok{0.73}\NormalTok{)}
\NormalTok{    r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, y)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{    ax.set\_title(title }\OperatorTok{+} \StringTok{" (corr: }\SpecialCharTok{\{\}}\StringTok{)"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(r.}\BuiltInTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)))}
    \ControlFlowTok{return}\NormalTok{ r}

\NormalTok{fig, axs }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize }\OperatorTok{=}\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\CommentTok{\# Just noise}
\NormalTok{x1, y1 }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{corr1 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], x1, y1, title }\OperatorTok{=} \StringTok{"noise"}\NormalTok{)}

\CommentTok{\# Strong linear}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y2 }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{*} \FloatTok{0.5} \OperatorTok{{-}} \DecValTok{1} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}
\NormalTok{corr2 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], x2, y2, title }\OperatorTok{=} \StringTok{"strong linear"}\NormalTok{)}

\CommentTok{\# Unequal spread}
\NormalTok{x3 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y3 }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{ x3}\OperatorTok{/}\DecValTok{3} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{)}\OperatorTok{*}\NormalTok{(x3)}\OperatorTok{/}\FloatTok{2.5}
\NormalTok{corr3 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], x3, y3, title }\OperatorTok{=} \StringTok{"strong linear"}\NormalTok{)}
\NormalTok{extent }\OperatorTok{=}\NormalTok{ axs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{].get\_window\_extent().transformed(fig.dpi\_scale\_trans.inverted())}

\CommentTok{\# Strong non{-}linear}
\NormalTok{x4 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y4 }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{np.sin(x3 }\OperatorTok{{-}} \FloatTok{1.5}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}
\NormalTok{corr4 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], x4, y4, title }\OperatorTok{=} \StringTok{"strong non{-}linear"}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{intro_to_modeling/intro_to_modeling_files/figure-pdf/cell-3-output-1.pdf}

}

\end{figure}

\hypertarget{alternate-form}{%
\subsection{Alternate Form}\label{alternate-form}}

When the variables \(y\) and \(x\) are measured in \emph{standard
units}, the regression line for predicting \(y\) based on \(x\) has
slope \(r\) and passes through the origin.

\[\hat{y}_{su} = r \cdot x_{su}\]

\includegraphics{intro_to_modeling/images/reg_line_1.png}

\begin{itemize}
\tightlist
\item
  In the original units, this becomes
\end{itemize}

\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]

\includegraphics{intro_to_modeling/images/reg_line_2.png}

\hypertarget{derivation}{%
\subsection{Derivation}\label{derivation}}

Starting from the top, we have our claimed form of the regression line,
and we want to show that it is equivalent to the optimal linear
regression line: \(\hat{y} = \hat{a} + \hat{b}x\).

Recall:

\begin{itemize}
\tightlist
\item
  \(\hat{b} = r \cdot \frac{\text{Standard Deviation of }y}{\text{Standard Deviation of }x}\)
\item
  \(\hat{a} = \text{average of }y - \text{slope}\cdot\text{average of }x\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colback=white, rightrule=.15mm, opacityback=0, breakable, leftrule=.75mm, arc=.35mm, toprule=.15mm, bottomrule=.15mm, left=2mm]

Proof:

\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]

Multiply by \(\sigma_y\), and add \(\bar{y}\) on both sides.

\[\hat{y} = \sigma_y \cdot r \cdot \frac{x - \bar{x}}{\sigma_x} + \bar{y}\]

Distribute coefficient \(\sigma_{y}\cdot r\) to the
\(\frac{x - \bar{x}}{\sigma_x}\) term

\[\hat{y} = (\frac{r\sigma_y}{\sigma_x} ) \cdot x + (\bar{y} - (\frac{r\sigma_y}{\sigma_x} ) \bar{x})\]

We now see that we have a line that matches our claim:

\begin{itemize}
\tightlist
\item
  slope:
  \(r\cdot\frac{\text{SD of y}}{\text{SD of x}} = r\cdot\frac{\sigma_y}{\sigma_x}\)
\item
  intercept: \(\bar{y} - \text{slope}\cdot \bar{x}\)
\end{itemize}

Note that the error for the i-th datapoint is: \(e_i = y_i - \hat{y_i}\)

\end{tcolorbox}

\hypertarget{the-modeling-process}{%
\section{The Modeling Process}\label{the-modeling-process}}

At a high level, a model is a way of representing a system. In Data 100,
we'll treat a model as some mathematical rule we use to describe the
relationship between variables.

What variables are we modeling? Typically, we use a subset of the
variables in our sample of collected data to model another variable in
this data. To put this more formally, say we have the following dataset
\(\mathcal{D}\):

\[\mathcal{D} = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\]

Each pair of values \((x_i, y_i)\) represents a datapoint. In a modeling
setting, we call these \textbf{observations}. \(y_i\) is the dependent
variable we are trying to model, also called an \textbf{output} or
\textbf{response}. \(x_i\) is the independent variable inputted into the
model to make predictions, also known as a \textbf{feature}.

Our goal in modeling is to use the observed data \(\mathcal{D}\) to
predict the output variable \(y_i\). We denote each prediction as
\(\hat{y}_i\) (read: ``y hat sub i'').

How do we generate these predictions? Some examples of models we'll
encounter in the next few lectures are given below:

\[\hat{y}_i = \theta\] \[\hat{y}_i = \theta_0 + \theta_1 x_i\]

The examples above are known as \textbf{parametric models}. They relate
the collected data, \(x_i\), to the prediction we make, \(\hat{y}_i\). A
few parameters (\(\theta\), \(\theta_0\), \(\theta_1\)) are used to
describe the relationship between \(x_i\) and \(\hat{y}_i\).

Notice that we don't immediately know the values of these parameters.
While the features, \(x_i\), are taken from our observed data, we need
to decide what values to give \(\theta\), \(\theta_0\), and \(\theta_1\)
ourselves. This is the heart of parametric modeling: \emph{what
parameter values should we choose so our model makes the best possible
predictions?}

To choose our model parameters, we'll work through the \textbf{modeling
process}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: how should we represent the world?
\item
  Choose a loss function: how do we quantify prediction error?
\item
  Fit the model: how do we choose the best parameters of our model given
  our data?
\item
  Evaluate model performance: how do we evaluate whether this process
  gave rise to a good model?
\end{enumerate}

\hypertarget{choosing-a-model}{%
\section{Choosing a Model}\label{choosing-a-model}}

Our first step is choosing a model: defining the mathematical rule that
describes the relationship between the features, \(x_i\), and
predictions \(\hat{y}_i\).

In
\href{https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html}{Data
8}, you learned about the \textbf{Simple Linear Regression (SLR) model}.
You learned that the model takes the form: \[\hat{y}_i = a + bx_i\]

In Data 100, we'll use slightly different notation: we will replace
\(a\) with \(\theta_0\) and \(b\) with \(\theta_1\). This will allow us
to use the same notation when we explore more complex models later on in
the course.

\[\hat{y}_i = \theta_0 + \theta_1 x_i\]

The parameters of the SLR model are \(\theta_0\), also called the
intercept term, and \(\theta_1\), also called the slope term. To create
an effective model, we want to choose values for \(\theta_0\) and
\(\theta_1\) that most accurately predict the output variable. The
``best'' fitting model parameters are given the special names:
\(\hat{\theta}_0\) and \(\hat{\theta}_1\); they are the specific
parameter values that allow our model to generate the best possible
predictions.

In Data 8, you learned that the best SLR model parameters are:
\[\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} \qquad \qquad \hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]

A quick reminder on notation:

\begin{itemize}
\tightlist
\item
  \(\bar{y}\) and \(\bar{x}\) indicate the mean value of \(y\) and
  \(x\), respectively
\item
  \(\sigma_y\) and \(\sigma_x\) indicate the standard deviations of
  \(y\) and \(x\)
\item
  \(r\) is the
  \href{https://inferentialthinking.com/chapters/15/1/Correlation.html\#the-correlation-coefficient}{correlation
  coefficient}, defined as the average of the product of \(x\) and \(y\)
  measured in standard units:
  \(\frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y})\)
\end{itemize}

In Data 100, we want to understand \emph{how} to derive these best model
coefficients. To do so, we'll introduce the concept of a loss function.

\hypertarget{choosing-a-loss-function}{%
\section{Choosing a Loss Function}\label{choosing-a-loss-function}}

We've talked about the idea of creating the ``best'' possible
predictions. This begs the question: how do we decide how ``good'' or
``bad'' our model's predictions are?

A \textbf{loss function} characterizes the cost, error, or fit resulting
from a particular choice of model or model parameters. This function,
\(L(y, \hat{y})\), quantifies how ``bad'' or ``far off'' a single
prediction by our model is from a true, observed value in our collected
data.

The choice of loss function for a particular model will affect the
accuracy and computational cost of estimation, and it'll also depend on
the estimation task at hand. For example,

\begin{itemize}
\tightlist
\item
  Are outputs quantitative or qualitative?
\item
  Do outliers matter?
\item
  Are all errors equally costly? (e.g., a false negative on a cancer
  test is arguably more dangerous than a false positive)
\end{itemize}

Regardless of the specific function used, a loss function should follow
two basic principles:

\begin{itemize}
\tightlist
\item
  If the prediction \(\hat{y}_i\) is \emph{close} to the actual value
  \(y_i\), loss should be low.
\item
  If the prediction \(\hat{y}_i\) is \emph{far} from the actual value
  \(y_i\), loss should be high.
\end{itemize}

Two common choices of loss function are squared loss and absolute loss.

\textbf{Squared loss}, also known as \textbf{L2 loss}, computes loss as
the square of the difference between the observed \(y_i\) and predicted
\(\hat{y}_i\): \[L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2\]

\textbf{Absolute loss}, also known as \textbf{L1 loss}, computes loss as
the absolute difference between the observed \(y_i\) and predicted
\(\hat{y}_i\): \[L(y_i, \hat{y}_i) = |y_i - \hat{y}_i|\]

L1 and L2 loss give us a tool for quantifying our model's performance on
a single data point. This is a good start, but ideally, we want to
understand how our model performs across our \emph{entire} dataset. A
natural way to do this is to compute the average loss across all data
points in the dataset. This is known as the \textbf{cost function},
\(\hat{R}(\theta)\):
\[\hat{R}(\theta) = \frac{1}{n} \sum^n_{i=1} L(y_i, \hat{y}_i)\]

The cost function has many names in the statistics literature. You may
also encounter the terms:

\begin{itemize}
\tightlist
\item
  Empirical risk (this is why we give the cost function the name \(R\))
\item
  Error function
\item
  Average loss
\end{itemize}

We can substitute our L1 and L2 loss into the cost function definition.
The \textbf{Mean Squared Error (MSE)} is the average squared loss across
a dataset: \[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

The \textbf{Mean Absolute Error (MAE)} is the average absolute loss
across a dataset:
\[\text{MAE}= \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\]

\hypertarget{fitting-the-model}{%
\section{Fitting the Model}\label{fitting-the-model}}

Now that we've established the concept of a loss function, we can return
to our original goal of choosing model parameters. Specifically, we want
to choose the best set of model parameters that will minimize the
model's cost on our dataset. This process is called fitting the model.

We know from calculus that a function is minimized when (1) its first
derivative is equal to zero and (2) its second derivative is positive.
We often call the function being minimized the \textbf{objective
function} (our objective is to find its minimum).

To find the optimal model parameter, we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the derivative of the cost function with respect to that
  parameter
\item
  Set the derivative equal to 0
\item
  Solve for the parameter
\end{enumerate}

We repeat this process for each parameter present in the model. For now,
we'll disregard the second derivative condition.

To help us make sense of this process, let's put it into action by
deriving the optimal model parameters for simple linear regression using
the mean squared error as our cost function. Remember: although the
notation may look tricky, all we are doing is following the three steps
above!

Step 1: take the derivative of the cost function with respect to each
model parameter. We substitute the SLR model,
\(\hat{y}_i = \theta_0+\theta_1 x_i\), into the definition of MSE above
and differentiate with respect to \(\theta_0\) and \(\theta_1\).
\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)^2\]

\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} y_i - \theta_0 - \theta_1 x_i\]

\[\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)x_i\]

Let's walk through these derivations in more depth, starting with the
derivative of MSE with respect to \(\theta_0\).

Given our MSE above, we know that:
\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{\partial}{\partial \theta_0} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]

Noting that the derivative of sum is equivalent to the sum of
derivatives, this then becomes:
\[ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_0} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]

We can then apply the chain rule.

\[ = \frac{1}{n} \sum_{i=1}^{n} 2 \cdot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-1)\]

Finally, we can simplify the constants, leaving us with our answer.

\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n}{(y_i - \theta_0 - \theta_1 x_i)}\]

Following the same procedure, we can take the derivative of MSE with
respect to \(\theta_1\).

\[\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{\partial}{\partial \theta_1} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]

\[ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_1} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]

\[ = \frac{1}{n} \sum_{i=1}^{n} 2 \dot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-x_i)\]

\[= \frac{-2}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}x_i\]

Step 2: set the derivatives equal to 0. After simplifying terms, this
produces two \textbf{estimating equations}. The best set of model
parameters \((\hat{\theta}_0, \hat{\theta}_1)\) \emph{must} satisfy
these two optimality conditions.
\[0 = \frac{-2}{n} \sum_{i=1}^{n} y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} y_i - \hat{y}_i = 0\]
\[0 = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i)x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)x_i = 0\]

Step 3: solve the estimating equations to compute estimates for
\(\hat{\theta}_0\) and \(\hat{\theta}_1\).

Taking the first equation gives the estimate of \(\hat{\theta}_0\):
\[\frac{1}{n} \sum_{i=1}^n y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i = 0 \]

\[\left(\frac{1}{n} \sum_{i=1}^n y_i \right) - \hat{\theta}_0 - \hat{\theta}_1\left(\frac{1}{n} \sum_{i=1}^n x_i \right) = 0\]

\[ \hat{\theta}_0 = \bar{y} - \hat{\theta}_1 \bar{x}\]

With a bit more maneuvering, the second equation gives the estimate of
\(\hat{\theta}_1\). Start by multiplying the first estimating equation
by \(\bar{x}\), then subtracting the result from the second estimating
equation.

\[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)x_i - \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)\bar{x} = 0 \]

\[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)(x_i - \bar{x}) = 0 \]

Next, plug in
\(\hat{y}_i = \hat{\theta}_0 + \hat{\theta}_1 x_i = \bar{y} + \hat{\theta}_1(x_i - \bar{x})\):

\[\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y} - \hat{\theta}_1(x - \bar{x}))(x_i - \bar{x}) = 0 \]

\[\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x}) = \hat{\theta}_1 \times \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]

By using the definition of correlation
\(\left(r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y}) \right)\)
and standard deviation
\(\left(\sigma_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \right)\),
we can conclude:
\[r \sigma_x \sigma_y = \hat{\theta}_1 \times \sigma_x^2\]
\[\hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]

Just as was given in Data 8!

Remember, this derivation found the optimal model parameters for SLR
when using the MSE cost function. If we had used a different model or
different loss function, we likely would have found different values for
the best model parameters. However, regardless of the model and loss
used, we can \emph{always} follow these three steps to fit the model.

\bookmarksetup{startatroot}

\hypertarget{constant-model-loss-and-transformations}{%
\chapter{Constant Model, Loss, and
Transformations}\label{constant-model-loss-and-transformations}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Derive the optimal model parameters for the constant model under MSE
  and MAE cost functions.
\item
  Evaluate the differences between MSE and MAE risk.
\item
  Understand the need for linearization of variables and apply the
  Tukey-Mosteller bulge diagram for transformations.
\end{itemize}

\end{tcolorbox}

Last time, we introduced the modeling process. We set up a framework to
predict target variables as functions of our features, following a set
workflow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model - how should we represent the world?
\item
  Choose a loss function - how do we quantify prediction error?
\item
  Fit the model - how do we choose the best parameter of our model given
  our data?
\item
  Evaluate model performance - how do we evaluate whether this process
  gave rise to a good model?
\end{enumerate}

To illustrate this process, we derived the optimal model parameters
under simple linear regression (SLR) with mean squared error (MSE) as
the cost function. A summary of the SLR modeling process is shown below:

In this lecture, we'll dive deeper into step 4 - evaluating model
performance - using SLR as an example. Additionally, we'll also explore
the modeling process with new models, continue familiarizing ourselves
with the modeling process by finding the best model parameters under a
new model, the constant model, and test out two different loss functions
to understand how our choice of loss influences model design. Later on,
we'll consider what happens when a linear model isn't the best choice to
capture trends in our data and what solutions there are to create better
models.

Before we get into Step 4, let's quickly review some important
terminology.

\hypertarget{prediction-vs.-estimation}{%
\subsection{Prediction vs.~Estimation}\label{prediction-vs.-estimation}}

The terms prediction and estimation are often used somewhat
interchangeably, but there is a subtle difference between them.
\textbf{Estimation} is the task of using data to calculate model
parameters. \textbf{Prediction} is the task of using a model to predict
outputs for unseen data. In our simple linear regression model,

\[\hat{y} = \hat{\theta_0} + \hat{\theta_1}\]

we \textbf{estimate} the parameters by minimizing average loss; then, we
\textbf{predict} using these estimations. \textbf{Least Squares
Estimation} is when we choose the parameters that minimize MSE.

\hypertarget{step-4-evaluating-the-slr-model}{%
\section{Step 4: Evaluating the SLR
Model}\label{step-4-evaluating-the-slr-model}}

Now that we've explored the mathematics behind (1) choosing a model, (2)
choosing a loss function, and (3) fitting the model, we're left with one
final question -- how ``good'' are the predictions made by this ``best''
fitted model? To determine this, we can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Visualize data and compute statistics:

  \begin{itemize}
  \tightlist
  \item
    Plot the original data.
  \item
    Compute each column's mean and standard deviation. If the mean and
    standard deviation of our predictions are close to those of the
    original observed \(y_i\)'s, we might be inclined to say that our
    model has done well.
  \item
    (If we're fitting a linear model) Compute the correlation \(r\). A
    large magnitude for the correlation coefficient between the feature
    and response variables could also indicate that our model has done
    well.
  \end{itemize}
\item
  Performance metrics:

  \begin{itemize}
  \tightlist
  \item
    We can take the \textbf{Root Mean Squared Error (RMSE)}.

    \begin{itemize}
    \tightlist
    \item
      It's the square root of the mean squared error (MSE), which is the
      average loss that we've been minimizing to determine optimal model
      parameters.
    \item
      RMSE is in the same units as \(y\).
    \item
      A lower RMSE indicates more ``accurate'' predictions, as we have a
      lower ``average loss'' across the data.
    \end{itemize}
  \end{itemize}

  \[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]
\item
  Visualization:

  \begin{itemize}
  \tightlist
  \item
    Look at the residual plot of \(e_i = y_i - \hat{y_i}\) to visualize
    the difference between actual and predicted values. The good
    residual plot should not show any pattern between input/features
    \(x_i\) and residual values \(e_i\).
  \end{itemize}
\end{enumerate}

To illustrate this process, let's take a look at \textbf{Anscombe's
quartet}.

\hypertarget{four-mysterious-datasets-anscombes-quartet}{%
\subsection{Four Mysterious Datasets (Anscombe's
quartet)}\label{four-mysterious-datasets-anscombes-quartet}}

Let's take a look at four different datasets.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\OperatorTok{\%}\NormalTok{matplotlib inline}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ itertools}
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Big font helper}
\KeywordTok{def}\NormalTok{ adjust\_fontsize(size}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
\NormalTok{    SMALL\_SIZE }\OperatorTok{=} \DecValTok{8}
\NormalTok{    MEDIUM\_SIZE }\OperatorTok{=} \DecValTok{10}
\NormalTok{    BIGGER\_SIZE }\OperatorTok{=} \DecValTok{12}
    \ControlFlowTok{if}\NormalTok{ size }\OperatorTok{!=} \VariableTok{None}\NormalTok{:}
\NormalTok{        SMALL\_SIZE }\OperatorTok{=}\NormalTok{ MEDIUM\_SIZE }\OperatorTok{=}\NormalTok{ BIGGER\_SIZE }\OperatorTok{=}\NormalTok{ size}

\NormalTok{    plt.rc(}\StringTok{"font"}\NormalTok{, size}\OperatorTok{=}\NormalTok{SMALL\_SIZE)  }\CommentTok{\# controls default text sizes}
\NormalTok{    plt.rc(}\StringTok{"axes"}\NormalTok{, titlesize}\OperatorTok{=}\NormalTok{SMALL\_SIZE)  }\CommentTok{\# fontsize of the axes title}
\NormalTok{    plt.rc(}\StringTok{"axes"}\NormalTok{, labelsize}\OperatorTok{=}\NormalTok{MEDIUM\_SIZE)  }\CommentTok{\# fontsize of the x and y labels}
\NormalTok{    plt.rc(}\StringTok{"xtick"}\NormalTok{, labelsize}\OperatorTok{=}\NormalTok{SMALL\_SIZE)  }\CommentTok{\# fontsize of the tick labels}
\NormalTok{    plt.rc(}\StringTok{"ytick"}\NormalTok{, labelsize}\OperatorTok{=}\NormalTok{SMALL\_SIZE)  }\CommentTok{\# fontsize of the tick labels}
\NormalTok{    plt.rc(}\StringTok{"legend"}\NormalTok{, fontsize}\OperatorTok{=}\NormalTok{SMALL\_SIZE)  }\CommentTok{\# legend fontsize}
\NormalTok{    plt.rc(}\StringTok{"figure"}\NormalTok{, titlesize}\OperatorTok{=}\NormalTok{BIGGER\_SIZE)  }\CommentTok{\# fontsize of the figure title}


\CommentTok{\# Helper functions}
\KeywordTok{def}\NormalTok{ standard\_units(x):}
    \ControlFlowTok{return}\NormalTok{ (x }\OperatorTok{{-}}\NormalTok{ np.mean(x)) }\OperatorTok{/}\NormalTok{ np.std(x)}


\KeywordTok{def}\NormalTok{ correlation(x, y):}
    \ControlFlowTok{return}\NormalTok{ np.mean(standard\_units(x) }\OperatorTok{*}\NormalTok{ standard\_units(y))}


\KeywordTok{def}\NormalTok{ slope(x, y):}
    \ControlFlowTok{return}\NormalTok{ correlation(x, y) }\OperatorTok{*}\NormalTok{ np.std(y) }\OperatorTok{/}\NormalTok{ np.std(x)}


\KeywordTok{def}\NormalTok{ intercept(x, y):}
    \ControlFlowTok{return}\NormalTok{ np.mean(y) }\OperatorTok{{-}}\NormalTok{ slope(x, y) }\OperatorTok{*}\NormalTok{ np.mean(x)}


\KeywordTok{def}\NormalTok{ fit\_least\_squares(x, y):}
\NormalTok{    theta\_0 }\OperatorTok{=}\NormalTok{ intercept(x, y)}
\NormalTok{    theta\_1 }\OperatorTok{=}\NormalTok{ slope(x, y)}
    \ControlFlowTok{return}\NormalTok{ theta\_0, theta\_1}


\KeywordTok{def}\NormalTok{ predict(x, theta\_0, theta\_1):}
    \ControlFlowTok{return}\NormalTok{ theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x}


\KeywordTok{def}\NormalTok{ compute\_mse(y, yhat):}
    \ControlFlowTok{return}\NormalTok{ np.mean((y }\OperatorTok{{-}}\NormalTok{ yhat) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}


\NormalTok{plt.style.use(}\StringTok{"default"}\NormalTok{)  }\CommentTok{\# Revert style to default mpl}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.style.use(}\StringTok{"default"}\NormalTok{)  }\CommentTok{\# Revert style to default mpl}
\NormalTok{NO\_VIZ, RESID, RESID\_SCATTER }\OperatorTok{=} \BuiltInTok{range}\NormalTok{(}\DecValTok{3}\NormalTok{)}


\KeywordTok{def}\NormalTok{ least\_squares\_evaluation(x, y, visualize}\OperatorTok{=}\NormalTok{NO\_VIZ):}
    \CommentTok{\# statistics}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"x\_mean : }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(x)}\SpecialCharTok{:.2f\}}\SpecialStringTok{, y\_mean : }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean(y)}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"x\_stdev: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{std(x)}\SpecialCharTok{:.2f\}}\SpecialStringTok{, y\_stdev: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{std(y)}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"r = Correlation(x, y): }\SpecialCharTok{\{}\NormalTok{correlation(x, y)}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}

    \CommentTok{\# Performance metrics}
\NormalTok{    ahat, bhat }\OperatorTok{=}\NormalTok{ fit\_least\_squares(x, y)}
\NormalTok{    yhat }\OperatorTok{=}\NormalTok{ predict(x, ahat, bhat)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\CharTok{\textbackslash{}t}\SpecialStringTok{heta\_0: }\SpecialCharTok{\{}\NormalTok{ahat}\SpecialCharTok{:.2f\}}\SpecialStringTok{, }\CharTok{\textbackslash{}t}\SpecialStringTok{heta\_1: }\SpecialCharTok{\{}\NormalTok{bhat}\SpecialCharTok{:.2f\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"RMSE: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{sqrt(compute\_mse(y, yhat))}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}

    \CommentTok{\# visualization}
\NormalTok{    fig, ax\_resid }\OperatorTok{=} \VariableTok{None}\NormalTok{, }\VariableTok{None}
    \ControlFlowTok{if}\NormalTok{ visualize }\OperatorTok{==}\NormalTok{ RESID\_SCATTER:}
\NormalTok{        fig, axs }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{        axs[}\DecValTok{0}\NormalTok{].scatter(x, y)}
\NormalTok{        axs[}\DecValTok{0}\NormalTok{].plot(x, yhat)}
\NormalTok{        axs[}\DecValTok{0}\NormalTok{].set\_title(}\StringTok{"LS fit"}\NormalTok{)}
\NormalTok{        ax\_resid }\OperatorTok{=}\NormalTok{ axs[}\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{elif}\NormalTok{ visualize }\OperatorTok{==}\NormalTok{ RESID:}
\NormalTok{        fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{        ax\_resid }\OperatorTok{=}\NormalTok{ plt.gca()}

    \ControlFlowTok{if}\NormalTok{ ax\_resid }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        ax\_resid.scatter(x, y }\OperatorTok{{-}}\NormalTok{ yhat, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{)}
\NormalTok{        ax\_resid.plot([}\DecValTok{4}\NormalTok{, }\DecValTok{14}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], color}\OperatorTok{=}\StringTok{"black"}\NormalTok{)}
\NormalTok{        ax\_resid.set\_title(}\StringTok{"Residuals"}\NormalTok{)}

    \ControlFlowTok{return}\NormalTok{ fig}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load in four different datasets: I, II, III, IV}
\NormalTok{x }\OperatorTok{=}\NormalTok{ [}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{14}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{]}
\NormalTok{y1 }\OperatorTok{=}\NormalTok{ [}\FloatTok{8.04}\NormalTok{, }\FloatTok{6.95}\NormalTok{, }\FloatTok{7.58}\NormalTok{, }\FloatTok{8.81}\NormalTok{, }\FloatTok{8.33}\NormalTok{, }\FloatTok{9.96}\NormalTok{, }\FloatTok{7.24}\NormalTok{, }\FloatTok{4.26}\NormalTok{, }\FloatTok{10.84}\NormalTok{, }\FloatTok{4.82}\NormalTok{, }\FloatTok{5.68}\NormalTok{]}
\NormalTok{y2 }\OperatorTok{=}\NormalTok{ [}\FloatTok{9.14}\NormalTok{, }\FloatTok{8.14}\NormalTok{, }\FloatTok{8.74}\NormalTok{, }\FloatTok{8.77}\NormalTok{, }\FloatTok{9.26}\NormalTok{, }\FloatTok{8.10}\NormalTok{, }\FloatTok{6.13}\NormalTok{, }\FloatTok{3.10}\NormalTok{, }\FloatTok{9.13}\NormalTok{, }\FloatTok{7.26}\NormalTok{, }\FloatTok{4.74}\NormalTok{]}
\NormalTok{y3 }\OperatorTok{=}\NormalTok{ [}\FloatTok{7.46}\NormalTok{, }\FloatTok{6.77}\NormalTok{, }\FloatTok{12.74}\NormalTok{, }\FloatTok{7.11}\NormalTok{, }\FloatTok{7.81}\NormalTok{, }\FloatTok{8.84}\NormalTok{, }\FloatTok{6.08}\NormalTok{, }\FloatTok{5.39}\NormalTok{, }\FloatTok{8.15}\NormalTok{, }\FloatTok{6.42}\NormalTok{, }\FloatTok{5.73}\NormalTok{]}
\NormalTok{x4 }\OperatorTok{=}\NormalTok{ [}\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{19}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{]}
\NormalTok{y4 }\OperatorTok{=}\NormalTok{ [}\FloatTok{6.58}\NormalTok{, }\FloatTok{5.76}\NormalTok{, }\FloatTok{7.71}\NormalTok{, }\FloatTok{8.84}\NormalTok{, }\FloatTok{8.47}\NormalTok{, }\FloatTok{7.04}\NormalTok{, }\FloatTok{5.25}\NormalTok{, }\FloatTok{12.50}\NormalTok{, }\FloatTok{5.56}\NormalTok{, }\FloatTok{7.91}\NormalTok{, }\FloatTok{6.89}\NormalTok{]}

\NormalTok{anscombe }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"I"}\NormalTok{: pd.DataFrame(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(x, y1)), columns}\OperatorTok{=}\NormalTok{[}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{]),}
    \StringTok{"II"}\NormalTok{: pd.DataFrame(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(x, y2)), columns}\OperatorTok{=}\NormalTok{[}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{]),}
    \StringTok{"III"}\NormalTok{: pd.DataFrame(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(x, y3)), columns}\OperatorTok{=}\NormalTok{[}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{]),}
    \StringTok{"IV"}\NormalTok{: pd.DataFrame(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(x4, y4)), columns}\OperatorTok{=}\NormalTok{[}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{]),}
\NormalTok{\}}

\CommentTok{\# Plot the scatter plot and line of best fit}
\NormalTok{fig, axs }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ i, dataset }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{([}\StringTok{"I"}\NormalTok{, }\StringTok{"II"}\NormalTok{, }\StringTok{"III"}\NormalTok{, }\StringTok{"IV"}\NormalTok{]):}
\NormalTok{    ans }\OperatorTok{=}\NormalTok{ anscombe[dataset]}
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ ans[}\StringTok{"x"}\NormalTok{], ans[}\StringTok{"y"}\NormalTok{]}
\NormalTok{    ahat, bhat }\OperatorTok{=}\NormalTok{ fit\_least\_squares(x, y)}
\NormalTok{    yhat }\OperatorTok{=}\NormalTok{ predict(x, ahat, bhat)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].scatter(x, y, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{)  }\CommentTok{\# plot the x, y points}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].plot(x, yhat)  }\CommentTok{\# plot the line of best fit}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_xlabel(}\SpecialStringTok{f"$x\_}\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{$"}\NormalTok{)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_ylabel(}\SpecialStringTok{f"$y\_}\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{$"}\NormalTok{)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_title(}\SpecialStringTok{f"Dataset }\SpecialCharTok{\{}\NormalTok{dataset}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-5-output-1.pdf}

}

\end{figure}

While these four sets of datapoints look very different, they actually
all have identical means \(\bar x\), \(\bar y\), standard deviations
\(\sigma_x\), \(\sigma_y\), correlation \(r\), and RMSE! If we only look
at these statistics, we would probably be inclined to say that these
datasets are similar.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ dataset }\KeywordTok{in}\NormalTok{ [}\StringTok{"I"}\NormalTok{, }\StringTok{"II"}\NormalTok{, }\StringTok{"III"}\NormalTok{, }\StringTok{"IV"}\NormalTok{]:}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"\textgreater{}\textgreater{}\textgreater{} Dataset }\SpecialCharTok{\{}\NormalTok{dataset}\SpecialCharTok{\}}\SpecialStringTok{:"}\NormalTok{)}
\NormalTok{    ans }\OperatorTok{=}\NormalTok{ anscombe[dataset]}
\NormalTok{    fig }\OperatorTok{=}\NormalTok{ least\_squares\_evaluation(ans[}\StringTok{"x"}\NormalTok{], ans[}\StringTok{"y"}\NormalTok{], visualize}\OperatorTok{=}\NormalTok{NO\_VIZ)}
    \BuiltInTok{print}\NormalTok{()}
    \BuiltInTok{print}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
>>> Dataset I:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.119


>>> Dataset II:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.119


>>> Dataset III:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.118


>>> Dataset IV:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.817
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.118

\end{verbatim}

We may also wish to visualize the model's \textbf{residuals}, defined as
the difference between the observed and predicted \(y_i\) value
(\(e_i = y_i - \hat{y}_i\)). This gives a high-level view of how ``off''
each prediction is from the true observed value. Recall that you
explored this concept in
\href{https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity\#detecting-heteroscedasticity}{Data
8}: a good regression fit should display no clear pattern in its plot of
residuals. The residual plots for Anscombe's quartet are displayed
below. Note how only the first plot shows no clear pattern to the
magnitude of residuals. This is an indication that SLR is not the best
choice of model for the remaining three sets of points.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Residual visualization}
\NormalTok{fig, axs }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ i, dataset }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{([}\StringTok{"I"}\NormalTok{, }\StringTok{"II"}\NormalTok{, }\StringTok{"III"}\NormalTok{, }\StringTok{"IV"}\NormalTok{]):}
\NormalTok{    ans }\OperatorTok{=}\NormalTok{ anscombe[dataset]}
\NormalTok{    x, y }\OperatorTok{=}\NormalTok{ ans[}\StringTok{"x"}\NormalTok{], ans[}\StringTok{"y"}\NormalTok{]}
\NormalTok{    ahat, bhat }\OperatorTok{=}\NormalTok{ fit\_least\_squares(x, y)}
\NormalTok{    yhat }\OperatorTok{=}\NormalTok{ predict(x, ahat, bhat)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].scatter(}
\NormalTok{        x, y }\OperatorTok{{-}}\NormalTok{ yhat, alpha}\OperatorTok{=}\FloatTok{0.6}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}
\NormalTok{    )  }\CommentTok{\# plot the x, y points}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].plot(}
\NormalTok{        x, np.zeros\_like(x), color}\OperatorTok{=}\StringTok{"black"}
\NormalTok{    )  }\CommentTok{\# plot the residual line}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_xlabel(}\SpecialStringTok{f"$x\_}\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{$"}\NormalTok{)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_ylabel(}\SpecialStringTok{f"$e\_}\SpecialCharTok{\{}\NormalTok{i}\OperatorTok{+}\DecValTok{1}\SpecialCharTok{\}}\SpecialStringTok{$"}\NormalTok{)}
\NormalTok{    axs[i }\OperatorTok{//} \DecValTok{2}\NormalTok{, i }\OperatorTok{\%} \DecValTok{2}\NormalTok{].set\_title(}\SpecialStringTok{f"Dataset }\SpecialCharTok{\{}\NormalTok{dataset}\SpecialCharTok{\}}\SpecialStringTok{ Residuals"}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-7-output-1.pdf}

}

\end{figure}

\hypertarget{constant-model-mse}{%
\section{Constant Model + MSE}\label{constant-model-mse}}

Now, we'll shift from the SLR model to the \textbf{constant model}, also
known as a summary statistic. The constant model is slightly different
from the simple linear regression model we've explored previously.
Rather than generating predictions from an inputted feature variable,
the constant model always \emph{predicts the same constant number}. This
ignores any relationships between variables. For example, let's say we
want to predict the number of drinks a boba shop sells in a day. Boba
tea sales likely depend on the time of year, the weather, how the
customers feel, whether school is in session, etc., but the constant
model ignores these factors in favor of a simpler model. In other words,
the constant model employs a \textbf{simplifying assumption}.

It is also a parametric, statistical model:

\[\hat{y} = \theta_0\]

\(\theta_0\) is the parameter of the constant model, just as
\(\theta_0\) and \(\theta_1\) were the parameters in SLR. Since our
parameter \(\theta_0\) is 1-dimensional (\(\theta_0 \in \mathbb{R}\)),
we now have no input to our model and will always predict
\(\hat{y} = \theta_0\).

\hypertarget{deriving-the-optimal-theta_0}{%
\subsection{\texorpdfstring{Deriving the optimal
\(\theta_0\)}{Deriving the optimal \textbackslash theta\_0}}\label{deriving-the-optimal-theta_0}}

Our task now is to determine what value of \(\theta_0\) best represents
the optimal model -- in other words, what number should we guess each
time to have the lowest possible \textbf{average loss} on our data?

Like before, we'll use Mean Squared Error (MSE). Recall that the MSE is
average squared loss (L2 loss) over the data
\(D = \{y_1, y_2, ..., y_n\}\).

\[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \hat{y_i})^2 \]

Our modeling process now looks like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: constant model
\item
  Choose a loss function: L2 loss
\item
  Fit the model
\item
  Evaluate model performance
\end{enumerate}

Given the \textbf{constant model} \(\hat{y} = \theta_0\), we can rewrite
the MSE equation as

\[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2 \]

We can fit \textbf{the model} by finding the optimal \(\hat{\theta_0}\)
that minimizes the MSE using a calculus approach.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate with respect to \(\theta_0\):
\end{enumerate}

\[
\begin{align}
\frac{d}{d\theta_0}\text{R}(\theta) & = \frac{d}{d\theta_0}(\frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2)
\\ &= \frac{1}{n}\sum^{n}_{i=1} \frac{d}{d\theta_0}  (y_i - \theta_0)^2 \quad \quad \text{a derivative of sums is a sum of derivatives}
\\ &= \frac{1}{n}\sum^{n}_{i=1} 2 (y_i - \theta_0) (-1) \quad \quad \text{chain rule}
\\ &= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \quad \quad \text{simply constants}
\end{align}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Set the derivative equation equal to 0:

  \[
  0 = {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})
  \]
\item
  Solve for \(\hat{\theta_0}\)
\end{enumerate}

\[
\begin{align}
0 &= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})
\\ &= \sum^{n}_{i=1} (y_i - \hat{\theta_0}) \quad \quad \text{divide both sides by} \frac{-2}{n}
\\ &= \left(\sum^{n}_{i=1} y_i\right) - \left(\sum^{n}_{i=1} \theta_0\right) \quad \quad \text{separate sums}
\\ &= \left(\sum^{n}_{i=1} y_i\right) - (n \cdot \hat{\theta_0}) \quad \quad  \text{c + c + … + c = nc}
\\ n \cdot \hat{\theta_0} &= \sum^{n}_{i=1} y_i
\\ \hat{\theta_0} &= \frac{1}{n} \sum^{n}_{i=1} y_i
\\ \hat{\theta_0} &= \bar{y}
\end{align}
\]

Let's take a moment to interpret this result.
\(\hat{\theta_0} = \bar{y}\) is the optimal parameter for constant model
+ MSE. It holds true regardless of what data sample you have, and it
provides some formal reasoning as to why the mean is such a common
summary statistic.

Our optimal model parameter is the value of the parameter that minimizes
the cost function. This minimum value of the cost function can be
expressed:

\[R(\hat{\theta_0}) = \min_{\theta_0} R(\theta_0)\]

To restate the above in plain English: we are looking at the value of
the cost function when it takes the best parameter as input. This
optimal model parameter, \(\hat{\theta_0}\), is the value of
\(\theta_0\) that minimizes the cost \(R\).

For modeling purposes, we care less about the minimum value of cost,
\(R(\hat{\theta_0})\), and more about the \emph{value of \(\theta\)}
that results in this lowest average loss. In other words, we concern
ourselves with finding the best parameter value such that:

\[\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)\]

That is, we want to find the \textbf{arg}ument \(\theta\) that
\textbf{min}imizes the cost function.

\hypertarget{comparing-two-different-models-both-fit-with-mse}{%
\subsection{Comparing Two Different Models, Both Fit with
MSE}\label{comparing-two-different-models-both-fit-with-mse}}

Now that we've explored the constant model with an L2 loss, we can
compare it to the SLR model that we learned last lecture. Consider the
dataset below, which contains information about the ages and lengths of
dugongs. Supposed we wanted to predict dugong ages:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3879}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4788}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Constant Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Simple Linear Regression
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
model & \(\hat{y} = \theta_0\) & \(\hat{y} = \theta_0 + \theta_1 x\) \\
data & sample of ages \(D = \{y_1, y_2, ..., y_n\}\) & sample of ages
\(D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\) \\
dimensions & \(\hat{\theta_0}\) is 1-D &
\(\hat{\theta} = [\hat{\theta_0}, \hat{\theta_1}]\) is 2-D \\
loss surface & 2-D
\includegraphics{constant_model_loss_transformations/images/constant_loss_surface.png}
& 3-D
\includegraphics{constant_model_loss_transformations/images/slr_loss_surface.png} \\
loss model &
\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\) &
\(\hat{R}(\theta_0, \theta_1) = \frac{1}{n}\sum^{n}_{i=1} (y_i - (\theta_0 + \theta_1 x))^2\) \\
RMSE & 7.72 & 4.31 \\
predictions visualized & rug plot
\includegraphics{constant_model_loss_transformations/images/dugong_rug.png}
& scatter plot
\includegraphics{constant_model_loss_transformations/images/dugong_scatter.png} \\
\end{longtable}

(Notice how the points for our SLR scatter plot are visually not a great
linear fit. We'll come back to this).

The code for generating the graphs and models is included below, but we
won't go over it in too much depth.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dugongs }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/dugongs.csv"}\NormalTok{)}
\NormalTok{data\_constant }\OperatorTok{=}\NormalTok{ dugongs[}\StringTok{"Age"}\NormalTok{]}
\NormalTok{data\_linear }\OperatorTok{=}\NormalTok{ dugongs[[}\StringTok{"Length"}\NormalTok{, }\StringTok{"Age"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Constant Model + MSE}
\NormalTok{plt.style.use(}\StringTok{\textquotesingle{}default\textquotesingle{}}\NormalTok{) }\CommentTok{\# Revert style to default mpl}
\NormalTok{adjust\_fontsize(size}\OperatorTok{=}\DecValTok{16}\NormalTok{)}
\OperatorTok{\%}\NormalTok{matplotlib inline}

\KeywordTok{def}\NormalTok{ mse\_constant(theta, data):}
    \ControlFlowTok{return}\NormalTok{ np.mean(np.array([(y\_obs }\OperatorTok{{-}}\NormalTok{ theta) }\OperatorTok{**} \DecValTok{2} \ControlFlowTok{for}\NormalTok{ y\_obs }\KeywordTok{in}\NormalTok{ data]), axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\NormalTok{thetas }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{42}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{l2\_loss\_thetas }\OperatorTok{=}\NormalTok{ mse\_constant(thetas, data\_constant)}

\CommentTok{\# Plotting the loss surface}
\NormalTok{plt.plot(thetas, l2\_loss\_thetas)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r\textquotesingle{}$\textbackslash{}theta\_0$\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r\textquotesingle{}MSE\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Optimal point}
\NormalTok{thetahat }\OperatorTok{=}\NormalTok{ np.mean(data\_constant)}
\NormalTok{plt.scatter([thetahat], [mse\_constant(thetahat, data\_constant)], s}\OperatorTok{=}\DecValTok{50}\NormalTok{, label }\OperatorTok{=} \VerbatimStringTok{r"$\textbackslash{}hat\{\textbackslash{}theta\}\_0$"}\NormalTok{)}
\NormalTok{plt.legend()}\OperatorTok{;}
\CommentTok{\# plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-9-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# SLR + MSE}
\KeywordTok{def}\NormalTok{ mse\_linear(theta\_0, theta\_1, data\_linear):}
\NormalTok{    data\_x, data\_y }\OperatorTok{=}\NormalTok{ data\_linear.iloc[:, }\DecValTok{0}\NormalTok{], data\_linear.iloc[:, }\DecValTok{1}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ np.mean(}
\NormalTok{        np.array([(y }\OperatorTok{{-}}\NormalTok{ (theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x)) }\OperatorTok{**} \DecValTok{2} \ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(data\_x, data\_y)]),}
\NormalTok{        axis}\OperatorTok{=}\DecValTok{0}\NormalTok{,}
\NormalTok{    )}


\CommentTok{\# plotting the loss surface}
\NormalTok{theta\_0\_values }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{80}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{)}
\NormalTok{theta\_1\_values }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{80}\NormalTok{)}
\NormalTok{mse\_values }\OperatorTok{=}\NormalTok{ np.array(}
\NormalTok{    [[mse\_linear(x, y, data\_linear) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ theta\_0\_values] }\ControlFlowTok{for}\NormalTok{ y }\KeywordTok{in}\NormalTok{ theta\_1\_values]}
\NormalTok{)}

\CommentTok{\# Optimal point}
\NormalTok{data\_x, data\_y }\OperatorTok{=}\NormalTok{ data\_linear.iloc[:, }\DecValTok{0}\NormalTok{], data\_linear.iloc[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{theta\_1\_hat }\OperatorTok{=}\NormalTok{ np.corrcoef(data\_x, data\_y)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{*}\NormalTok{ np.std(data\_y) }\OperatorTok{/}\NormalTok{ np.std(data\_x)}
\NormalTok{theta\_0\_hat }\OperatorTok{=}\NormalTok{ np.mean(data\_y) }\OperatorTok{{-}}\NormalTok{ theta\_1\_hat }\OperatorTok{*}\NormalTok{ np.mean(data\_x)}

\CommentTok{\# Create the 3D plot}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{"3d"}\NormalTok{)}

\NormalTok{X, Y }\OperatorTok{=}\NormalTok{ np.meshgrid(theta\_0\_values, theta\_1\_values)}
\NormalTok{surf }\OperatorTok{=}\NormalTok{ ax.plot\_surface(}
\NormalTok{    X, Y, mse\_values, cmap}\OperatorTok{=}\StringTok{"viridis"}\NormalTok{, alpha}\OperatorTok{=}\FloatTok{0.6}
\NormalTok{)  }\CommentTok{\# Use alpha to make it slightly transparent}

\CommentTok{\# Scatter point using matplotlib}
\NormalTok{sc }\OperatorTok{=}\NormalTok{ ax.scatter(}
\NormalTok{    [theta\_0\_hat],}
\NormalTok{    [theta\_1\_hat],}
\NormalTok{    [mse\_linear(theta\_0\_hat, theta\_1\_hat, data\_linear)],}
\NormalTok{    marker}\OperatorTok{=}\StringTok{"o"}\NormalTok{,}
\NormalTok{    color}\OperatorTok{=}\StringTok{"red"}\NormalTok{,}
\NormalTok{    s}\OperatorTok{=}\DecValTok{100}\NormalTok{,}
\NormalTok{    label}\OperatorTok{=}\StringTok{"theta hat"}\NormalTok{,}
\NormalTok{)}

\CommentTok{\# Create a colorbar}
\NormalTok{cbar }\OperatorTok{=}\NormalTok{ fig.colorbar(surf, ax}\OperatorTok{=}\NormalTok{ax, shrink}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, aspect}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\NormalTok{cbar.set\_label(}\StringTok{"Cost Value"}\NormalTok{)}

\NormalTok{ax.set\_title(}\StringTok{"MSE for different $}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{theta\_0, }\CharTok{\textbackslash{}\textbackslash{}}\StringTok{theta\_1$"}\NormalTok{)}
\NormalTok{ax.set\_xlabel(}\StringTok{"$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{theta\_0$"}\NormalTok{)}
\NormalTok{ax.set\_ylabel(}\StringTok{"$}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{theta\_1$"}\NormalTok{)}
\NormalTok{ax.set\_zlabel(}\StringTok{"MSE"}\NormalTok{)}

\CommentTok{\# plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0, 'MSE')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-10-output-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predictions}
\NormalTok{yobs }\OperatorTok{=}\NormalTok{ data\_linear[}\StringTok{"Age"}\NormalTok{]  }\CommentTok{\# The true observations y}
\NormalTok{xs }\OperatorTok{=}\NormalTok{ data\_linear[}\StringTok{"Length"}\NormalTok{]  }\CommentTok{\# Needed for linear predictions}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(yobs)  }\CommentTok{\# Predictions}

\NormalTok{yhats\_constant }\OperatorTok{=}\NormalTok{ [thetahat }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n)]  }\CommentTok{\# Not used, but food for thought}
\NormalTok{yhats\_linear }\OperatorTok{=}\NormalTok{ [theta\_0\_hat }\OperatorTok{+}\NormalTok{ theta\_1\_hat }\OperatorTok{*}\NormalTok{ x }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Constant Model Rug Plot}
\CommentTok{\# In case we\textquotesingle{}re in a weird style state}
\NormalTok{sns.set\_theme()}
\NormalTok{adjust\_fontsize(size}\OperatorTok{=}\DecValTok{16}\NormalTok{)}
\OperatorTok{\%}\NormalTok{matplotlib inline}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\FloatTok{1.5}\NormalTok{))}
\NormalTok{sns.rugplot(yobs, height}\OperatorTok{=}\FloatTok{0.25}\NormalTok{, lw}\OperatorTok{=}\DecValTok{2}\NormalTok{) }\OperatorTok{;}
\NormalTok{plt.axvline(thetahat, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{, label}\OperatorTok{=}\VerbatimStringTok{r"$\textbackslash{}hat\{\textbackslash{}theta\}\_0$"}\NormalTok{)}\OperatorTok{;}
\NormalTok{plt.legend()}
\NormalTok{plt.yticks([])}\OperatorTok{;}
\CommentTok{\# plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-12-output-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# SLR model scatter plot }
\CommentTok{\# In case we\textquotesingle{}re in a weird style state}
\NormalTok{sns.set\_theme()}
\NormalTok{adjust\_fontsize(size}\OperatorTok{=}\DecValTok{16}\NormalTok{)}
\OperatorTok{\%}\NormalTok{matplotlib inline}

\NormalTok{sns.scatterplot(x}\OperatorTok{=}\NormalTok{xs, y}\OperatorTok{=}\NormalTok{yobs)}
\NormalTok{plt.plot(xs, yhats\_linear, color}\OperatorTok{=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{, lw}\OperatorTok{=}\DecValTok{4}\NormalTok{)}\OperatorTok{;}
\CommentTok{\# plt.savefig(\textquotesingle{}dugong\_line.png\textquotesingle{}, bbox\_inches = \textquotesingle{}tight\textquotesingle{});}
\CommentTok{\# plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-13-output-1.pdf}

}

\end{figure}

Interpreting the RMSE (Root Mean Squared Error):

\begin{itemize}
\tightlist
\item
  Because the constant error is \textbf{HIGHER} than the linear error,
\item
  The constant model is \textbf{WORSE} than the linear model (at least
  for this metric).
\end{itemize}

\hypertarget{constant-model-mae}{%
\section{Constant Model + MAE}\label{constant-model-mae}}

We see now that changing the model used for prediction leads to a wildly
different result for the optimal model parameter. What happens if we
instead change the loss function used in model evaluation?

This time, we will consider the constant model with L1 (absolute loss)
as the loss function. This means that the average loss will be expressed
as the \textbf{Mean Absolute Error (MAE)}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: constant model
\item
  Choose a loss function: L1 loss
\item
  Fit the model
\item
  Evaluate model performance
\end{enumerate}

\hypertarget{deriving-the-optimal-theta_0-1}{%
\subsection{\texorpdfstring{Deriving the optimal
\(\theta_0\)}{Deriving the optimal \textbackslash theta\_0}}\label{deriving-the-optimal-theta_0-1}}

Recall that the MAE is average \textbf{absolute} loss (L1 loss) over the
data \(D = \{y_1, y_2, ..., y_n\}\).

\[\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \hat{y_i}| \]

Given the constant model \(\hat{y} = \theta_0\), we can write the MAE
as:

\[\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \]

To fit the model, we find the optimal parameter value \(\hat{\theta_0}\)
that minimizes the MAE by differentiating using a calculus approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate with respect to \(\hat{\theta_0}\):
\end{enumerate}

\[
\begin{align}
\hat{R}(\theta_0) &= \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \\
\frac{d}{d\theta_0} R(\theta_0) &= \frac{d}{d\theta_0} \left(\frac{1}{n} \sum^{n}_{i=1} |y_i - \theta_0| \right) \\
&= \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0|
\end{align}
\]

\begin{itemize}
\tightlist
\item
  Here, we seem to have run into a problem: the derivative of an
  absolute value is undefined when the argument is 0 (i.e.~when
  \(y_i = \theta_0\)). For now, we'll ignore this issue. It turns out
  that disregarding this case doesn't influence our final result.
\item
  To perform the derivative, consider two cases. When \(\theta_0\) is
  \emph{less than or equal to} \(y_i\), the term \(y_i - \theta_0\) will
  be positive and the absolute value has no impact. When \(\theta_0\) is
  \emph{greater than} \(y_i\), the term \(y_i - \theta_0\) will be
  negative. Applying the absolute value will convert this to a positive
  value, which we can express by saying
  \(-(y_i - \theta_0) = \theta_0 - y_i\).
\end{itemize}

\[|y_i - \theta_0| = \begin{cases} y_i - \theta_0 \quad \text{ if } \theta_0 \le y_i \\ \theta_0 - y_i \quad \text{if }\theta_0 > y_i \end{cases}\]

\begin{itemize}
\tightlist
\item
  Taking derivatives:
\end{itemize}

\[\frac{d}{d\theta_0} |y_i - \theta_0| = \begin{cases} \frac{d}{d\theta_0} (y_i - \theta_0) = -1 \quad \text{if }\theta_0 < y_i \\ \frac{d}{d\theta_0} (\theta_0 - y_i) = 1 \quad \text{if }\theta_0 > y_i \end{cases}\]

\begin{itemize}
\tightlist
\item
  This means that we obtain a different value for the derivative for
  data points where \(\theta_0 < y_i\) and where \(\theta_0 > y_i\). We
  can summarize this by saying:
\end{itemize}

\[
\frac{d}{d\theta_0} R(\theta_0) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0| \\
= \frac{1}{n} \left[\sum_{\theta_0 < y_i} (-1) + \sum_{\theta_0 > y_i} (+1) \right]
\]

\begin{itemize}
\tightlist
\item
  In other words, we take the sum of values for \(i = 1, 2, ..., n\):

  \begin{itemize}
  \tightlist
  \item
    \(-1\) if our observation \(y_i\) is \emph{greater than} our
    prediction \(\hat{\theta_0}\)
  \item
    \(+1\) if our observation \(y_i\) is \emph{smaller than} our
    prediction \(\hat{\theta_0}\)
  \end{itemize}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Set the derivative equation equal to 0:
  \[ 0 = \frac{1}{n}\sum_{\hat{\theta_0} < y_i} (-1) + \frac{1}{n}\sum_{\hat{\theta_0} > y_i} (+1) \]
\item
  Solve for \(\hat{\theta_0}\):
  \[ 0 = -\frac{1}{n}\sum_{\hat{\theta_0} < y_i} (1) + \frac{1}{n}\sum_{\hat{\theta_0} > y_i} (1)\]
\end{enumerate}

\[\sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]

Thus, the constant model parameter \(\theta = \hat{\theta_0}\) that
minimizes MAE must satisfy:

\[ \sum_{\hat{\theta_0} < y_i} (1) = \sum_{\hat{\theta_0} > y_i} (1) \]

In other words, the number of observations greater than \(\theta_0\)
must be equal to the number of observations less than \(\theta_0\);
there must be an equal number of points on the left and right sides of
the equation. This is the definition of median, so our optimal value is
\[ \hat{\theta_0} = median(y) \]

\hypertarget{summary-loss-optimization-calculus-and-critical-points}{%
\section{Summary: Loss Optimization, Calculus, and Critical
Points}\label{summary-loss-optimization-calculus-and-critical-points}}

First, define the \textbf{objective function} as average loss.

\begin{itemize}
\tightlist
\item
  Plug in L1 or L2 loss.
\item
  Plug in the model so that the resulting expression is a function of
  \(\theta\).
\end{itemize}

Then, find the minimum of the objective function:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate with respect to \(\theta\).
\item
  Set equal to 0.
\item
  Solve for \(\hat{\theta}\).
\item
  (If we have multiple parameters) repeat steps 1-3 with partial
  derivatives.
\end{enumerate}

Recall critical points from calculus: \(R(\hat{\theta})\) could be a
minimum, maximum, or saddle point!

\begin{itemize}
\tightlist
\item
  We should technically also perform the second derivative test, i.e.,
  show \(R''(\hat{\theta}) > 0\).
\item
  MSE has a property---\textbf{convexity}---that guarantees that
  \(R(\hat{\theta})\) is a global minimum.
\item
  The proof of convexity for MAE is beyond this course.
\end{itemize}

\hypertarget{comparing-loss-functions}{%
\section{Comparing Loss Functions}\label{comparing-loss-functions}}

We've now tried our hand at fitting a model under both MSE and MAE cost
functions. How do the two results compare?

Let's consider a dataset where each entry represents the number of
drinks sold at a bubble tea store each day. We'll fit a constant model
to predict the number of drinks that will be sold tomorrow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{33}\NormalTok{])}
\NormalTok{drinks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([20, 21, 22, 29, 33])
\end{verbatim}

From our derivations above, we know that the optimal model parameter
under MSE cost is the mean of the dataset. Under MAE cost, the optimal
parameter is the median of the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.mean(drinks), np.median(drinks)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(25.0, 22.0)
\end{verbatim}

If we plot each empirical risk function across several possible values
of \(\theta\), we find that each \(\hat{\theta}\) does indeed correspond
to the lowest value of error:

Notice that the MSE above is a \textbf{smooth} function -- it is
differentiable at all points, making it easy to minimize using numerical
methods. The MAE, in contrast, is not differentiable at each of its
``kinks.'' We'll explore how the smoothness of the cost function can
impact our ability to apply numerical optimization in a few weeks.

How do outliers affect each cost function? Imagine we replace the
largest value in the dataset with 1000. The mean of the data increases
substantially, while the median is nearly unaffected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_with\_outlier }\OperatorTok{=}\NormalTok{ np.append(drinks, }\DecValTok{1033}\NormalTok{)}
\NormalTok{display(drinks\_with\_outlier)}
\NormalTok{np.mean(drinks\_with\_outlier), np.median(drinks\_with\_outlier)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([  20,   21,   22,   29,   33, 1033])
\end{verbatim}

\begin{verbatim}
(193.0, 25.5)
\end{verbatim}

This means that under the MSE, the optimal model parameter
\(\hat{\theta}\) is strongly affected by the presence of outliers. Under
the MAE, the optimal parameter is not as influenced by outlying data. We
can generalize this by saying that the MSE is \textbf{sensitive} to
outliers, while the MAE is \textbf{robust} to outliers.

Let's try another experiment. This time, we'll add an additional,
non-outlying datapoint to the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_with\_additional\_observation }\OperatorTok{=}\NormalTok{ np.append(drinks, }\DecValTok{35}\NormalTok{)}
\NormalTok{drinks\_with\_additional\_observation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([20, 21, 22, 29, 33, 35])
\end{verbatim}

When we again visualize the cost functions, we find that the MAE now
plots a horizontal line between 22 and 29. This means that there are
\emph{infinitely} many optimal values for the model parameter: any value
\(\hat{\theta} \in [22, 29]\) will minimize the MAE. In contrast, the
MSE still has a single best value for \(\hat{\theta}\). In other words,
the MSE has a \textbf{unique} solution for \(\hat{\theta}\); the MAE is
not guaranteed to have a single unique solution.

To summarize our example,

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3879}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4788}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MSE (Mean Squared Loss)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MAE (Mean Absolute Loss)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Loss Function &
\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\) &
\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0|\) \\
Optimal \(\hat{\theta_0}\) & \(\hat{\theta_0} = mean(y) = \bar{y}\) &
\(\hat{\theta_0} = median(y)\) \\
Loss Surface & & \\
Shape & \textbf{Smooth} - easy to minimize using numerical methods (in a
few weeks) & \textbf{Piecewise} - at each of the ``kinks,'' it's not
differentiable. Harder to minimize. \\
Outliers & \textbf{Sensitive} to outliers (since they change mean
substantially). Sensitivity also depends on the dataset size. &
\textbf{More robust} to outliers. \\
\(\hat{\theta_0}\) Uniqueness & \textbf{Unique} \(\hat{\theta_0}\) &
\textbf{Infinitely many} \(\hat{\theta_0}\)s \\
\end{longtable}

\hypertarget{transformations-to-fit-linear-models}{%
\section{Transformations to fit Linear
Models}\label{transformations-to-fit-linear-models}}

At this point, we have an effective method of fitting models to predict
linear relationships. Given a feature variable and target, we can apply
our four-step process to find the optimal model parameters.

A key word above is \emph{linear}. When we computed parameter estimates
earlier, we assumed that \(x_i\) and \(y_i\) shared a roughly linear
relationship. Data in the real world isn't always so straightforward,
but we can transform the data to try and obtain linearity.

The \textbf{Tukey-Mosteller Bulge Diagram} is a useful tool for
summarizing what transformations can linearize the relationship between
two variables. To determine what transformations might be appropriate,
trace the shape of the ``bulge'' made by your data. Find the quadrant of
the diagram that matches this bulge. The transformations shown on the
vertical and horizontal axes of this quadrant can help improve the fit
between the variables.

Note that:

\begin{itemize}
\tightlist
\item
  There are multiple solutions. Some will fit better than others.
\item
  sqrt and log make a value ``smaller.''
\item
  Raising to a power makes a value ``bigger.''
\item
  Each of these transformations equates to increasing or decreasing the
  scale of an axis.
\end{itemize}

Other goals in addition to linearity are possible, for example, making
data appear more symmetric. Linearity allows us to fit lines to the
transformed data.

Let's revisit our dugongs example. The lengths and ages are plotted
below:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \textasciigrave{}corrcoef\textasciigrave{} computes the correlation coefficient between two variables}
\CommentTok{\# \textasciigrave{}std\textasciigrave{} finds the standard deviation}
\NormalTok{x }\OperatorTok{=}\NormalTok{ dugongs[}\StringTok{"Length"}\NormalTok{]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ dugongs[}\StringTok{"Age"}\NormalTok{]}
\NormalTok{r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, y)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{theta\_1 }\OperatorTok{=}\NormalTok{ r }\OperatorTok{*}\NormalTok{ np.std(y) }\OperatorTok{/}\NormalTok{ np.std(x)}
\NormalTok{theta\_0 }\OperatorTok{=}\NormalTok{ np.mean(y) }\OperatorTok{{-}}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ np.mean(x)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{200}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].scatter(x, y)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_ylabel(}\StringTok{"Age"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].scatter(x, y)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x, theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x, }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_ylabel(}\StringTok{"Age"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0, 0.5, 'Age')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-18-output-2.pdf}

}

\end{figure}

Looking at the plot on the left, we see that there is a slight curvature
to the data points. Plotting the SLR curve on the right results in a
poor fit.

For SLR to perform well, we'd like there to be a rough linear trend
relating \texttt{"Age"} and \texttt{"Length"}. What is making the raw
data deviate from a linear relationship? Notice that the data points
with \texttt{"Length"} greater than 2.6 have disproportionately high
values of \texttt{"Age"} relative to the rest of the data. If we could
manipulate these data points to have lower \texttt{"Age"} values, we'd
``shift'' these points downwards and reduce the curvature in the data.
Applying a logarithmic transformation to \(y_i\) (that is, taking
\(\log(\) \texttt{"Age"} \()\) ) would achieve just that.

An important word on \(\log\): in Data 100 (and most upper-division STEM
courses), \(\log\) denotes the natural logarithm with base \(e\). The
base-10 logarithm, where relevant, is indicated by \(\log_{10}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ np.log(y)}

\NormalTok{r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, z)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{theta\_1 }\OperatorTok{=}\NormalTok{ r }\OperatorTok{*}\NormalTok{ np.std(z) }\OperatorTok{/}\NormalTok{ np.std(x)}
\NormalTok{theta\_0 }\OperatorTok{=}\NormalTok{ np.mean(z) }\OperatorTok{{-}}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ np.mean(x)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{200}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].scatter(x, z)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}log\{(Age)\}$"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].scatter(x, z)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x, theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x, }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}log\{(Age)\}$"}\NormalTok{)}

\NormalTok{plt.subplots\_adjust(wspace}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-19-output-1.pdf}

}

\end{figure}

Our SLR fit looks a lot better! We now have a new target variable: the
SLR model is now trying to predict the \emph{log} of \texttt{"Age"},
rather than the untransformed \texttt{"Age"}. In other words, we are
applying the transformation \(z_i = \log{(y_i)}\). Notice that the
resulting model is still \textbf{linear in the parameters}
\(\theta = [\theta_0, \theta_1]\). The SLR model becomes:

\[\hat{\log{y}} = \theta_0 + \theta_1 x\]
\[\hat{z} = \theta_0 + \theta_1 x\]

It turns out that this linearized relationship can help us understand
the underlying relationship between \(x\) and \(y\). If we rearrange the
relationship above, we find:

\[
\log{(y)} = \theta_0 + \theta_1 x \\
y = e^{\theta_0 + \theta_1 x} \\
y = (e^{\theta_0})e^{\theta_1 x} \\
y_i = C e^{k x}
\]

For some constants \(C\) and \(k\).

\(y\) is an \emph{exponential} function of \(x\). Applying an
exponential fit to the untransformed variables corroborates this
finding.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.figure(dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{))}

\NormalTok{plt.scatter(x, y)}
\NormalTok{plt.plot(x, np.exp(theta\_0) }\OperatorTok{*}\NormalTok{ np.exp(theta\_1 }\OperatorTok{*}\NormalTok{ x), }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Age"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0, 0.5, 'Age')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-20-output-2.pdf}

}

\end{figure}

You may wonder: why did we choose to apply a log transformation
specifically? Why not some other function to linearize the data?

Practically, many other mathematical operations that modify the relative
scales of \texttt{"Age"} and \texttt{"Length"} could have worked here.

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

Multiple linear regression is an extension of simple linear regression
that adds additional features to the model. The multiple linear
regression model takes the form:

\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]

Our predicted value of \(y\), \(\hat{y}\), is a linear combination of
the single \textbf{observations} (features), \(x_i\), and the
parameters, \(\theta_i\).

We'll dive deeper into Multiple Linear Regression in the next lecture.

\hypertarget{bonus-calculating-constant-model-mse-using-an-algebraic-trick}{%
\section{Bonus: Calculating Constant Model MSE Using an Algebraic
Trick}\label{bonus-calculating-constant-model-mse-using-an-algebraic-trick}}

Earlier, we calculated the constant model MSE using calculus. It turns
out that there is a much more elegant way of performing this same
minimization algebraically, without using calculus at all.

In this calculation, we use the fact that the \textbf{sum of deviations
from the mean is \(0\)} or that \(\sum_{i=1}^{n} (y_i - \bar{y}) = 0\).

Let's quickly walk through the proof for this: \[
\begin{align}
\sum_{i=1}^{n} (y_i - \bar{y}) &= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \bar{y} \\
 &= \sum_{i=1}^{n} y_i - n\bar{y} \\
 &= \sum_{i=1}^{n} y_i - n\frac{1}{n}\sum_{i=1}^{n}y_i \\
 &= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n}y_i \\
 & = 0
\end{align}
\]

In our calculations, we'll also be using the definition of the variance
as a sample. As a refresher:

\[\sigma_y^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})^2\]

Getting into our calculation for MSE minimization:

\[
\begin{align}
R(\theta) &= {\frac{1}{n}}\sum^{n}_{i=1} (y_i - \theta)^2
\\ &= \frac{1}{n}\sum^{n}_{i=1} [(y_i - \bar{y}) + (\bar{y} - \theta)]^2\quad \quad \text{using trick that a-b can be written as (a-c) + (c-b) } \\
&\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \space \space \text{where a, b, and c are any numbers}
\\ &= \frac{1}{n}\sum^{n}_{i=1} [(y_i - \bar{y})^2 + 2(y_i - \bar{y})(\bar{y} - \theta) + (\bar{y} - \theta)^2]
\\ &= \frac{1}{n}[\sum^{n}_{i=1}(y_i - \bar{y})^2 + 2(\bar{y} - \theta)\sum^{n}_{i=1}(y_i - \bar{y}) + n(\bar{y} - \theta)^2] \quad \quad  \text{distribute sum to individual terms}
\\ &= \frac{1}{n}\sum^{n}_{i=1}(y_i - \bar{y})^2 + \frac{2}{n}(\bar{y} - \theta)\cdot0 + (\bar{y} - \theta)^2 \quad \quad  \text{sum of deviations from mean is 0}
\\ &= \sigma_y^2 + (\bar{y} - \theta)^2
\end{align}
\]

Since variance can't be negative, we know that our first term,
\(\sigma_y^2\) is greater than or equal to \(0\). Also note, that
\textbf{the first term doesn't involve \(\theta\) at all}, meaning
changing our model won't change this value. For the purposes of
determining \$\hat{\theta}\#, we can then essentially ignore this term.

Looking at the second term, \((\bar{y} - \theta)^2\), since it is
squared, we know it must be greater than or equal to \(0\). As this term
does involve \(\theta\), picking the value of \(\theta\) that minimizes
this term will allow us to minimize our average loss. For the second
term to equal \(0\), \(\theta = \bar{y}\), or in other words,
\(\hat{\theta} = \bar{y} = mean(y)\).

\hypertarget{note}{%
\paragraph{Note}\label{note}}

In the derivation above, we decompose the expected loss, \(R(\theta)\),
into two key components: the variance of the data, \(\sigma_y^2\), and
the square of the bias, \((\bar{y} - \theta)^2\). This decomposition is
insightful for understanding the behavior of estimators in statistical
models.

\begin{itemize}
\item
  \textbf{Variance, \(\sigma_y^2\)}: This term represents the spread of
  the data points around their mean, \(\bar{y}\), and is a measure of
  the data's inherent variability. Importantly, it does not depend on
  the choice of \(\theta\), meaning it's a fixed property of the data.
  Variance serves as an indicator of the data's dispersion and is
  crucial in understanding the dataset's structure, but it remains
  constant regardless of how we adjust our model parameter \(\theta\).
\item
  \textbf{Bias Squared, \((\bar{y} - \theta)^2\)}: This term captures
  the bias of the estimator, defined as the square of the difference
  between the mean of the data points, \(\bar{y}\), and the parameter
  \(\theta\). The bias quantifies the systematic error introduced when
  estimating \(\theta\). Minimizing this term is essential for improving
  the accuracy of the estimator. When \(\theta = \bar{y}\), the bias is
  \(0\), indicating that the estimator is unbiased for the parameter it
  estimates. This highlights a critical principle in statistical
  estimation: choosing \(\theta\) to be the sample mean, \(\bar{y}\),
  minimizes the average loss, rendering the estimator both efficient and
  unbiased for the population mean.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{ordinary-least-squares}{%
\chapter{Ordinary Least Squares}\label{ordinary-least-squares}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Define linearity with respect to a vector of parameters \(\theta\).
\item
  Understand the use of matrix notation to express multiple linear
  regression.
\item
  Interpret ordinary least squares as the minimization of the norm of
  the residual vector.
\item
  Compute performance metrics for multiple linear regression.
\end{itemize}

\end{tcolorbox}

We've now spent a number of lectures exploring how to build effective
models -- we introduced the SLR and constant models, selected cost
functions to suit our modeling task, and applied transformations to
improve the linear fit.

Throughout all of this, we considered models of one feature
(\(\hat{y}_i = \theta_0 + \theta_1 x_i\)) or zero features
(\(\hat{y}_i = \theta_0\)). As data scientists, we usually have access
to datasets containing \emph{many} features. To make the best models we
can, it will be beneficial to consider all of the variables available to
us as inputs to a model, rather than just one. In today's lecture, we'll
introduce \textbf{multiple linear regression} as a framework to
incorporate multiple features into a model. We will also learn how to
accelerate the modeling process -- specifically, we'll see how linear
algebra offers us a powerful set of tools for understanding model
performance.

\hypertarget{ols-problem-formulation}{%
\section{OLS Problem Formulation}\label{ols-problem-formulation}}

\hypertarget{multiple-linear-regression-1}{%
\subsection{Multiple Linear
Regression}\label{multiple-linear-regression-1}}

Multiple linear regression is an extension of simple linear regression
that adds additional features to the model. The multiple linear
regression model takes the form:

\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]

Our predicted value of \(y\), \(\hat{y}\), is a linear combination of
the single \textbf{observations} (features), \(x_i\), and the
parameters, \(\theta_i\).

We can explore this idea further by looking at a dataset containing
aggregate per-player data from the 2018-19 NBA season, downloaded from
\href{https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019}{Kaggle}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{nba }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{\textquotesingle{}data/nba18{-}19.csv\textquotesingle{}}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{nba.index.name }\OperatorTok{=} \VariableTok{None} \CommentTok{\# Drops name of index (players are ordered by rank)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nba.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lllrlrrrrrrrrrrrrrrrrrrrrrrrrr}
\toprule
{} &                  Player & Pos &  Age &   Tm &   G &  GS &    MP &   FG &   FGA &    FG\% &   3P &  3PA &    3P\% &   2P &   2PA &    2P\% &   eFG\% &   FT &  FTA &    FT\% &  ORB &  DRB &  TRB &  AST &  STL &  BLK &  TOV &   PF &   PTS \\
\midrule
1 &  Álex Abrines\textbackslash abrinal01 &  SG &   25 &  OKC &  31 &   2 &  19.0 &  1.8 &   5.1 &  0.357 &  1.3 &  4.1 &  0.323 &  0.5 &   1.0 &  0.500 &  0.487 &  0.4 &  0.4 &  0.923 &  0.2 &  1.4 &  1.5 &  0.6 &  0.5 &  0.2 &  0.5 &  1.7 &   5.3 \\
2 &      Quincy Acy\textbackslash acyqu01 &  PF &   28 &  PHO &  10 &   0 &  12.3 &  0.4 &   1.8 &  0.222 &  0.2 &  1.5 &  0.133 &  0.2 &   0.3 &  0.667 &  0.278 &  0.7 &  1.0 &  0.700 &  0.3 &  2.2 &  2.5 &  0.8 &  0.1 &  0.4 &  0.4 &  2.4 &   1.7 \\
3 &  Jaylen Adams\textbackslash adamsja01 &  PG &   22 &  ATL &  34 &   1 &  12.6 &  1.1 &   3.2 &  0.345 &  0.7 &  2.2 &  0.338 &  0.4 &   1.1 &  0.361 &  0.459 &  0.2 &  0.3 &  0.778 &  0.3 &  1.4 &  1.8 &  1.9 &  0.4 &  0.1 &  0.8 &  1.3 &   3.2 \\
4 &  Steven Adams\textbackslash adamsst01 &   C &   25 &  OKC &  80 &  80 &  33.4 &  6.0 &  10.1 &  0.595 &  0.0 &  0.0 &  0.000 &  6.0 &  10.1 &  0.596 &  0.595 &  1.8 &  3.7 &  0.500 &  4.9 &  4.6 &  9.5 &  1.6 &  1.5 &  1.0 &  1.7 &  2.6 &  13.9 \\
5 &   Bam Adebayo\textbackslash adebaba01 &   C &   21 &  MIA &  82 &  28 &  23.3 &  3.4 &   5.9 &  0.576 &  0.0 &  0.2 &  0.200 &  3.4 &   5.7 &  0.588 &  0.579 &  2.0 &  2.8 &  0.735 &  2.0 &  5.3 &  7.3 &  2.2 &  0.9 &  0.8 &  1.5 &  2.5 &   8.9 \\
\bottomrule
\end{tabular}

Let's say we are interested in predicting the number of points
(\texttt{PTS}) an athlete will score in a basketball game this season.

Suppose we want to fit a linear model by using some characteristics, or
\textbf{features} of a player. Specifically, we'll focus on field goals,
assists, and 3-point attempts.

\begin{itemize}
\tightlist
\item
  \texttt{FG}, the average number of (2-point) field goals per game
\item
  \texttt{AST}, the average number of assists per game
\item
  \texttt{3PA}, the average number of 3-point field goals attempted per
  game
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nba[[}\StringTok{\textquotesingle{}FG\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}AST\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}3PA\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}PTS\textquotesingle{}}\NormalTok{]].head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrrrr}
\toprule
{} &   FG &  AST &  3PA &   PTS \\
\midrule
1 &  1.8 &  0.6 &  4.1 &   5.3 \\
2 &  0.4 &  0.8 &  1.5 &   1.7 \\
3 &  1.1 &  1.9 &  2.2 &   3.2 \\
4 &  6.0 &  1.6 &  0.0 &  13.9 \\
5 &  3.4 &  2.2 &  0.2 &   8.9 \\
\bottomrule
\end{tabular}

Because we are now dealing with many parameter values, we've collected
them all into a \textbf{parameter vector} with dimensions
\((p+1) \times 1\) to keep things tidy. Remember that \(p\) represents
the number of features we have (in this case, 3).

\[\theta = \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix}\]

We are working with two vectors here: a row vector representing the
observed data, and a column vector containing the model parameters. The
multiple linear regression model is \textbf{equivalent to the dot
(scalar) product of the observation vector and parameter vector}.

\[[1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \theta = [1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]

Notice that we have inserted 1 as the first value in the observation
vector. When the dot product is computed, this 1 will be multiplied with
\(\theta_0\) to give the intercept of the regression model. We call this
1 entry the \textbf{intercept} or \textbf{bias} term.

Given that we have three features here, we can express this model as:
\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:\theta_3 x_{3}\]

Our features are represented by \(x_1\) (\texttt{FG}), \(x_2\)
(\texttt{AST}), and \(x_3\) (\texttt{3PA}) with each having
correpsonding parameters, \(\theta_1\), \(\theta_2\), and \(\theta_3\).

In statistics, this model + loss is called \textbf{Ordinary Least
Squares (OLS)}. The solution to OLS is the minimizing loss for
parameters \(\hat{\theta}\), also called the \textbf{least squares
estimate}.

\hypertarget{linear-algebra-approach}{%
\subsection{Linear Algebra Approach}\label{linear-algebra-approach}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra Review: Vector Dot Product}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

The \textbf{dot product (or inner product)} is a vector operation that:

\begin{itemize}
\tightlist
\item
  Can only be carried out on two vectors of the \textbf{same length}
\item
  Sums up the products of the corresponding entries of the two vectors
\item
  Returns a single number
\end{itemize}

For example, let \[ 
\begin{align}
\vec{u} = \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}, \vec{v} = \begin{bmatrix}1 \\ 1 \\ 1\end{bmatrix}
\end{align}
\]

The dot product between \(\vec{u}\) and \(\vec{v}\) is \[
\begin{align}
\vec{u} \cdot \vec{v} &= \vec{u}^T \vec{v} = \vec{v}^T \vec{u} \\
  &= 1 \cdot 1 + 2 \cdot 1 + 3 \cdot 1 \\ 
  &= 6
\end{align}
\]

While not in scope, note that we can also interpret the dot product
geometrically:

\begin{itemize}
\tightlist
\item
  It is the product of three things: the \textbf{magnitude} of both
  vectors, and the \textbf{cosine} of the angles between them:
  \[\vec{u} \cdot \vec{v} = ||\vec{u}|| \cdot ||\vec{v}|| \cdot {cos \theta}\]
\end{itemize}

\end{tcolorbox}

We now know how to generate a single prediction from multiple observed
features. Data scientists usually work at scale -- that is, they want to
build models that can produce many predictions, all at once. The vector
notation we introduced above gives us a hint on how we can expedite
multiple linear regression. We want to use the tools of linear algebra.

Let's think about how we can apply what we did above. To accommodate for
the fact that we're considering several feature variables, we'll adjust
our notation slightly. Each observation can now be thought of as a row
vector with an entry for each of \(p\) features.

To make a prediction from the \emph{first} observation in the data, we
take the dot product of the parameter vector and \emph{first}
observation vector. To make a prediction from the \emph{second}
observation, we would repeat this process to find the dot product of the
parameter vector and the \emph{second} observation vector. If we wanted
to find the model predictions for each observation in the dataset, we'd
repeat this process for all \(n\) observations in the data.

\[\hat{y}_1 = \theta_0 + \theta_1 x_{11} + \theta_2 x_{12} + ... + \theta_p x_{1p} = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\]
\[\hat{y}_2 = \theta_0 + \theta_1 x_{21} + \theta_2 x_{22} + ... + \theta_p x_{2p} = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}] \theta\]
\[\vdots\]
\[\hat{y}_n = \theta_0 + \theta_1 x_{n1} + \theta_2 x_{n2} + ... + \theta_p x_{np} = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]

Our observed data is represented by \(n\) row vectors, each with
dimension \((p+1)\). We can collect them all into a single matrix, which
we call \(\mathbb{X}\).

The matrix \(\mathbb{X}\) is known as the \textbf{design matrix}. It
contains all observed data for each of our \(p\) features, where each
\textbf{row} corresponds to one \textbf{observation}, and each
\textbf{column} corresponds to a \textbf{feature}. It often (but not
always) contains an additional column of all ones to represent the
\textbf{intercept} or \textbf{bias column}.

To review what is happening in the design matrix: each row represents a
single observation. For example, a student in Data 100. Each column
represents a feature. For example, the ages of students in Data 100.
This convention allows us to easily transfer our previous work in
DataFrames over to this new linear algebra perspective.

The multiple linear regression model can then be restated in terms of
matrices: \[
\Large
\mathbb{\hat{Y}} = \mathbb{X} \theta
\]

Here, \(\mathbb{\hat{Y}}\) is the \textbf{prediction vector} with \(n\)
elements (\(\mathbb{\hat{Y}} \in \mathbb{R}^{n}\)); it contains the
prediction made by the model for each of the \(n\) input observations.
\(\mathbb{X}\) is the \textbf{design matrix} with dimensions
\(\mathbb{X} \in \mathbb{R}^{n \times (p + 1)}\), and \(\theta\) is the
\textbf{parameter vector} with dimensions
\(\theta \in \mathbb{R}^{(p + 1)}\). Note that our \textbf{true output}
\(\mathbb{Y}\) is also a vector with \(n\) elements
(\(\mathbb{Y} \in \mathbb{R}^{n}\)).

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra Review: Linearity}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

An expression is \textbf{linear in \(\theta\)} (a set of parameters) if
it is a linear combination of the elements of the set. Checking if an
expression can separate into a matrix product of two terms -- a
\textbf{vector of \(\theta\)} s, and a matrix/vector \textbf{not
involving \(\theta\)} -- is a good indicator of linearity.

For example, consider the vector
\(\theta = [\theta_0, \theta_1, \theta_2]\)

\begin{itemize}
\tightlist
\item
  \(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\) is linear in theta, and
  we can separate it into a matrix product of two terms:
\end{itemize}

\[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]

\begin{itemize}
\tightlist
\item
  \(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\) is
  \emph{not} linear in theta, as the \(\theta_1\) term is squared, and
  the \(\theta_2\) term is logged. We cannot separate it into a matrix
  product of two terms.
\end{itemize}

\end{tcolorbox}

\hypertarget{mean-squared-error}{%
\subsection{Mean Squared Error}\label{mean-squared-error}}

We now have a new approach to understanding models in terms of vectors
and matrices. To accompany this new convention, we should update our
understanding of risk functions and model fitting.

Recall our definition of MSE:
\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

At its heart, the MSE is a measure of \emph{distance} -- it gives an
indication of how ``far away'' the predictions are from the true values,
on average.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra: L2 Norm}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

When working with vectors, this idea of ``distance'' or the vector's
\textbf{size/length} is represented by the \textbf{norm}. More
precisely, the distance between two vectors \(\vec{a}\) and \(\vec{b}\)
can be expressed as:
\[||\vec{a} - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\]

The double bars are mathematical notation for the norm. The subscript 2
indicates that we are computing the L2, or squared norm.

The two norms we need to know for Data 100 are the L1 and L2 norms
(sound familiar?). In this note, we'll focus on L2 norm. We'll dive into
L1 norm in future lectures.

For the n-dimensional vector
\[\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\]
its \textbf{L2 vector norm} is

\[||\vec{x}||_2 = \sqrt{(x_1)^2 + (x_2)^2 + \ldots + (x_n)^2} = \sqrt{\sum_{i=1}^n (x_i)^2}\]

The L2 vector norm is a generalization of the Pythagorean theorem in
\(n\) dimensions. Thus, it can be used as a measure of the
\textbf{length} of a vector or even as a measure of the
\textbf{distance} between two vectors.

\end{tcolorbox}

We can express the MSE as a squared L2 norm if we rewrite it in terms of
the prediction vector, \(\hat{\mathbb{Y}}\), and true target vector,
\(\mathbb{Y}\):

\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} (||\mathbb{Y} - \hat{\mathbb{Y}}||_2)^2\]

Here, the superscript 2 outside of the parentheses means that we are
\emph{squaring} the norm. If we plug in our linear model
\(\hat{\mathbb{Y}} = \mathbb{X} \theta\), we find the MSE cost function
in vector notation:

\[R(\theta) = \frac{1}{n} (||\mathbb{Y} - \mathbb{X} \theta||_2)^2\]

Under the linear algebra perspective, our new task is to fit the optimal
parameter vector \(\theta\) such that the cost function is minimized.
Equivalently, we wish to minimize the norm
\[||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2.\]

We can restate this goal in two ways:

\begin{itemize}
\tightlist
\item
  Minimize the \textbf{distance} between the vector of true values,
  \(\mathbb{Y}\), and the vector of predicted values,
  \(\mathbb{\hat{Y}}\)
\item
  Minimize the \textbf{length} of the \textbf{residual vector}, defined
  as: \[e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}
           y_1 - \hat{y}_1 \\
           y_2 - \hat{y}_2 \\
           \vdots \\
           y_n - \hat{y}_n
         \end{bmatrix}\]
\end{itemize}

\hypertarget{a-note-on-terminology-for-multiple-linear-regression}{%
\subsection{A Note on Terminology for Multiple Linear
Regression}\label{a-note-on-terminology-for-multiple-linear-regression}}

There are several equivalent terms in the context of regression. The
ones we use most often for this course are bolded.

\begin{itemize}
\tightlist
\item
  \(x\) can be called a

  \begin{itemize}
  \tightlist
  \item
    \textbf{Feature(s)}
  \item
    Covariate(s)
  \item
    \textbf{Independent variable(s)}
  \item
    Explanatory variable(s)
  \item
    Predictor(s)
  \item
    Input(s)
  \item
    Regressor(s)
  \end{itemize}
\item
  \(y\) can be called an

  \begin{itemize}
  \tightlist
  \item
    \textbf{Output}
  \item
    Outcome
  \item
    \textbf{Response}
  \item
    Dependent variable
  \end{itemize}
\item
  \(\hat{y}\) can be called a

  \begin{itemize}
  \tightlist
  \item
    \textbf{Prediction}
  \item
    Predicted response
  \item
    Estimated value
  \end{itemize}
\item
  \(\theta\) can be called a

  \begin{itemize}
  \tightlist
  \item
    \textbf{Weight(s)}
  \item
    \textbf{Parameter(s)}
  \item
    Coefficient(s)
  \end{itemize}
\item
  \(\hat{\theta}\) can be called a

  \begin{itemize}
  \tightlist
  \item
    \textbf{Estimator(s)}
  \item
    \textbf{Optimal parameter(s)}
  \end{itemize}
\item
  A datapoint \((x, y)\) is also called an observation.
\end{itemize}

\hypertarget{geometric-derivation}{%
\section{Geometric Derivation}\label{geometric-derivation}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra: Span}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Recall that the \textbf{span} or \textbf{column space} of a matrix
\(\mathbb{X}\) (denoted \(span(\mathbb{X})\)) is the set of all possible
linear combinations of the matrix's columns. In other words, the span
represents every point in space that could possibly be reached by adding
and scaling some combination of the matrix columns. Additionally, if
each column of \(\mathbb{X}\) has length \(n\), \(span(\mathbb{X})\) is
a subspace of \(\mathbb{R}^{n}\).

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra: Matrix-Vector Multiplication}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

There are 2 ways we can think about matrix-vector multiplication

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  So far, we've thought of our model as horizontally stacked predictions
  per datapoint
\item
  However, it is helpful sometimes to think of matrix-vector
  multiplication as performed by columns. We can also think of
  \(\mathbb{Y}\) as a \emph{linear combination of feature vectors},
  scaled by \emph{parameters}.
\end{enumerate}

\end{tcolorbox}

Up until now, we've mostly thought of our model as a scalar product
between horizontally stacked observations and the parameter vector. We
can also think of \(\hat{\mathbb{Y}}\) as a \textbf{linear combination
of feature vectors}, scaled by the \textbf{parameters}. We use the
notation \(\mathbb{X}_{:, i}\) to denote the \(i\)th column of the
design matrix. You can think of this as following the same convention as
used when calling \texttt{.iloc} and \texttt{.loc}. ``:'' means that we
are taking all entries in the \(i\)th column.

\[
\hat{\mathbb{Y}} = 
\theta_0 \begin{bmatrix}
           1 \\
           1 \\
           \vdots \\
           1
         \end{bmatrix} + \theta_1 \begin{bmatrix}
           x_{11} \\
           x_{21} \\
           \vdots \\
           x_{n1}
         \end{bmatrix} + \ldots + \theta_p \begin{bmatrix}
           x_{1p} \\
           x_{2p} \\
           \vdots \\
           x_{np}
         \end{bmatrix}
         = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p \mathbb{X}_{:,\:p+1}\]

This new approach is useful because it allows us to take advantage of
the properties of linear combinations.

Because the prediction vector, \(\hat{\mathbb{Y}} = \mathbb{X} \theta\),
is a \textbf{linear combination} of the columns of \(\mathbb{X}\), we
know that the \textbf{predictions are contained in the span of
\(\mathbb{X}\)}. That is, we know that
\(\mathbb{\hat{Y}} \in \text{Span}(\mathbb{X})\).

The diagram below is a simplified view of \(\text{Span}(\mathbb{X})\),
assuming that each column of \(\mathbb{X}\) has length \(n\). Notice
that the columns of \(\mathbb{X}\) define a subspace of
\(\mathbb{R}^n\), where each point in the subspace can be reached by a
linear combination of \(\mathbb{X}\)'s columns. The prediction vector
\(\mathbb{\hat{Y}}\) lies somewhere in this subspace.

Examining this diagram, we find a problem. The vector of true values,
\(\mathbb{Y}\), could theoretically lie \emph{anywhere} in
\(\mathbb{R}^n\) space -- its exact location depends on the data we
collect out in the real world. However, our multiple linear regression
model can only make predictions in the subspace of \(\mathbb{R}^n\)
spanned by \(\mathbb{X}\). Remember the model fitting goal we
established in the previous section: we want to generate predictions
such that the distance between the vector of true values,
\(\mathbb{Y}\), and the vector of predicted values,
\(\mathbb{\hat{Y}}\), is minimized. This means that \textbf{we want
\(\mathbb{\hat{Y}}\) to be the vector in \(\text{Span}(\mathbb{X})\)
that is closest to \(\mathbb{Y}\)}.

Another way of rephrasing this goal is to say that we wish to minimize
the length of the residual vector \(e\), as measured by its \(L_2\)
norm.

The vector in \(\text{Span}(\mathbb{X})\) that is closest to
\(\mathbb{Y}\) is always the \textbf{orthogonal projection} of
\(\mathbb{Y}\) onto \(\text{Span}(\mathbb{X}).\) Thus, we should choose
the parameter vector \(\theta\) that makes the \textbf{residual vector
orthogonal to any vector in \(\text{Span}(\mathbb{X})\)}. You can
visualize this as the vector created by dropping a perpendicular line
from \(\mathbb{Y}\) onto the span of \(\mathbb{X}\).

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Linear Algebra: Orthogonality}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Recall that two vectors \(\vec{a}\) and \(\vec{b}\) are orthogonal if
their dot product is zero: \(\vec{a}^{T}\vec{b} = 0\).

A vector \(v\) is \textbf{orthogonal} to the span of a matrix \(M\) if
and only if \(v\) is orthogonal to \textbf{each column} in \(M\). Put
together, a vector \(v\) is orthogonal to \(\text{Span}(M)\) if:

\[M^Tv = \vec{0}\]

Note that \(\vec{0}\) represents the \textbf{zero vector}, a
\(d\)-length vector full of 0s.

\end{tcolorbox}

Remember our goal is to find \(\hat{\theta}\) such that we minimize the
objective function \(R(\theta)\). Equivalently, this is the
\(\hat{\theta}\) such that the residual vector
\(e = \mathbb{Y} - \mathbb{X} \hat{\theta}\) is orthogonal to
\(\text{Span}(\mathbb{X})\).

Looking at the definition of orthogonality of
\(\mathbb{Y} - \mathbb{X}\hat{\theta}\) to \(span(\mathbb{X})\), we can
write: \[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]

Let's then rearrange the terms:
\[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} = \vec{0}\]

And finally, we end up with the \textbf{normal equation}:
\[\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]

Any vector \(\theta\) that minimizes MSE on a dataset must satisfy this
equation.

If \(\mathbb{X}^T \mathbb{X}\) is invertible, we can conclude:
\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}\]

This is called the \textbf{least squares estimate} of \(\theta\): it is
the value of \(\theta\) that minimizes the squared loss.

Note that the least squares estimate was derived under the assumption
that \(\mathbb{X}^T \mathbb{X}\) is \emph{invertible}. This condition
holds true when \(\mathbb{X}^T \mathbb{X}\) is full column rank, which,
in turn, happens when \(\mathbb{X}\) is full column rank. The proof for
why \(\mathbb{X}\) needs to be full column rank is optional and in the
Bonus section at the end.

\hypertarget{evaluating-model-performance}{%
\section{Evaluating Model
Performance}\label{evaluating-model-performance}}

Our geometric view of multiple linear regression has taken us far! We
have identified the optimal set of parameter values to minimize MSE in a
model of multiple features. Now, we want to understand how well our
fitted model performs.

\hypertarget{rmse}{%
\subsection{RMSE}\label{rmse}}

One measure of model performance is the \textbf{Root Mean Squared
Error}, or RMSE. The RMSE is simply the square root of MSE. Taking the
square root converts the value back into the original, non-squared units
of \(y_i\), which is useful for understanding the model's performance. A
low RMSE indicates more ``accurate'' predictions -- that there is a
lower average loss across the dataset.

\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]

\hypertarget{residual-plots}{%
\subsection{Residual Plots}\label{residual-plots}}

When working with SLR, we generated plots of the residuals against a
single feature to understand the behavior of residuals. When working
with several features in multiple linear regression, it no longer makes
sense to consider a single feature in our residual plots. Instead,
multiple linear regression is evaluated by making plots of the residuals
against the predicted values. As was the case with SLR, a multiple
linear model performs well if its residual plot shows no patterns.

\hypertarget{multiple-r2}{%
\subsection{\texorpdfstring{Multiple
\(R^2\)}{Multiple R\^{}2}}\label{multiple-r2}}

For SLR, we used the correlation coefficient to capture the association
between the target variable and a single feature variable. In a multiple
linear model setting, we will need a performance metric that can account
for multiple features at once. \textbf{Multiple \(R^2\)}, also called
the \textbf{coefficient of determination}, is the \textbf{proportion of
variance} of our \textbf{fitted values} (predictions) \(\hat{y}_i\) to
our true values \(y_i\). It ranges from 0 to 1 and is effectively the
\emph{proportion} of variance in the observations that the \textbf{model
explains}.

\[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]

Note that for OLS with an intercept term, for example
\(\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_px_p\),
\(R^2\) is equal to the square of the correlation between \(y\) and
\(\hat{y}\). On the other hand for SLR, \(R^2\) is equal to \(r^2\), the
correlation between \(x\) and \(y\). The proof of these last two
properties is out of scope for this course.

Additionally, as we add more features, our fitted values tend to become
closer and closer to our actual values. Thus, \(R^2\) increases.

Adding more features doesn't always mean our model is better though!
We'll see why later in the course.

\hypertarget{ols-properties}{%
\section{OLS Properties}\label{ols-properties}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When using the optimal parameter vector, our residuals
  \(e = \mathbb{Y} - \hat{\mathbb{Y}}\) are orthogonal to
  \(span(\mathbb{X})\).
\end{enumerate}

\[\mathbb{X}^Te = 0 \]

\begin{tcolorbox}[enhanced jigsaw, colback=white, rightrule=.15mm, opacityback=0, breakable, leftrule=.75mm, arc=.35mm, toprule=.15mm, bottomrule=.15mm, left=2mm]

Proof:

\begin{itemize}
\tightlist
\item
  The optimal parameter vector, \(\hat{\theta}\), solves the normal
  equations
  \(\implies \hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\)
\end{itemize}

\[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]

\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{X}\hat{\theta}\]

\begin{itemize}
\tightlist
\item
  Any matrix multiplied with its own inverse is the identity matrix
  \(\mathbb{I}\)
\end{itemize}

\[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y} = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For all linear models with an \textbf{intercept term}, the \textbf{sum
  of residuals is zero}.
\end{enumerate}

\[\sum_i^n e_i = 0\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, rightrule=.15mm, opacityback=0, breakable, leftrule=.75mm, arc=.35mm, toprule=.15mm, bottomrule=.15mm, left=2mm]

Proof:

\begin{itemize}
\tightlist
\item
  For all linear models with an \textbf{intercept term}, the average of
  the predicted \(y\) values is equal to the average of the true \(y\)
  values. \[\bar{y} = \bar{\hat{y}}\]
\item
  Rewriting the sum of residuals as two separate sums,
  \[\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i\]
\item
  Each respective sum is a multiple of the average of the sum.
  \[\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0\]
\end{itemize}

\end{tcolorbox}

To summarize:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unique?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Constant Model + MSE & \(\hat{y} = \theta_0\) &
\(\hat{\theta}_0 = mean(y) = \bar{y}\) & \textbf{Yes}. Any set of values
has a unique mean. \\
Constant Model + MAE & \(\hat{y} = \theta_0\) &
\(\hat{\theta}_0 = median(y)\) & \textbf{Yes}, if odd. \textbf{No}, if
even. Return the average of the middle 2 values. \\
Simple Linear Regression + MSE & \(\hat{y} = \theta_0 + \theta_1x\) &
\(\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x}\)
\(\hat{\theta}_1 = r\frac{\sigma_y}{\sigma_x}\) & \textbf{Yes}. Any set
of non-constant* values has a unique mean, SD, and correlation
coefficient. \\
\textbf{OLS} (Linear Model + MSE) &
\(\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}\) &
\(\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\) &
\textbf{Yes}, if \(\mathbb{X}\) is full column rank (all columns are
linearly independent, \# of datapoints
\textgreater\textgreater\textgreater{} \# of features). \\
\end{longtable}

\hypertarget{bonus-uniqueness-of-the-solution}{%
\section{Bonus: Uniqueness of the
Solution}\label{bonus-uniqueness-of-the-solution}}

The Least Squares estimate \(\hat{\theta}\) is \textbf{unique} if and
only if \(\mathbb{X}\) is \textbf{full column rank}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, rightrule=.15mm, opacityback=0, breakable, leftrule=.75mm, arc=.35mm, toprule=.15mm, bottomrule=.15mm, left=2mm]

Proof:

\begin{itemize}
\tightlist
\item
  We know the solution to the normal equation
  \(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{X}^T\mathbb{Y}\) is the
  least square estimate that minimizes the squared loss.
\item
  \(\hat{\theta}\) has a \textbf{unique} solution \(\iff\) the square
  matrix \(\mathbb{X}^T\mathbb{X}\) is \textbf{invertible} \(\iff\)
  \(\mathbb{X}^T\mathbb{X}\) is full rank.

  \begin{itemize}
  \tightlist
  \item
    The \textbf{column} rank of a square matrix is the max number of
    linearly independent columns it contains.
  \item
    An \(n\) x \(n\) square matrix is deemed full column rank when all
    of its columns are linearly independent. That is, its rank would be
    equal to \(n\).
  \item
    \(\mathbb{X}^T\mathbb{X}\) has shape \((p + 1) \times (p + 1)\), and
    therefore has max rank \(p + 1\).
  \end{itemize}
\item
  \(rank(\mathbb{X}^T\mathbb{X})\) = \(rank(\mathbb{X})\) (proof out of
  scope).
\item
  Therefore, \(\mathbb{X}^T\mathbb{X}\) has rank \(p + 1\) \(\iff\)
  \(\mathbb{X}\) has rank \(p + 1\) \(\iff \mathbb{X}\) is full column
  rank.
\end{itemize}

\end{tcolorbox}

Therefore, if \(\mathbb{X}\) is not full column rank, we will not have
unique estimates. This can happen for two major reasons.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If our design matrix \(\mathbb{X}\) is ``\textbf{wide}'':

  \begin{itemize}
  \tightlist
  \item
    If n \textless{} p, then we have way more features (columns) than
    observations (rows).
  \item
    Then \(rank(\mathbb{X})\) = min(n, p+1) \textless{} p+1, so
    \(\hat{\theta}\) is not unique.
  \item
    Typically we have n \textgreater\textgreater{} p so this is less of
    an issue.
  \end{itemize}
\item
  If our design matrix \(\mathbb{X}\) has features that are
  \textbf{linear combinations} of other features:

  \begin{itemize}
  \tightlist
  \item
    By definition, rank of \(\mathbb{X}\) is number of linearly
    independent columns in \(\mathbb{X}\).
  \item
    Example: If ``Width'', ``Height'', and ``Perimeter'' are all
    columns,

    \begin{itemize}
    \tightlist
    \item
      Perimeter = 2 * Width + 2 * Height \(\rightarrow\) \(\mathbb{X}\)
      is not full rank.
    \end{itemize}
  \item
    Important with one-hot encoding (to discuss later).
  \end{itemize}
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{sklearn-and-gradient-descent}{%
\chapter{sklearn and Gradient
Descent}\label{sklearn-and-gradient-descent}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Apply the \texttt{sklearn} library for model creation and training
\item
  Optimizing complex models
\item
  Identifying cases where straight calculus or geometric arguments won't
  help solve the loss function
\item
  Applying gradient descent for numerical optimization
\end{itemize}

\end{tcolorbox}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ plotly.express }\ImportTok{as}\NormalTok{ px}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{pd.options.mode.chained\_assignment }\OperatorTok{=} \VariableTok{None}  \CommentTok{\# default=\textquotesingle{}warn\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{sklearn}{%
\section{\texorpdfstring{\texttt{sklearn}}{sklearn}}\label{sklearn}}

\hypertarget{implementing-derived-formulas-in-code}{%
\subsection{Implementing Derived Formulas in
Code}\label{implementing-derived-formulas-in-code}}

Throughout this lecture, we'll refer to the \texttt{penguins} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{penguins }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"penguins"}\NormalTok{)}
\NormalTok{penguins }\OperatorTok{=}\NormalTok{ penguins[penguins[}\StringTok{"species"}\NormalTok{] }\OperatorTok{==} \StringTok{"Adelie"}\NormalTok{].dropna()}
\NormalTok{penguins.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrrrrl}
\toprule
{} & species &     island &  bill\_length\_mm &  bill\_depth\_mm &  flipper\_length\_mm &  body\_mass\_g &     sex \\
\midrule
0 &  Adelie &  Torgersen &            39.1 &           18.7 &              181.0 &       3750.0 &    Male \\
1 &  Adelie &  Torgersen &            39.5 &           17.4 &              186.0 &       3800.0 &  Female \\
2 &  Adelie &  Torgersen &            40.3 &           18.0 &              195.0 &       3250.0 &  Female \\
4 &  Adelie &  Torgersen &            36.7 &           19.3 &              193.0 &       3450.0 &  Female \\
5 &  Adelie &  Torgersen &            39.3 &           20.6 &              190.0 &       3650.0 &    Male \\
\bottomrule
\end{tabular}

Our goal will be to predict the value of the \texttt{"bill\_depth\_mm"}
for a particular penguin given its \texttt{"flipper\_length\_mm"} and
\texttt{"body\_mass\_g"}. We'll also add a bias column of all ones to
represent the intercept term of our models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a bias column of all ones to \textasciigrave{}penguins\textasciigrave{}}
\NormalTok{penguins[}\StringTok{"bias"}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(penguins), dtype}\OperatorTok{=}\BuiltInTok{int}\NormalTok{) }

\CommentTok{\# Define the design matrix, X...}
\CommentTok{\# Note that we use .to\_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form}
\NormalTok{X }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"bias"}\NormalTok{, }\StringTok{"flipper\_length\_mm"}\NormalTok{, }\StringTok{"body\_mass\_g"}\NormalTok{]].to\_numpy()}

\CommentTok{\# ...as well as the target variable, Y}
\CommentTok{\# Again, we use .to\_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"bill\_depth\_mm"}\NormalTok{]].to\_numpy()}
\end{Highlighting}
\end{Shaded}

In the lecture on ordinary least squares, we expressed multiple linear
regression using matrix notation.

\[\hat{\mathbb{Y}} = \mathbb{X}\theta\]

We used a geometric approach to derive the following expression for the
optimal model parameters:

\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\]

That's a whole lot of matrix manipulation. How do we implement it in
\texttt{python}?

There are three operations we need to perform here: multiplying
matrices, taking transposes, and finding inverses.

\begin{itemize}
\tightlist
\item
  To perform matrix multiplication, use the \texttt{@} operator
\item
  To take a transpose, call the \texttt{.T} attribute of an
  \texttt{NumPy} array or \texttt{DataFrame}
\item
  To compute an inverse, use \texttt{NumPy}'s in-built method
  \texttt{np.linalg.inv}
\end{itemize}

Putting this all together, we can compute the OLS estimate for the
optimal model parameters, stored in the array \texttt{theta\_hat}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_hat }\OperatorTok{=}\NormalTok{ np.linalg.inv(X.T }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{@}\NormalTok{ X.T }\OperatorTok{@}\NormalTok{ Y}
\NormalTok{theta\_hat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[1.10029953e+01],
       [9.82848689e-03],
       [1.47749591e-03]])
\end{verbatim}

To make predictions using our optimized parameter values, we
matrix-multiply the design matrix with the parameter vector:

\[\hat{\mathbb{Y}} = \mathbb{X}\theta\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_hat }\OperatorTok{=}\NormalTok{ X }\OperatorTok{@}\NormalTok{ theta\_hat}
\NormalTok{pd.DataFrame(Y\_hat).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &          0 \\
\midrule
0 &  18.322561 \\
1 &  18.445578 \\
2 &  17.721412 \\
3 &  17.997254 \\
4 &  18.263268 \\
\bottomrule
\end{tabular}

\hypertarget{the-sklearn-workflow}{%
\subsection{\texorpdfstring{The \texttt{sklearn}
Workflow}{The sklearn Workflow}}\label{the-sklearn-workflow}}

We've already saved a lot of time (and avoided tedious calculations) by
translating our derived formulas into code. However, we still had to go
through the process of writing out the linear algebra ourselves.

To make life \emph{even easier}, we can turn to the \texttt{sklearn}
\href{https://scikit-learn.org/stable/}{\texttt{python} library}.
\texttt{sklearn} is a robust library of machine learning tools used
extensively in research and industry. It is the standard for simple
machine learning tasks and gives us a wide variety of in-built modeling
frameworks and methods, so we'll keep returning to \texttt{sklearn}
techniques as we progress through Data 100.

Regardless of the specific type of model being implemented,
\texttt{sklearn} follows a standard set of steps for creating a model:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Import the \texttt{LinearRegression} model from \texttt{sklearn}

\begin{verbatim}
from sklearn.linear_model import LinearRegression
\end{verbatim}
\item
  Create a model object. This generates a new instance of the model
  class. You can think of it as making a new ``copy'' of a standard
  ``template'' for a model. In code, this looks like:

\begin{verbatim}
my_model = LinearRegression()
\end{verbatim}
\item
  Fit the model to the \texttt{X} design matrix and \texttt{Y} target
  vector. This calculates the optimal model parameters ``behind the
  scenes'' without us explicitly working through the calculations
  ourselves. The fitted parameters are then stored within the model for
  use in future predictions:

\begin{verbatim}
my_model.fit(X, Y)
\end{verbatim}
\item
  Use the fitted model to make predictions on the \texttt{X} input data
  using \texttt{.predict}.

\begin{verbatim}
my_model.predict(X)
\end{verbatim}
\end{enumerate}

To extract the fitted parameters, we can use:

\begin{verbatim}
my_model.coef_

my_model.intercept_
\end{verbatim}

Let's put this into action with our multiple regression task!

\textbf{1. Initialize an instance of the model class}

\texttt{sklearn} stores ``templates'' of useful models for machine
learning. We begin the modeling process by making a ``copy'' of one of
these templates for our own use. Model initialization looks like
\texttt{ModelClass()}, where \texttt{ModelClass} is the type of model we
wish to create.

For now, let's create a linear regression model using
\texttt{LinearRegression}.

\texttt{my\_model} is now an instance of the \texttt{LinearRegression}
class. You can think of it as the ``idea'' of a linear regression model.
We haven't trained it yet, so it doesn't know any model parameters and
cannot be used to make predictions. In fact, we haven't even told it
what data to use for modeling! It simply waits for further instructions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\end{Highlighting}
\end{Shaded}

\textbf{2. Train the model using \texttt{.fit}}

Before the model can make predictions, we will need to fit it to our
training data. When we fit the model, \texttt{sklearn} will run gradient
descent behind the scenes to determine the optimal model parameters. It
will then save these model parameters to our model instance for future
use.

All \texttt{sklearn} model classes include a \texttt{.fit} method, which
is used to fit the model. It takes in two inputs: the design matrix,
\texttt{X}, and the target variable, \texttt{Y}.

Let's start by fitting a model with just one feature: the flipper
length. We create a design matrix \texttt{X} by pulling out the
\texttt{"flipper\_length\_mm"} column from the \texttt{DataFrame}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# .fit expects a 2D data design matrix, so we use double brackets to extract a DataFrame}
\NormalTok{X }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{]]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{]}

\NormalTok{my\_model.fit(X, Y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LinearRegression()
\end{verbatim}

Notice that we use \textbf{double brackets} to extract this column. Why
double brackets instead of just single brackets? The \texttt{.fit}
method, by default, expects to receive \textbf{2-dimensional} data --
some kind of data that includes both rows and columns. Writing
\texttt{penguins{[}"flipper\_length\_mm"{]}} would return a 1D
\texttt{Series}, causing \texttt{sklearn} to error. We avoid this by
writing \texttt{penguins{[}{[}"flipper\_length\_mm"{]}{]}} to produce a
2D \texttt{DataFrame}.

And in just three lines of code, our model has run gradient descent to
determine the optimal model parameters! Our single-feature model takes
the form:

\[\text{bill depth} = \theta_0 + \theta_1 \text{flipper length}\]

Note that \texttt{LinearRegression} will automatically include an
intercept term.

The fitted model parameters are stored as attributes of the model
instance. \texttt{my\_model.intercept\_} will return the value of
\(\hat{\theta}_0\) as a scalar. \texttt{my\_model.coef\_} will return
all values \(\hat{\theta}_1, \hat{\theta}_1, ...\) in an array. Because
our model only contains one feature, we see just the value of
\(\hat{\theta}_1\) in the cell below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The intercept term, theta\_0}
\NormalTok{my\_model.intercept\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
7.297305899612306
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All parameters theta\_1, ..., theta\_p}
\NormalTok{my\_model.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.05812622])
\end{verbatim}

\textbf{3. Use the fitted model to make predictions}

Now that the model has been trained, we can use it to make predictions!
To do so, we use the \texttt{.predict} method. \texttt{.predict} takes
in one argument: the design matrix that should be used to generate
predictions. To understand how the model performs on the training set,
we would pass in the training data. Alternatively, to make predictions
on unseen data, we would pass in a new dataset that wasn't used to train
the model.

Below, we call \texttt{.predict} to generate model predictions on the
original training data. As before, we use double brackets to ensure that
we extract 2-dimensional data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y\_hat\_one\_feature }\OperatorTok{=}\NormalTok{ my\_model.predict(penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{]])}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The RMSE of the model is }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{sqrt(np.mean((Y}\OperatorTok{{-}}\NormalTok{Y\_hat\_one\_feature)}\OperatorTok{**}\DecValTok{2}\NormalTok{))}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The RMSE of the model is 1.1549363099239012
\end{verbatim}

What if we wanted a model with two features?

\[\text{bill depth} = \theta_0 + \theta_1 \text{flipper length} + \theta_2 \text{body mass}\]

We repeat this three-step process by intializing a new model object,
then calling \texttt{.fit} and \texttt{.predict} as before.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: initialize LinearRegression model}
\NormalTok{two\_feature\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}

\CommentTok{\# Step 2: fit the model}
\NormalTok{X\_two\_features }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{, }\StringTok{"body\_mass\_g"}\NormalTok{]]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{]}

\NormalTok{two\_feature\_model.fit(X\_two\_features, Y)}

\CommentTok{\# Step 3: make predictions}
\NormalTok{Y\_hat\_two\_features }\OperatorTok{=}\NormalTok{ two\_feature\_model.predict(X\_two\_features)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"The RMSE of the model is }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{sqrt(np.mean((Y}\OperatorTok{{-}}\NormalTok{Y\_hat\_two\_features)}\OperatorTok{**}\DecValTok{2}\NormalTok{))}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The RMSE of the model is 0.9881331104079044
\end{verbatim}

We can also see that we obtain the same predictions using
\texttt{sklearn} as we did when applying the ordinary least squares
formula before!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.DataFrame(\{}\StringTok{"Y\_hat from OLS"}\NormalTok{:np.squeeze(Y\_hat), }\StringTok{"Y\_hat from sklearn"}\NormalTok{:Y\_hat\_two\_features\}).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrr}
\toprule
{} &  Y\_hat from OLS &  Y\_hat from sklearn \\
\midrule
0 &       18.322561 &           18.322561 \\
1 &       18.445578 &           18.445578 \\
2 &       17.721412 &           17.721412 \\
3 &       17.997254 &           17.997254 \\
4 &       18.263268 &           18.263268 \\
\bottomrule
\end{tabular}

\hypertarget{gradient-descent}{%
\section{Gradient Descent}\label{gradient-descent}}

At this point, we've grown quite familiar with the process of choosing a
model and a corresponding loss function and optimizing parameters by
choosing the values of \(\theta\) that minimize the loss function. So
far, we've optimized \(\theta\) by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using calculus to take the derivative of the loss function with
  respect to \(\theta\), setting it equal to 0, and solving for
  \(\theta\).
\item
  Using the geometric argument of orthogonality to derive the OLS
  solution
  \(\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\).
\end{enumerate}

One thing to note, however, is that the techniques we used above can
only be applied if we make some big assumptions. For the calculus
approach, we assumed that the loss function was differentiable at all
points and that we could algebraically solve for the zero points of the
derivative; for the geometric approach, OLS \emph{only} applies when
using a linear model with MSE loss. What happens when we have more
complex models with different, more complex loss functions? The
techniques we've learned so far will not work, so we need a new
optimization technique: \textbf{gradient descent}.

\begin{quote}
\textbf{BIG IDEA}: use an iterative algorithm to numerically compute the
minimum of the loss.
\end{quote}

\hypertarget{minimizing-an-arbitrary-1d-function}{%
\subsection{Minimizing an Arbitrary 1D
Function}\label{minimizing-an-arbitrary-1d-function}}

Let's consider an arbitrary function. Our goal is to find the value of
\(x\) that minimizes this function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ arbitrary(x):}
    \ControlFlowTok{return}\NormalTok{ (x}\OperatorTok{**}\DecValTok{4} \OperatorTok{{-}} \DecValTok{15}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{3} \OperatorTok{+} \DecValTok{80}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{180}\OperatorTok{*}\NormalTok{x }\OperatorTok{+} \DecValTok{144}\NormalTok{)}\OperatorTok{/}\DecValTok{10}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-naive-approach-guess-and-check}{%
\subsubsection{The Naive Approach: Guess and
Check}\label{the-naive-approach-guess-and-check}}

Above, we saw that the minimum is somewhere around 5.3. Let's see if we
can figure out how to find the exact minimum algorithmically from
scratch. One very slow (and terrible) way would be manual
guess-and-check.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{arbitrary(}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.0
\end{verbatim}

A somewhat better (but still slow) approach is to use brute force to try
out a bunch of x values and return the one that yields the lowest loss.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ simple\_minimize(f, xs):}
    \CommentTok{\# Takes in a function f and a set of values xs. }
    \CommentTok{\# Calculates the value of the function f at all values x in xs}
    \CommentTok{\# Takes the minimum value of f(x) and returns the corresponding value x }
\NormalTok{    y }\OperatorTok{=}\NormalTok{ [f(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]  }
    \ControlFlowTok{return}\NormalTok{ xs[np.argmin(y)]}

\NormalTok{guesses }\OperatorTok{=}\NormalTok{ [}\FloatTok{5.3}\NormalTok{, }\FloatTok{5.31}\NormalTok{, }\FloatTok{5.32}\NormalTok{, }\FloatTok{5.33}\NormalTok{, }\FloatTok{5.34}\NormalTok{, }\FloatTok{5.35}\NormalTok{]}
\NormalTok{simple\_minimize(arbitrary, guesses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5.33
\end{verbatim}

This process is essentially the same as before where we made a graphical
plot, it's just that we're only looking at 20 selected points.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xs }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{sparse\_xs }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{)}

\NormalTok{ys }\OperatorTok{=}\NormalTok{ arbitrary(xs)}
\NormalTok{sparse\_ys }\OperatorTok{=}\NormalTok{ arbitrary(sparse\_xs)}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ px.line(x }\OperatorTok{=}\NormalTok{ xs, y }\OperatorTok{=}\NormalTok{ arbitrary(xs))}
\NormalTok{fig.add\_scatter(x }\OperatorTok{=}\NormalTok{ sparse\_xs, y }\OperatorTok{=}\NormalTok{ arbitrary(sparse\_xs), mode }\OperatorTok{=} \StringTok{"markers"}\NormalTok{)}
\NormalTok{fig.update\_layout(showlegend}\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{fig.update\_layout(autosize}\OperatorTok{=}\VariableTok{False}\NormalTok{, width}\OperatorTok{=}\DecValTok{800}\NormalTok{, height}\OperatorTok{=}\DecValTok{600}\NormalTok{)}
\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

This basic approach suffers from three major flaws:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If the minimum is outside our range of guesses, the answer will be
  completely wrong.
\item
  Even if our range of guesses is correct, if the guesses are too
  coarse, our answer will be inaccurate.
\item
  It is \emph{very} computationally inefficient, considering potentially
  vast numbers of guesses that are useless.
\end{enumerate}

\hypertarget{scipy.optimize.minimize}{%
\subsubsection{\texorpdfstring{\texttt{Scipy.optimize.minimize}}{Scipy.optimize.minimize}}\label{scipy.optimize.minimize}}

One way to minimize this mathematical function is to use the
\texttt{scipy.optimize.minimize} function. It takes a function and a
starting guess and tries to find the minimum.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# takes a function f and a starting point x0 and returns a readout }
\CommentTok{\# with the optimal input value of x which minimizes f}
\NormalTok{minimize(arbitrary, x0 }\OperatorTok{=} \FloatTok{3.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: -0.13827491292966557
        x: [ 2.393e+00]
      nit: 3
      jac: [ 6.486e-06]
 hess_inv: [[ 7.385e-01]]
     nfev: 20
     njev: 10
\end{verbatim}

\texttt{scipy.optimize.minimize} is great. It may also seem a bit
magical. How could you write a function that can find the minimum of any
mathematical function? There are a number of ways to do this, which
we'll explore in today's lecture, eventually arriving at the important
idea of \textbf{gradient descent}, which is the principle that
\texttt{scipy.optimize.minimize} uses.

It turns out that under the hood, the \texttt{fit} method for
\texttt{LinearRegression} models uses gradient descent. Gradient descent
is also how much of machine learning works, including even advanced
neural network models.

In Data 100, the gradient descent process will usually be invisible to
us, hidden beneath an abstraction layer. However, to be good data
scientists, it's important that we know the underlying principles that
optimization functions harness to find optimal parameters.

\hypertarget{digging-into-gradient-descent}{%
\subsubsection{Digging into Gradient
Descent}\label{digging-into-gradient-descent}}

Looking at the function across this domain, it is clear that the
function's minimum value occurs around \(\theta = 5.3\). Let's pretend
for a moment that we \emph{couldn't} see the full view of the cost
function. How would we guess the value of \(\theta\) that minimizes the
function?

It turns out that the first derivative of the function can give us a
clue. In the plots below, the line indicates the value of the derivative
of each value of \(\theta\). The derivative is negative where it is red
and positive where it is green.

Say we make a guess for the minimizing value of \(\theta\). Remember
that we read plots from left to right, and assume that our starting
\(\theta\) value is to the left of the optimal \(\hat{\theta}\). If the
guess ``undershoots'' the true minimizing value -- our guess for
\(\theta\) is lower than the value of the \(\hat{\theta}\) that
minimizes the function -- the derivative will be \textbf{negative}. This
means that if we increase \(\theta\) (move further to the right), then
we \textbf{can decrease} our loss function further. If this guess
``overshoots'' the true minimizing value, the derivative will be
positive, implying the converse.

We can use this pattern to help formulate our next guess for the optimal
\(\hat{\theta}\). Consider the case where we've undershot \(\theta\) by
guessing too low of a value. We'll want our next guess to be greater in
value than our previous guess -- that is, we want to shift our guess to
the right. You can think of this as following the slope ``downhill'' to
the function's minimum value.

If we've overshot \(\hat{\theta}\) by guessing too high of a value,
we'll want our next guess to be lower in value -- we want to shift our
guess for \(\hat{\theta}\) to the left.

In other words, the derivative of the function at each point tells us
the direction of our next guess.

\begin{itemize}
\tightlist
\item
  A negative slope means we want to step to the right, or move in the
  \emph{positive} direction.
\item
  A positive slope means we want to step to the left, or move in the
  \emph{negative} direction.
\end{itemize}

\hypertarget{algorithm-attempt-1}{%
\subsubsection{Algorithm Attempt 1}\label{algorithm-attempt-1}}

Armed with this knowledge, let's try to see if we can use the derivative
to optimize the function.

We start by making some guess for the minimizing value of \(x\). Then,
we look at the derivative of the function at this value of \(x\), and
step downhill in the \emph{opposite} direction. We can express our new
rule as a recurrence relation:

\[x^{(t+1)} = x^{(t)} - \frac{d}{dx} f(x^{(t)})\]

Translating this statement into English: we obtain \textbf{our next
guess} for the minimizing value of \(x\) at timestep \(t+1\)
(\(x^{(t+1)}\)) by taking \textbf{our last guess} (\(x^{(t)}\)) and
subtracting the \textbf{derivative of the function} at that point
(\(\frac{d}{dx} f(x^{(t)})\)).

A few steps are shown below, where the old step is shown as a
transparent point, and the next step taken is the green-filled dot.

Looking pretty good! We do have a problem though -- once we arrive close
to the minimum value of the function, our guesses ``bounce'' back and
forth past the minimum without ever reaching it.

In other words, each step we take when updating our guess moves us too
far. We can address this by decreasing the size of each step.

\hypertarget{algorithm-attempt-2}{%
\subsubsection{Algorithm Attempt 2}\label{algorithm-attempt-2}}

Let's update our algorithm to use a \textbf{learning rate} (also
sometimes called the step size), which controls how far we move with
each update. We represent the learning rate with \(\alpha\).

\[x^{(t+1)} = x^{(t)} - \alpha \frac{d}{dx} f(x^{(t)})\]

A small \(\alpha\) means that we will take small steps; a large
\(\alpha\) means we will take large steps. When do we stop updating? We
stop updating either after a fixed number of updates or after a
subsequent update doesn't change much.

Updating our function to use \(\alpha=0.3\), our algorithm successfully
\textbf{converges} (settles on a solution and stops updating
significantly, or at all) on the minimum value.

\hypertarget{convexity}{%
\subsection{Convexity}\label{convexity}}

In our analysis above, we focused our attention on the global minimum of
the loss function. You may be wondering: what about the local minimum
that's just to the left?

If we had chosen a different starting guess for \(\theta\), or a
different value for the learning rate \(\alpha\), our algorithm may have
gotten ``stuck'' and converged on the local minimum, rather than on the
true optimum value of loss.

If the loss function is \textbf{convex}, gradient descent is guaranteed
to converge and find the global minimum of the objective function.
Formally, a function \(f\) is convex if:
\[tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)\] for all \(a, b\) in the domain
of \(f\) and \(t \in [0, 1]\).

To put this into words: if you drew a line between any two points on the
curve, all values on the curve must be \emph{on or below} the line.
Importantly, any local minimum of a convex function is also its global
minimum so we avoid the situation where the algorithm converges on some
critical point that is not the minimum of the function.

In summary, non-convex loss functions can cause problems with
optimization. This means that our choice of loss function is a key
factor in our modeling process. It turns out that MSE \emph{is} convex,
which is a major reason why it is such a popular choice of loss
function. Gradient descent is only guaranteed to converge (given enough
iterations and an appropriate step size) for convex functions.

\hypertarget{gradient-descent-in-1-dimension}{%
\subsection{Gradient Descent in 1
Dimension}\label{gradient-descent-in-1-dimension}}

\begin{quote}
\textbf{Terminology clarification}: In past lectures, we have used
``loss'' to refer to the error incurred on a \emph{single} datapoint. In
applications, we usually care more about the average error across
\emph{all} datapoints. Going forward, we will take the ``model's loss''
to mean the model's average error across the dataset. This is sometimes
also known as the empirical risk, cost function, or objective function.
\[L(\theta) = R(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y, \hat{y})\]
\end{quote}

In our discussion above, we worked with some arbitrary function \(f\).
As data scientists, we will almost always work with gradient descent in
the context of optimizing \emph{models} -- specifically, we want to
apply gradient descent to find the minimum of a \emph{loss function}. In
a modeling context, our goal is to minimize a loss function by choosing
the minimizing model \emph{parameters}.

Recall our modeling workflow from the past few lectures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define a model with some parameters \(\theta_i\)
\item
  Choose a loss function
\item
  Select the values of \(\theta_i\) that minimize the loss function on
  the data
\end{enumerate}

Gradient descent is a powerful technique for completing this last task.
By applying the gradient descent algorithm, we can select values for our
parameters \(\theta_i\) that will lead to the model having minimal loss
on the training data.

When using gradient descent in a modeling context, we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make guesses for the minimizing \(\theta_i\)
\item
  Compute the derivative of the loss function \(L\)
\end{enumerate}

We can ``translate'' our gradient descent rule from before by replacing
\(x\) with \(\theta\) and \(f\) with \(L\):

\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta} L(\theta^{(t)})\]

\hypertarget{gradient-descent-on-the-tips-dataset}{%
\subsubsection{\texorpdfstring{Gradient Descent on the \texttt{tips}
Dataset}{Gradient Descent on the tips Dataset}}\label{gradient-descent-on-the-tips-dataset}}

To see this in action, let's consider a case where we have a linear
model with no offset. We want to predict the tip (y) given the price of
a meal (x). To do this, we

\begin{itemize}
\tightlist
\item
  Choose a model: \(\hat{y} = \theta_1 x\),
\item
  Choose a loss function:
  \(L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\).
\end{itemize}

Let's apply our \texttt{gradient\_descent} function from before to
optimize our model on the \texttt{tips} dataset. We will try to select
the best parameter \(\theta_i\) to predict the \texttt{tip} \(y\) from
the \texttt{total\_bill} \(x\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"tips"}\NormalTok{)}
\NormalTok{df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrllllr}
\toprule
{} &  total\_bill &   tip &     sex & smoker &  day &    time &  size \\
\midrule
0 &       16.99 &  1.01 &  Female &     No &  Sun &  Dinner &     2 \\
1 &       10.34 &  1.66 &    Male &     No &  Sun &  Dinner &     3 \\
2 &       21.01 &  3.50 &    Male &     No &  Sun &  Dinner &     3 \\
3 &       23.68 &  3.31 &    Male &     No &  Sun &  Dinner &     2 \\
4 &       24.59 &  3.61 &  Female &     No &  Sun &  Dinner &     4 \\
\bottomrule
\end{tabular}

We can visualize the value of the MSE on our dataset for different
possible choices of \(\theta_1\). To optimize our model, we want to
select the value of \(\theta_1\) that leads to the lowest MSE.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.graph\_objects }\ImportTok{as}\NormalTok{ go}

\KeywordTok{def}\NormalTok{ derivative\_arbitrary(x):}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{4}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{3} \OperatorTok{{-}} \DecValTok{45}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{160}\OperatorTok{*}\NormalTok{x }\OperatorTok{{-}} \DecValTok{180}\NormalTok{)}\OperatorTok{/}\DecValTok{10}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ go.Figure()}
\NormalTok{roots }\OperatorTok{=}\NormalTok{ np.array([}\FloatTok{2.3927}\NormalTok{, }\FloatTok{3.5309}\NormalTok{, }\FloatTok{5.3263}\NormalTok{])}

\NormalTok{fig.add\_trace(go.Scatter(x }\OperatorTok{=}\NormalTok{ xs, y }\OperatorTok{=}\NormalTok{ arbitrary(xs), }
\NormalTok{                         mode }\OperatorTok{=} \StringTok{"lines"}\NormalTok{, name }\OperatorTok{=} \StringTok{"f"}\NormalTok{))}
\NormalTok{fig.add\_trace(go.Scatter(x }\OperatorTok{=}\NormalTok{ xs, y }\OperatorTok{=}\NormalTok{ derivative\_arbitrary(xs), }
\NormalTok{                         mode }\OperatorTok{=} \StringTok{"lines"}\NormalTok{, name }\OperatorTok{=} \StringTok{"df"}\NormalTok{, line }\OperatorTok{=}\NormalTok{ \{}\StringTok{"dash"}\NormalTok{: }\StringTok{"dash"}\NormalTok{\}))}
\NormalTok{fig.add\_trace(go.Scatter(x }\OperatorTok{=}\NormalTok{ np.array(roots), y }\OperatorTok{=} \DecValTok{0}\OperatorTok{*}\NormalTok{roots, }
\NormalTok{                         mode }\OperatorTok{=} \StringTok{"markers"}\NormalTok{, name }\OperatorTok{=} \StringTok{"df = zero"}\NormalTok{, marker\_size }\OperatorTok{=} \DecValTok{12}\NormalTok{))}
\NormalTok{fig.update\_layout(font\_size }\OperatorTok{=} \DecValTok{20}\NormalTok{, yaxis\_range}\OperatorTok{=}\NormalTok{[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{])}
\NormalTok{fig.update\_layout(autosize}\OperatorTok{=}\VariableTok{False}\NormalTok{, width}\OperatorTok{=}\DecValTok{800}\NormalTok{, height}\OperatorTok{=}\DecValTok{600}\NormalTok{)}
\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

To apply gradient descent, we need to compute the derivative of the loss
function with respect to our parameter \(\theta_1\).

\begin{itemize}
\tightlist
\item
  Given our loss function,
  \[L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\]
\item
  We take the derivative with respect to \(\theta_1\)
  \[\frac{\partial}{\partial \theta_{1}} L(\theta_1^{(t)}) = \frac{-2}{n} \sum_{i=1}^n (y_i - \theta_1^{(t)} x_i) x_i\]
\item
  Which results in the gradient descent update rule
  \[\theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{d}{d\theta}L(\theta_1^{(t)})\]
\end{itemize}

for some learning rate \(\alpha\).

Implementing this in code, we can visualize the MSE loss on the
\texttt{tips} data. \textbf{MSE is convex}, so there is one global
minimum.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gradient\_descent(df, initial\_guess, alpha, n):}
    \CommentTok{"""Performs n steps of gradient descent on df using learning rate alpha starting}
\CommentTok{       from initial\_guess. Returns a numpy array of all guesses over time."""}
\NormalTok{    guesses }\OperatorTok{=}\NormalTok{ [initial\_guess]}
\NormalTok{    current\_guess }\OperatorTok{=}\NormalTok{ initial\_guess}
    \ControlFlowTok{while} \BuiltInTok{len}\NormalTok{(guesses) }\OperatorTok{\textless{}}\NormalTok{ n:}
\NormalTok{        current\_guess }\OperatorTok{=}\NormalTok{ current\_guess }\OperatorTok{{-}}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ df(current\_guess)}
\NormalTok{        guesses.append(current\_guess)}
        
    \ControlFlowTok{return}\NormalTok{ np.array(guesses)}

\KeywordTok{def}\NormalTok{ mse\_single\_arg(theta\_1):}
    \CommentTok{"""Returns the MSE on our data for the given theta1"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ df[}\StringTok{"total\_bill"}\NormalTok{]}
\NormalTok{    y\_obs }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{]}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x}
    \ControlFlowTok{return}\NormalTok{ np.mean((y\_hat }\OperatorTok{{-}}\NormalTok{ y\_obs) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}

\KeywordTok{def}\NormalTok{ mse\_loss\_derivative\_single\_arg(theta\_1):}
    \CommentTok{"""Returns the derivative of the MSE on our data for the given theta1"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ df[}\StringTok{"total\_bill"}\NormalTok{]}
\NormalTok{    y\_obs }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{]}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x}
    
    \ControlFlowTok{return}\NormalTok{ np.mean(}\DecValTok{2} \OperatorTok{*}\NormalTok{ (y\_hat }\OperatorTok{{-}}\NormalTok{ y\_obs) }\OperatorTok{*}\NormalTok{ x)}

\NormalTok{loss\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"theta\_1"}\NormalTok{:np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{), }\StringTok{"MSE"}\NormalTok{:[mse\_single\_arg(theta\_1) }\ControlFlowTok{for}\NormalTok{ theta\_1 }\KeywordTok{in}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{1.5}\NormalTok{, }\DecValTok{1}\NormalTok{)]\})}

\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ gradient\_descent(mse\_loss\_derivative\_single\_arg, }\OperatorTok{{-}}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.0001}\NormalTok{, }\DecValTok{100}\NormalTok{)}

\NormalTok{plt.plot(loss\_df[}\StringTok{"theta\_1"}\NormalTok{], loss\_df[}\StringTok{"MSE"}\NormalTok{])}
\NormalTok{plt.scatter(trajectory, [mse\_single\_arg(guess) }\ControlFlowTok{for}\NormalTok{ guess }\KeywordTok{in}\NormalTok{ trajectory], c}\OperatorTok{=}\StringTok{"white"}\NormalTok{, edgecolor}\OperatorTok{=}\StringTok{"firebrick"}\NormalTok{)}
\NormalTok{plt.scatter(trajectory[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{], mse\_single\_arg(trajectory[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]), c}\OperatorTok{=}\StringTok{"firebrick"}\NormalTok{)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}theta\_1$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r"$L(\textbackslash{}theta\_1)$"}\NormalTok{)}\OperatorTok{;}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Final guess for theta\_1: }\SpecialCharTok{\{}\NormalTok{trajectory[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Final guess for theta_1: 0.14369554654231262
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-21-output-2.pdf}

}

\end{figure}

\hypertarget{gradient-descent-on-multi-dimensional-models}{%
\subsection{Gradient Descent on Multi-Dimensional
Models}\label{gradient-descent-on-multi-dimensional-models}}

The function we worked with above was one-dimensional -- we were only
minimizing the function with respect to a single parameter, \(\theta\).
However, models usually have a cost function with multiple parameters
that need to be optimized. For example, simple linear regression has 2
parameters: \[\hat{y} + \theta_0 + \theta_1x\] and multiple linear
regression has \(p+1\) parameters:
\[\mathbb{Y} = \theta_0 + \theta_1 \Bbb{X}_{:,1} + \theta_2 \Bbb{X}_{:,2} + \cdots + \theta_p \Bbb{X}_{:,p}\]

We'll need to expand gradient descent so we can update our guesses for
all model parameters all in one go.

With multiple parameters to optimize, we consider a \textbf{loss
surface}, or the model's loss for a particular \emph{combination} of
possible parameter values.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ plotly.graph\_objects }\ImportTok{as}\NormalTok{ go}


\KeywordTok{def}\NormalTok{ mse\_loss(theta, X, y\_obs):}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ X }\OperatorTok{@}\NormalTok{ theta}
    \ControlFlowTok{return}\NormalTok{ np.mean((y\_hat }\OperatorTok{{-}}\NormalTok{ y\_obs) }\OperatorTok{**} \DecValTok{2}\NormalTok{)    }

\NormalTok{tips\_with\_bias }\OperatorTok{=}\NormalTok{ df.copy()}
\NormalTok{tips\_with\_bias[}\StringTok{"bias"}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\NormalTok{tips\_with\_bias }\OperatorTok{=}\NormalTok{ tips\_with\_bias[[}\StringTok{"bias"}\NormalTok{, }\StringTok{"total\_bill"}\NormalTok{]]}

\NormalTok{uvalues }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{vvalues }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{(u,v) }\OperatorTok{=}\NormalTok{ np.meshgrid(uvalues, vvalues)}
\NormalTok{thetas }\OperatorTok{=}\NormalTok{ np.vstack((u.flatten(),v.flatten()))}

\KeywordTok{def}\NormalTok{ mse\_loss\_single\_arg(theta):}
    \ControlFlowTok{return}\NormalTok{ mse\_loss(theta, tips\_with\_bias, df[}\StringTok{"tip"}\NormalTok{])}

\NormalTok{MSE }\OperatorTok{=}\NormalTok{ np.array([mse\_loss\_single\_arg(t) }\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in}\NormalTok{ thetas.T])}

\NormalTok{loss\_surface }\OperatorTok{=}\NormalTok{ go.Surface(x}\OperatorTok{=}\NormalTok{u, y}\OperatorTok{=}\NormalTok{v, z}\OperatorTok{=}\NormalTok{np.reshape(MSE, u.shape))}

\NormalTok{ind }\OperatorTok{=}\NormalTok{ np.argmin(MSE)}
\NormalTok{optimal\_point }\OperatorTok{=}\NormalTok{ go.Scatter3d(name }\OperatorTok{=} \StringTok{"Optimal Point"}\NormalTok{,}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ [thetas.T[ind,}\DecValTok{0}\NormalTok{]], y }\OperatorTok{=}\NormalTok{ [thetas.T[ind,}\DecValTok{1}\NormalTok{]], }
\NormalTok{    z }\OperatorTok{=}\NormalTok{ [MSE[ind]],}
\NormalTok{    marker}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(size}\OperatorTok{=}\DecValTok{10}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{))}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ go.Figure(data}\OperatorTok{=}\NormalTok{[loss\_surface, optimal\_point])}
\NormalTok{fig.update\_layout(scene }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    xaxis\_title }\OperatorTok{=} \StringTok{"theta0"}\NormalTok{,}
\NormalTok{    yaxis\_title }\OperatorTok{=} \StringTok{"theta1"}\NormalTok{,}
\NormalTok{    zaxis\_title }\OperatorTok{=} \StringTok{"MSE"}\NormalTok{), autosize}\OperatorTok{=}\VariableTok{False}\NormalTok{, width}\OperatorTok{=}\DecValTok{800}\NormalTok{, height}\OperatorTok{=}\DecValTok{600}\NormalTok{)}

\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

We can also visualize a bird's-eye view of the loss surface from above
using a contour plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{contour }\OperatorTok{=}\NormalTok{ go.Contour(x}\OperatorTok{=}\NormalTok{u[}\DecValTok{0}\NormalTok{], y}\OperatorTok{=}\NormalTok{v[:, }\DecValTok{0}\NormalTok{], z}\OperatorTok{=}\NormalTok{np.reshape(MSE, u.shape))}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ go.Figure(contour)}
\NormalTok{fig.update\_layout(}
\NormalTok{    xaxis\_title }\OperatorTok{=} \StringTok{"theta0"}\NormalTok{,}
\NormalTok{    yaxis\_title }\OperatorTok{=} \StringTok{"theta1"}\NormalTok{, autosize}\OperatorTok{=}\VariableTok{False}\NormalTok{, width}\OperatorTok{=}\DecValTok{800}\NormalTok{, height}\OperatorTok{=}\DecValTok{600}\NormalTok{)}

\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\hypertarget{the-gradient-vector}{%
\subsubsection{The Gradient Vector}\label{the-gradient-vector}}

As before, the derivative of the loss function tells us the best way
towards the minimum value.

On a 2D (or higher) surface, the best way to go down (gradient) is
described by a \emph{vector}.

\begin{quote}
Math Aside: Partial Derivatives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  For an equation with multiple variables, we take a \textbf{partial
  derivative} by differentiating with respect to just one variable at a
  time. The partial derivative is denoted with a \(\partial\).
  Intuitively, we want to see how the function changes if we only vary
  one variable while holding other variables constant.
\item
  Using \(f(x, y) = 3x^2 + y\) as an example,

  \begin{itemize}
  \tightlist
  \item
    taking the partial derivative with respect to x and treating y as a
    constant gives us \(\frac{\partial f}{\partial x} = 6x\)
  \item
    taking the partial derivative with respect to y and treating x as a
    constant gives us \(\frac{\partial f}{\partial y} = 1\)
  \end{itemize}
\end{itemize}
\end{quote}

For the \emph{vector} of parameter values
\(\vec{\theta} = \begin{bmatrix}  \theta_{0} \\  \theta_{1} \\  \end{bmatrix}\),
we take the \emph{partial derivative} of loss with respect to each
parameter: \(\frac{\partial L}{\partial \theta_0}\) and
\(\frac{\partial L}{\partial \theta_1}\).

\begin{quote}
For example, consider the 2D function:
\[f(\theta_0, \theta_1) = 8 \theta_0^2 + 3\theta_0\theta_1\] For a
function of 2 variables \(f(\theta_0, \theta_1)\), we define the
gradient \[
\begin{align}
\frac{\partial f}{\partial \theta_{0}} &= 16\theta_0 + 3\theta_1 \\
\frac{\partial f}{\partial \theta_{1}} &= 3\theta_0 \\
\nabla_{\vec{\theta}} f(\vec{\theta}) &=  \begin{bmatrix} 16\theta_0 + 3\theta_1 \\ 3\theta_0 \\ \end{bmatrix}
\end{align}
\]
\end{quote}

The \textbf{gradient vector} of a generic function of \(p+1\) variables
is therefore
\[\nabla_{\vec{\theta}} L =  \begin{bmatrix} \frac{\partial L}{\partial \theta_0} \\ \frac{\partial L}{\partial \theta_1} \\ \vdots \end{bmatrix}\]
where \(\nabla_\theta L\) always points in the downhill direction of the
surface. We can interpret each gradient as: ``If I nudge the \(i\)th
model weight, what happens to loss?''

We can use this to update our 1D gradient rule for models with multiple
parameters.

\begin{itemize}
\item
  Recall our 1D update rule:
  \[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]
\item
  For models with multiple parameters, we work in terms of vectors:
  \[\begin{bmatrix}
           \theta_{0}^{(t+1)} \\
           \theta_{1}^{(t+1)} \\
           \vdots
         \end{bmatrix} = \begin{bmatrix}
           \theta_{0}^{(t)} \\
           \theta_{1}^{(t)} \\
           \vdots
         \end{bmatrix} - \alpha \begin{bmatrix}
           \frac{\partial L}{\partial \theta_{0}} \\
           \frac{\partial L}{\partial \theta_{1}} \\
           \vdots \\
         \end{bmatrix}\]
\item
  Written in a more compact form,
  \[\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)}) \]

  \begin{itemize}
  \tightlist
  \item
    \(\theta\) is a vector with our model weights
  \item
    \(L\) is the loss function
  \item
    \(\alpha\) is the learning rate (ours is constant, but other
    techniques use an \(\alpha\) that decreases over time)
  \item
    \(\vec{\theta}^{(t)}\) is the current value of \(\theta\)
  \item
    \(\vec{\theta}^{(t+1)}\) is the next value of \(\theta\)
  \item
    \(\nabla_{\vec{\theta}} L(\theta^{(t)})\) is the gradient of the
    loss function evaluated at the current \(\vec{\theta}^{(t)}\)
  \end{itemize}
\end{itemize}

\hypertarget{batch-gradient-descent-and-stochastic-gradient-descent}{%
\subsection{Batch Gradient Descent and Stochastic Gradient
Descent}\label{batch-gradient-descent-and-stochastic-gradient-descent}}

Formally, the algorithm we derived above is called \textbf{batch
gradient descent.} For each iteration of the algorithm, the derivative
of loss is computed across the \emph{entire} batch of all \(n\)
datapoints. While this update rule works well in theory, it is not
practical in most circumstances. For large datasets (with perhaps
billions of datapoints), finding the gradient across all the data is
incredibly computationally taxing; gradient descent will converge slowly
because each individual update is slow.

\textbf{Stochastic (mini-batch) gradient descent} tries to address this
issue. In stochastic descent, only a \emph{sample} of the full dataset
is used at each update. We estimate the true gradient of the loss
surface using just that sample of data. The \textbf{batch size} is the
number of data points used in each sample. The sampling strategy is
generally without replacement (data is shuffled and batch size examples
are selected one at a time.)

Each complete ``pass'' through the data is known as a \textbf{training
epoch}. After shuffling the data, in a single \textbf{training epoch} of
stochastic gradient descent, we

\begin{itemize}
\tightlist
\item
  Compute the gradient on the first x\% of the data. Update the
  parameter guesses.
\item
  Compute the gradient on the next x\% of the data. Update the parameter
  guesses.
\item
  \(\dots\)
\item
  Compute the gradient on the last x\% of the data. Update the parameter
  guesses.
\end{itemize}

Every data point appears once in a single training epoch. We then
perform several training epochs until we're satisfied.

Batch gradient descent is a deterministic technique -- because the
entire dataset is used at each update iteration, the algorithm will
always advance towards the minimum of the loss surface. In contrast,
stochastic gradient descent involve an element of randomness. Since only
a subset of the full data is used to update the guess for
\(\vec{\theta}\) at each iteration, there's a chance the algorithm will
not progress towards the true minimum of loss with each update. Over the
longer term, these stochastic techniques should still converge towards
the optimal solution.

The diagrams below represent a ``bird's eye view'' of a loss surface
from above. Notice that batch gradient descent takes a direct path
towards the optimal \(\hat{\theta}\). Stochastic gradient descent, in
contrast, ``hops around'' on its path to the minimum point on the loss
surface. This reflects the randomness of the sampling process at each
update step.

To summarize the tradeoffs of batch size:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
-
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smaller Batch Size
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Larger Batch Size
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Pros & More frequent gradient updates & Leverage hardware acceleration
to improve overall system performance and higher quality gradient
updates \\
Cons & More variability in the gradient estimates & Less frequent
gradient updates \\
\end{longtable}

The typical solution is to set batch size to ensure sufficient hardware
utilization.

\bookmarksetup{startatroot}

\hypertarget{feature-engineering}{%
\chapter{Feature Engineering}\label{feature-engineering}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Recognize the value of feature engineering as a tool to improve model
  performance
\item
  Implement polynomial feature generation and one hot encoding
\item
  Understand the interactions between model complexity, model variance,
  and training error
\end{itemize}

\end{tcolorbox}

At this point, we've grown quite familiar with the modeling process.
We've introduced the concept of loss, used it to fit several types of
models, and, most recently, extended our analysis to multiple
regression. Along the way, we've forged our way through the mathematics
of deriving the optimal model parameters in all its gory detail. It's
time to make our lives a little easier -- let's implement the modeling
process in code!

In this lecture, we'll explore two techniques for model fitting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Translating our derived formulas for regression to \texttt{python}
\item
  Using \texttt{python}'s \texttt{sklearn} package
\end{enumerate}

With our new programming frameworks in hand, we will also add
sophistication to our models by introducing more complex features to
enhance model performance.

\hypertarget{gradient-descent-cont.}{%
\section{Gradient Descent Cont.}\label{gradient-descent-cont.}}

Before we dive into feature engineering, let's quickly review gradient
descent, which we covered in the last lecture. Recall that gradient
descent is a powerful technique for choosing the model parameters that
minimize the loss function.

\hypertarget{gradient-descent-review}{%
\subsection{Gradient Descent Review}\label{gradient-descent-review}}

As we learned earlier, we set the derivative of the loss function to
zero and solve to determine the optimal parameters \(\theta\) that
minimize loss. For a loss surface in 2D (or higher), the best way to
minimize loss is to ``walk'' down the loss surface until we reach our
optimal parameters \(\vec{\theta}\). The \textbf{gradient vector} tells
us which direction to ``walk'' in.

For example, the \emph{vector} of parameter values
\(\vec{\theta} = \begin{bmatrix}  \theta_{0} \\  \theta_{1} \\  \end{bmatrix}\)
gives us a two parameter model (d = 2). To calculate our gradient
vector, we can take the \emph{partial derivative} of loss with respect
to each parameter: \(\frac{\partial L}{\partial \theta_0}\) and
\(\frac{\partial L}{\partial \theta_1}\).

Its \textbf{gradient vector} would then be the 2D vector:
\[\nabla_{\vec{\theta}} L =  \begin{bmatrix} \frac{\partial L}{\partial \theta_0} \\ \frac{\partial L}{\partial \theta_1} \end{bmatrix}\]

Note that \(-\nabla_{\vec{\theta}} L\) always points in the
\textbf{downhill direction} of the surface.

Recall that we also discussed the gradient descent update rule, where we
nudge \(\theta\) in a negative gradient direction until \(\theta\)
converges.

As a refresher, the rule is as follows:
\[\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\vec{\theta}^{(t)}) \]

\begin{itemize}
\tightlist
\item
  \(\theta\) is a vector with our model weights
\item
  \(L\) is the loss function
\item
  \(\alpha\) is the learning rate
\item
  \(\vec{\theta}^{(t)}\) is the current value of \(\theta\)
\item
  \(\vec{\theta}^{(t+1)}\) is the next value of \(\theta\)
\item
  \(\nabla_{\vec{\theta}} L(\vec{\theta}^{(t)})\) is the gradient of the
  loss function evaluated at the current \(\theta\):
  \[\frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))\]
\end{itemize}

Let's now walk through an example of calculating and updating the
gradient vector. Say our model and loss are: \[\begin{align}
f_{\vec{\theta}}(\vec{x}) &= \vec{x}^T\vec{\theta} = \theta_0x_0 + \theta_1x_1
\\l(y, \hat{y}) &= (y - \hat{y})^2
\end{align}
\]

Plugging in \(f_{\vec{\theta}}(\vec{x})\) for \(\hat{y}\), our loss
function becomes
\(l(\vec{\theta}, \vec{x}, \hat{y}) = (y_i - \theta_0x_0 - \theta_1x_1)^2\).

To calculate our gradient vector, we can start by computing the partial
derivative of the loss function with respect to \(\theta_0\):
\[\frac{\partial}{\partial \theta_{0}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_0)\]

Let's now do the same but with respect to \(\theta_1\):
\[\frac{\partial}{\partial \theta_{1}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_1)\]

Putting this together, our gradient vector is:
\[\nabla_{\theta} l(\vec{\theta}, \vec{x}, y_i) =  \begin{bmatrix} -2(y_i - \theta_0x_0 - \theta_1x_1)(x_0) \\ -2(y_i - \theta_0x_0 - \theta_1x_1)(x_1) \end{bmatrix}\]

Remember that we need to keep updating \(\theta\) until the algorithm
\textbf{converges} to a solution and stops updating significantly (or at
all). When updating \(\theta\), we'll have a fixed number of updates and
subsequent updates will be quite small (we won't change \(\theta\) by
much).

\hypertarget{stochastic-mini-batch-gradient-descent}{%
\subsection{Stochastic (Mini-batch) Gradient
Descent}\label{stochastic-mini-batch-gradient-descent}}

Let's now dive deeper into gradient and stochastic gradient descent. In
the previous lecture, we discussed how finding the gradient across all
the data is extremeley computationally taxing and takes a lot of
resources to calculate.

We know that the solution to the normal equation is
\(\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\).
Let's break this down and determine the computational complexity for
this solution.

Let \(n\) be the number of samples (rows) and \(d\) be the number of
features (columns).

\begin{itemize}
\tightlist
\item
  Computing \((\mathbb{X}^{\top}\mathbb{X})\) takes \(O(nd^2)\) time,
  and it's inverse takes another \(O(d^3)\) time to calculate; overall,
  \((\mathbb{X}^{\top}\mathbb{X})^{-1}\) takes \(O(nd^2) + O(d^3)\)
  time.
\item
  \(\mathbb{X}^{\top}\mathbb{Y}\) takes \(O(nd)\) time.
\item
  Multiplying \((\mathbb{X}^{\top}\mathbb{X})^{-1}\) and
  \(\mathbb{X}^{\top}\mathbb{Y}\) takes \(O(d^2)\) time.
\end{itemize}

In total, calculating the solution to the normal equation takes
\(O(nd^2) + O(d^3) + O(nd) + O(d^2)\) time. We can see that
\(O(nd^2) + O(d^3)\) dominates the complexity --- this can be
problematic for high-dimensional models and very large datasets.

On the other hand, the time complexity of a single gradient descent step
takes only \(O(nd)\) time.

Suppose we run \(T\) iterations. The final complexity would then be
\(O(Tnd)\). Typically, \(n\) is much larger than \(T\) and \(d\). How
can we reduce the cost of this algorithm using a technique from Data
100? Do we really need to use \(n\) data points? We don't! Instead, we
can use stochastic gradient descent.

We know that our true gradient of
\(\nabla_{\vec{\theta}} L (\vec{\theta^{(t)}}) = \frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))\)
has a time complexity of \(O(nd)\). Instead of using all \(n\) samples
to calculate the true gradient of the loss surface, let's use a sample
of our data to approximate. Say we sample \(b\) records
(\(s_1, \cdots, s_b\)) from our \(n\) datapoints. Our new (stochastic)
gradient descent function will be
\(\nabla_{\vec{\theta}} L (\vec{\theta^{(t)}}) = \frac{1}{b}\sum_{i=1}^{b}\nabla_{\vec{\theta}} l(y_{s_i}, f_{\vec{\theta}^{(t)}}(X_{s_i}))\)
and will now have a time complexity of \(O(bd)\), which is much faster!

Stochastic gradient descent helps us approximate the gradient while also
reducing the time complexity and computational cost. The time complexity
scales with the number of datapoints selected in the sample. To sample
data, there are two approaches we can use:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Shuffle the data and select samples one at a time.
\item
  Take a simple random sample for each gradient computation.
\end{enumerate}

But how do we decide our mini-batch size (\(b\)), or the number of
datapoints in our sample? The original stochastic gradient descent
algorithm uses \(b=1\) so that only one sample is used to approximate
the gradient at a time. Although we don't use such a small mini-batch
size often, \(b\) typically is small. When choosing \(b\), there are
several factors to consider: a larger batch size results in a better
gradient estimate, parallelism, and other systems factors. On the other
hand, a smaller batch size will be faster and have more frequent
updates. It is up to data scientists to balance the tradeoff between
batch size and time complexity.

Summarizing our two gradient descent techniques:

\begin{itemize}
\tightlist
\item
  \textbf{(Batch) Gradient Descent}: Gradient descent computes the
  \textbf{true} descent and always descends towards the true minimum of
  the loss. While accurate, it can often be computationally expensive.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{(Minibatch) Stochastic gradient descent}: Stochastic gradient
  descent \textbf{approximates} the true gradient descent. It may not
  descend towards the true minimum with each update, but it's often less
  computationally expensive than batch gradient descent.
\end{itemize}

\hypertarget{feature-engineering-1}{%
\section{Feature Engineering}\label{feature-engineering-1}}

At this point in the course, we've equipped ourselves with some powerful
techniques to build and optimize models. We've explored how to develop
models of multiple variables, as well as how to transform variables to
help \textbf{linearize} a dataset and fit these models to maximize their
performance.

All of this was done with one major caveat: the regression models we've
worked with so far are all \textbf{linear in the input variables}. We've
assumed that our predictions should be some combination of linear
variables. While this works well in some cases, the real world isn't
always so straightforward. We'll learn an important method to address
this issue -- feature engineering -- and consider some new problems that
can arise when we do so.

Feature engineering is the process of \emph{transforming} raw features
into \emph{more informative features} that can be used in modeling or
EDA tasks and improve model performance.

Feature engineering allows you to:

\begin{itemize}
\tightlist
\item
  Capture domain knowledge
\item
  Express non-linear relationships using linear models
\item
  Use non-numeric (qualitative) features in models
\end{itemize}

\hypertarget{feature-functions}{%
\section{Feature Functions}\label{feature-functions}}

A \textbf{feature function} describes the transformations we apply to
raw features in a dataset to create a design matrix of transformed
features. We typically denote the feature function as \(\Phi\) (the
Greek letter ``phi'' that we use to represent the true function). When
we apply the feature function to our original dataset \(\mathbb{X}\),
the result, \(\Phi(\mathbb{X})\), is a transformed design matrix ready
to be used in modeling.

For example, we might design a feature function that computes the square
of an existing feature and adds it to the design matrix. In this case,
our existing matrix \([x]\) is transformed to \([x, x^2]\). Its
\emph{dimension} increases from 1 to 2. Often, the dimension of the
\emph{featurized} dataset increases as seen here.

The new features introduced by the feature function can then be used in
modeling. Often, we use the symbol \(\phi_i\) to represent transformed
features after feature engineering.

\[
\begin{align}
\hat{y} &= \theta_0 + \theta_1 x + \theta_2 x^2 \\
\hat{y} &= \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2
\end{align}
\]

In matrix notation, the symbol \(\Phi\) is sometimes used to denote the
design matrix after feature engineering has been performed. Note that in
the usage below, \(\Phi\) is now a feature-engineered matrix, rather
than a function.

\[\hat{\mathbb{Y}} = \Phi \theta\]

More formally, we describe a feature function as transforming the
original \(\mathbb{R}^{n \times p}\) dataset \(\mathbb{X}\) to a
featurized \(\mathbb{R}^{n \times p'}\) dataset \(\mathbb{\Phi}\), where
\(p'\) is typically greater than \(p\).

\[\mathbb{X} \in \mathbb{R}^{n \times p} \longrightarrow \Phi \in \mathbb{R}^{n \times p'}\]

\hypertarget{one-hot-encoding}{%
\section{One Hot Encoding}\label{one-hot-encoding}}

Feature engineering opens up a whole new set of possibilities for
designing better-performing models. As you will see in lab and homework,
feature engineering is one of the most important parts of the entire
modeling process.

A particularly powerful use of feature engineering is to allow us to
perform regression on \emph{non-numeric} features. \textbf{One hot
encoding} is a feature engineering technique that generates numeric
features from categorical data, allowing us to use our usual methods to
fit a regression model on the data.

To illustrate how this works, we'll refer back to the \texttt{tips}
dataset from previous lectures. Consider the \texttt{"day"} column of
the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ sklearn.linear\_model }\ImportTok{as}\NormalTok{ lm}
\NormalTok{tips }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"tips"}\NormalTok{)}
\NormalTok{tips.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrllllr}
\toprule
{} &  total\_bill &   tip &     sex & smoker &  day &    time &  size \\
\midrule
0 &       16.99 &  1.01 &  Female &     No &  Sun &  Dinner &     2 \\
1 &       10.34 &  1.66 &    Male &     No &  Sun &  Dinner &     3 \\
2 &       21.01 &  3.50 &    Male &     No &  Sun &  Dinner &     3 \\
3 &       23.68 &  3.31 &    Male &     No &  Sun &  Dinner &     2 \\
4 &       24.59 &  3.61 &  Female &     No &  Sun &  Dinner &     4 \\
\bottomrule
\end{tabular}

At first glance, it doesn't seem possible to fit a regression model to
this data -- we can't directly perform any mathematical operations on
the entry ``Sun''.

To resolve this, we instead create a new table with a feature for each
unique value in the original \texttt{"day"} column. We then iterate
through the \texttt{"day"} column. For each entry in \texttt{"day"} we
fill the corresponding feature in the new table with 1. All other
features are set to 0.

In short, each category of a categorical variable gets its own feature

Value = 1 if a row belongs to the category

Value = 0 otherwise

The \texttt{OneHotEncoder} class of \texttt{sklearn}
(\href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\#sklearn.preprocessing.OneHotEncoder.get_feature_names_out}{documentation})
offers a quick way to perform this one-hot encoding. You will explore
its use in detail in the lab. For now, recognize that we follow a very
similar workflow to when we were working with the
\texttt{LinearRegression} class: we initialize a \texttt{OneHotEncoder}
object, fit it to our data, and finally use \texttt{.transform} to apply
the fitted encoder.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OneHotEncoder}

\CommentTok{\# Initialize a OneHotEncoder object}
\NormalTok{ohe }\OperatorTok{=}\NormalTok{ OneHotEncoder()}

\CommentTok{\# Fit the encoder}
\NormalTok{ohe.fit(tips[[}\StringTok{"day"}\NormalTok{]])}

\CommentTok{\# Use the encoder to transform the raw "day" feature}
\NormalTok{encoded\_day }\OperatorTok{=}\NormalTok{ ohe.transform(tips[[}\StringTok{"day"}\NormalTok{]]).toarray()}
\NormalTok{encoded\_day\_df }\OperatorTok{=}\NormalTok{ pd.DataFrame(encoded\_day, columns}\OperatorTok{=}\NormalTok{ohe.get\_feature\_names\_out())}

\NormalTok{encoded\_day\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrr}
\toprule
{} &  day\_Fri &  day\_Sat &  day\_Sun &  day\_Thur \\
\midrule
0 &      0.0 &      0.0 &      1.0 &       0.0 \\
1 &      0.0 &      0.0 &      1.0 &       0.0 \\
2 &      0.0 &      0.0 &      1.0 &       0.0 \\
3 &      0.0 &      0.0 &      1.0 &       0.0 \\
4 &      0.0 &      0.0 &      1.0 &       0.0 \\
\bottomrule
\end{tabular}

The one-hot encoded features can then be used in the design matrix to
train a model:

\[\hat{y} = \theta_1 (\text{total}\_\text{bill}) + \theta_2 (\text{size}) + \theta_3 (\text{day}\_\text{Fri}) + \theta_4 (\text{day}\_\text{Sat}) + \theta_5 (\text{day}\_\text{Sun}) + \theta_6 (\text{day}\_\text{Thur})\]

Or in shorthand:

\[\hat{y} = \theta_{1}\phi_{1} + \theta_{2}\phi_{2} + \theta_{3}\phi_{3} + \theta_{4}\phi_{4} + \theta_{5}\phi_{5} + \theta_{6}\phi_{6}\]

Now, the \texttt{day} feature (or rather, the four new boolean features
that represent day) can be used to fit a model.

Using \texttt{sklearn} to fit the new model, we can determine the model
coefficients, allowing us to understand how each feature impacts the
predicted tip.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{data\_w\_ohe }\OperatorTok{=}\NormalTok{ tips[[}\StringTok{"total\_bill"}\NormalTok{, }\StringTok{"size"}\NormalTok{, }\StringTok{"day"}\NormalTok{]].join(encoded\_day\_df).drop(columns }\OperatorTok{=} \StringTok{"day"}\NormalTok{)}
\NormalTok{ohe\_model }\OperatorTok{=}\NormalTok{ lm.LinearRegression(fit\_intercept}\OperatorTok{=}\VariableTok{False}\NormalTok{) }\CommentTok{\#Tell sklearn to not add an additional bias column. Why?}
\NormalTok{ohe\_model.fit(data\_w\_ohe, tips[}\StringTok{"tip"}\NormalTok{])}

\NormalTok{pd.DataFrame(\{}\StringTok{"Feature"}\NormalTok{:data\_w\_ohe.columns, }\StringTok{"Model Coefficient"}\NormalTok{:ohe\_model.coef\_\})}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llr}
\toprule
{} &     Feature &  Model Coefficient \\
\midrule
0 &  total\_bill &           0.092994 \\
1 &        size &           0.187132 \\
2 &     day\_Fri &           0.745787 \\
3 &     day\_Sat &           0.621129 \\
4 &     day\_Sun &           0.732289 \\
5 &    day\_Thur &           0.668294 \\
\bottomrule
\end{tabular}

For example, when looking at the coefficient for \texttt{day\_Fri}, we
can now understand the impact of it being Friday on the predicted tip
--- if it is a Friday, the predicted tip increases by approximately
\$0.75.

When one-hot encoding, keep in mind that any set of one-hot encoded
columns will always sum to a column of all ones, representing the bias
column. More formally, the bias column is a linear combination of the
OHE columns.

We must be careful not to include this bias column in our design matrix.
Otherwise, there will be linear dependence in the model, meaning
\(\mathbb{X}^{\top}\mathbb{X}\) would no longer be invertible, and our
OLS estimate
\(\hat{\theta} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
fails.

To resolve this issue, we simply omit one of the one-hot encoded columns
\emph{or} do not include an intercept term. The adjusted design matrices
are shown below.

Either approach works --- we still retain the same information as the
omitted column being a linear combination of the remaining columns.

\hypertarget{polynomial-features}{%
\section{Polynomial Features}\label{polynomial-features}}

We have encountered a few cases now where models with linear features
have performed poorly on datasets that show clear non-linear curvature.

As an example, consider the \texttt{vehicles} dataset, which contains
information about cars. Suppose we want to use the \texttt{hp}
(horsepower) of a car to predict its \texttt{"mpg"} (gas mileage in
miles per gallon). If we visualize the relationship between these two
variables, we see a non-linear curvature. Fitting a linear model to
these variables results in a high (poor) value of RMSE.

\[\hat{y} = \theta_0 + \theta_1 (\text{hp})\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.options.mode.chained\_assignment }\OperatorTok{=} \VariableTok{None} 
\NormalTok{vehicles }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"mpg"}\NormalTok{).dropna().rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{"horsepower"}\NormalTok{: }\StringTok{"hp"}\NormalTok{\}).sort\_values(}\StringTok{"hp"}\NormalTok{)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ vehicles[[}\StringTok{"hp"}\NormalTok{]]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"mpg"}\NormalTok{]}

\NormalTok{hp\_model }\OperatorTok{=}\NormalTok{ lm.LinearRegression()}
\NormalTok{hp\_model.fit(X, Y)}
\NormalTok{hp\_model\_predictions }\OperatorTok{=}\NormalTok{ hp\_model.predict(X)}

\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{sns.scatterplot(data}\OperatorTok{=}\NormalTok{vehicles, x}\OperatorTok{=}\StringTok{"hp"}\NormalTok{, y}\OperatorTok{=}\StringTok{"mpg"}\NormalTok{)}
\NormalTok{plt.plot(vehicles[}\StringTok{"hp"}\NormalTok{], hp\_model\_predictions, c}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{)}\OperatorTok{;}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"MSE of model with (hp) feature: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean((Y}\OperatorTok{{-}}\NormalTok{hp\_model\_predictions)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
MSE of model with (hp) feature: 23.943662938603108
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{feature_engineering/feature_engineering_files/figure-pdf/cell-5-output-2.pdf}

}

\end{figure}

As we can see from the plot, the data follows a curved line rather than
a straight one. To capture this non-linearity, we can incorporate
\textbf{non-linear} features. Let's introduce a \textbf{polynomial}
term, \(\text{hp}^2\), into our regression model. The model now takes
the form:

\[\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)\]
\[\hat{y} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2\]

How can we fit a model with non-linear features? We can use the exact
same techniques as before: ordinary least squares, gradient descent, or
\texttt{sklearn}. This is because our new model is still a
\textbf{linear model}. Although it contains non-linear \emph{features},
it is linear with respect to the model \emph{parameters}. All of our
previous work on fitting models was done under the assumption that we
were working with linear models. Because our new model is still linear,
we can apply our existing methods to determine the optimal parameters.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a hp\^{}2 feature to the design matrix}
\NormalTok{X }\OperatorTok{=}\NormalTok{ vehicles[[}\StringTok{"hp"}\NormalTok{]]}
\NormalTok{X[}\StringTok{"hp\^{}2"}\NormalTok{] }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"hp"}\NormalTok{]}\OperatorTok{**}\DecValTok{2}

\CommentTok{\# Use sklearn to fit the model}
\NormalTok{hp2\_model }\OperatorTok{=}\NormalTok{ lm.LinearRegression()}
\NormalTok{hp2\_model.fit(X, Y)}
\NormalTok{hp2\_model\_predictions }\OperatorTok{=}\NormalTok{ hp2\_model.predict(X)}

\NormalTok{sns.scatterplot(data}\OperatorTok{=}\NormalTok{vehicles, x}\OperatorTok{=}\StringTok{"hp"}\NormalTok{, y}\OperatorTok{=}\StringTok{"mpg"}\NormalTok{)}
\NormalTok{plt.plot(vehicles[}\StringTok{"hp"}\NormalTok{], hp2\_model\_predictions, c}\OperatorTok{=}\StringTok{"tab:red"}\NormalTok{)}\OperatorTok{;}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"MSE of model with (hp\^{}2) feature: }\SpecialCharTok{\{}\NormalTok{np}\SpecialCharTok{.}\NormalTok{mean((Y}\OperatorTok{{-}}\NormalTok{hp2\_model\_predictions)}\OperatorTok{**}\DecValTok{2}\NormalTok{)}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
MSE of model with (hp^2) feature: 18.984768907617223
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{feature_engineering/feature_engineering_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

Looking a lot better! By incorporating a squared feature, we are able to
capture the curvature of the dataset. Our model is now a parabola
centered on our data. Notice that our new model's error has decreased
relative to the original model with linear features.

\hypertarget{complexity-and-overfitting}{%
\section{Complexity and Overfitting}\label{complexity-and-overfitting}}

We've seen now that feature engineering allows us to build all sorts of
features to improve the performance of the model. In particular, we saw
that designing a more complex feature (squaring \texttt{hp} in the
\texttt{vehicles} data previously) substantially improved the model's
ability to capture non-linear relationships. To take full advantage of
this, we might be inclined to design increasingly complex features.
Consider the following three models, each of different order (the
maximum exponent power of each model):

\begin{itemize}
\tightlist
\item
  Model with order 2:
  \(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)\)
\item
  Model with order 3:
  \(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3)\)
\item
  Model with order 4:
  \(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3) + \theta_4 (\text{hp}^4)\)
\end{itemize}

As we can see in the plots above, MSE continues to decrease with each
additional polynomial term. To visualize it further, let's plot models
as the complexity increases from 0 to 7:

When we use our model to make predictions on the same data that was used
to fit the model, we find that the MSE decreases with each additional
polynomial term (as our model gets more complex). The \textbf{training
error} is the model's error when generating predictions from the same
data that was used for training purposes. We can conclude that the
training error goes down as the complexity of the model increases.

This seems like good news -- when working on the \textbf{training data},
we can improve model performance by designing increasingly complex
models.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Math Fact: Polynomial Degrees}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Given \(N\) overlapping data points, we can always find a polynomial of
degree \(N-1\) that goes through all those points.

For example, there always exists a degree-4 polynomial curve that can
perfectly model a dataset of 5 datapoints:

\end{tcolorbox}

However, high model complexity comes with its own set of issues. When
building the \texttt{vehicles} models above, we trained the models on
the \emph{entire} dataset and then evaluated their performance on this
same dataset. In reality, we are likely to instead train the model on a
\emph{sample} from the population, then use it to make predictions on
data it didn't encounter during training.

Let's walk through a more realistic example. Say we are given a training
dataset of just 6 datapoints and want to train a model to then make
predictions on a \emph{different} set of points. We may be tempted to
make a highly complex model (e.g., degree 5), especially given it makes
perfect predictions on the training data as clear on the left. However,
as shown in the graph on the right, this model would perform
\emph{horribly} on the rest of the population!

This phenomenon called \textbf{overfitting}. The model effectively just
memorized the training data it encountered when it was fitted, leaving
it unable to \textbf{generalize} well to data it didn't encounter during
training. This is a problem: we want models that are generalizable to
``unseen'' data.

Additionally, since complex models are sensitive to the specific dataset
used to train them, they have high \textbf{variance}. A model with high
variance tends to \emph{vary} more dramatically when trained on
different datasets. Going back to our example above, we can see our
degree-5 model varies erratically when we fit it to different samples of
6 points from \texttt{vehicles}.

We now face a dilemma: we know that we can \textbf{decrease training
error} by increasing model complexity, but models that are \emph{too}
complex start to overfit and can't be reapplied to new datasets due to
\textbf{high variance}.

We can see that there is a clear trade-off that comes from the
complexity of our model. As model complexity increases, the model's
error on the training data decreases. At the same time, the model's
variance tends to increase.

The takeaway here: we need to strike a balance in the complexity of our
models; we want models that are generalizable to ``unseen'' data. A
model that is too simple won't be able to capture the key relationships
between our variables of interest; a model that is too complex runs the
risk of overfitting.

This begs the question: how do we control the complexity of a model?
Stay tuned for Lecture 17 on Cross-Validation and Regularization!

\hypertarget{bonus-stochastic-gradient-descent-in-pytorch}{%
\section{\texorpdfstring{{[}Bonus{]} Stochastic Gradient Descent in
\texttt{PyTorch}}{{[}Bonus{]} Stochastic Gradient Descent in PyTorch}}\label{bonus-stochastic-gradient-descent-in-pytorch}}

While this material is out of scope for Data 100, it is useful if you
plan to enter a career in data science!

In practice, you will use software packages such as \texttt{PyTorch}
when computing gradients and implementing gradient descent. You'll often
follow three main steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample a batch of the data.
\item
  Compute the loss and the gradient.
\item
  Update your gradient until you reach an appropriate estimate of the
  true gradient.
\end{enumerate}

If you want to learn more, this
\href{https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html}{Intro
to PyTorch tutorial} is a great resource to get started!

\bookmarksetup{startatroot}

\hypertarget{case-study-in-human-contexts-and-ethics}{%
\chapter{Case Study in Human Contexts and
Ethics}\label{case-study-in-human-contexts-and-ethics}}

\textbf{Note:} Given the nuanced nature of some of the arguments made in
the lecture, it is highly recommended that you view the lecture
recording given by Professor Ari Edmundson to fully engage and
understand the material. The course notes will have the same broader
structure but are by no means comprehensive.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Learn about the ethical dilemmas that data scientists face.
\item
  Examine the Cook County Assessor's Office and Property Appraisal case
  study for fairness in housing appraisal.
\item
  Know how to critique models using contextual knowledge about data.
\end{itemize}

\end{tcolorbox}

\begin{quote}
\textbf{Disclaimer}: The following note discusses issues of structural
racism. Some of the items in this note may be sensitive and may or may
not be the opinions, ideas, and beliefs of the students who collected
the materials. The Data 100 course staff tries its best to only present
information that is relevant for teaching the lessons at hand.
\end{quote}

As data scientists, our goal is to wrangle data, recognize patterns and
use them to make predictions within a certain context. However, it is
often easy to abstract data away from its original context. In previous
lectures, we've explored datasets like \texttt{elections},
\texttt{babynames}, and \texttt{world\_bank} to learn fundamental
techniques for working with data, but rarely do we stop to ask questions
like ``How/when was this data collected?'' or ``Are there any inherent
biases in the data that could affect results?''. It turns out that
inquiries like these profoundly affect how data scientists approach a
task and convey their findings. This lecture explores these ethical
dilemmas through the lens of a case study.

Let's immerse ourselves in the real-world story of data scientists
working for an organization called the Cook County Assessor's Office
(CCAO) located in Chicago, Illinois. Their job is to \textbf{estimate
the values of houses} in order to \textbf{assign property taxes}. This
is because the tax burden in this area is determined by the estimated
\textbf{value} of a house rather than its price. Since value changes
over time and has no obvious indicators, the CCAO created a
\textbf{model} to estimate the values of houses. In this note, we will
dig deep into biases that arose in the model, the consequences to human
lives, and what we can learn from this example to avoid the same
mistakes in the future.

\hypertarget{the-problem}{%
\section{The Problem}\label{the-problem}}

What prompted the formation of the CCAO and led to the development of
this model? In 2017, an
\href{https://apps.chicagotribune.com/news/watchdog/cook-county-property-tax-divide/assessments.html}{investigative
report} by the \emph{Chicago Tribune} uncovered a major scandal in the
property assessment system managed by the CCAO under the watch of former
County Assessor Joseph Berrios. Working with experts from the University
of Chicago, the \emph{Chicago Tribune} journalists found that the CCAO's
model for estimating house value perpetuated a highly regressive tax
system that disproportionately burdened African-American and Latinx
homeowners in Cook County. How did the journalists demonstrate this
disparity?

The image above shows two standard metrics to estimate the fairness of
assessments: the
\href{https://www.realestateagent.com/real-estate-glossary/real-estate/coefficient-of-dispersion.html}{coefficient
of dispersion} and
\href{https://leg.wa.gov/House/Committees/FIN/Documents/2009/RatioText.pdf}{price-related
differential}. How they're calculated is out of scope for this class,
but you can assume that these metrics have been rigorously tested by
experts in the field and are a good indication of fairness. As we see
above, calculating these metrics for the Cook County prices revealed
that the pricing created by the CCAO did not fall in acceptable ranges.
While this on its own is \textbf{not the entire} story, it was a good
indicator that \textbf{something fishy was going on}.

This prompted journalists to investigate if the CCAO's model itself was
producing fair tax rates. When accounting for the homeowner's income,
they found that the model actually produced a \textbf{regressive} tax
rate (see figure above). A tax rate is \textbf{regressive} if the
percentage tax rate is higher for individuals with lower net income; it
is \textbf{progressive} if the percentage tax rate is higher for
individuals with higher net income.

Digging further, journalists found that the model was not only
regressive and unfair to lower-income individuals, but it was also
unfair to non-white homeowners (see figure above). The likelihood of a
property being under- or over-assessed was highly dependent on the
owner's race, and that did not sit well with many homeowners.

\hypertarget{spotlight-appeals}{%
\subsection{Spotlight: Appeals}\label{spotlight-appeals}}

What was the cause of such a major issue? It might be easy to simply
blame ``biased'' algorithms, but the main issue was not a faulty model.
Instead, it was largely due to the \textbf{appeals system} which enabled
the wealthy and privileged to more easily and successfully challenge
their assessments. Once given the CCAO model's initial assessment of
their home's value, homeowners could choose to appeal to a board of
elected officials to try and change the listed value of their home and,
consequently, how much they are taxed. In theory, this sounds like a
very fair system: a human being oversees the final pricing of houses
rather than a computer algorithm. In reality, this ended up exacerbating
the problem.

\begin{quote}
``Appeals are a good thing,'' Thomas Jaconetty, deputy assessor for
valuation and appeals, said in an interview. ``The goal here is
fairness. We made the numbers. We can change them.''
\end{quote}

We can borrow lessons from
\href{https://www.britannica.com/topic/critical-race-theory}{Critical
Race Theory} ------ on the surface, everyone has the legal right to try
and appeal the value of their home. However, not everyone has an
\emph{equal ability} to do so. Those who have the money to hire tax
lawyers to appeal for them have a drastically higher chance of trying
and succeeding in their appeal (see above figure). Many homeowners who
appealed were generally under-assessed compared to homeowners who did
not (see figure below). Clearly, the model is part of a deeper
institutional pattern rife with potential corruption.

In fact, Chicago boasts a large and thriving tax attorney industry
dedicated precisely to appealing property assessments, reflected in the
growing number of appeals in Cook County in the 21st century. Given
wealthier, whiter neighborhoods typically have greater access to
lawyers, they often appealed more and won reductions far more often than
their less wealthy neighbors. In other words, those with higher incomes
pay less in property tax, tax lawyers can grow their business due to
their role in appeals, and politicians are socially connected to the
aforementioned tax lawyers and wealthy homeowners. All these
stakeholders have reasons to advertise the appeals system as an integral
part of a fair system; after all, it serves to benefit them. Here lies
the value in asking questions: a system that seems fair on the surface
may, in reality, be unfair upon taking a closer look.

\hypertarget{human-impacts}{%
\subsection{Human Impacts}\label{human-impacts}}

What happened as a result of this corrupt system? As the \emph{Chicago
Tribune} reported, many African American and Latino homeowners purchased
homes only to find their houses were later appraised at levels far
higher than what they paid. As a result, homeowners were now responsible
for paying significantly more in taxes every year than initially
budgeted, putting them at risk of not being able to afford their homes
and losing them.

The impact of the housing model extends beyond the realm of home
ownership and taxation ------ the issues of justice go much deeper. This
model perpetrated much older patterns of racially discriminatory
practices in Chicago and across the United States. Unfortunately, it is
no accident that this happened in Chicago, one of the most segregated
cities in the United States
(\href{https://fivethirtyeight.com/features/the-most-diverse-cities-are-often-the-most-segregated/}{source}).
These factors are central to informing us, as data scientists, about
what is at stake.

\hypertarget{spotlight-intersection-of-real-estate-and-race}{%
\subsection{Spotlight: Intersection of Real Estate and
Race}\label{spotlight-intersection-of-real-estate-and-race}}

Before we dive into how the CCAO used data science to ``solve'' this
problem, let's briefly go through the history of discriminatory housing
practices in the United States to give more context on the gravity and
urgency of this situation.

Housing and real estate, among other factors, have been one of the most
significant and enduring drivers of structural racism and racial
inequality in the United States since the Civil War. It is one of the
main areas where inequalities are created and reproduced. In the early
20th century,
\href{https://www.history.com/topics/early-20th-century-us/jim-crow-laws}{Jim
Crow} laws were explicit in forbidding people of color from utilizing
the same facilities ------ such as buses, bathrooms, and pools ------ as
white individuals. This set of practices by government actors in
combination with overlapping practices driven by the private real estate
industry further served to make neighborhoods increasingly segregated.

Although advancements in civil rights have been made, the spirit of the
laws is alive in many parts of the US. In the 1920s and 1930s, it was
illegal for governments to actively segregate neighborhoods according to
race, but other methods were available for achieving the same ends. One
of the most notorious practices was \textbf{redlining}: the federal
housing agencies' process of distinguishing neighborhoods in a city in
terms of relative risk. The goal was to increase access to homeownership
for low-income Americans. In practice, however, it allowed real estate
professionals to legally perpetuate segregation. The federal housing
agencies deemed predominantly African American neighborhoods as high
risk and colored them in red ------ hence the name redlining
------~making it nearly impossible for African Americans to own a home.

The origins of the data that made these maps possible lay in a kind of
``racial data revolution'' in the private real estate industry beginning
in the 1920s. Segregation was established and reinforced in part through
the work of real estate agents who were also very concerned with
establishing reliable methods for predicting the value of a home. The
effects of these practices continue to resonate today.

Source: Colin Koopman, How We Became Our Data (2019) p.~137

\hypertarget{the-response-cook-county-open-data-initiative}{%
\section{The Response: Cook County Open Data
Initiative}\label{the-response-cook-county-open-data-initiative}}

The response to this problem started in politics. A new assessor, Fritz
Kaegi, was elected and created a new mandate with two goals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Distributional equity in property taxation, meaning that properties of
  the same value are treated alike during assessments.
\item
  Creating a new Office of Data Science.
\end{enumerate}

He wanted to not only create a more accurate algorithmic model but also
to design a new system to address the problems with the CCAO.

Let's frame this problem through the lens of the data science lifecycle.

\hypertarget{questionproblem-formulation}{%
\subsection{1. Question/Problem
Formulation}\label{questionproblem-formulation}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Driving Questions}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  What do we want to know?
\item
  What problems are we trying to solve?
\item
  What are the hypotheses we want to test?
\item
  What are our metrics for success?
\end{itemize}

\end{tcolorbox}

The old system was unfair because it was systemically inaccurate; it
made one kind of error for one group, and another kind of error for
another. Its goal was to ``create a robust pipeline that accurately
assesses property values at scale and is fair'', and in turn, they
defined fairness as accuracy: ``the ability of our pipeline to
accurately assess all residential property values, accounting for
disparities in geography, information, etc.'' Thus, the plan ------ make
the system more fair ------ was already framed in terms of a task
appropriate to a data scientist: make the assessments more accurate (or
more precisely, minimize errors in a particular way).

The idea here is that if the model is more accurate it will also
(perhaps necessarily) become more fair, which is a big assumption. There
are, in a sense, two different problems ------ make accurate
assessments, and make a fair system. Treating these two problems as one
makes it a more straightforward issue that can be solved technically
(with a good model) but does raise the question of if fairness and
accuracy are one and the same.

For now, let's just talk about the technical part of this ------
accuracy. For you, the data scientist, this part might feel more
comfortable. We can determine some metrics of success and frame a social
problem as a data science problem.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Definitions: Fairness and Transparency}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

The definitions, as given by the Cook County Assessor's Office, are
given below:

\begin{itemize}
\tightlist
\item
  Fairness: The ability of our pipeline to accurately assess property
  values, accounting for disparities in geography, information, etc.
\item
  Transparency: The ability of the data science department to share and
  explain pipeline results and decisions to both internal and external
  stakeholders
\end{itemize}

\end{tcolorbox}

The new Office of Data Science started by framing the problem and
redefining their goals. They determined that they needed to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Accurately, uniformly, and impartially assess the value of a home and
  accurately predict the sale price of a home within the next year by:

  \begin{itemize}
  \tightlist
  \item
    Following international standards (e.g., coefficient of dispersion)
  \item
    Predicting the value of all homes with as little total error as
    possible
  \end{itemize}
\item
  Create a robust pipeline that accurately assesses property values at
  scale and is fair to all people by:

  \begin{itemize}
  \tightlist
  \item
    Disrupting the circuit of corruption (Board of Review appeals
    process)
  \item
    Eliminating regressivity
  \item
    Engendering trust in the system among all stakeholders
  \end{itemize}
\end{enumerate}

The goals defined above lead us to ask the question: what does it
actually mean to accurately assess property values, and what role does
``scale'' play?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is an assessment of a home's value?
\item
  What makes one assessment more accurate than another?
\item
  What makes one batch of assessments more accurate than another batch?
\end{enumerate}

Each of the above questions leads to a slew of more questions.
Considering just the first question, one answer could be that an
assessment is an estimate of the value of a home. This leads to more
inquiries: what is the value of a home? What determines it? How do we
know? For this class, we take it to be the house's market value, or how
much it would sell for.

Unfortunately, if you are the county assessor, it becomes hard to
determine property values with this definition. After all, you can't
make everyone sell their house every year. And as many properties
haven't been sold in decades, every year that passes makes that previous
sale less reliable as an indicator.

So how would one generate reliable estimates? You're probably thinking,
well, with data about homes and their sale prices you can probably
predict the value of a property reliably. Even if you're not a data
scientist, you might know there are websites like Zillow and RedFin that
estimate what properties would sell for and constantly update them. They
don't know the value, but they estimate them. How do you think they do
this? Let's start with the data ------ which is the next step in the
lifecycle.

\hypertarget{data-acquisition-and-cleaning}{%
\subsection{2. Data Acquisition and
Cleaning}\label{data-acquisition-and-cleaning}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Driving Questions}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  What data do we have, and what data do we need?
\item
  How will we sample more data?
\item
  Is our data representative of the population we want to study?
\end{itemize}

\end{tcolorbox}

To generate estimates, the data scientists used two datasets. The first
contained all recorded sales data from 2013 to 2019. The second
contained property characteristics, including a property identification
number and physical characteristics (e.g., age, bedroom, baths, square
feet, neighborhood, site desirability, etc.).

As they examined the datasets, they asked the questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How was this data collected?
\item
  When was this data collected?
\item
  Who collected this data?
\item
  For what purposes was the data collected?
\item
  How and why were particular categories created?
\end{enumerate}

With so much data available, data scientists worked to see how all the
different data points correlated with each other and with the sales
prices. By discovering patterns in datasets containing known sale prices
and characteristics of similar and nearby properties, training a model
on this data, and applying it to all the properties without sales data,
it was now possible to create a linear model that could predict the sale
price (``fair market value'') of unsold properties.

Some other key questions data scientists asked about the data were:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are any attributes of a house differentially reported? How might these
  attributes be differentially reported?
\item
  How are ``improvements'' like renovations tracked and updated?
\item
  Which data is missing, and for which neighborhoods or populations is
  it missing?
\item
  What other data sources or attributes might be valuable?
\end{enumerate}

Attributes can have different likelihoods of appearing in the data. For
example, housing data in the floodplain geographic region of Chicago
were less represented than other regions.

Features can also be reported at different rates. Improvements in homes,
which tend to increase property value, were unlikely to be reported by
the homeowners.

Additionally, they found that there was simply more missing data in
lower-income neighborhoods.

\hypertarget{exploratory-data-analysis}{%
\subsection{3. Exploratory Data
Analysis}\label{exploratory-data-analysis}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Driving Questions}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  How is our data organized, and what does it contain?
\item
  Do we already have relevant data?
\item
  What are the biases, anomalies, or other issues with the data?
\item
  How do we transform the data to enable effective analysis?
\end{itemize}

\end{tcolorbox}

Before the modeling step, they investigated a multitude of crucial
questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Which attributes are most predictive of sales price?
\item
  Is the data uniformly distributed?
\item
  Do all neighborhoods have recent data? Do all neighborhoods have the
  same granularity?\\
\item
  Do some neighborhoods have missing or outdated data?
\end{enumerate}

They found that certain features, such as bedroom number, were much more
useful in determining house value for certain neighborhoods than for
others. This informed them that different models should be used
depending on the neighborhood.

They also noticed that low-income neighborhoods had disproportionately
spottier data. This informed them that they needed to develop new data
collection practices - including finding new sources of data.

\hypertarget{prediction-and-inference}{%
\subsection{4. Prediction and
Inference}\label{prediction-and-inference}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Driving Questions}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  What does the data say about the world?
\item
  Does it answer our questions or accurately solve the problem?
\item
  How robust are our conclusions, and can we trust the predictions?
\end{itemize}

\end{tcolorbox}

Rather than using a singular model to predict sale prices (``fair market
value'') of unsold properties, the CCAO predicts sale prices using
machine learning models that discover patterns in data sets containing
known sale prices and characteristics of \textbf{similar and nearby
properties}. It uses different model weights for each neighborhood.

Compared to traditional mass appraisal, the CCAO's new approach is more
granular and more sensitive to neighborhood variations.

But how do we know if an assessment is accurate? We can see how our
model performs when predicting the sales prices of properties it wasn't
trained on! We can then evaluate how ``close'' our estimate was to the
actual sales price, using Root Mean Square Error (RMSE). However, is
RMSE a good proxy for fairness in this context?

Broad metrics of error like RMSE can be limiting when evaluating the
``fairness'' of a property appraisal system. RMSE does not tell us
anything about the distribution of errors, whether the errors are
positive or negative, and the relative size of the errors. It does not
tell us anything about the regressivity of the model, instead just
giving a rough measure of our model's overall error.

Even with a low RMSE, we can't guarantee a fair model. The error we see
(no matter how small) may be a result of our model overvaluing less
expensive homes and undervaluing more expensive homes.

Regarding accuracy, it's important to ask what makes a batch of
assessments better or more accurate than another batch of assessments.
The value of a home that a model predicts is relational. It's a product
of the interaction of social and technical elements so property
assessment involves social trust.

Why should any particular individual believe that the model is accurate
for their property? Why should any individual trust the model?

To foster public trust, the CCAO focuses on ``transparency'', putting
data, models, and the pipeline onto GitLab. By doing so, they can better
equate the production of ``accurate assessments'' with ``fairness''.

There's a lot more to be said here on the relationship between accuracy,
fairness, and metrics we tend to use when evaluating our models. Given
the nuanced nature of the argument, it is recommended you view the
corresponding lecture as the course notes are not as comprehensive for
this portion of the lecture.

\hypertarget{results-and-conclusions}{%
\subsection{5. Results and Conclusions}\label{results-and-conclusions}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Driving Questions}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  How successful is the system for each goal?

  \begin{itemize}
  \tightlist
  \item
    Accuracy/uniformity of the model
  \item
    Fairness and transparency that eliminates regressivity and engenders
    trust
  \end{itemize}
\item
  How do you know?
\end{itemize}

\end{tcolorbox}

Unfortunately, it may be naive to hope that a more accurate and
transparent algorithm will translate into more fair outcomes in
practice. Even if our model is perfectly optimized according to the
standards of fairness we've set, there is no guarantee that people will
actually pay their expected share of taxes as determined by the model.
While it is a good step in the right direction, maintaining a level of
social trust is key to ensuring people pay their fair share.

Despite all their best efforts, the CCAO is still struggling to create
fair assessments and engender trust.

Stories like
\href{https://www.axios.com/local/chicago/2022/12/01/why-chicagos-property-tax-bills-so-high}{the
one} show that total taxes for residential properties went up overall
(because commercial taxes went down). But looking at the distribution,
we can see that the biggest increases occurred in wealthy neighborhoods,
and the biggest decreases occurred in poorer, predominantly Black
neighborhoods. So maybe there was some success after all?

However, it'll ultimately be hard to overcome the propensity of the
board of review to reduce the tax burden of the rich, preventing the
CCAO from creating a truly fair system. This is in part because there
are many cases where the model makes big, frustrating mistakes. In some
cases like
\href{https://www.axios.com/local/chicago/2023/05/22/cook-county-property-tax-appeal-process}{this
one}, it is due to spotty data.

\hypertarget{summary-questions-to-consider}{%
\section{Summary: Questions to
Consider}\label{summary-questions-to-consider}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Question/Problem Formulation

  \begin{itemize}
  \tightlist
  \item
    Who is responsible for framing the problem?
  \item
    Who are the stakeholders? How are they involved in the problem
    framing?
  \item
    What do you bring to the table? How does your positionality affect
    your understanding of the problem?
  \item
    What are the narratives that you're tapping into?
  \end{itemize}
\item
  Data Acquisition and Cleaning

  \begin{itemize}
  \tightlist
  \item
    Where does the data come from?
  \item
    Who collected it? For what purpose?
  \item
    What kinds of collecting and recording systems and techniques were
    used?
  \item
    How has this data been used in the past?
  \item
    What restrictions are there on access to the data, and what enables
    you to have access?
  \end{itemize}
\item
  Exploratory Data Analysis \& Visualization

  \begin{itemize}
  \tightlist
  \item
    What kind of personal or group identities have become salient in
    this data?
  \item
    Which variables became salient, and what kinds of relationships do
    we see between them?
  \item
    Do any of the relationships made visible lend themselves to
    arguments that might be potentially harmful to a particular
    community?
  \end{itemize}
\item
  Prediction and Inference

  \begin{itemize}
  \tightlist
  \item
    What does the prediction or inference do in the world?
  \item
    Are the results useful for the intended purposes?
  \item
    Are there benchmarks to compare the results?
  \item
    How are your predictions and inferences dependent upon the larger
    system in which your model works?
  \end{itemize}
\end{enumerate}

\hypertarget{key-takeaways}{%
\section{Key Takeaways}\label{key-takeaways}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Accuracy is a necessary, but not sufficient, condition of a fair
  system.
\item
  Fairness and transparency are context-dependent and
  \textbf{sociotechnical} concepts.
\item
  Learn to work with contexts, and consider how your data analysis will
  reshape them.
\item
  Keep in mind the power, and limits, of data analysis.
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{cross-validation-and-regularization}{%
\chapter{Cross Validation and
Regularization}\label{cross-validation-and-regularization}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Recognize the need for validation and test sets to preview model
  performance on unseen data
\item
  Apply cross-validation to select model hyperparameters
\item
  Understand the conceptual basis for L1 and L2 regularization
\end{itemize}

\end{tcolorbox}

At the end of the Feature Engineering lecture (Lecture 14), we arrived
at the issue of fine-tuning model complexity. We identified that a model
that's too complex can lead to overfitting while a model that's too
simple can lead to underfitting. This brings us to a natural question:
how do we control model complexity to avoid under- and overfitting?

To answer this question, we will need to address two things: first, we
need to understand \emph{when} our model begins to overfit by assessing
its performance on unseen data. We can achieve this through
\textbf{cross-validation}. Secondly, we need to introduce a technique to
adjust the complexity of our models ourselves -- to do so, we will apply
\textbf{regularization}.

\hypertarget{cross-validation}{%
\section{Cross-validation}\label{cross-validation}}

\hypertarget{training-test-and-validation-sets}{%
\subsection{Training, Test, and Validation
Sets}\label{training-test-and-validation-sets}}

From the last lecture, we learned that \emph{increasing} model
complexity \emph{decreased} our model's training error but
\emph{increased} its variance. This makes intuitive sense: adding more
features causes our model to fit more closely to data it encountered
during training, but it generalizes worse to new data that hasn't been
seen before. For this reason, a low training error is not always
representative of our model's underlying performance -- we need to also
assess how well it performs on unseen data to ensure that it is not
overfitting.

Truly, the only way to know when our model overfits is by evaluating it
on unseen data. Unfortunately, that means we need to wait for more data.
This may be very expensive and time-consuming.

How should we proceed? In this section, we will build up a viable
solution to this problem.

\hypertarget{test-sets}{%
\subsubsection{Test Sets}\label{test-sets}}

The simplest approach to avoid overfitting is to keep some of our data
``secret'' from ourselves. We can set aside a random portion of our full
dataset to use \emph{only} for testing purposes. The datapoints in this
\textbf{test set} will \emph{not} be used to fit the model. Instead, we
will:

\begin{itemize}
\tightlist
\item
  Use the remaining portion of our dataset -- now called the
  \textbf{training set} -- to run ordinary least squares, gradient
  descent, or some other technique to train our model,
\item
  Take the fitted model and use it to make predictions on datapoints in
  the test set. The model's performance on the test set (expressed as
  the MSE, RMSE, etc.) is now indicative of how well it can make
  predictions on \emph{unseen} data
\end{itemize}

Importantly, the optimal model parameters were found by \emph{only}
considering the data in the training set. After the model has been
fitted to the training data, we do not change any parameters before
making predictions on the test set. Importantly, we only ever make
predictions on the test set \textbf{once} after all model design has
been completely finalized. We treat the test set performance as the
final test of how well a model does. To reiterate, the test set is only
ever touched once: to compute the performance of the model after all
fine-tuning has been completed.

The process of sub-dividing our dataset into training and test sets is
known as a \textbf{train-test split}. Typically, between 10\% and 20\%
of the data is allocated to the test set.

In \texttt{sklearn}, the \texttt{train\_test\_split} function
(\href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html}{documentation})
of the \texttt{model\_selection} module allows us to automatically
generate train-test splits.

We will work with the \texttt{vehicles} dataset from previous lectures.
As before, we will attempt to predict the \texttt{mpg} of a vehicle from
transformations of its \texttt{hp}. In the cell below, we allocate 20\%
of the full dataset to testing, and the remaining 80\% to training.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ warnings}
\NormalTok{warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Load the dataset and construct the design matrix}
\NormalTok{vehicles }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"mpg"}\NormalTok{).rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"horsepower"}\NormalTok{:}\StringTok{"hp"}\NormalTok{\}).dropna()}
\NormalTok{X }\OperatorTok{=}\NormalTok{ vehicles[[}\StringTok{"hp"}\NormalTok{]]}
\NormalTok{X[}\StringTok{"hp\^{}2"}\NormalTok{] }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"hp"}\NormalTok{]}\OperatorTok{**}\DecValTok{2}
\NormalTok{X[}\StringTok{"hp\^{}3"}\NormalTok{] }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"hp"}\NormalTok{]}\OperatorTok{**}\DecValTok{3}
\NormalTok{X[}\StringTok{"hp\^{}4"}\NormalTok{] }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"hp"}\NormalTok{]}\OperatorTok{**}\DecValTok{4}

\NormalTok{Y }\OperatorTok{=}\NormalTok{ vehicles[}\StringTok{"mpg"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}

\CommentTok{\# \textasciigrave{}test\_size\textasciigrave{} specifies the proportion of the full dataset that should be allocated to testing}
\CommentTok{\# \textasciigrave{}random\_state\textasciigrave{} makes our results reproducible for educational purposes}
\NormalTok{X\_train, X\_test, Y\_train, Y\_test }\OperatorTok{=}\NormalTok{ train\_test\_split(}
\NormalTok{        X, }
\NormalTok{        Y, }
\NormalTok{        test\_size}\OperatorTok{=}\FloatTok{0.2}\NormalTok{, }
\NormalTok{        random\_state}\OperatorTok{=}\DecValTok{220}
\NormalTok{    )}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Size of full dataset: }\SpecialCharTok{\{}\NormalTok{X}\SpecialCharTok{.}\NormalTok{shape[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Size of training set: }\SpecialCharTok{\{}\NormalTok{X\_train}\SpecialCharTok{.}\NormalTok{shape[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Size of test set: }\SpecialCharTok{\{}\NormalTok{X\_test}\SpecialCharTok{.}\NormalTok{shape[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{ points"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Size of full dataset: 392 points
Size of training set: 313 points
Size of test set: 79 points
\end{verbatim}

After performing our train-test split, we fit a model to the training
set and assess its performance on the test set.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sklearn.linear\_model }\ImportTok{as}\NormalTok{ lm}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}

\NormalTok{model }\OperatorTok{=}\NormalTok{ lm.LinearRegression()}

\CommentTok{\# Fit to the training set}
\NormalTok{model.fit(X\_train, Y\_train)}

\CommentTok{\# Calculate errors}
\NormalTok{train\_error }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y\_train, model.predict(X\_train))}
\NormalTok{test\_error }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y\_test, model.predict(X\_test))}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Training error: }\SpecialCharTok{\{}\NormalTok{train\_error}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Test error: }\SpecialCharTok{\{}\NormalTok{test\_error}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Training error: 17.85851684101209
Test error: 23.192405629701074
\end{verbatim}

\hypertarget{validation-sets}{%
\subsubsection{Validation Sets}\label{validation-sets}}

Now, what if we were dissatisfied with our test set performance? With
our current framework, we'd be stuck. As outlined previously, assessing
model performance on the test set is the \emph{final} stage of the model
design process; we can't go back and adjust our model based on the new
discovery that it is overfitting. If we did, then we would be
\emph{factoring in information from the test set} to design our model.
The test error would no longer be a true representation of the model's
performance on \emph{unseen} data!

Our solution is to introduce a \textbf{validation set}. A validation set
is a random portion of the \emph{training set} that is set aside for
assessing model performance while the model is \emph{still being
developed}. The process for using a validation set is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform a train-test split.
\item
  Set the test set aside; we will not touch it until the very end of the
  model design process.
\item
  Set aside a portion of the training set to be used for validation.
\item
  Fit the model parameters to the datapoints contained in the remaining
  portion of the training set.
\item
  Assess the model's performance on the validation set. Adjust the model
  as needed, re-fit it to the remaining portion of the training set,
  then re-evaluate it on the validation set. Repeat as necessary until
  you are satisfied.
\item
  After \emph{all} model development is complete, assess the model's
  performance on the test set. This is the final test of how well the
  model performs on unseen data. No further modifications should be made
  to the model.
\end{enumerate}

The process of creating a validation set is called a \textbf{validation
split}.

Note that the validation error behaves quite differently from the
training error explored previously. As the model becomes more complex,
it makes better predictions on the training data; the variance of the
model typically increases as model complexity increases. Validation
error, on the other hand, decreases \emph{then increases} as we increase
model complexity. This reflects the transition from under- to
overfitting: at low model complexity, the model underfits because it is
not complex enough to capture the main trends in the data; at high model
complexity, the model overfits because it ``memorizes'' the training
data too closely.

We can update our understanding of the relationships between error,
complexity, and model variance:

Our goal is to train a model with complexity near the orange dotted line
-- this is where our model minimizes the validation error. Note that
this relationship is a simplification of the real-world, but it's a good
enough approximation for the purposes of Data 100.

\hypertarget{k-fold-cross-validation}{%
\subsection{K-Fold Cross-Validation}\label{k-fold-cross-validation}}

Introducing a validation set gave us an ``extra'' chance to assess model
performance on another set of unseen data. We are able to finetune the
model design based on its performance on this one set of validation
data.

But what if, by random chance, our validation set just happened to
contain many outliers? It is possible that the validation datapoints we
set aside do not actually represent other unseen data that the model
might encounter. Ideally, we would like to validate our model's
performance on several different unseen datasets. This would give us
greater confidence in our understanding of how the model behaves on new
data.

Let's think back to our validation framework. Earlier, we set aside
\(x\)\% of our training data (say, 20\%) to use for validation.

In the example above, we set aside the first 20\% of training datapoints
for the validation set. This was an arbitrary choice. We could have set
aside \emph{any} 20\% portion of the training data for validation. In
fact, there are 5 non-overlapping ``chunks'' of training points that we
could have designated as the validation set.

The common term for one of these chunks is a \textbf{fold}. In the
example above, we had 5 folds, each containing 20\% of the training
data. This gives us a new perspective: we really have \emph{5}
validation sets ``hidden'' in our training set.

In \textbf{cross-validation}, we perform validation splits for each fold
in the training set. For a dataset with \(K\) folds, we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick one fold to be the validation fold
\item
  Fit the model to training data from every fold \emph{other} than the
  validation fold
\item
  Compute the model's error on the validation fold and record it
\item
  Repeat for all \(K\) folds
\end{enumerate}

The \textbf{cross-validation error} is then the \emph{average} error
across all \(K\) validation folds. In the example below, the
cross-validation error is the mean of validation errors \#1 to \#5.

\hypertarget{model-selection-workflow}{%
\subsection{Model Selection Workflow}\label{model-selection-workflow}}

At this stage, we have refined our model selection workflow. We begin by
performing a train-test split to set aside a test set for the final
evaluation of model performance. Then, we alternate between adjusting
our design matrix and computing the cross-validation error to finetune
the model's design. In the example below, we illustrate the use of
4-fold cross-validation to help inform model design.

\hypertarget{hyperparameters}{%
\subsection{Hyperparameters}\label{hyperparameters}}

An important use of cross-validation is for \textbf{hyperparameter}
selection. A hyperparameter is some value in a model that is chosen
\emph{before} the model is fit to any data. This means that it is
distinct from the \emph{model parameters}, \(\theta_i\), because its
value is selected \emph{before} the training process begins. We cannot
use our usual techniques -- calculus, ordinary least squares, or
gradient descent -- to choose its value. Instead, we must decide it
ourselves.

Some examples of hyperparameters in Data 100 are:

\begin{itemize}
\tightlist
\item
  The degree of our polynomial model (recall that we selected the degree
  before creating our design matrix and calling \texttt{.fit})
\item
  The learning rate, \(\alpha\), in gradient descent
\item
  The regularization penalty, \(\lambda\) (to be introduced later this
  lecture)
\end{itemize}

To select a hyperparameter value via cross-validation, we first list out
several ``guesses'' for what the best hyperparameter may be. For each
guess, we then run cross-validation to compute the cross-validation
error incurred by the model when using that choice of hyperparameter
value. We then select the value of the hyperparameter that resulted in
the lowest cross-validation error.

For example, we may wish to use cross-validation to decide what value we
should use for \(\alpha\), which controls the step size of each gradient
descent update. To do so, we list out some possible guesses for the best
\(\alpha\), like 0.1, 1, and 10. For each possible value, we perform
cross-validation to see what error the model has when we use that value
of \(\alpha\) to train it.

\hypertarget{regularization}{%
\section{Regularization}\label{regularization}}

We've now addressed the first of our two goals for today: creating a
framework to assess model performance on unseen data. Now, we'll discuss
our second objective: developing a technique to adjust model complexity.
This will allow us to directly tackle the issues of under- and
overfitting.

Earlier, we adjusted the complexity of our polynomial model by tuning a
hyperparameter -- the degree of the polynomial. We tested out several
different polynomial degrees, computed the validation error for each,
and selected the value that minimized the validation error. Tweaking the
``complexity'' was simple; it was only a matter of adjusting the
polynomial degree.

In most machine learning problems, complexity is defined differently
from what we have seen so far. Today, we'll explore two different
definitions of complexity: the \emph{squared} and \emph{absolute}
magnitude of \(\theta_i\) coefficients.

\hypertarget{constraining-model-parameters}{%
\subsection{Constraining Model
Parameters}\label{constraining-model-parameters}}

Think back to our work using gradient descent to descend down a loss
surface. You may find it helpful to refer back to the Gradient Descent
note to refresh your memory. Our aim was to find the combination of
model parameters that the smallest, minimum loss. We visualized this
using a contour map by plotting possible parameter values on the
horizontal and vertical axes, which allows us to take a bird's eye view
above the loss surface. Notice that the contour map has \(p=2\)
parameters for ease of visualization. We want to find the model
parameters corresponding to the lowest point on the loss surface.

Let's review our current modeling framework.

\[\hat{\mathbb{Y}} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2 + \ldots + \theta_p \phi_p\]

Recall that we represent our features with \(\phi_i\) to reflect the
fact that we have performed feature engineering.

Previously, we restricted model complexity by limiting the total number
of features present in the model. We only included a limited number of
polynomial features at a time; all other polynomials were excluded from
the model.

What if, instead of fully removing particular features, we kept all
features and used each one only a ``little bit''? If we put a limit on
how \emph{much} each feature can contribute to the predictions, we can
still control the model's complexity without the need to manually
determine how many features should be removed.

What do we mean by a ``little bit''? Consider the case where some
parameter \(\theta_i\) is close to or equal to 0. Then, feature
\(\phi_i\) barely impacts the prediction -- the feature is weighted by
such a small value that its presence doesn't significantly change the
value of \(\hat{\mathbb{Y}}\). If we restrict how large each parameter
\(\theta_i\) can be, we restrict how much feature \(\phi_i\) contributes
to the model. This has the effect of \emph{reducing} model complexity.

In \textbf{regularization}, we restrict model complexity by putting a
limit on the \emph{magnitudes} of the model parameters \(\theta_i\).

What do these limits look like? Suppose we specify that the sum of all
absolute parameter values can be no greater than some number \(Q\). In
other words:

\[\sum_{i=1}^p |\theta_i| \leq Q\]

where \(p\) is the total number of parameters in the model. You can
think of this as us giving our model a ``budget'' for how it distributes
the magnitudes of each parameter. If the model assigns a large value to
some \(\theta_i\), it may have to assign a small value to some other
\(\theta_j\). This has the effect of increasing feature \(\phi_i\)'s
influence on the predictions while decreasing the influence of feature
\(\phi_j\). The model will need to be strategic about how the parameter
weights are distributed -- ideally, more ``important'' features will
receive greater weighting.

Notice that the intercept term, \(\theta_0\), is excluded from this
constraint. \textbf{We typically do not regularize the intercept term}.

Now, let's think back to gradient descent and visualize the loss surface
as a contour map. As a refresher, a loss surface means that each point
represents the model's loss for a particular combination of
\(\theta_1\), \(\theta_2\). Let's say our goal is to find the
combination of parameters that gives us the lowest loss.

With no constraint, the optimal \(\hat{\theta}\) is in the center. We
denote this as \(\hat{\theta}_\text{No Reg}\).

Applying this constraint limits what combinations of model parameters
are valid. We can now only consider parameter combinations with a total
absolute sum less than or equal to our number \(Q\). For our 2D example,
the constraint \(\sum_{i=1}^p |\theta_i| \leq Q\) can be rewritten as
\(|\theta_0| + |\theta_1| \leq Q\). This means that we can only assign
our \emph{regularized} parameter vector \(\hat{\theta}_{\text{Reg}}\) to
positions in the green diamond below.

We can no longer select the parameter vector that \emph{truly} minimizes
the loss surface, \(\hat{\theta}_{\text{No Reg}}\), because this
combination of parameters does not lie within our allowed region.
Instead, we select whatever allowable combination brings us
\emph{closest} to the true minimum loss, which is depicted by the red
point below.

Notice that, under regularization, our optimized \(\theta_1\) and
\(\theta_2\) values are much smaller than they were without
regularization (indeed, \(\theta_1\) has decreased to 0). The model has
\emph{decreased in complexity} because we have limited how much our
features contribute to the model. In fact, by setting its parameter to
0, we have effectively removed the influence of feature \(\phi_1\) from
the model altogether.

If we change the value of \(Q\), we change the region of allowed
parameter combinations. The model will still choose the combination of
parameters that produces the lowest loss -- the closest point in the
constrained region to the true minimizer,
\(\hat{\theta}_{\text{No Reg}}\).

When \(Q\) is small, we severely restrict the size of our parameters.
\(\theta_i\)s are small in value, and features \(\phi_i\) only
contribute a little to the model. The allowed region of model parameters
contracts, and the model becomes much simpler:

When \(Q\) is large, we do not restrict our parameter sizes by much.
\(\theta_i\)s are large in value, and features \(\phi_i\) contribute
more to the model. The allowed region of model parameters expands, and
the model becomes more complex:

Consider the extreme case of when \(Q\) is extremely large. In this
situation, our restriction has essentially no effect, and the allowed
region includes the OLS solution!

Now what if \(Q\) was extremely small? Most parameters are then set to
(essentially) 0.

\begin{itemize}
\tightlist
\item
  If the model has no intercept term:
  \(\hat{\mathbb{Y}} = (0)\phi_1 + (0)\phi_2 + \ldots = 0\).
\item
  If the model has an intercept term:
  \(\hat{\mathbb{Y}} = (0)\phi_1 + (0)\phi_2 + \ldots = \theta_0\).
  Remember that the intercept term is excluded from the constraint -
  this is so we avoid the situation where we always predict 0.
\end{itemize}

Let's summarize what we have seen.

\hypertarget{l1-lasso-regularization}{%
\subsection{L1 (LASSO) Regularization}\label{l1-lasso-regularization}}

How do we actually apply our constraint
\(\sum_{i=1}^p |\theta_i| \leq Q\)? We will do so by modifying the
\emph{objective function} that we seek to minimize when fitting a model.

Recall our ordinary least squares objective function: our goal was to
find parameters that minimize the model's mean squared error:

\[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\]

To apply our constraint, we need to rephrase our minimization goal as:

\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such that} \sum_{i=1}^p |\theta_i| \leq Q\]

Unfortunately, we can't directly use this formulation as our objective
function -- it's not easy to mathematically optimize over a constraint.
Instead, we will apply the magic of the
\href{https://en.wikipedia.org/wiki/Duality_(optimization)}{Lagrangian
Duality}. The details of this are out of scope (take EECS 127 if you're
interested in learning more), but the end result is very useful. It
turns out that minimizing the following \emph{augmented} objective
function is \emph{equivalent} to our minimization goal above.

\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2 + \lambda \sum_{i=1}^p \vert \theta_i \vert\]
\[ = ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p |\theta_i|\]
\[ = ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda || \theta ||_1\]

The last two expressions include the MSE expressed using vector
notation, and the last expression writes \(\sum_{i=1}^p |\theta_i|\) as
it's \textbf{L1 norm} equivalent form, \(|| \theta ||_1\).

Notice that we've replaced the constraint with a second term in our
objective function. We're now minimizing a function with an additional
regularization term that \emph{penalizes large coefficients}. In order
to minimize this new objective function, we'll end up balancing two
components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Keeping the model's error on the training data low, represented by the
  term
  \(\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 x_{i, 1} + \theta_2 x_{i, 2} + \ldots + \theta_p x_{i, p}))^2\)
\item
  Keeping the magnitudes of model parameters low, represented by the
  term \(\lambda \sum_{i=1}^p |\theta_i|\)
\end{enumerate}

The \(\lambda\) factor controls the degree of regularization. Roughly
speaking, \(\lambda\) is related to our \(Q\) constraint from before by
the rule \(\lambda \approx \frac{1}{Q}\). To understand why, let's
consider two extreme examples. Recall that our goal is to minimize the
cost function:
\(||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda || \theta ||_1\).

\begin{itemize}
\item
  Assume \(\lambda \rightarrow \infty\). Then,
  \(\lambda || \theta ||_1\) dominates the cost function. In order to
  neutralize the \(\infty\) and minimize this term, we set
  \(\theta_j = 0\) for all \(j \ge 1\). This is a very constrained model
  that is mathematically equivalent to the constant model
\item
  Assume \(\lambda \rightarrow 0\). Then, \(\lambda || \theta ||_1=0\).
  Minimizing the cost function is equivalent to minimizing
  \(\frac{1}{n} || Y - X\theta ||_2^2\), our usual MSE loss function.
  The act of minimizing MSE loss is just our familiar OLS, and the
  optimal solution is the global minimum
  \(\hat{\theta} = \hat\theta_{No Reg.}\).
\end{itemize}

We call \(\lambda\) the \textbf{regularization penalty hyperparameter};
it needs to be determined \emph{prior} to training the model, so we must
find the best value via cross-validation.

The process of finding the optimal \(\hat{\theta}\) to minimize our new
objective function is called \textbf{L1 regularization}. It is also
sometimes known by the acronym ``LASSO'', which stands for ``Least
Absolute Shrinkage and Selection Operator.''

Unlike ordinary least squares, which can be solved via the closed-form
solution
\(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\),
\textbf{there is no closed-form solution for the optimal parameter
vector under L1 regularization}. Instead, we use the \texttt{Lasso}
model class of \texttt{sklearn}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sklearn.linear\_model }\ImportTok{as}\NormalTok{ lm}

\CommentTok{\# The alpha parameter represents our lambda term}
\NormalTok{lasso\_model }\OperatorTok{=}\NormalTok{ lm.Lasso(alpha}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\NormalTok{lasso\_model.fit(X\_train, Y\_train)}

\NormalTok{lasso\_model.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-2.54932056e-01, -9.48597165e-04,  8.91976284e-06, -1.22872290e-08])
\end{verbatim}

Notice that all model coefficients are very small in magnitude. In fact,
some of them are so small that they are essentially 0. An important
characteristic of L1 regularization is that many model parameters are
set to 0. In other words, LASSO effectively \textbf{selects only a
subset} of the features. The reason for this comes back to our loss
surface and allowed ``diamond'' regions from earlier -- we can often get
closer to the lowest loss contour at a corner of the diamond than along
an edge.

When a model parameter is set to 0 or close to 0, its corresponding
feature is essentially removed from the model. We say that L1
regularization performs \textbf{feature selection} because, by setting
the parameters of unimportant features to 0, LASSO ``selects'' which
features are more useful for modeling. L1 regularization indicates that
the features with non-zero parameters are more informative for modeling
than those with parameters set to zero.

\hypertarget{scaling-features-for-regularization}{%
\subsection{Scaling Features for
Regularization}\label{scaling-features-for-regularization}}

The regularization procedure we just performed had one subtle issue. To
see what it is, let's take a look at the design matrix for our
\texttt{lasso\_model}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_train.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrr}
\toprule
{} &     hp &     hp\textasciicircum 2 &       hp\textasciicircum 3 &         hp\textasciicircum 4 \\
\midrule
259 &   85.0 &   7225.0 &   614125.0 &   52200625.0 \\
129 &   67.0 &   4489.0 &   300763.0 &   20151121.0 \\
207 &  102.0 &  10404.0 &  1061208.0 &  108243216.0 \\
302 &   70.0 &   4900.0 &   343000.0 &   24010000.0 \\
71  &   97.0 &   9409.0 &   912673.0 &   88529281.0 \\
\bottomrule
\end{tabular}

Our features -- \texttt{hp}, \texttt{hp\^{}2}, \texttt{hp\^{}3}, and
\texttt{hp\^{}4} -- are on drastically different numeric scales! The
values contained in \texttt{hp\^{}4} are orders of magnitude larger than
those contained in \texttt{hp}. This can be a problem because the value
of \texttt{hp\^{}4} will naturally contribute more to each predicted
\(\hat{y}\) because it is so much greater than the values of the other
features. For \texttt{hp} to have much of an impact at all on the
prediction, it must be scaled by a large model parameter.

By inspecting the fitted parameters of our model, we see that this is
the case -- the parameter for \texttt{hp} is much larger in magnitude
than the parameter for \texttt{hp\^{}4}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pd.DataFrame(\{}\StringTok{"Feature"}\NormalTok{:X\_train.columns, }\StringTok{"Parameter"}\NormalTok{:lasso\_model.coef\_\})}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llr}
\toprule
{} & Feature &     Parameter \\
\midrule
0 &      hp & -2.549321e-01 \\
1 &    hp\textasciicircum 2 & -9.485972e-04 \\
2 &    hp\textasciicircum 3 &  8.919763e-06 \\
3 &    hp\textasciicircum 4 & -1.228723e-08 \\
\bottomrule
\end{tabular}

Recall that by applying regularization, we give our a model a ``budget''
for how it can allocate the values of model parameters. For \texttt{hp}
to have much of an impact on each prediction, LASSO is forced to
``spend'' more of this budget on the parameter for \texttt{hp}.

We can avoid this issue by \textbf{scaling} the data before
regularizing. This is a process where we convert all features to the
same numeric scale. A common way to scale data is to perform
\textbf{standardization} such that all features have mean 0 and standard
deviation 1; essentially, we replace everything with its Z-score.

\[z_i = \frac{x_i - \mu}{\sigma}\]

\hypertarget{l2-ridge-regularization}{%
\subsection{L2 (Ridge) Regularization}\label{l2-ridge-regularization}}

In all of our work above, we considered the constraint
\(\sum_{i=1}^p |\theta_i| \leq Q\) to limit the complexity of the model.
What if we had applied a different constraint?

In \textbf{L2 regularization}, also known as \textbf{ridge regression},
we constrain the model such that the sum of the \emph{squared}
parameters must be less than some number \(Q\). This constraint takes
the form:

\[\sum_{i=1}^p \theta_i^2 \leq Q\]

As before, \textbf{we typically do not regularize the intercept term}.

In our 2D example, the constraint becomes
\(\theta_1^2 + \theta_2^2 \leq Q\). Can you see how this is similar to
the equation for a circle, \(x^2 + y^2 = r^2\)? The allowed region of
parameters for a given value of \(Q\) is now shaped like a ball.

If we modify our objective function like before, we find that our new
goal is to minimize the function:
\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2\:\text{such that} \sum_{i=1}^p \theta_i^2 \leq Q\]

Notice that all we have done is change the constraint on the model
parameters. The first term in the expression, the MSE, has not changed.

Using Lagrangian Duality (again, out of scope for Data 100), we can
re-express our objective function as:
\[\frac{1}{n} \sum_{i=1}^n (y_i - (\theta_0 + \theta_1 \phi_{i, 1} + \theta_2 \phi_{i, 2} + \ldots + \theta_p \phi_{i, p}))^2 + \lambda \sum_{i=1}^p \theta_i^2\]
\[= ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda \sum_{i=1}^p \theta_i^2\]
\[= ||\mathbb{Y} - \mathbb{X}\theta||_2^2 + \lambda || \theta ||_2^2\]

The last two expressions include the MSE expressed using vector
notation, and the last expression writes \(\sum_{i=1}^p \theta_i^2\) as
it's \textbf{L2 norm} equivalent form, \(|| \theta ||_2^2\).

When applying L2 regularization, our goal is to minimize this updated
objective function.

Unlike L1 regularization, L2 regularization \emph{does} have a
closed-form solution for the best parameter vector when regularization
is applied:

\[\hat\theta_{\text{ridge}} = (\mathbb{X}^{\top}\mathbb{X} + n\lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\]

This solution exists \textbf{even if \(\mathbb{X}\) is not full column
rank}. This is a major reason why L2 regularization is often used -- it
can produce a solution even when there is colinearity in the features.
We will discuss the concept of colinearity in a future lecture, but we
will not derive this result in Data 100, as it involves a fair bit of
matrix calculus.

In \texttt{sklearn}, we perform L2 regularization using the
\texttt{Ridge} class. It runs gradient descent to minimize the L2
objective function. Notice that we scale the data before regularizing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge\_model }\OperatorTok{=}\NormalTok{ lm.Ridge(alpha}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\CommentTok{\# alpha represents the hyperparameter lambda}
\NormalTok{ridge\_model.fit(X\_train, Y\_train)}

\NormalTok{ridge\_model.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([ 5.89130559e-02, -6.42445915e-03,  4.44468157e-05, -8.83981945e-08])
\end{verbatim}

\hypertarget{regression-summary}{%
\section{Regression Summary}\label{regression-summary}}

Our regression models are summarized below. Note the objective function
is what the gradient descent optimizer minimizes.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0605}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1423}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0534}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0569}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3238}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3630}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Loss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Regularization
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Objective Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Solution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
OLS & \(\hat{\mathbb{Y}} = \mathbb{X}\theta\) & MSE & None &
\(\frac{1}{n} \|\mathbb{Y}-\mathbb{X} \theta\|^2_2\) &
\(\hat{\theta}_{OLS} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)
if \(\mathbb{X}\) is full column rank \\
Ridge & \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) & MSE & L2 &
\(\frac{1}{n} \|\mathbb{Y}-\mathbb{X}\theta\|^2_2 + \lambda \sum_{i=1}^p \theta_i^2\)
&
\(\hat{\theta}_{ridge} = (\mathbb{X}^{\top}\mathbb{X} + n \lambda I)^{-1}\mathbb{X}^{\top}\mathbb{Y}\) \\
LASSO & \(\hat{\mathbb{Y}} = \mathbb{X} \theta\) & MSE & L1 &
\(\frac{1}{n} \|\mathbb{Y}-\mathbb{X}\theta\|^2_2 + \lambda \sum_{i=1}^p \vert \theta_i \vert\)
& No closed form solution \\
\end{longtable}

\bookmarksetup{startatroot}

\hypertarget{random-variables}{%
\chapter{Random Variables}\label{random-variables}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Define a random variable in terms of its distribution
\item
  Compute the expectation and variance of a random variable
\item
  Gain familiarity with the Bernoulli and binomial random variables
\end{itemize}

\end{tcolorbox}

In the past few lectures, we've examined the role of complexity in
influencing model performance. We've considered model complexity in the
context of a tradeoff between two competing factors: model variance and
training error.

So far, our analysis has been mostly qualitative. We've acknowledged
that our choice of model complexity needs to strike a balance between
model variance and training error, but we haven't yet discussed
\emph{why} exactly this tradeoff exists.

To better understand the origin of this tradeoff, we will need to dive
into \textbf{random variables}. The next two course notes on probability
will be a brief digression from our work on modeling so we can build up
the concepts needed to understand this so-called \textbf{bias-variance
tradeoff}. In specific, we will cover:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Random Variables: introduce random variables, considering the concepts
  of expectation, variance, and covariance
\item
  Estimators, Bias, and Variance: re-express the ideas of model variance
  and training error in terms of random variables and use this new
  perspective to investigate our choice of model complexity
\end{enumerate}

We'll go over just enough probability to help you understand its
implications for modeling, but if you want to go a step further, take
Data 140, CS 70, and/or EECS 126.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Data 8 Recap}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Recall the following concepts from Data 8:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sample mean: The mean of the random sample
\item
  Central Limit Theorem: If you draw a large random sample with
  replacement, then, regardless of the population distribution, the
  probability distribution of the sample mean

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    is roughly normal
  \item
    is centered at the population mean
  \item
    has an
    \(SD = \frac{\text{population SD}}{\sqrt{\text{sample size}}}\)
  \end{enumerate}
\end{enumerate}

\end{tcolorbox}

In Data 100, we want to understand the broader relationship between the
following:

\begin{itemize}
\tightlist
\item
  \textbf{Population parameter}: a number that describes something about
  the population
\item
  \textbf{Sample statistic}: an estimate of the number computed on a
  sample
\end{itemize}

\hypertarget{random-variables-and-distributions}{%
\section{Random Variables and
Distributions}\label{random-variables-and-distributions}}

Suppose we generate a set of random data, like a random sample from some
population. A \textbf{random variable} is a \emph{function} from the
outcome of a random event to a number.

It is \emph{random} since our sample was drawn at random; it is
\emph{variable} because its exact value depends on how this random
sample came out. As such, the domain or input of our random variable is
all possible outcomes for some random event in a \emph{sample space},
and its range or output is the real number line. We typically denote
random variables with uppercase letters, such as \(X\) or \(Y\). In
contrast, note that regular variables tend to be denoted using lowercase
letters. Sometimes we also use uppercase letters to refer to matrices
(such as your design matrix \(\mathbb{X}\)), but we will do our best to
be clear with the notation.

To motivate what this (rather abstract) definition means, let's consider
the following examples:

\hypertarget{example-tossing-a-coin}{%
\subsection{Example: Tossing a Coin}\label{example-tossing-a-coin}}

Let's formally define a fair coin toss. A fair coin can land on heads
(\(H\)) or tails (\(T\)), each with a probability of 0.5. With these
possible outcomes, we can define a random variable \(X\) as:
\[X = \begin{cases} 
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails} 
   \end{cases}\]

\(X\) is a function with a domain, or input, of \(\{H, T\}\) and a
range, or output, of \(\{1, 0\}\). In practice, while we don't use the
following function notation, you could write the above as
\[X = \begin{cases}  X(H) = 1 \\ X(T) = 0 \end{cases}\]

\hypertarget{example-sampling-data-100-students}{%
\subsection{Example: Sampling Data 100
Students}\label{example-sampling-data-100-students}}

Suppose we draw a random sample \(s\) of size 3 from all students
enrolled in Data 100.

We can define \(Y\) as the number of data science students in our
sample. Its domain is all possible samples of size 3, and its range is
\(\{0, 1, 2, 3\}\).

Note that we can use random variables in mathematical expressions to
create new random variables.

For example, let's say we sample 3 students at random from lecture and
look at their midterm scores. Let \(X_1\), \(X_2\), and \(X_3\)
represent each student's midterm grade.

We can use these random variables to create a new random variable,
\(Y\), which represents the average of the 3 scores:
\(Y = (X_1 + X_2 + X_3)/3\).

As we're creating this random variable, a few questions arise:

\begin{itemize}
\tightlist
\item
  What can we say about the distribution of \(Y\)?
\item
  How does it depend on the distribution of \(X_1\), \(X_2\), and
  \(X_3\)?
\end{itemize}

But, what exactly is a distribution? Let's dive into this!

\hypertarget{distributions}{%
\subsection{Distributions}\label{distributions}}

To define any random variable \(X\), we need to be able to specify 2
things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Possible values}: the set of values the random variable can
  take on.
\item
  \textbf{Probabilities}: the set of probabilities describing how the
  total probability of 100\% is split over the possible values.
\end{enumerate}

If \(X\) is discrete (has a finite number of possible values), the
probability that a random variable \(X\) takes on the value \(x\) is
given by \(P(X=x)\), and probabilities must sum to 1:
\(\sum_{\text{all } x} P(X=x) = 1\),

We can often display this using a \textbf{probability distribution
table}. In the coin toss example, the probability distribution table of
\(X\) is given by.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
\(x\) & \(P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & \(\frac{1}{2}\) \\
1 & \(\frac{1}{2}\) \\
\end{longtable}

The \textbf{distribution} of a random variable \(X\) describes how the
total probability of 100\% is split across all the possible values of
\(X\), and it fully defines a random variable. If you know the
distribution of a random variable you can:

\begin{itemize}
\tightlist
\item
  compute properties of the random variables and derived variables
\item
  simulate the random variables by randomly picking values of \(X\)
  according to its distribution using \texttt{np.random.choice},
  \texttt{df.sample}, or
  \texttt{scipy.stats.\textless{}dist\textgreater{}.rvs(...)}
\end{itemize}

The distribution of a discrete random variable can also be represented
using a histogram. If a variable is \textbf{continuous}, meaning it can
take on infinitely many values, we can illustrate its distribution using
a density curve.

We often don't know the (true) distribution and instead compute an
empirical distribution. If you flip a coin 3 times and get \{H, H, T\},
you may ask ------ what is the probability that the coin will land
heads? We can come up with an \textbf{empirical estimate} of
\(\frac{2}{3}\), though the true probability might be \(\frac{1}{2}\).

Probabilities are areas. For discrete random variables, the \emph{area
of the red bars} represents the probability that a discrete random
variable \(X\) falls within those values. For continuous random
variables, the \emph{area under the curve} represents the probability
that a discrete random variable \(Y\) falls within those values.

If we sum up the total area of the bars/under the density curve, we
should get 100\%, or 1.

We can show the distribution of \(Y\) in the following tables. The table
on the left lists all possible samples of \(s\) and the number of times
they can appear (\(Y(s)\)). We can use this to calculate the values for
the table on the right, a \textbf{probability distribution table}.

Rather than fully write out a probability distribution or show a
histogram, there are some common distributions that come up frequently
when doing data science. These distributions are specified by some
\textbf{parameters}, which are constants that specify the shape of the
distribution. In terms of notation, the `\textasciitilde{}' means ``has
the probability distribution of''.

These common distributions are listed below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Bernoulli(\(p\)): If \(X\) \textasciitilde{} Bernoulli(\(p\)), then
  \(X\) takes on a value 1 with probability \(p\), and 0 with
  probability \(1 - p\). Bernoulli random variables are also termed the
  ``indicator'' random variables.
\item
  Binomial(\(n\), \(p\)): If \(X\) \textasciitilde{} Binomial(\(n\),
  \(p\)), then \(X\) counts the number of 1s in \(n\) independent
  Bernoulli(\(p\)) trials.
\item
  Categorical(\(p_1, ..., p_k\)) of values: The probability of each
  value is 1 / (number of possible values).
\item
  Uniform on the unit interval (0, 1): The density is flat at 1 on (0,
  1) and 0 elsewhere. We won't get into what density means as much here,
  but intuitively, this is saying that there's an equally likely chance
  of getting any value on the interval (0, 1).
\item
  Normal(\(\mu\), \(\sigma^2\)): The probability density is specified by
  \(\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\).
  This bell-shaped distribution comes up fairly often in data, in part
  due to the Central Limit Theorem you saw back in Data 8.
\end{enumerate}

\hypertarget{expectation-and-variance}{%
\section{Expectation and Variance}\label{expectation-and-variance}}

There are several ways to describe a random variable. The methods shown
above ------ a table of all samples \(s, X(s)\), distribution table
\(P(X=x)\), and histograms ------ are all definitions that \emph{fully
describe} a random variable. Often, it is easier to describe a random
variable using some \emph{numerical summary} rather than fully defining
its distribution. These numerical summaries are numbers that
characterize some properties of the random variable. Because they give a
``summary'' of how the variable tends to behave, they are \emph{not}
random. Instead, think of them as a static number that describes a
certain property of the random variable. In Data 100, we will focus our
attention on the expectation and variance of a random variable.

\hypertarget{expectation}{%
\subsection{Expectation}\label{expectation}}

The \textbf{expectation} of a random variable \(X\) is the
\textbf{weighted average} of the values of \(X\), where the weights are
the probabilities of each value occurring. There are two equivalent ways
to compute the expectation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Apply the weights one \emph{sample} at a time:
  \[\mathbb{E}[X] = \sum_{\text{all possible } s} X(s) P(s)\].
\item
  Apply the weights one possible \emph{value} at a time:
  \[\mathbb{E}[X] = \sum_{\text{all possible } x} x P(X=x)\]
\end{enumerate}

The latter is more commonly used as we are usually just given the
distribution, not all possible samples.

We want to emphasize that the expectation is a \emph{number}, not a
random variable. Expectation is a generalization of the average, and it
has the same units as the random variable. It is also the center of
gravity of the probability distribution histogram, meaning if we
simulate the variable many times, it is the long-run average of the
simulated values.

\hypertarget{example-1-coin-toss}{%
\subsubsection{Example 1: Coin Toss}\label{example-1-coin-toss}}

Going back to our coin toss example, we define a random variable \(X\)
as: \[X = \begin{cases} 
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails} 
   \end{cases}\]

We can calculate its expectation \(\mathbb{E}[X]\) using the second
method of applying the weights one possible value at a time:
\[\begin{align}
 \mathbb{E}[X] &= \sum_{x} x P(X=x) \\
 &= 1 * 0.5 + 0 * 0.5 \\
 &= 0.5
\end{align}\]

Note that \(\mathbb{E}[X] = 0.5\) is not a possible value of \(X\); it's
an average. \textbf{The expectation of X does not need to be a possible
value of X}.

\hypertarget{example-2-1}{%
\subsubsection{Example 2}\label{example-2-1}}

Consider the random variable \(X\):

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
\(x\) & \(P(X=x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
3 & 0.1 \\
4 & 0.2 \\
6 & 0.4 \\
8 & 0.3 \\
\end{longtable}

To calculate it's expectation, \[\begin{align}
 \mathbb{E}[X] &= \sum_{x} x P(X=x) \\
 &= 3 * 0.1 + 4 * 0.2 + 6 * 0.4 + 8 * 0.3 \\
 &= 0.3 + 0.8 + 2.4 + 2.4 \\
 &= 5.9
\end{align}\]

Again, note that \(\mathbb{E}[X] = 5.9\) is not a possible value of
\(X\); it's an average. \textbf{The expectation of X does not need to be
a possible value of X}.

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

The \textbf{variance} of a random variable is a measure of its chance
error. It is defined as the expected squared deviation from the
expectation of \(X\). Put more simply, variance asks: how far does \(X\)
typically vary from its average value, just by chance? What is the
spread of \(X\)'s distribution?

\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]

The units of variance are the square of the units of \(X\). To get it
back to the right scale, use the standard deviation of \(X\):
\[\text{SD}(X) = \sqrt{\text{Var}(X)}\]

Like with expectation, \textbf{variance and standard deviation are
numbers, not random variables}! Variance helps us describe the
variability of a random variable. It is the expected squared error
between the random variable and its expected value. As you will see
shortly, we can use variance to help us quantify the chance error that
arises when using a sample \(X\) to estimate the population mean.

By
\href{https://www.inferentialthinking.com/chapters/14/2/Variability.html\#Chebychev\textquotesingle{}s-Bounds}{Chebyshev's
inequality}, which you saw in Data 8, no matter what the shape of the
distribution of \(X\) is, the vast majority of the probability lies in
the interval ``expectation plus or minus a few SDs.''

If we expand the square and use properties of expectation, we can
re-express variance as the \textbf{computational formula for variance}.

\[\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]

This form is often more convenient to use when computing the variance of
a variable by hand, and it is also useful in Mean Squared Error
calculations, as \(\mathbb{E}[X^2] = \text{Var}(X)\) if \(X\) is
centered and \(E(X)=0\).

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Proof}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\[\begin{align}
   \text{Var}(X) &= \mathbb{E}[(X-\mathbb{E}[X])^2] \\
   &= \mathbb{E}(X^2 - 2X\mathbb{E}(X) + (\mathbb{E}(X))^2) \\
   &= \mathbb{E}(X^2) - 2 \mathbb{E}(X)\mathbb{E}(X) +( \mathbb{E}(X))^2\\
   &= \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{align}\]

\end{tcolorbox}

How do we compute \(\mathbb{E}[X^2]\)? Any function of a random variable
is \emph{also} a random variable. That means that by squaring \(X\),
we've created a new random variable. To compute \(\mathbb{E}[X^2]\), we
can simply apply our definition of expectation to the random variable
\(X^2\).

\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x)\]

\hypertarget{example-die}{%
\subsection{Example: Die}\label{example-die}}

Let \(X\) be the outcome of a single fair die roll. \(X\) is a random
variable defined as \[X = \begin{cases} 
      \frac{1}{6}, \text{if } x \in \{1,2,3,4,5,6\} \\
      0, \text{otherwise} 
   \end{cases}\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{What's the expectation, \(\mathbb{E}[X]?\)}, colframe=quarto-callout-caution-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\[ \begin{align} 
         \mathbb{E}[X] &= 1\big(\frac{1}{6}\big) + 2\big(\frac{1}{6}\big) + 3\big(\frac{1}{6}\big) + 4\big(\frac{1}{6}\big) + 5\big(\frac{1}{6}\big) + 6\big(\frac{1}{6}\big) \\
         &= \big(\frac{1}{6}\big)( 1 + 2 + 3 + 4 + 5 + 6) \\
         &= \frac{7}{2}
      \end{align}\]

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{What's the variance, \(\text{Var}(X)?\)}, colframe=quarto-callout-caution-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Using Approach 1 (definition): \[\begin{align} 
      \text{Var}(X) &= \big(\frac{1}{6}\big)((1 - \frac{7}{2})^2 + (2 - \frac{7}{2})^2 + (3 - \frac{7}{2})^2 + (4 - \frac{7}{2})^2 + (5 - \frac{7}{2})^2 + (6 - \frac{7}{2})^2) \\
      &= \frac{35}{12}
   \end{align}\]

Using Approach 2 (property):
\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x) = \frac{91}{6}\]
\[\text{Var}(X) = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12}\]

\end{tcolorbox}

We can summarize our discussion so far in the following diagram:

\hypertarget{sums-of-random-variables}{%
\section{Sums of Random Variables}\label{sums-of-random-variables}}

Often, we will work with multiple random variables at the same time. A
function of a random variable is also a random variable. If you create
multiple random variables based on your sample, then functions of those
random variables are also random variables.

For example, if \(X_1, X_2, ..., X_n\) are random variables, then so are
all of these:

\begin{itemize}
\tightlist
\item
  \(X_n^2\)
\item
  \(\#\{i : X_i > 10\}\)
\item
  \(\text{max}(X_1, X_2, ..., X_n)\)
\item
  \(\frac{1}{n} \sum_{i=1}^n (X_i - c)^2\)
\item
  \(\frac{1}{n} \sum_{i=1}^n X_i\)
\end{itemize}

Many functions of random variables that we are interested in (e.g.,
counts, means) involve sums of random variables, so let's dive deeper
into the properties of sums of random variables.

\hypertarget{properties-of-expectation}{%
\subsection{Properties of Expectation}\label{properties-of-expectation}}

Instead of simulating full distributions, we often just compute
expectation and variance directly. Recall the definition of expectation:
\[\mathbb{E}[X] = \sum_{x} x P(X=x)\]

From it, we can derive some useful properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Linearity of expectation}. The expectation of the linear
  transformation \(aX+b\), where \(a\) and \(b\) are constants, is:
\end{enumerate}

\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Proof}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\[\begin{align}
        \mathbb{E}[aX+b] &= \sum_{x} (ax + b) P(X=x) \\
        &= \sum_{x} (ax P(X=x) + bP(X=x)) \\
        &= a\sum_{x}P(X=x) + b\sum_{x}P(X=x)\\
        &= a\mathbb{E}(X) + b * 1
    \end{align}\]

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Expectation is also linear in \emph{sums} of random variables.
\end{enumerate}

\[\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Proof}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\[\begin{align}
    \mathbb{E}[X+Y] &= \sum_{s} (X+Y)(s) P(s) \\
    &= \sum_{s} (X(s)P(s) + Y(s)P(s)) \\
    &= \sum_{s} X(s)P(s) + \sum_{s} Y(s)P(s)\\
    &= \mathbb{E}[X] + \mathbb{E}[Y]
\end{align}\]

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  If \(g\) is a non-linear function, then in general,
  \[\mathbb{E}[g(X)] \neq g(\mathbb{E}[X])\] For example, if \(X\) is -1
  or 1 with equal probability, then \(\mathbb{E}[X] = 0\), but
  \(\mathbb{E}[X^2] = 1 \neq 0\).
\end{enumerate}

\hypertarget{properties-of-variance}{%
\subsection{Properties of Variance}\label{properties-of-variance}}

Let's now get into the properties of variance. Recall the definition of
variance: \[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]

Combining it with the properties of expectation, we can derive some
useful properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Unlike expectation, variance is \emph{non-linear}. The variance of the
  linear transformation \(aX+b\) is:
  \[\text{Var}(aX+b) = a^2 \text{Var}(X)\]
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Subsequently, \[\text{SD}(aX+b) = |a| \text{SD}(X)\]
\item
  The full proof of this fact can be found using the definition of
  variance. As general intuition, consider that \(aX+b\) scales the
  variable \(X\) by a factor of \(a\), then shifts the distribution of
  \(X\) by \(b\) units.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Proof}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

We know that \[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]

In order to compute \(\text{Var}(aX+b)\), consider that a shift by \(b\)
units does not affect spread, so \(\text{Var}(aX+b) = \text{Var}(aX)\).

Then, \[\begin{align}
    \text{Var}(aX+b) &= \text{Var}(aX) \\
    &= E((aX)^2) - (E(aX))^2 \\
    &= E(a^2 X^2) - (aE(X))^2\\
    &= a^2 (E(X^2) - (E(X))^2) \\
    &= a^2 \text{Var}(X)
\end{align}\]

\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Shifting the distribution by \(b\) \emph{does not} impact the
  \emph{spread} of the distribution. Thus,
  \(\text{Var}(aX+b) = \text{Var}(aX)\).
\item
  Scaling the distribution by \(a\) \emph{does} impact the spread of the
  distribution.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Variance of sums of random variables is affected by the (in)dependence
  of the random variables.
  \[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{cov}(X,Y)\]
  \[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}\]
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Proof}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

The variance of a sum is affected by the dependence between the two
random variables that are being added. Let's expand the definition of
\(\text{Var}(X + Y)\) to see what's going on.

To simplify the math, let \(\mu_x = \mathbb{E}[X]\) and
\(\mu_y = \mathbb{E}[Y]\).

\[ \begin{align}
\text{Var}(X + Y) &= \mathbb{E}[(X+Y- \mathbb{E}(X+Y))^2] \\
&= \mathbb{E}[((X - \mu_x) + (Y - \mu_y))^2] \\
&= \mathbb{E}[(X - \mu_x)^2 + 2(X - \mu_x)(Y - \mu_y) + (Y - \mu_y)^2] \\
&= \mathbb{E}[(X - \mu_x)^2] + \mathbb{E}[(Y - \mu_y)^2] + \mathbb{E}[(X - \mu_x)(Y - \mu_y)] \\
&= \text{Var}(X) + \text{Var}(Y) + \mathbb{E}[(X - \mu_x)(Y - \mu_y)] 
\end{align}\]

\end{tcolorbox}

\hypertarget{covariance-and-correlation}{%
\subsection{Covariance and
Correlation}\label{covariance-and-correlation}}

We define the \textbf{covariance} of two random variables as the
expected product of deviations from expectation. Put more simply,
covariance is a generalization of variance to variance:
\(\text{Cov}(X, X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X)\)

\[\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\]

We can treat the covariance as a measure of association. Remember the
definition of correlation given when we first established SLR?

\[r(X, Y) = \mathbb{E}\left[\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right] = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}\]

It turns out we've been quietly using covariance for some time now! If
\(X\) and \(Y\) are independent, then \(\text{Cov}(X, Y) =0\) and
\(r(X, Y) = 0\). Note, however, that the converse is not always true:
\(X\) and \(Y\) could have \(\text{Cov}(X, Y) = r(X, Y) = 0\) but not be
independent.

\hypertarget{equal-vs.-identically-distributed-vs.-i.i.d}{%
\subsection{Equal vs.~Identically Distributed
vs.~i.i.d}\label{equal-vs.-identically-distributed-vs.-i.i.d}}

Suppose that we have two random variables \(X\) and \(Y\):

\begin{itemize}
\tightlist
\item
  \(X\) and \(Y\) are \textbf{equal} if \(X(s) = Y(s)\) for every sample
  \(s\). Regardless of the exact sample drawn, \(X\) is always equal to
  \(Y\).
\item
  \(X\) and \(Y\) are \textbf{identically distributed} if the
  distribution of \(X\) is equal to the distribution of \(Y\). We say
  ``\(X\) and \(Y\) are equal in distribution.'' That is, \(X\) and
  \(Y\) take on the same set of possible values, and each of these
  possible values is taken with the same probability. On any specific
  sample \(s\), identically distributed variables do \emph{not}
  necessarily share the same value. If \(X = Y\), then \(X\) and \(Y\)
  are identically distributed; however, the converse is not true (ex:
  \(Y = 7 - X\), \(X\) is a die)
\item
  \(X\) and \(Y\) are \textbf{independent and identically distributed
  (i.i.d)} if

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    The variables are identically distributed.
  \item
    Knowing the outcome of one variable does not influence our belief of
    the outcome of the other.
  \end{enumerate}
\end{itemize}

Note that in Data 100, you'll never be expected to prove that random
variables are i.i.d.

Now let's walk through an example. Say \(X_1\) and \(X_2\) be numbers on
rolls of two fair die. \(X_1\) and \(X_2\) are i.i.d, so \(X_1\) and
\(X_2\) have the same distribution. However, the sums
\(Y = X_1 + X_1 = 2X_1\) and \(Z=X_1+X_2\) have different distributions
but the same expectation.

However, \(Y = X_1\) has a larger variance.

\hypertarget{example-bernoulli-random-variable}{%
\subsection{Example: Bernoulli Random
Variable}\label{example-bernoulli-random-variable}}

To get some practice with the formulas discussed so far, let's derive
the expectation and variance for a Bernoulli(\(p\)) random variable. If
\(X\) \textasciitilde{} Bernoulli(\(p\)),

\(\mathbb{E}[X] = 1 \cdot p + 0 \cdot (1 - p) = p\)

To compute the variance, we will use the computational formula. We first
find that: \(\mathbb{E}[X^2] = 1^2 \cdot p + 0^2 \cdot (1 - p) = p\)

From there, let's calculate our variance:
\(\text{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = p - p^2 = p(1-p)\)

\hypertarget{example-binomial-random-variable}{%
\subsection{Example: Binomial Random
Variable}\label{example-binomial-random-variable}}

Let \(Y\) \textasciitilde{} Binomial(\(n\), \(p\)). We can think of
\(Y\) as being the sum of \(n\) i.i.d. Bernoulli(\(p\)) random
variables. Mathematically, this translates to

\[Y = \sum_{i=1}^n X_i\]

where \(X_i\) is the indicator of a success on trial \(i\).

Using linearity of expectation,

\[\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\]

For the variance, since each \(X_i\) is independent of the other,
\(\text{Cov}(X_i, X_j) = 0\),

\[\text{Var}(Y) =  \sum_{i=1}^n \text{Var}[X_i] = np(1-p)\]

\hypertarget{summary-2}{%
\subsection{Summary}\label{summary-2}}

\begin{itemize}
\tightlist
\item
  Let \(X\) be a random variable with distribution \(P(X=x)\).

  \begin{itemize}
  \tightlist
  \item
    \(\mathbb{E}[X] = \sum_{x} x P(X=x)\)
  \item
    \(\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\)
  \end{itemize}
\item
  Let \(a\) and \(b\) be scalar values.

  \begin{itemize}
  \tightlist
  \item
    \(\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\)
  \item
    \(\text{Var}(aX+b) = a^2 \text{Var}(X)\)
  \end{itemize}
\item
  Let \(Y\) be another random variable.

  \begin{itemize}
  \tightlist
  \item
    \(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)
  \item
    \(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)\)
  \end{itemize}
\end{itemize}

Note that \(\text{Cov}(X,Y)\) would equal 0 if \(X\) and \(Y\) are
independent.

\bookmarksetup{startatroot}

\hypertarget{estimators-bias-and-variance}{%
\chapter{Estimators, Bias, and
Variance}\label{estimators-bias-and-variance}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Explore commonly seen random variables like Bernoulli and Binomial
  distributions
\item
  Apply the Central Limit Theorem to approximate parameters of a
  population
\item
  Use sampled data to model an estimation of and infer the true
  underlying distribution
\item
  Estimate the true population distribution from a sample using the
  bootstrapping technique
\end{itemize}

\end{tcolorbox}

Last time, we introduced the idea of random variables: numerical
functions of a sample. Most of our work in the last lecture was done to
build a background in probability and statistics. Now that we've
established some key ideas, we're in a good place to apply what we've
learned to our original goal -- understanding how the randomness of a
sample impacts the model design process.

In this lecture, we will delve more deeply into the idea of fitting a
model to a sample. We'll explore how to re-express our modeling process
in terms of random variables and use this new understanding to steer
model complexity.

\hypertarget{common-random-variables}{%
\section{Common Random Variables}\label{common-random-variables}}

There are several cases of random variables that appear often and have
useful properties. Below are the ones we will explore further in this
course. The numbers in parentheses are the parameters of a random
variable, which are constants. Parameters define a random variable's
shape (i.e., distribution) and its values. For this lecture, we'll focus
more heavily on the bolded random variables and their special
properties, but you should familiarize yourself with all the ones listed
below:

\begin{itemize}
\tightlist
\item
  \textbf{Bernoulli(\(p\))}

  \begin{itemize}
  \tightlist
  \item
    Takes on value 1 with probability \(p\), and 0 with probability
    \((1 - p)\).
  \item
    AKA the ``indicator'' random variable.
  \item
    Let \(X\) be a Bernoulli(\(p\)) random variable.

    \begin{itemize}
    \tightlist
    \item
      \(\mathbb{E}[X] = 1 * p + 0 * (1-p) = p\)

      \begin{itemize}
      \tightlist
      \item
        \(\mathbb{E}[X^2] = 1^2 * p + 0 * (1-p) = p\)
      \end{itemize}
    \item
      \(\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = p - p^2 = p(1-p)\)
    \end{itemize}
  \end{itemize}
\item
  \textbf{Binomial(\(n\), \(p\))}

  \begin{itemize}
  \tightlist
  \item
    Number of 1s in \(n\) independent Bernoulli(\(p\)) trials.
  \item
    Let \(Y\) be a Binomial(\(n\), \(p\)) random variable.

    \begin{itemize}
    \tightlist
    \item
      The distribution of \(Y\) is given by the binomial formula, and we
      can write \(Y = \sum_{i=1}^n X_i\) where:

      \begin{itemize}
      \tightlist
      \item
        \(X_i\) s the indicator of success on trial i. \(X_i = 1\) if
        trial i is a success, else 0.
      \item
        All \(X_i\) are i.i.d. and Bernoulli(\(p\)).
      \end{itemize}
    \item
      \(\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\)
    \item
      \(\text{Var}(X) = \sum_{i=1}^n \text{Var}(X_i) = np(1-p)\)

      \begin{itemize}
      \tightlist
      \item
        \(X_i\)'s are independent, so \(\text{Cov}(X_i, X_j) = 0\) for
        all i, j.
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  Uniform on a finite set of values

  \begin{itemize}
  \tightlist
  \item
    The probability of each value is
    \(\frac{1}{\text{(number of possible values)}}\).
  \item
    For example, a standard/fair die.
  \end{itemize}
\item
  Uniform on the unit interval (0, 1)

  \begin{itemize}
  \tightlist
  \item
    Density is flat at 1 on (0, 1) and 0 elsewhere.
  \end{itemize}
\item
  Normal(\(\mu, \sigma^2\)), a.k.a Gaussian

  \begin{itemize}
  \tightlist
  \item
    \(f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right)\)
  \end{itemize}
\end{itemize}

\hypertarget{example}{%
\subsection{Example}\label{example}}

Suppose you win cash based on the number of heads you get in a series of
20 coin flips. Let \(X_i = 1\) if the \(i\)-th coin is heads, \(0\)
otherwise. Which payout strategy would you choose?

A. \(Y_A = 10 * X_1 + 10 * X_2\)

B. \(Y_B = \sum_{i=1}^{20} X_i\)

C. \(Y_C = 20 * X_1\)

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-caution-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Solution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Let \(X_1, X_2, ... X_{20}\) be 20 i.i.d Bernoulli(0.5) random
variables. Since the \(X_i\)'s are independent,
\(\text{Cov}(X_i, X_j) = 0\) for all pairs \(i, j\). Additionally, Since
\(X_i\) is Bernoulli(0.5), we know that \(\mathbb{E}[X] = p = 0.5\) and
\(\text{Var}(X) = p(1-p) = 0.25\). We can calculate the following for
each scenario:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
A. \(Y_A = 10 * X_1 + 10 * X_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
B. \(Y_B = \sum_{i=1}^{20} X_i\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
C. \(Y_C = 20 * X_1\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Expectation & \(\mathbb{E}[Y_A] = 10 (0.5) + 10(0.5) = 10\) &
\(\mathbb{E}[Y_B] = 0.5 + ... + 0.5 = 10\) &
\(\mathbb{E}[Y_C] = 20(0.5) = 10\) \\
Variance & \(\text{Var}(Y_A) = 10^2 (0.25) + 10^2 (0.25) = 50\) &
\(\text{Var}(Y_B) = 0.25 + ... + 0.25 = 5\) &
\(\text{Var}(Y_C) = 20^2 (0.25) = 100\) \\
Standard Deviation & \(\text{SD}(Y_A) \approx 7.07\) &
\(\text{SD}(Y_B) \approx 2.24\) & \(\text{SD}(Y_C) = 10\) \\
\end{longtable}

As we can see, all the scenarios have the same expected value but
different variances. The higher the variance, the greater the risk and
uncertainty, so the ``right'' strategy depends on your personal
preference. Would you choose the ``safest'' option B, the most ``risky''
option C, or somewhere in the middle (option A)?

\end{tcolorbox}

\hypertarget{sample-statistics}{%
\section{Sample Statistics}\label{sample-statistics}}

Today, we've talked extensively about populations; if we know the
distribution of a random variable, we can reliably compute expectation,
variance, functions of the random variable, etc. Note that:

\begin{itemize}
\tightlist
\item
  The distribution of a \emph{population} describes how a random
  variable behaves across \emph{all} individuals of interest.
\item
  The distribution of a \emph{sample} describes how a random variable
  behaves in a \emph{specific sample} from the population.
\end{itemize}

In Data Science, however, we often do not have access to the whole
population, so we don't know its distribution. As such, we need to
collect a sample and use its distribution to estimate or infer
properties of the population. In cases like these, we can take several
samples of size \(n\) from the population (an easy way to do this is
using \texttt{df.sample(n,\ replace=True)}), and compute the mean of
each \emph{sample}. When sampling, we make the (big) assumption that we
sample uniformly at random \emph{with replacement} from the population;
each observation in our sample is a random variable drawn i.i.d from our
population distribution. Remember that our sample mean is a random
variable since it depends on our randomly drawn sample! On the other
hand, our population mean is simply a number (a fixed value).

\hypertarget{sample-mean}{%
\subsection{Sample Mean}\label{sample-mean}}

Consider an i.i.d. sample \(X_1, X_2, ..., X_n\) drawn from a population
with mean 𝜇 and SD 𝜎. We define the sample mean as
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i\]

The expectation of the sample mean is given by: \[\begin{align} 
    \mathbb{E}[\bar{X}_n] &= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] \\
    &= \frac{1}{n} (n \mu) \\
    &= \mu 
\end{align}\]

The variance is given by: \[\begin{align} 
    \text{Var}(\bar{X}_n) &= \frac{1}{n^2} \text{Var}( \sum_{i=1}^n X_i) \\
    &=  \frac{1}{n^2} \left( \sum_{i=1}^n \text{Var}(X_i) \right) \\
    &=  \frac{1}{n^2} (n \sigma^2) = \frac{\sigma^2}{n}
\end{align}\]

\(\bar{X}_n\) is approximately normally distributed by the Central Limit
Theorem (CLT).

\hypertarget{central-limit-theorem}{%
\subsection{Central Limit Theorem}\label{central-limit-theorem}}

In
\href{https://inferentialthinking.com/chapters/14/4/Central_Limit_Theorem.html?}{Data
8} and in the previous lecture, you encountered the \textbf{Central
Limit Theorem (CLT)}. This is a powerful theorem for estimating the
distribution of a population with mean \(\mu\) and standard deviation
\(\sigma\) from a collection of smaller samples. The CLT tells us that
if an i.i.d sample of size \(n\) is large, then the probability
distribution of the \textbf{sample mean} is \textbf{roughly normal} with
mean \(\mu\) and SD of \(\frac{\sigma}{\sqrt{n}}\). More generally, any
theorem that provides the rough distribution of a statistic and
\textbf{doesn't need the distribution of the population} is valuable to
data scientists! This is because we rarely know a lot about the
population.

Importantly, the CLT assumes that each observation in our samples is
drawn i.i.d from the distribution of the population. In addition, the
CLT is accurate only when \(n\) is ``large'', but what counts as a
``large'' sample size depends on the specific distribution. If a
population is highly symmetric and unimodal, we could need as few as
\(n=20\); if a population is very skewed, we need a larger \(n\). If in
doubt, you can bootstrap the sample mean and see if the bootstrapped
distribution is bell-shaped. Classes like Data 140 investigate this idea
in great detail.

For a more in-depth demo, check out
\href{https://onlinestatbook.com/stat_sim/sampling_dist/}{onlinestatbook}.

\hypertarget{using-the-sample-mean-to-estimate-the-population-mean}{%
\subsection{Using the Sample Mean to Estimate the Population
Mean}\label{using-the-sample-mean-to-estimate-the-population-mean}}

Now let's say we want to use the sample mean to \textbf{estimate} the
population mean, for example, the average height of Cal undergraduates.
We can typically collect a \textbf{single sample}, which has just one
average. However, what if we happened, by random chance, to draw a
sample with a different mean or spread than that of the population? We
might get a skewed view of how the population behaves (consider the
extreme case where we happen to sample the exact same value \(n\)
times!).

For example, notice the difference in variation between these two
distributions that are different in sample size. The distribution with a
bigger sample size (\(n=800\)) is tighter around the mean than the
distribution with a smaller sample size (\(n=200\)). Try plugging in
these values into the standard deviation equation for the sample mean to
make sense of this!

Applying the CLT allows us to make sense of all of this and resolve this
issue. By drawing many samples, we can consider how the sample
distribution varies across multiple subsets of the data. This allows us
to approximate the properties of the population without the need to
survey every single member.

Given this potential variance, it is also important that we consider the
\textbf{average value and spread} of all possible sample means, and what
this means for how big \(n\) should be. For every sample size, the
expected value of the sample mean is the population mean:
\[\mathbb{E}[\bar{X}_n] = \mu\] We call the sample mean an
\textbf{unbiased estimator} of the population mean and will explore this
idea more in the next lecture.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Data 8 Recap: Square Root Law}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

The square root law
(\href{https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html\#the-square-root-law}{Data
8}) states that if you increase the sample size by a factor, the SD of
the sample mean decreases by the square root of the factor.
\[\text{SD}(\bar{X_n}) = \frac{\sigma}{\sqrt{n}}\] The sample mean is
more likely to be close to the population mean if we have a larger
sample size.

\end{tcolorbox}

\hypertarget{prediction-and-inference-1}{%
\section{Prediction and Inference}\label{prediction-and-inference-1}}

At this point in the course, we've spent a great deal of time working
with models. When we first introduced the idea of modeling a few weeks
ago, we did so in the context of \textbf{prediction}: using models to
make \emph{accurate predictions} about unseen data. Another reason we
might build models is to better understand complex phenomena in the
world around us. \textbf{Inference} is the task of using a model to
infer the true underlying relationships between the feature and response
variables. For example, if we are working with a set of housing data,
\emph{prediction} might ask: given the attributes of a house, how much
is it worth? \emph{Inference} might ask: how much does having a local
park impact the value of a house?

A major goal of inference is to draw conclusions about the full
population of data given only a random sample. To do this, we aim to
estimate the value of a \textbf{parameter}, which is a numerical
function of the \emph{population} (for example, the population mean
\(\mu\)). We use a collected sample to construct a \textbf{statistic},
which is a numerical function of the random \emph{sample} (for example,
the sample mean \(\bar{X}_n\)). It's helpful to think ``p'' for
``parameter'' and ``population,'' and ``s'' for ``sample'' and
``statistic.''

Since the sample represents a \emph{random} subset of the population,
any statistic we generate will likely deviate from the true population
parameter, and it \emph{could have been different}. We say that the
sample statistic is an \textbf{estimator} of the true population
parameter. Notationally, the population parameter is typically called
\(\theta\), while its estimator is denoted by \(\hat{\theta}\).

To address our inference question, we aim to construct estimators that
closely estimate the value of the population parameter. We evaluate how
``good'' an estimator is by answering three questions:

\begin{itemize}
\tightlist
\item
  How close is our answer to the parameter? \textbf{(Risk / MSE)}
  \[ MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)]^2\]
\item
  Do we get the right answer for the parameter, on average?
  \textbf{(Bias)}
  \[\text{Bias}(\hat{\theta}) = E[\hat{\theta} - \theta] = E[\hat{\theta}] - \theta\]
\item
  How variable is the answer? \textbf{(Variance)}
  \[Var(\hat{\theta}) = E[(\theta - E[\theta])^2] \]
\end{itemize}

This relationship can be illustrated with an archery analogy. Imagine
that the center of the target is the \(\theta\) and each arrow
corresponds to a separate parameter estimate \(\hat{\theta}\)

Ideally, we want our estimator to have low bias and low variance, but
how can we mathematically quantify that? See
Section~\ref{sec-bias-variance-tradeoff} for more detail.

\hypertarget{prediction-as-estimation}{%
\subsection{Prediction as Estimation}\label{prediction-as-estimation}}

Now that we've established the idea of an estimator, let's see how we
can apply this learning to the modeling process. To do so, we'll take a
moment to formalize our data collection and models in the language of
random variables.

Say we are working with an input variable, \(x\), and a response
variable, \(Y\). We assume that \(Y\) and \(x\) are linked by some
relationship \(g\); in other words, \(Y = g(x)\) where \(g\) represents
some ``universal truth'' or ``law of nature'' that defines the
underlying relationship between \(x\) and \(Y\). In the image below,
\(g\) is denoted by the red line.

As data scientists, however, we have no way of directly ``seeing'' the
underlying relationship \(g\). The best we can do is collect observed
data out in the real world to try to understand this relationship.
Unfortunately, the data collection process will always have some
inherent error (think of the randomness you might encounter when taking
measurements in a scientific experiment). We say that each observation
comes with some random error or \textbf{noise} term, \(\epsilon\) (read:
``epsilon''). This error is assumed to be a random variable with
expectation \(\mathbb{E}(\epsilon)=0\), variance
\(\text{Var}(\epsilon) = \sigma^2\), and be i.i.d. across each
observation. The existence of this random noise means that our
observations, \(Y(x)\), are \emph{random variables}.

We can only observe our random sample of data, represented by the blue
points above. From this sample, we want to estimate the true
relationship \(g\). We do this by constructing the model \(\hat{Y}(x)\)
to estimate \(g\).

\[\text{True relationship: } g(x)\]

\[\text{Observed relationship: }Y = g(x) + \epsilon\]

\[\text{Prediction: }\hat{Y}(x)\]

When building models it is also important to note that our choice of
features will also signficantly impact our estimation. In the plot
below, you can see how the different models (green and purple) can lead
to different estimates.

\hypertarget{estimating-a-linear-relationship}{%
\subsubsection{Estimating a Linear
Relationship}\label{estimating-a-linear-relationship}}

If we assume that the true relationship \(g\) is linear, we can express
the response as \(Y = f_{\theta}(x)\), where our true relationship is
modeled by \[Y = g(x) + \epsilon\]
\[ f_{\theta}(x) = Y = \theta_0 + \sum_{j=1}^p \theta_j x_j + \epsilon\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Which expressions are random?}, colframe=quarto-callout-warning-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

In our two equations above, the true relationship
\(g(x) = \theta_0 + \sum_{j=1}^p \theta_j x_j\) is not random, but
\(\epsilon\) is random. Hence, \(Y = f_{\theta}(x)\) is also random.

\end{tcolorbox}

This true relationship has true, unobservable parameters \(\theta\), and
it has random noise \(\epsilon\), so we can never observe the true
relationship. Instead, the next best thing we can do is obtain a sample
\(\Bbb{X}\), \(\Bbb{Y}\) of \(n\) observed relationships, \((x, Y)\) and
use it to train a model and obtain an estimate of \(\hat{\theta}\)
\[\hat{Y}(x) = f_{\hat{\theta}}(x) = \hat{\theta_0} + \sum_{j=1}^p \hat{\theta_j} x_j\]

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Which expressions are random?}, colframe=quarto-callout-warning-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

In our estimating equation above, our sample \(\Bbb{X}\), \(\Bbb{Y}\)
are random (often due to human error). Hence, the estimates we calculate
from our samples \(\hat{\theta}\) are also random, so our predictor
\(\hat{Y}(x)\) is also random.

\end{tcolorbox}

Now taking a look at our original equations, we can see that they both
have differing sources of randomness. For our observed relationship,
\(Y = g(x) + \epsilon\), \(\epsilon\) represents errors which occur
during or after the observation or measurement process. For the
estimation model, the data we have is a random sample collected from the
population, which was constructed from decisions made before the
measurement process.

\hypertarget{sec-bias-variance-tradeoff}{%
\section{Bias-Variance Tradeoff}\label{sec-bias-variance-tradeoff}}

Recall the model and the data we generated from that model in the last
section:

\[\text{True relationship: } g(x)\]

\[\text{Observed relationship: }Y = g(x) + \epsilon\]

\[\text{Prediction: }\hat{Y}(x)\]

With this reformulated modeling goal, we can now revisit the
Bias-Variance Tradeoff from two lectures ago (shown below):

In today's lecture, we'll explore a more mathematical version of the
graph you see above by introducing the terms model risk, observation
variance, model bias, and model variance. Eventually, we'll work our way
up to an updated version of the Bias-Variance Tradeoff graph that you
see below

\hypertarget{model-risk}{%
\subsection{Model Risk}\label{model-risk}}

\textbf{Model risk} is defined as the mean square prediction error of
the random variable \(\hat{Y}\). It is an expectation across \emph{all}
samples we could have possibly gotten when fitting the model, which we
can denote as random variables \(X_1, X_2, \ldots, X_n, Y\). Model risk
considers the model's performance on any sample that is theoretically
possible, rather than the specific data that we have collected.

\[\text{model risk }=E\left[(Y-\hat{Y(x)})^2\right]\]

What is the origin of the error encoded by model risk? Note that there
are two types of errors:

\begin{itemize}
\tightlist
\item
  Chance errors: happen due to randomness alone

  \begin{itemize}
  \tightlist
  \item
    Source 1 \textbf{(Observation Variance)}: randomness in new
    observations \(Y\) due to random noise \(\epsilon\)
  \item
    Source 2 \textbf{(Model Variance)}: randomness in the sample we used
    to train the models, as samples \(X_1, X_2, \ldots, X_n, Y\) are
    random
  \end{itemize}
\item
  \textbf{(Model Bias)}: non-random error due to our model being
  different from the true underlying function \(g\)
\end{itemize}

Recall the data-generating process we established earlier. There is a
true underlying relationship \(g\), observed data (with random noise)
\(Y\), and model \(\hat{Y}\).

To better understand model risk, we'll zoom in on a single data point in
the plot above.

Remember that \(\hat{Y}(x)\) is a random variable -- it is the
prediction made for \(x\) after being fit on the specific sample used
for training. If we had used a different sample for training, a
different prediction might have been made for this value of \(x\). To
capture this, the diagram above considers both the prediction
\(\hat{Y}(x)\) made for a particular random training sample, and the
\emph{expected} prediction across all possible training samples,
\(E[\hat{Y}(x)]\).

We can use this simplified diagram to break down the prediction error
into smaller components. First, start by considering the error on a
single prediction, \(Y(x)-\hat{Y}(x)\).

We can identify three components of this error.

That is, the error can be written as:

\[Y(x)-\hat{Y}(x) = \epsilon + \left(g(x)-E\left[\hat{Y}(x)\right]\right) + \left(E\left[\hat{Y}(x)\right] - \hat{Y}(x)\right)\]
\[\newline   \]

The model risk is the expected square of the expression above,
\(E\left[(Y(x)-\hat{Y}(x))^2\right]\). If we square both sides and then
take the expectation, we will get the following decomposition of model
risk:

\[E\left[(Y(x)-\hat{Y}(x))^2\right] = E[\epsilon^2] + \left(g(x)-E\left[\hat{Y}(x)\right]\right)^2 + E\left[\left(E\left[\hat{Y}(x)\right] - \hat{Y}(x)\right)^2\right]\]

It looks like we are missing some cross-product terms when squaring the
right-hand side, but it turns out that all of those cross-product terms
are zero. The detailed derivation is out of scope for this class, but a
proof is included at the end of this note for your reference.

This expression may look complicated at first glance, but we've actually
already defined each term earlier in this lecture! Let's look at them
term by term.

\hypertarget{observation-variance}{%
\subsubsection{Observation Variance}\label{observation-variance}}

The first term in the above decomposition is \(E[\epsilon^2]\). Remember
\(\epsilon\) is the random noise when observing \(Y\), with expectation
\(\mathbb{E}(\epsilon)=0\) and variance
\(\text{Var}(\epsilon) = \sigma^2\). We can show that \(E[\epsilon^2]\)
is the variance of \(\epsilon\): \[
\begin{align*}
\text{Var}(\epsilon) &= E[\epsilon^2] + \left(E[\epsilon]\right)^2\\
&= E[\epsilon^2] + 0^2\\
&= \sigma^2.
\end{align*}
\]

This term describes how variable the random error \(\epsilon\) (and
\(Y\)) is for each observation. This is called the \textbf{observation
variance}. It exists due to the randomness in our observations \(Y\). It
is a form of \emph{chance error} we talked about in the Sampling
lecture.

\[\text{observation variance} = \text{Var}(\epsilon) = \sigma^2.\]

The observation variance results from measurement errors when observing
data or missing information that acts like noise. To reduce this
observation variance, we could try to get more precise measurements, but
it is often beyond the control of data scientists. Because of this, the
observation variance \(\sigma^2\) is sometimes called ``irreducible
error.''

\hypertarget{model-variance}{%
\subsubsection{Model Variance}\label{model-variance}}

We will then look at the last term:
\(E\left[\left(E\left[\hat{Y}(x)\right] - \hat{Y}(x)\right)^2\right]\).
If you recall the definition of variance from the last lecture, this is
precisely \(\text{Var}(\hat{Y}(x))\). We call this the \textbf{model
variance}.

It describes how much the prediction \(\hat{Y}(x)\) tends to vary when
we fit the model on different samples. Remember the sample we collect
can come out very differently, thus the prediction \(\hat{Y}(x)\) will
also be different. The model variance describes this variability due to
the randomness in our sampling process. Like observation variance, it is
also a form of \emph{chance error}---even though the sources of
randomness are different.

\[\text{model variance} = \text{Var}(\hat{Y}(x)) = E\left[\left(\hat{Y}(x) - E\left[\hat{Y}(x)\right]\right)^2\right]\]

The main reason for the large model variance is because of
\textbf{overfitting}: we paid too much attention to the details in our
sample that small differences in our random sample lead to large
differences in the fitted model. To remediate this, we try to reduce
model complexity (e.g.~take out some features and limit the magnitude of
estimated model coefficients) and not fit our model on the noises.

\hypertarget{model-bias}{%
\subsubsection{Model Bias}\label{model-bias}}

Finally, the second term is
\(\left(g(x)-E\left[\hat{Y}(x)\right]\right)^2\). What is this? The term
\(E\left[\hat{Y}(x)\right] - g(x)\) is called the \textbf{model bias}.

Remember that \(g(x)\) is the fixed underlying truth and \(\hat{Y}(x)\)
is our fitted model, which is random. Model bias therefore measures how
far off \(g(x)\) and \(\hat{Y}(x)\) are on average over all possible
samples.

\[\text{model bias} = E\left[\hat{Y}(x) - g(x)\right] = E\left[\hat{Y}(x)\right] - g(x)\]

The model bias is not random; it's an average measure for a specific
individual \(x\). If bias is positive, our model tends to overestimate
\(g(x)\); if it's negative, our model tends to underestimate \(g(x)\).
And if it's 0, we can say that our model is \textbf{unbiased}.

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Unbiased Estimators}, colframe=quarto-callout-tip-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

An \textbf{unbiased model} has a \(\text{model bias } = 0\). In other
words, our model predicts \(g(x)\) on average.

Similarly, we can define bias for estimators like the mean. The sample
mean is an \textbf{unbiased estimator} of the population mean, as by
CLT, \(\mathbb{E}[\bar{X}_n] = \mu\). Therefore, the
\(\text{estimator bias } = \mathbb{E}[\bar{X}_n] - \mu = 0\).

\end{tcolorbox}

There are two main reasons for large model biases:

\begin{itemize}
\tightlist
\item
  Underfitting: our model is too simple for the data
\item
  Lack of domain knowledge: we don't understand what features are useful
  for the response variable
\end{itemize}

To fix this, we increase model complexity (but we don't want to
overfit!) or consult domain experts to see which models make sense. You
can start to see a tradeoff here: if we increase model complexity, we
decrease the model bias, but we also risk increasing the model variance.

\hypertarget{the-decomposition}{%
\subsection{The Decomposition}\label{the-decomposition}}

To summarize:

\begin{itemize}
\tightlist
\item
  The \textbf{model risk},
  \(\mathbb{E}\left[(Y(x)-\hat{Y}(x))^2\right]\), is the mean squared
  prediction error of the model. It is an expectation and is therefore a
  \textbf{fixed number} (for a given x).
\item
  The \textbf{observation variance}, \(\sigma^2\), is the variance of
  the random noise in the observations. It describes how variable the
  random error \(\epsilon\) is for each observation and \textbf{cannot
  be addressed by modeling}.
\item
  The \textbf{model bias}, \(\mathbb{E}\left[\hat{Y}(x)\right]-g(x)\),
  is how ``off'' the \(\hat{Y}(x)\) is as an estimator of the true
  underlying relationship \(g(x)\).
\item
  The \textbf{model variance}, \(\text{Var}(\hat{Y}(x))\), describes how
  much the prediction \(\hat{Y}(x)\) tends to vary when we fit the model
  on different samples.
\end{itemize}

The above definitions enable us to simplify the decomposition of model
risk before as:

\[ E[(Y(x) - \hat{Y}(x))^2] = \sigma^2 + (E[\hat{Y}(x)] - g(x))^2 + \text{Var}(\hat{Y}(x)) \]
\[\text{model risk } = \text{observation variance} + (\text{model bias})^2 \text{+ model variance}\]

This is known as the \textbf{bias-variance tradeoff}. What does it mean?
Remember that the model risk is a measure of the model's performance.
Our goal in building models is to keep model risk low; this means that
we will want to ensure that each component of model risk is kept at a
small value.

Observation variance is an inherent, random part of the data collection
process. We aren't able to reduce the observation variance, so we'll
focus our attention on the model bias and model variance.

In the Feature Engineering lecture, we considered the issue of
overfitting. We saw that the model's error or bias tends to decrease as
model complexity increases --- if we design a highly complex model, it
will tend to make predictions that are closer to the true relationship
\(g\). At the same time, model variance tends to \emph{increase} as
model complexity increases; a complex model may overfit to the training
data, meaning that small differences in the random samples used for
training lead to large differences in the fitted model. We have a
problem. To decrease model bias, we could increase the model's
complexity, which would lead to overfitting and an increase in model
variance. Alternatively, we could decrease model variance by decreasing
the model's complexity at the cost of increased model bias due to
underfitting.

We need to strike a balance. Our goal in model creation is to use a
complexity level that is high enough to keep bias low, but not so high
that model variance is large.

\hypertarget{bonus-proof-of-bias-variance-decomposition}{%
\section{{[}Bonus{]} Proof of Bias-Variance
Decomposition}\label{bonus-proof-of-bias-variance-decomposition}}

This section walks through the detailed derivation of the Bias-Variance
Decomposition in the Bias-Variance Tradeoff section above, and this
content is out of scope.

\begin{tcolorbox}[enhanced jigsaw, colback=white, rightrule=.15mm, opacityback=0, breakable, leftrule=.75mm, arc=.35mm, toprule=.15mm, bottomrule=.15mm, left=2mm]

\textbf{Click to show}\vspace{2mm}

We want to prove that the model risk can be decomposed as

\[
\begin{align*}
E\left[(Y(x)-\hat{Y}(x))^2\right] &= E[\epsilon^2] + \left(g(x)-E\left[\hat{Y}(x)\right]\right)^2 + E\left[\left(E\left[\hat{Y}(x)\right] - \hat{Y}(x)\right)^2\right].
\end{align*}
\]

To prove this, we will first need the following lemma:

If \(V\) and \(W\) are independent random variables then
\(E[VW] = E[V]E[W]\).

We will prove this in the discrete finite case. Trust that it's true in
greater generality.

The job is to calculate the weighted average of the values of \(VW\),
where the weights are the probabilities of those values. Here goes.

\begin{align*}
E[VW] ~ &= ~ \sum_v\sum_w vwP(V=v \text{ and } W=w) \\
&= ~ \sum_v\sum_w vwP(V=v)P(W=w) ~~~~ \text{by independence} \\
&= ~ \sum_v vP(V=v)\sum_w wP(W=w) \\
&= ~ E[V]E[W]
\end{align*}

Now we go into the actual proof:

\hypertarget{goal}{%
\subsection{Goal}\label{goal}}

Decompose the model risk into recognizable components.

\hypertarget{step-1}{%
\subsection{Step 1}\label{step-1}}

\[
\begin{align*}
\text{model risk} ~ &= ~ E\left[\left(Y - \hat{Y}(x)\right)^2 \right] \\
&= ~ E\left[\left(g(x) + \epsilon - \hat{Y}(x)\right)^2 \right] \\
&= ~ E\left[\left(\epsilon + \left(g(x)- \hat{Y}(x)\right)\right)^2 \right] \\
&= ~ E\left[\epsilon^2\right] + 2E\left[\epsilon \left(g(x)- \hat{Y}(x)\right)\right] + E\left[\left(g(x) - \hat{Y}(x)\right)^2\right]\\
\end{align*}
\]

On the right hand side:

\begin{itemize}
\tightlist
\item
  The first term is the observation variance \(\sigma^2\).
\item
  The cross product term is 0 because \(\epsilon\) is independent of
  \(g(x) - \hat{Y}(x)\) and \(E(\epsilon) = 0\)
\item
  The last term is the mean squared difference between our predicted
  value and the value of the true function at \(x\)
\end{itemize}

\hypertarget{step-2}{%
\subsection{Step 2}\label{step-2}}

At this stage we have

\[
\text{model risk} ~ = ~ E\left[\epsilon^2\right] + E\left[\left(g(x) - \hat{Y}(x)\right)^2\right]
\]

We don't yet have a good understanding of \(g(x) - \hat{Y}(x)\). But we
do understand the deviation
\(D_{\hat{Y}(x)} = \hat{Y}(x) - E\left[\hat{Y}(x)\right]\). We know that

\begin{itemize}
\tightlist
\item
  \(E\left[D_{\hat{Y}(x)}\right] ~ = ~ 0\)
\item
  \(E\left[D_{\hat{Y}(x)}^2\right] ~ = ~ \text{model variance}\)
\end{itemize}

So let's add and subtract \(E\left[\hat{Y}(x)\right]\) and see if that
helps.

\[
g(x) - \hat{Y}(x) ~ = ~ \left(g(x) - E\left[\hat{Y}(x)\right] \right) + \left(E\left[\hat{Y}(x)\right] - \hat{Y}(x)\right) 
\]

The first term on the right hand side is the model bias at \(x\). The
second term is \(-D_{\hat{Y}(x)}\). So

\[
g(x) - \hat{Y}(x) ~ = ~ \text{model bias} - D_{\hat{Y}(x)}
\]

\hypertarget{step-3}{%
\subsection{Step 3}\label{step-3}}

Remember that the model bias at \(x\) is a constant, not a random
variable. Think of it as your favorite number, say 10. Then \[
\begin{align*}
E\left[ \left(g(x) - \hat{Y}(x)\right)^2 \right] ~ &= ~ \text{model bias}^2 - 2(\text{model bias})E\left[D_{\hat{Y}(x)}\right] + E\left[D_{\hat{Y}(x)}^2\right] \\
&= ~ \text{model bias}^2 - 0 + \text{model variance} \\
&= ~ \text{model bias}^2 + \text{model variance}
\end{align*}
\]

Again, the cross-product term is \(0\) because
\(E\left[D_{\hat{Y}(x)}\right] ~ = ~ 0\).

\hypertarget{step-4-bias-variance-decomposition}{%
\subsection{Step 4: Bias-Variance
Decomposition}\label{step-4-bias-variance-decomposition}}

In Step 2, we had:

\[
\text{model risk} ~ = ~ \text{observation variance} + E\left[\left(g(x) - \hat{Y}(x)\right)^2\right]
\]

Step 3 showed:

\[
E\left[ \left(g(x) - \hat{Y}(x)\right)^2 \right] ~ = ~ \text{model bias}^2 + \text{model variance}
\]

Thus, we have proven the bias-variance decomposition:

\[
\text{model risk} = \text{observation variance} + \text{model bias}^2 + \text{model variance}.
\]

That is,

\[
E\left[(Y(x)-\hat{Y}(x))^2\right] = \sigma^2 + \left(E\left[\hat{Y}(x)\right] - g(x)\right)^2 + E\left[\left(\hat{Y}(x)-E\left[\hat{Y}(x)\right]\right)^2\right]
\]

\end{tcolorbox}

\bookmarksetup{startatroot}

\hypertarget{causal-inference-and-confounding}{%
\chapter{Causal Inference and
Confounding}\label{causal-inference-and-confounding}}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Learning Outcomes}, colframe=quarto-callout-note-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

\begin{itemize}
\tightlist
\item
  Construct confidence intervals for hypothesis testing using
  bootstrapping
\item
  Understand the assumptions we make and their impact on our regression
  inference
\item
  Explore ways to overcome issues of multicollinearity
\item
  Compare regression correlation and causation
\end{itemize}

\end{tcolorbox}

Last time, we introduced the idea of random variables and how they
affect the data and model we construct. We also demonstrated the
decomposition of model risk from a fitted model and dived into the
bias-variance tradeoff.

In this lecture, we will explore regression inference via hypothesis
testing, understand how to use bootstrapping under the right
assumptions, and consider the environment of understanding causality in
theory and in practice.

\hypertarget{parameter-inference-interpreting-regression-coefficients}{%
\section{Parameter Inference: Interpreting Regression
Coefficients}\label{parameter-inference-interpreting-regression-coefficients}}

There are two main reasons why we build models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Prediction}: using our model to make accurate predictions
  about unseen data
\item
  \textbf{Inference}: using our model to draw conclusions about the
  underlying relationship(s) between our features and response. We want
  to understand the complex phenomena occurring in the world we live in.
  While training is the process of fitting a model, inference is the
  \emph{process of making predictions}.
\end{enumerate}

Recall the framework we established in the last lecture. The
relationship between datapoints is given by \(Y = g(x) + \epsilon\),
where \(g(x)\) is the \emph{true underlying relationship}, and
\(\epsilon\) represents randomness. If we assume \(g(x)\) is linear, we
can express this relationship in terms of the unknown, true model
parameters \(\theta\).

\[f_{\theta}(x) = g(x) + \epsilon = \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p + \epsilon\]

Our model attempts to estimate each true population parameter
\(\theta_i\) using the sample estimates \(\hat{\theta}_i\) calculated
from the design matrix \(\Bbb{X}\) and response vector \(\Bbb{Y}\).

\[f_{\hat{\theta}}(x) = \hat{\theta}_0 + \hat{\theta}_1 x_1 + \ldots + \hat{\theta}_p x_p\]

Let's pause for a moment. At this point, we're very used to working with
the idea of a model parameter. But what exactly does each coefficient
\(\theta_i\) actually \emph{mean}? We can think of each \(\theta_i\) as
a \emph{slope} of the linear model. If all other variables are held
constant, a unit change in \(x_i\) will result in a \(\theta_i\) change
in \(f_{\theta}(x)\). Broadly speaking, a large value of \(\theta_i\)
means that the feature \(x_i\) has a large effect on the response;
conversely, a small value of \(\theta_i\) means that \(x_i\) has little
effect on the response. In the extreme case, if the true parameter
\(\theta_i\) is 0, then the feature \(x_i\) has \textbf{no effect} on
\(Y(x)\).

If the true parameter \(\theta_i\) for a particular feature is 0, this
tells us something pretty significant about the world: there is no
underlying relationship between \(x_i\) and \(Y(x)\)! But how can we
test if a parameter is actually 0? As a baseline, we go through our
usual process of drawing a sample, using this data to fit a model, and
computing an estimate \(\hat{\theta}_i\). However, we also need to
consider that if our random sample comes out differently, we may find a
different result for \(\hat{\theta}_i\). To infer if the true parameter
\(\theta_i\) is 0, we want to draw our conclusion from the distribution
of \(\hat{\theta}_i\) estimates we could have drawn across all other
random samples. This is where
\href{https://inferentialthinking.com/chapters/11/Testing_Hypotheses.html}{hypothesis
testing} comes in handy!

To test if the true parameter \(\theta_i\) is 0, we construct a
\textbf{hypothesis test} where our null hypothesis states that the true
parameter \(\theta_i\) is 0, and the alternative hypothesis states that
the true parameter \(\theta_i\) is \emph{not} 0. If our p-value is
smaller than our cutoff value (usually p = 0.05), we reject the null
hypothesis in favor of the alternative hypothesis.

\hypertarget{review-bootstrap-resampling}{%
\section{Review: Bootstrap
Resampling}\label{review-bootstrap-resampling}}

To determine the properties (e.g., variance) of the sampling
distribution of an estimator, we'd need access to the population.
Ideally, we'd want to consider all possible samples in the population,
compute an estimate for each sample, and study the distribution of those
estimates.

However, this can be quite expensive and time-consuming. Even more
importantly, we don't have access to the population ------ we only have
\emph{one} random sample from the population. How can we consider all
possible samples if we only have one?

Bootstrapping comes in handy here! With bootstrapping, we treat our
random sample as a ``population'' and resample from it \emph{with
replacement}. Intuitively, a random sample resembles the population (if
it is big enough), so a random \emph{resample} also resembles a random
sample of the population. When sampling, there are a couple things to
keep in mind:

\begin{itemize}
\tightlist
\item
  We need to sample the same way we constructed the original sample.
  Typically, this involves taking a simple random sample with
  replacement.
\item
  New samples must be the same size as the original sample. We need to
  accurately model the variability of our estimates.
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, colback=white, colbacktitle=quarto-callout-warning-color!10!white, coltitle=black, breakable, leftrule=.75mm, bottomrule=.15mm, bottomtitle=1mm, rightrule=.15mm, opacityback=0, toptitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Why must we resample \emph{with replacement}?}, colframe=quarto-callout-warning-color-frame, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, left=2mm]

Given an original sample of size \(n\), we want a resample that has the
same size \(n\) as the original. Sampling \emph{without} replacement
will give us the original sample with shuffled rows. Hence, when we
calculate summary statistics like the average, our sample \emph{without}
replacement will always have the same average as the original sample,
defeating the purpose of a bootstrap.

\end{tcolorbox}

Bootstrap resampling is a technique for estimating the sampling
distribution of an estimator. To execute it, we can follow the
pseudocode below:

\begin{verbatim}
collect a random sample of size n (called the bootstrap population)

initiate a list of estimates

repeat 10,000 times:
    resample with replacement from the bootstrap population
    apply estimator f to the resample
    store in list

list of estimates is the bootstrapped sampling distribution of f
\end{verbatim}

How well does bootstrapping actually represent our population? The
bootstrapped sampling distribution of an estimator does not exactly
match the sampling distribution of that estimator, but it is often
close. Similarly, the variance of the bootstrapped distribution is often
close to the true variance of the estimator. The example below displays
the results of different bootstraps from a \emph{known} population using
a sample size of \(n=50\).

In the real world, we don't know the population distribution. The center
of the bootstrapped distribution is the estimator applied to our
original sample, so we have no way of understanding the estimator's true
expected value; the center and spread of our bootstrap are
\emph{approximations}. The quality of our bootstrapped distribution also
depends on the quality of our original sample. If our original sample
was not representative of the population (like Sample 5 in the image
above), then the bootstrap is next to useless. In general, bootstrapping
works better for \emph{large samples}, when the population distribution
is \emph{not heavily skewed} (no outliers), and when the estimator is
\emph{``low variance''} (insensitive to extreme values).

Although our bootstrapped sample distribution does not exactly match the
sampling distribution of the population, we can see that it is
relatively close. This demonstrates the benefit of bootstrapping ------
without knowing the actual population distribution, we can still roughly
approximate the true slope for the model by using only a single random
sample of 20 cars.

\hypertarget{collinearity}{%
\section{Collinearity}\label{collinearity}}

\hypertarget{hypothesis-testing-through-bootstrap-snowy-plover-demo}{%
\subsection{Hypothesis Testing Through Bootstrap: Snowy Plover
Demo}\label{hypothesis-testing-through-bootstrap-snowy-plover-demo}}

We can conduct the hypothesis testing described earlier through
\textbf{bootstrapping} (this equivalence can be proven through the
\href{https://stats.stackexchange.com/questions/179902/confidence-interval-p-value-duality-vs-frequentist-interpretation-of-cis}{duality
argument}, which is out of scope for this class). We use bootstrapping
to compute approximate 95\% confidence intervals for each \(\theta_i\).
If the interval doesn't contain 0, we reject the null hypothesis at the
p=5\% level. Otherwise, the data is consistent with the null, as the
true parameter \emph{could possibly} be 0.

To show an example of this hypothesis testing process, we'll work with
the \href{https://www.audubon.org/field-guide/bird/snowy-plover}{snowy
plover} dataset throughout this section. The data are about the eggs and
newly hatched chicks of the Snowy Plover. The data were collected at the
Point Reyes National Seashore by a former
\href{https://openlibrary.org/books/OL2038693M/BLSS_the_Berkeley_interactive_statistical_system}{student
at Berkeley}. Here's a
\href{http://cescos.fau.edu/jay/eps/articles/snowyplover.html}{parent
bird and some eggs}.

Note that \texttt{Egg\ Length} and \texttt{Egg\ Breadth} (widest
diameter) are measured in millimeters, and \texttt{Egg\ Weight} and
\texttt{Bird\ Weight} are measured in grams. For reference, a standard
paper clip weighs about one gram.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{eggs }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/snowy\_plover.csv"}\NormalTok{)}
\NormalTok{eggs.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrrrr}
\toprule
{} &  egg\_weight &  egg\_length &  egg\_breadth &  bird\_weight \\
\midrule
0 &         7.4 &       28.80 &        21.84 &          5.2 \\
1 &         7.7 &       29.04 &        22.45 &          5.4 \\
2 &         7.9 &       29.36 &        22.48 &          5.6 \\
3 &         7.5 &       30.10 &        21.71 &          5.3 \\
4 &         8.3 &       30.17 &        22.75 &          5.9 \\
\bottomrule
\end{tabular}

Our goal will be to predict the weight of a newborn plover chick, which
we assume follows the true relationship \(Y = f_{\theta}(x)\) below.

\[\text{bird\_weight} = \theta_0 + \theta_1 \text{egg\_weight} + \theta_2 \text{egg\_length} + \theta_3 \text{egg\_breadth} + \epsilon\]

Note that for each \(i\), the parameter \(\theta_i\) is a fixed number,
but it is unobservable. We can only estimate it. The random error
\(\epsilon\) is also unobservable, but it is assumed to have expectation
0 and be independent and identically distributed across eggs.

Say we wish to determine if the \texttt{egg\_weight} impacts the
\texttt{bird\_weight} of a chick -- we want to infer if \(\theta_1\) is
equal to 0.

First, we define our hypotheses:

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis}: the true parameter \(\theta_1\) is 0; any
  variation is due to random chance.
\item
  \textbf{Alternative hypothesis}: the true parameter \(\theta_1\) is
  not 0.
\end{itemize}

Next, we use our data to fit a model \(\hat{Y} = f_{\hat{\theta}}(x)\)
that approximates the relationship above. This gives us the
\textbf{observed value} of \(\hat{\theta}_1\) from our data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{X }\OperatorTok{=}\NormalTok{ eggs[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ eggs[}\StringTok{"bird\_weight"}\NormalTok{]}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{model.fit(X, Y)}

\CommentTok{\# This gives an array containing the fitted model parameter estimates}
\NormalTok{thetas }\OperatorTok{=}\NormalTok{ model.coef\_}

\CommentTok{\# Put the parameter estimates in a nice table for viewing}
\NormalTok{display(pd.DataFrame(}
\NormalTok{  [model.intercept\_] }\OperatorTok{+} \BuiltInTok{list}\NormalTok{(model.coef\_),}
\NormalTok{  columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}theta\_hat\textquotesingle{}}\NormalTok{],}
\NormalTok{  index}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}intercept\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}egg\_weight\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}egg\_length\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}egg\_breadth\textquotesingle{}}\NormalTok{]}
\NormalTok{))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"RMSE"}\NormalTok{, np.mean((Y }\OperatorTok{{-}}\NormalTok{ model.predict(X)) }\OperatorTok{**} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  theta\_hat \\
\midrule
intercept   &  -4.605670 \\
egg\_weight  &   0.431229 \\
egg\_length  &   0.066570 \\
egg\_breadth &   0.215914 \\
\bottomrule
\end{tabular}

\begin{verbatim}
RMSE 0.04547085380275766
\end{verbatim}

Our single sample of data gives us the value of
\(\hat{\theta}_1=0.431\). To get a sense of how this estimate might vary
if we were to draw different random samples, we will use
\href{https://inferentialthinking.com/chapters/13/2/Bootstrap.html?}{bootstrapping}.
As a refresher, to construct a bootstrap sample, we will draw a resample
from the collected data that:

\begin{itemize}
\tightlist
\item
  Has the same sample size as the collected data
\item
  Is drawn with replacement (this ensures that we don't draw the exact
  same sample every time!)
\end{itemize}

We draw a bootstrap sample, use this sample to fit a model, and record
the result for \(\hat{\theta}_1\) on this bootstrapped sample. We then
repeat this process many times to generate a \textbf{bootstrapped
empirical distribution} of \(\hat{\theta}_1\). This gives us an estimate
of what the true distribution of \(\hat{\theta}_1\) across all possible
samples might look like.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set a random seed so you generate the same random sample as staff}
\CommentTok{\# In the "real world", we wouldn\textquotesingle{}t do this}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\CommentTok{\# Set the sample size of each bootstrap sample}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(eggs)}

\CommentTok{\# Create a list to store all the bootstrapped estimates}
\NormalTok{estimates }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Generate a bootstrap resample from \textasciigrave{}eggs\textasciigrave{} and find an estimate for theta\_1 using this sample. }
\CommentTok{\# Repeat 10000 times.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
    \CommentTok{\# draw a bootstrap sample}
\NormalTok{    bootstrap\_resample }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[}\StringTok{"bird\_weight"}\NormalTok{]}
    
    \CommentTok{\# use bootstrapped sample to fit a model}
\NormalTok{    bootstrap\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model.fit(X\_bootstrap, Y\_bootstrap)}
\NormalTok{    bootstrap\_thetas }\OperatorTok{=}\NormalTok{ bootstrap\_model.coef\_}
    
    \CommentTok{\# record the result for theta\_1}
\NormalTok{    estimates.append(bootstrap\_thetas[}\DecValTok{0}\NormalTok{])}
    
\CommentTok{\# calculate the 95\% confidence interval }
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.percentile(estimates, }\FloatTok{2.5}\NormalTok{, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.percentile(estimates, }\FloatTok{97.5}\NormalTok{, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{conf\_interval }\OperatorTok{=}\NormalTok{ (lower, upper)}
\NormalTok{conf\_interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.25864811956848754, 1.1034243854204049)
\end{verbatim}

Our bootstrapped 95\% confidence interval for \(\theta_1\) is
\([-0.259, 1.103]\). Immediately, we can see that 0 \emph{is} indeed
contained in this interval -- this means that we \emph{cannot} conclude
that \(\theta_1\) is non-zero! More formally, we fail to reject the null
hypothesis (that \(\theta_1\) is 0) under a 5\% p-value cutoff.

We can repeat this process to construct 95\% confidence intervals for
the other parameters of the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\NormalTok{theta\_0\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_1\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_2\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_3\_estimates }\OperatorTok{=}\NormalTok{ []}


\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
\NormalTok{    bootstrap\_resample }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[}\StringTok{"bird\_weight"}\NormalTok{]}
    
\NormalTok{    bootstrap\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model.fit(X\_bootstrap, Y\_bootstrap)}
\NormalTok{    bootstrap\_theta\_0 }\OperatorTok{=}\NormalTok{ bootstrap\_model.intercept\_}
\NormalTok{    bootstrap\_theta\_1, bootstrap\_theta\_2, bootstrap\_theta\_3 }\OperatorTok{=}\NormalTok{ bootstrap\_model.coef\_}
    
\NormalTok{    theta\_0\_estimates.append(bootstrap\_theta\_0)}
\NormalTok{    theta\_1\_estimates.append(bootstrap\_theta\_1)}
\NormalTok{    theta\_2\_estimates.append(bootstrap\_theta\_2)}
\NormalTok{    theta\_3\_estimates.append(bootstrap\_theta\_3)}
    
\NormalTok{theta\_0\_lower, theta\_0\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_0\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_0\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_1\_lower, theta\_1\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_1\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_1\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_2\_lower, theta\_2\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_2\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_2\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_3\_lower, theta\_3\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_3\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_3\_estimates, }\FloatTok{97.5}\NormalTok{)}

\CommentTok{\# Make a nice table to view results}
\NormalTok{pd.DataFrame(\{}\StringTok{"lower"}\NormalTok{:[theta\_0\_lower, theta\_1\_lower, theta\_2\_lower, theta\_3\_lower], }\StringTok{"upper"}\NormalTok{:[theta\_0\_upper, }\OperatorTok{\textbackslash{}}
\NormalTok{                theta\_1\_upper, theta\_2\_upper, theta\_3\_upper]\}, index}\OperatorTok{=}\NormalTok{[}\StringTok{"theta\_0"}\NormalTok{, }\StringTok{"theta\_1"}\NormalTok{, }\StringTok{"theta\_2"}\NormalTok{, }\StringTok{"theta\_3"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lrr}
\toprule
{} &      lower &     upper \\
\midrule
theta\_0 & -15.278542 &  5.161473 \\
theta\_1 &  -0.258648 &  1.103424 \\
theta\_2 &  -0.099138 &  0.208557 \\
theta\_3 &  -0.257141 &  0.758155 \\
\bottomrule
\end{tabular}

Something's off here. Notice that 0 is included in the 95\% confidence
interval for \emph{every} parameter of the model. Using the
interpretation we outlined above, this would suggest that we can't say
for certain that \emph{any} of the input variables impact the response
variable! This makes it seem like our model can't make any predictions
-- and yet, each model we fit in our bootstrap experiment above could
very much make predictions of \(Y\).

How can we explain this result? Think back to how we first interpreted
the parameters of a linear model. We treated each \(\theta_i\) as a
slope, where a unit increase in \(x_i\) leads to a \(\theta_i\) increase
in \(Y\), \textbf{if all other variables are held constant}. It turns
out that this last assumption is very important. If variables in our
model are somehow related to one another, then it might not be possible
to have a change in one of them while holding the others constant. This
means that our interpretation framework is no longer valid! In the
models we fit above, we incorporated \texttt{egg\_length},
\texttt{egg\_breadth}, and \texttt{egg\_weight} as input variables.
These variables are very likely related to one another -- an egg with
large \texttt{egg\_length} and \texttt{egg\_breadth} will likely be
heavy in \texttt{egg\_weight}. This means that the model parameters
cannot be meaningfully interpreted as slopes.

To support this conclusion, we can visualize the relationships between
our feature variables. Notice the strong positive association between
the features.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.pairplot(eggs[[}\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{, }\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{\textquotesingle{}bird\_weight\textquotesingle{}}\NormalTok{]])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/seaborn/axisgrid.py:118: UserWarning:

The figure layout has changed to tight
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{inference_causality/inference_causality_files/figure-pdf/cell-6-output-2.pdf}

}

\end{figure}

This issue is known as \textbf{collinearity}, sometimes also called
\textbf{multicollinearity}. Collinearity occurs when one feature can be
predicted fairly accurately by a linear combination of the other
features, which happens when one feature is highly correlated with the
others.

Why is collinearity a problem? Its consequences span several aspects of
the modeling process:

\begin{itemize}
\tightlist
\item
  \textbf{Inference}: Slopes can't be interpreted for an inference task.
\item
  \textbf{Model Variance}: If features strongly influence one another,
  even small changes in the sampled data can lead to large changes in
  the estimated slopes.
\item
  \textbf{Unique Solution}: If one feature is a linear combination of
  the other features, the design matrix will not be full rank, and
  \(\mathbb{X}^{\top}\mathbb{X}\) is not invertible. This means that
  least squares does not have a unique solution. See
  \href{https://ds100.org/course-notes/ols/ols.html\#bonus-uniqueness-of-the-solution}{this
  section} of Course Note 12 for more on this.
\end{itemize}

The take-home point is that we need to be careful with what features we
select for modeling. If two features likely encode similar information,
it is often a good idea to choose only one of them as an input variable.

\hypertarget{a-simpler-model}{%
\subsection{A Simpler Model}\label{a-simpler-model}}

Let us now consider a more interpretable model: we instead assume a true
relationship using only egg weight:

\[f_\theta(x) = \theta_0 + \theta_1 \text{egg\_weight} + \epsilon\]

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{X\_int }\OperatorTok{=}\NormalTok{ eggs[[}\StringTok{"egg\_weight"}\NormalTok{]]}
\NormalTok{Y\_int }\OperatorTok{=}\NormalTok{ eggs[}\StringTok{"bird\_weight"}\NormalTok{]}

\NormalTok{model\_int }\OperatorTok{=}\NormalTok{ LinearRegression()}

\NormalTok{model\_int.fit(X\_int, Y\_int)}

\CommentTok{\# This gives an array containing the fitted model parameter estimates}
\NormalTok{thetas\_int }\OperatorTok{=}\NormalTok{ model\_int.coef\_}

\CommentTok{\# Put the parameter estimates in a nice table for viewing}
\NormalTok{pd.DataFrame(\{}\StringTok{"theta\_hat"}\NormalTok{:[model\_int.intercept\_, thetas\_int[}\DecValTok{0}\NormalTok{]]\}, index}\OperatorTok{=}\NormalTok{[}\StringTok{"theta\_0"}\NormalTok{, }\StringTok{"theta\_1"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
/Users/Ishani/micromamba/lib/python3.9/site-packages/IPython/core/formatters.py:342: FutureWarning:

In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.
\end{verbatim}

\begin{tabular}{lr}
\toprule
{} &  theta\_hat \\
\midrule
theta\_0 &  -0.058272 \\
theta\_1 &   0.718515 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\CommentTok{\# Set a random seed so you generate the same random sample as staff}
\CommentTok{\# In the "real world", we wouldn\textquotesingle{}t do this}
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\CommentTok{\# Set the sample size of each bootstrap sample}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(eggs)}

\CommentTok{\# Create a list to store all the bootstrapped estimates}
\NormalTok{estimates\_int }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Generate a bootstrap resample from \textasciigrave{}eggs\textasciigrave{} and find an estimate for theta\_1 using this sample. }
\CommentTok{\# Repeat 10000 times.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
\NormalTok{    bootstrap\_resample\_int }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap\_int }\OperatorTok{=}\NormalTok{ bootstrap\_resample\_int[[}\StringTok{"egg\_weight"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap\_int }\OperatorTok{=}\NormalTok{ bootstrap\_resample\_int[}\StringTok{"bird\_weight"}\NormalTok{]}
    
\NormalTok{    bootstrap\_model\_int }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model\_int.fit(X\_bootstrap\_int, Y\_bootstrap\_int)}
\NormalTok{    bootstrap\_thetas\_int }\OperatorTok{=}\NormalTok{ bootstrap\_model\_int.coef\_}
    
\NormalTok{    estimates\_int.append(bootstrap\_thetas\_int[}\DecValTok{0}\NormalTok{])}

\NormalTok{plt.figure(dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{sns.histplot(estimates\_int, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}hat\{\textbackslash{}theta\}\_1$"}\NormalTok{)}
\NormalTok{plt.title(}\VerbatimStringTok{r"Bootstrapped estimates $\textbackslash{}hat\{\textbackslash{}theta\}\_1$ Under the Interpretable Model"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{inference_causality/inference_causality_files/figure-pdf/cell-8-output-1.pdf}

}

\end{figure}

Notice how the interpretable model performs almost as well as our other
model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}

\NormalTok{rmse }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y, model.predict(X))}
\NormalTok{rmse\_int }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y\_int, model\_int.predict(X\_int))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}RMSE of Original Model: }\SpecialCharTok{\{}\NormalTok{rmse}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}RMSE of Interpretable Model: }\SpecialCharTok{\{}\NormalTok{rmse\_int}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RMSE of Original Model: 0.04547085380275766
RMSE of Interpretable Model: 0.046493941375556846
\end{verbatim}

Yet, the confidence interval for the true parameter \(\theta_{1}\) does
not contain zero.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower\_int }\OperatorTok{=}\NormalTok{ np.percentile(estimates\_int, }\FloatTok{2.5}\NormalTok{)}
\NormalTok{upper\_int }\OperatorTok{=}\NormalTok{ np.percentile(estimates\_int, }\FloatTok{97.5}\NormalTok{)}

\NormalTok{conf\_interval\_int }\OperatorTok{=}\NormalTok{ (lower\_int, upper\_int)}
\NormalTok{conf\_interval\_int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.6029335250209633, 0.8208401738546206)
\end{verbatim}

In retrospect, it's no surprise that the weight of an egg best predicts
the weight of a newly-hatched chick.

A model with highly correlated variables prevents us from interpreting
how the variables are related to the prediction.

\hypertarget{reminder-assumptions-matter}{%
\subsection{Reminder: Assumptions
Matter}\label{reminder-assumptions-matter}}

Keep the following in mind: All inference assumes that the regression
model holds.

\begin{itemize}
\tightlist
\item
  If the model doesn't hold, the inference might not be valid.
\item
  If the
  \href{https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html?highlight=p\%20value\%20confidence\%20interval\#care-in-using-the-bootstrap-percentile-method}{assumptions
  of the bootstrap} don't hold\ldots{}

  \begin{itemize}
  \tightlist
  \item
    Sample size n is large
  \item
    Sample is representative of population distribution (drawn i.i.d.,
    unbiased)
  \end{itemize}

  \ldots then the results of the bootstrap might not be valid.
\end{itemize}

\hypertarget{bonus-content}{%
\section{{[}Bonus Content{]}}\label{bonus-content}}

Note: the content in this section is out of scope.

\hypertarget{prediction-vs-causation}{%
\subsection{Prediction vs Causation}\label{prediction-vs-causation}}

The difference between correlation/prediction vs.~causation is best
illustrated through examples.

Some questions about \textbf{correlation / prediction} include:

\begin{itemize}
\tightlist
\item
  Are homes with granite countertops worth more money?
\item
  Is college GPA higher for students who win a certain scholarship?
\item
  Are breastfed babies less likely to develop asthma?
\item
  Do cancer patients given some aggressive treatment have a higher
  5-year survival rate?
\item
  Are people who smoke more likely to get cancer?
\end{itemize}

While these may sound like causal questions, they are not! Questions
about \textbf{causality} are about the \textbf{effects} of
\textbf{interventions} (not just passive observation). For example:

\begin{itemize}
\tightlist
\item
  How much do granite countertops \textbf{raise} the value of a house?
\item
  Does getting the scholarship \textbf{improve} students' GPAs?
\item
  Does breastfeeding \textbf{protect} babies against asthma?
\item
  Does the treatment \textbf{improve} cancer survival?
\item
  Does smoking \textbf{cause} cancer?
\end{itemize}

Note, however, that regression coefficients are sometimes called
``effects'', which can be deceptive!

When using data alone, \textbf{predictive questions} (i.e., are
breastfed babies healthier?) can be answered, but \textbf{causal
questions} (i.e., does breastfeeding improve babies' health?) cannot.
The reason for this is that there are many possible causes for our
predictive question. For example, possible explanations for why
breastfed babies are healthier on average include:

\begin{itemize}
\tightlist
\item
  \textbf{Causal effect:} breastfeeding makes babies healthier
\item
  \textbf{Reverse causality:} healthier babies more likely to
  successfully breastfeed
\item
  \textbf{Common cause:} healthier / richer parents have healthier
  babies and are more likely to breastfeed
\end{itemize}

We cannot tell which explanations are true (or to what extent) just by
observing (\(x\),\(y\)) pairs. Additionally, causal questions implicitly
involve \textbf{counterfactuals}, events that didn't happen. For
example, we could ask, \textbf{would} the \textbf{same} breastfed babies
have been less healthy \textbf{if} they hadn't been breastfed?
Explanation 1 from above implies they would be, but explanations 2 and 3
do not.

\hypertarget{confounders}{%
\subsection{Confounders}\label{confounders}}

Let T represent a treatment (for example, alcohol use) and Y represent
an outcome (for example, lung cancer).

A \textbf{confounder} is a variable that affects both T and Y,
distorting the correlation between them. Using the example above, rich
parents could be a confounder for breastfeeding and a baby's health.
Confounders can be a measured covariate (a feature) or an unmeasured
variable we don't know about, and they generally cause problems, as the
relationship between T and Y is affected by data we cannot see. We
commonly \emph{assume that all confounders are observed} (this is also
called \textbf{ignorability}).

\hypertarget{how-to-perform-causal-inference}{%
\subsection{How to perform causal
inference?}\label{how-to-perform-causal-inference}}

In a \textbf{randomized experiment}, participants are randomly assigned
into two groups: treatment and control. A treatment is applied
\emph{only} to the treatment group. We assume ignorability and gather as
many measurements as possible so that we can compare them between the
control and treatment groups to determine whether or not the treatment
has a true effect or is just a confounding factor.

However, often, randomly assigning treatments is impractical or
unethical. For example, assigning a treatment of cigarettes to test the
effect of smoking on the lungs would not only be impractical but also
unethical.

An alternative to bypass this issue is to utilize \textbf{observational
studies}. This can be done by obtaining two participant groups separated
based on some identified treatment variable. Unlike randomized
experiments, however, we cannot assume ignorability here: the
participants could have separated into two groups based on other
covariates! In addition, there could also be unmeasured confounders.



\end{document}
