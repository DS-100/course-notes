% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Principles and Techniques of Data Science},
  pdfauthor={Kanu Grover; Bella Crouch},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Principles and Techniques of Data Science}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Data 100}
\author{Kanu Grover \and Bella Crouch}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, enhanced, sharp corners, frame hidden, boxrule=0pt, breakable, borderline west={3pt}{0pt}{shadecolor}]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

\hypertarget{about-the-course-notes}{%
\section*{About the Course Notes}\label{about-the-course-notes}}
\addcontentsline{toc}{section}{About the Course Notes}

\markright{About the Course Notes}

This text was developed for the Spring 2023 Edition of the UC Berkeley
course Data 100: Principles and Techniques of Data Science.

As this project is in development during the Spring 2023 semester, the
course notes may be in flux. We appreciate your understanding. If you
spot any errors or would like to suggest any changes, please email us.
\textbf{Email}: data100.instructors@berkeley.edu

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Understand the stages of the data science lifecycle.
\end{itemize}

\end{tcolorbox}

Data science is an interdisciplinary field with a variety of
applications. The field is rapidly evolving; many of the key technical
underpinnings in modern-day data science have been popularized during
the early 21\textsuperscript{st} century.

A true mastery of data science requires a deep theoretical understanding
and strong grasp of domain expertise. This course will help you build on
the former -- specifically, the foundation of your technical knowledge.
To do so, we've organized concepts in Data 100 around the \textbf{data
science lifecycle}: an iterative process that encompasses the various
statistical and computational building blocks of data science.

\hypertarget{data-science-lifecycle}{%
\section{Data Science Lifecycle}\label{data-science-lifecycle}}

The data science lifecycle is a high-level overview of the data science
workflow. It's a cycle of stages that a data scientist should explore as
they conduct a thorough analysis of a data-driven problem.

There are many variations of the key ideas present in the data science
lifecycle. In Data 100, we visualize the stages of the lifecycle using a
flow diagram. Notice how there are two entry points.

\hypertarget{ask-a-question}{%
\subsection{Ask a Question}\label{ask-a-question}}

Whether by curiosity or necessity, data scientists will constantly ask
questions. For example, in the business world, data scientists may be
interested in predicting the profit generated by a certain investment.
In the field of medicine, they may ask whether some patients are more
likely than others to benefit from a treatment.

Posing questions is one of the primary ways the data science lifecycle
begins. It helps to fully define the question. Here are some things you
should ask yourself before framing a question.

\begin{itemize}
\tightlist
\item
  What do we want to know?

  \begin{itemize}
  \tightlist
  \item
    A question that is too ambiguous may lead to confusion.
  \end{itemize}
\item
  What problems are we trying to solve?

  \begin{itemize}
  \tightlist
  \item
    The goal of asking a question should be clear in order to justify
    your efforts to stakeholders.
  \end{itemize}
\item
  What are the hypotheses we want to test?

  \begin{itemize}
  \tightlist
  \item
    This gives a clear perspective from which to analyze final results.
  \end{itemize}
\item
  What are the metrics for our success?

  \begin{itemize}
  \tightlist
  \item
    This gives a clear point to know when to finish the project.
  \end{itemize}
\end{itemize}

\hypertarget{obtain-data}{%
\subsection{Obtain Data}\label{obtain-data}}

The second entry point to the lifecycle is by obtaining data. A careful
analysis of any problem requires the use of data. Data may be readily
available to us, or we may have to embark on a process to collect it.
When doing so, its crucial to ask the following:

\begin{itemize}
\tightlist
\item
  What data do we have and what data do we need?

  \begin{itemize}
  \tightlist
  \item
    Define the units of the data (people, cities, points in time, etc.)
    and what features to measure.
  \end{itemize}
\item
  How will we sample more data?

  \begin{itemize}
  \tightlist
  \item
    Scrape the web, collect manually, etc.
  \end{itemize}
\item
  Is our data representative of the population we want to study?

  \begin{itemize}
  \tightlist
  \item
    If our data is not representative of our population of interest,
    then we can come to incorrect conclusions.
  \end{itemize}
\end{itemize}

Key procedures: \emph{data acquisition}, \emph{data cleaning}

\hypertarget{understand-the-data}{%
\subsection{Understand the Data}\label{understand-the-data}}

Raw data itself is not inherently useful. It's impossible to discern all
the patterns and relationships between variables without carefully
investigating them. Therefore, translating pure data to actionable
insights is a key job of a data scientist. For example, we may choose to
ask:

\begin{itemize}
\tightlist
\item
  How is our data organized and what does it contain?

  \begin{itemize}
  \tightlist
  \item
    Knowing what the data says about the world helps us better
    understand the world.
  \end{itemize}
\item
  Do we have relevant data?

  \begin{itemize}
  \tightlist
  \item
    If the data we have collected is not useful to the question at hand,
    then we must collected more data.
  \end{itemize}
\item
  What are the biases, anomalies, or other issues with the data?

  \begin{itemize}
  \tightlist
  \item
    These can lead to many false conclusions if ignored, so data
    scientists must always be aware of these issues.
  \end{itemize}
\item
  How do we transform the data to enable effective analysis?

  \begin{itemize}
  \tightlist
  \item
    Data is not always easy to interpret at first glance, so a data
    scientist should reveal these hidden insights.
  \end{itemize}
\end{itemize}

Key procedures: \emph{exploratory data analysis}, \emph{data
visualization}.

\hypertarget{understand-the-world}{%
\subsection{Understand the World}\label{understand-the-world}}

After observing the patterns in our data, we can begin answering our
question. This may require that we predict a quantity (machine
learning), or measure the effect of some treatment (inference).

From here, we may choose to report our results, or possibly conduct more
analysis. We may not be satisfied by our findings, or our initial
exploration may have brought up new questions that require a new data.

\begin{itemize}
\tightlist
\item
  What does the data say about the world?

  \begin{itemize}
  \tightlist
  \item
    Given our models, the data will lead us to certain conclusions about
    the real world.\\
  \end{itemize}
\item
  Does it answer our questions or accurately solve the problem?

  \begin{itemize}
  \tightlist
  \item
    If our model and data can not accomplish our goals, then we must
    reform our question, model, or both.\\
  \end{itemize}
\item
  How robust are our conclusions and can we trust the predictions?

  \begin{itemize}
  \tightlist
  \item
    Inaccurate models can lead to untrue conclusions.
  \end{itemize}
\end{itemize}

Key procedures: \emph{model creation}, \emph{prediction},
\emph{inference}.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

The data science lifecycle is meant to be a set of general guidelines
rather than a hard list of requirements. In our journey exploring the
lifecycle, we'll cover both the underlying theory and technologies used
in data science, and we hope you'll build an appreciation for the field.

With that, let's begin by introducing one of the most important tools in
exploratory data analysis: \texttt{pandas}.

\bookmarksetup{startatroot}

\hypertarget{pandas-i}{%
\chapter{Pandas I}\label{pandas-i}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Build familiarity with basic \texttt{pandas} syntax
\item
  Learn the methods of selecting and filtering data from a DataFrame.
\item
  Understand the differences between DataFrames and Series
\end{itemize}

\end{tcolorbox}

Data scientists work with data stored in a variety of formats. The
primary focus of this class is in understanding tabular data -- one of
the most widely used formats in data science. This note introduces
DataFrames, which are among the most popular representations of tabular
data. We'll also introduce \texttt{pandas}, the standard Python package
for manipulating data in DataFrames.

\hypertarget{introduction-to-exploratory-data-analysis}{%
\section{Introduction to Exploratory Data
Analysis}\label{introduction-to-exploratory-data-analysis}}

Imagine you collected, or have been given a box of data. What do you do
next?

The first step is to clean your data. \textbf{Data cleaning} often
corrects issues in the structure and formatting of data, including
missing values and unit conversions.

Data scientists have coined the term \textbf{exploratory data analysis
(EDA)} to describe the process of transforming raw data to insightful
observations. EDA is an \emph{open-ended} analysis of transforming,
visualizing, and summarizing patterns in data. In order to conduct EDA,
we first need to familiarize ourselves with \texttt{pandas} -- an
important programming tool.

\hypertarget{introduction-to-pandas}{%
\section{Introduction to Pandas}\label{introduction-to-pandas}}

\texttt{pandas} is a data analysis library to make data cleaning and
analysis fast and convenient in Python.

The \texttt{pandas} library adopts many coding idioms from
\texttt{NumPy}. The biggest difference is that \texttt{pandas} is
designed for working with tabular data, one of the most common data
formats (and the focus of Data 100).

Before writing any code, we must import \texttt{pandas} into our Python
environment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# \textasciigrave{}pd\textasciigrave{} is the conventional alias for Pandas, as \textasciigrave{}np\textasciigrave{} is for NumPy}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\end{Highlighting}
\end{Shaded}

\hypertarget{series-dataframes-and-indices}{%
\section{Series, DataFrames, and
Indices}\label{series-dataframes-and-indices}}

There are three fundamental data structures in \texttt{pandas}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Series}: 1D labeled array data; best thought of as columnar
  data
\item
  \textbf{DataFrame}: 2D tabular data with rows and columns
\item
  \textbf{Index}: A sequence of row/column labels
\end{enumerate}

DataFrames, Series, and Indices can be represented visually in the
following diagram.

\includegraphics{pandas_1/images/df_series_index.png}

Notice how the \textbf{DataFrame} is a two dimensional object -- it
contains both rows and columns. The \textbf{Series} above is a singular
column of this DataFrame, namely the \texttt{Candidate} column. Both
contain an \textbf{Index}, or a shared list of row labels (the integers
from 0 to 5, inclusive).

\hypertarget{series}{%
\subsection{Series}\label{series}}

A Series represents a column of a DataFrame; more generally, it can be
any 1-dimensional array-like object containing values of the same type
with associated data labels, called its index.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{s }\OperatorTok{=}\NormalTok{ pd.Series([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0    -1
1    10
2     2
dtype: int64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.array }\CommentTok{\# Data contained within the Series}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<PandasArray>
[-1, 10, 2]
Length: 3, dtype: int64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index }\CommentTok{\# The Index of the Series}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RangeIndex(start=0, stop=3, step=1)
\end{verbatim}

By default, row indices in \texttt{pandas} are a sequential list of
integers beginning from 0. Optionally, a list of desired indices can be
passed to the \texttt{index} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OperatorTok{=}\NormalTok{ pd.Series([}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{2}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
a    -1
b    10
c     2
dtype: int64
\end{verbatim}

Indices can also be changed after initialization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s.index }\OperatorTok{=}\NormalTok{ [}\StringTok{"first"}\NormalTok{, }\StringTok{"second"}\NormalTok{, }\StringTok{"third"}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
first     -1
second    10
third      2
dtype: int64
\end{verbatim}

\hypertarget{selection-in-series}{%
\subsubsection{Selection in Series}\label{selection-in-series}}

Similar to an array, we can select a single value or a set of values
from a Series. There are 3 primary methods of selecting data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A single index label
\item
  A list of index labels
\item
  A filtering condition
\end{enumerate}

Let's define the following Series \texttt{ser}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser }\OperatorTok{=}\NormalTok{ pd.Series([}\DecValTok{4}\NormalTok{, }\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{6}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"a"}\NormalTok{, }\StringTok{"b"}\NormalTok{, }\StringTok{"c"}\NormalTok{, }\StringTok{"d"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(ser)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
a    4
b   -2
c    0
d    6
dtype: int64
\end{verbatim}

\hypertarget{a-single-index-label}{%
\paragraph{A Single Index Label}\label{a-single-index-label}}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(ser[}\StringTok{"a"}\NormalTok{]) }\CommentTok{\# Notice how the return value is a single array element}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4
\end{verbatim}

\hypertarget{a-list-of-index-labels}{%
\paragraph{A List of Index Labels}\label{a-list-of-index-labels}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser[[}\StringTok{"a"}\NormalTok{, }\StringTok{"c"}\NormalTok{]] }\CommentTok{\# Notice how the return value is another Series}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  0 \\
\midrule
a &  4 \\
c &  0 \\
\bottomrule
\end{tabular}

\hypertarget{a-filtering-condition}{%
\paragraph{A Filtering Condition}\label{a-filtering-condition}}

Perhaps the most interesting (and useful) method of selecting data from
a Series is with a filtering condition.

We first must apply a vectorized boolean operation to our Series that
encodes the filter conditon.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser }\OperatorTok{\textgreater{}} \DecValTok{0} \CommentTok{\# Filter condition: select all elements greater than 0}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &      0 \\
\midrule
a &   True \\
b &  False \\
c &  False \\
d &   True \\
\bottomrule
\end{tabular}

Upon ``indexing'' in our Series with this condition, \texttt{pandas}
selects only the rows with \texttt{True} values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ser[ser }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  0 \\
\midrule
a &  4 \\
d &  6 \\
\bottomrule
\end{tabular}

\hypertarget{dataframes}{%
\subsection{DataFrames}\label{dataframes}}

In Data 8, you encountered the \texttt{Table} class of the
\texttt{datascience} library, which represented tabular data. In Data
100, we'll be using the \texttt{DataFrame} class of the \texttt{pandas}
library.

Here is an example of a DataFrame that contains election data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{)}
\NormalTok{elections}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &               Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0   &  1824 &          Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1   &  1824 &       John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2   &  1828 &          Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3   &  1828 &       John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4   &  1832 &          Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
5   &  1832 &              Henry Clay &    National Republican &        484205 &   loss &  37.603628 \\
6   &  1832 &            William Wirt &           Anti-Masonic &        100715 &   loss &   7.821583 \\
7   &  1836 &       Hugh Lawson White &                   Whig &        146109 &   loss &  10.005985 \\
8   &  1836 &        Martin Van Buren &             Democratic &        763291 &    win &  52.272472 \\
9   &  1836 &  William Henry Harrison &                   Whig &        550816 &   loss &  37.721543 \\
10  &  1840 &        Martin Van Buren &             Democratic &       1128854 &   loss &  46.948787 \\
11  &  1840 &  William Henry Harrison &                   Whig &       1275583 &    win &  53.051213 \\
12  &  1844 &              Henry Clay &                   Whig &       1300004 &   loss &  49.250523 \\
13  &  1844 &              James Polk &             Democratic &       1339570 &    win &  50.749477 \\
14  &  1848 &              Lewis Cass &             Democratic &       1223460 &   loss &  42.552229 \\
15  &  1848 &        Martin Van Buren &              Free Soil &        291501 &   loss &  10.138474 \\
16  &  1848 &          Zachary Taylor &                   Whig &       1360235 &    win &  47.309296 \\
17  &  1852 &         Franklin Pierce &             Democratic &       1605943 &    win &  51.013168 \\
18  &  1852 &            John P. Hale &              Free Soil &        155210 &   loss &   4.930283 \\
19  &  1852 &          Winfield Scott &                   Whig &       1386942 &   loss &  44.056548 \\
20  &  1856 &          James Buchanan &             Democratic &       1835140 &    win &  45.306080 \\
21  &  1856 &         John C. Frémont &             Republican &       1342345 &   loss &  33.139919 \\
22  &  1856 &        Millard Fillmore &               American &        873053 &   loss &  21.554001 \\
23  &  1860 &         Abraham Lincoln &             Republican &       1855993 &    win &  39.699408 \\
24  &  1860 &               John Bell &   Constitutional Union &        590901 &   loss &  12.639283 \\
25  &  1860 &    John C. Breckinridge &    Southern Democratic &        848019 &   loss &  18.138998 \\
26  &  1860 &      Stephen A. Douglas &    Northern Democratic &       1380202 &   loss &  29.522311 \\
27  &  1864 &         Abraham Lincoln &         National Union &       2211317 &    win &  54.951512 \\
28  &  1864 &     George B. McClellan &             Democratic &       1812807 &   loss &  45.048488 \\
29  &  1868 &         Horatio Seymour &             Democratic &       2708744 &   loss &  47.334695 \\
30  &  1868 &           Ulysses Grant &             Republican &       3013790 &    win &  52.665305 \\
31  &  1872 &          Horace Greeley &     Liberal Republican &       2834761 &   loss &  44.071406 \\
32  &  1872 &           Ulysses Grant &             Republican &       3597439 &    win &  55.928594 \\
33  &  1876 &        Rutherford Hayes &             Republican &       4034142 &    win &  48.471624 \\
34  &  1876 &        Samuel J. Tilden &             Democratic &       4288546 &   loss &  51.528376 \\
35  &  1880 &         James B. Weaver &              Greenback &        308649 &   loss &   3.352344 \\
36  &  1880 &          James Garfield &             Republican &       4453337 &    win &  48.369234 \\
37  &  1880 &  Winfield Scott Hancock &             Democratic &       4444976 &   loss &  48.278422 \\
38  &  1884 &         Benjamin Butler &          Anti-Monopoly &        134294 &   loss &   1.335838 \\
39  &  1884 &        Grover Cleveland &             Democratic &       4914482 &    win &  48.884933 \\
40  &  1884 &         James G. Blaine &             Republican &       4856905 &   loss &  48.312208 \\
41  &  1884 &           John St. John &            Prohibition &        147482 &   loss &   1.467021 \\
42  &  1888 &          Alson Streeter &            Union Labor &        146602 &   loss &   1.288861 \\
43  &  1888 &       Benjamin Harrison &             Republican &       5443633 &    win &  47.858041 \\
44  &  1888 &         Clinton B. Fisk &            Prohibition &        249819 &   loss &   2.196299 \\
45  &  1888 &        Grover Cleveland &             Democratic &       5534488 &   loss &  48.656799 \\
46  &  1892 &       Benjamin Harrison &             Republican &       5176108 &   loss &  42.984101 \\
47  &  1892 &        Grover Cleveland &             Democratic &       5553898 &    win &  46.121393 \\
48  &  1892 &         James B. Weaver &               Populist &       1041028 &   loss &   8.645038 \\
49  &  1892 &            John Bidwell &            Prohibition &        270879 &   loss &   2.249468 \\
50  &  1896 &          John M. Palmer &    National Democratic &        134645 &   loss &   0.969566 \\
51  &  1896 &         Joshua Levering &            Prohibition &        131312 &   loss &   0.945565 \\
52  &  1896 &  William Jennings Bryan &             Democratic &       6509052 &   loss &  46.871053 \\
53  &  1896 &        William McKinley &             Republican &       7112138 &    win &  51.213817 \\
54  &  1900 &         John G. Woolley &            Prohibition &        210864 &   loss &   1.526821 \\
55  &  1900 &  William Jennings Bryan &             Democratic &       6370932 &   loss &  46.130540 \\
56  &  1900 &        William McKinley &             Republican &       7228864 &    win &  52.342640 \\
57  &  1904 &         Alton B. Parker &             Democratic &       5083880 &   loss &  37.685116 \\
58  &  1904 &          Eugene V. Debs &              Socialist &        402810 &   loss &   2.985897 \\
59  &  1904 &        Silas C. Swallow &            Prohibition &        259102 &   loss &   1.920637 \\
60  &  1904 &      Theodore Roosevelt &             Republican &       7630557 &    win &  56.562787 \\
61  &  1904 &        Thomas E. Watson &               Populist &        114070 &   loss &   0.845563 \\
62  &  1908 &          Eugene V. Debs &              Socialist &        420852 &   loss &   2.850866 \\
63  &  1908 &        Eugene W. Chafin &            Prohibition &        254087 &   loss &   1.721194 \\
64  &  1908 &  William Jennings Bryan &             Democratic &       6408979 &   loss &  43.414640 \\
65  &  1908 &            William Taft &             Republican &       7678335 &    win &  52.013300 \\
66  &  1912 &          Eugene V. Debs &              Socialist &        901551 &   loss &   6.004354 \\
67  &  1912 &        Eugene W. Chafin &            Prohibition &        208156 &   loss &   1.386325 \\
68  &  1912 &      Theodore Roosevelt &            Progressive &       4122721 &   loss &  27.457433 \\
69  &  1912 &            William Taft &             Republican &       3486242 &   loss &  23.218466 \\
70  &  1912 &          Woodrow Wilson &             Democratic &       6296284 &    win &  41.933422 \\
71  &  1916 &         Allan L. Benson &              Socialist &        590524 &   loss &   3.194193 \\
72  &  1916 &    Charles Evans Hughes &             Republican &       8548728 &   loss &  46.240779 \\
73  &  1916 &             Frank Hanly &            Prohibition &        221302 &   loss &   1.197041 \\
74  &  1916 &          Woodrow Wilson &             Democratic &       9126868 &    win &  49.367987 \\
75  &  1920 &        Aaron S. Watkins &            Prohibition &        188787 &   loss &   0.708351 \\
76  &  1920 &          Eugene V. Debs &              Socialist &        913693 &   loss &   3.428282 \\
77  &  1920 &            James M. Cox &             Democratic &       9139661 &   loss &  34.293063 \\
78  &  1920 &   Parley P. Christensen &           Farmer–Labor &        265398 &   loss &   0.995804 \\
79  &  1920 &          Warren Harding &             Republican &      16144093 &    win &  60.574501 \\
80  &  1924 &         Calvin Coolidge &             Republican &      15723789 &    win &  54.329113 \\
81  &  1924 &           John W. Davis &             Democratic &       8386242 &   loss &  28.976291 \\
82  &  1924 &      Robert La Follette &            Progressive &       4831706 &   loss &  16.694596 \\
83  &  1928 &                Al Smith &             Democratic &      15015464 &   loss &  40.902853 \\
84  &  1928 &          Herbert Hoover &             Republican &      21427123 &    win &  58.368524 \\
85  &  1928 &           Norman Thomas &              Socialist &        267478 &   loss &   0.728623 \\
86  &  1932 &      Franklin Roosevelt &             Democratic &      22821277 &    win &  57.672125 \\
87  &  1932 &          Herbert Hoover &             Republican &      15761254 &   loss &  39.830594 \\
88  &  1932 &           Norman Thomas &              Socialist &        884885 &   loss &   2.236211 \\
89  &  1932 &       William Z. Foster &              Communist &        103307 &   loss &   0.261069 \\
90  &  1936 &              Alf Landon &             Republican &      16679543 &   loss &  36.648285 \\
91  &  1936 &      Franklin Roosevelt &             Democratic &      27752648 &    win &  60.978107 \\
92  &  1936 &           Norman Thomas &              Socialist &        187910 &   loss &   0.412876 \\
93  &  1936 &           William Lemke &                  Union &        892378 &   loss &   1.960733 \\
94  &  1940 &      Franklin Roosevelt &             Democratic &      27313945 &    win &  54.871202 \\
95  &  1940 &           Norman Thomas &              Socialist &        116599 &   loss &   0.234237 \\
96  &  1940 &         Wendell Willkie &             Republican &      22347744 &   loss &  44.894561 \\
97  &  1944 &      Franklin Roosevelt &             Democratic &      25612916 &    win &  53.773801 \\
98  &  1944 &         Thomas E. Dewey &             Republican &      22017929 &   loss &  46.226199 \\
99  &  1948 &        Claude A. Watson &            Prohibition &        103708 &   loss &   0.212747 \\
100 &  1948 &            Harry Truman &             Democratic &      24179347 &    win &  49.601536 \\
101 &  1948 &        Henry A. Wallace &            Progressive &       1157328 &   loss &   2.374144 \\
102 &  1948 &           Norman Thomas &              Socialist &        139569 &   loss &   0.286312 \\
103 &  1948 &          Strom Thurmond &              Dixiecrat &       1175930 &   loss &   2.412304 \\
104 &  1948 &         Thomas E. Dewey &             Republican &      21991292 &   loss &  45.112958 \\
105 &  1952 &         Adlai Stevenson &             Democratic &      27375090 &   loss &  44.446312 \\
106 &  1952 &       Dwight Eisenhower &             Republican &      34075529 &    win &  55.325173 \\
107 &  1952 &        Vincent Hallinan &            Progressive &        140746 &   loss &   0.228516 \\
108 &  1956 &         Adlai Stevenson &             Democratic &      26028028 &   loss &  42.174464 \\
109 &  1956 &       Dwight Eisenhower &             Republican &      35579180 &    win &  57.650654 \\
110 &  1956 &      T. Coleman Andrews &         States' Rights &        107929 &   loss &   0.174883 \\
111 &  1960 &            John Kennedy &             Democratic &      34220984 &    win &  50.082561 \\
112 &  1960 &           Richard Nixon &             Republican &      34108157 &   loss &  49.917439 \\
113 &  1964 &         Barry Goldwater &             Republican &      27175754 &   loss &  38.655297 \\
114 &  1964 &          Lyndon Johnson &             Democratic &      43127041 &    win &  61.344703 \\
115 &  1968 &          George Wallace &   American Independent &       9901118 &   loss &  13.571218 \\
116 &  1968 &         Hubert Humphrey &             Democratic &      31271839 &   loss &  42.863537 \\
117 &  1968 &           Richard Nixon &             Republican &      31783783 &    win &  43.565246 \\
118 &  1972 &         George McGovern &             Democratic &      29173222 &   loss &  37.670670 \\
119 &  1972 &         John G. Schmitz &   American Independent &       1100868 &   loss &   1.421524 \\
120 &  1972 &           Richard Nixon &             Republican &      47168710 &    win &  60.907806 \\
121 &  1976 &         Eugene McCarthy &            Independent &        740460 &   loss &   0.911649 \\
122 &  1976 &             Gerald Ford &             Republican &      39148634 &   loss &  48.199499 \\
123 &  1976 &            Jimmy Carter &             Democratic &      40831881 &    win &  50.271900 \\
124 &  1976 &           Lester Maddox &   American Independent &        170274 &   loss &   0.209640 \\
125 &  1976 &          Roger MacBride &            Libertarian &        172557 &   loss &   0.212451 \\
126 &  1976 &      Thomas J. Anderson &               American &        158271 &   loss &   0.194862 \\
127 &  1980 &          Barry Commoner &               Citizens &        233052 &   loss &   0.270182 \\
128 &  1980 &                Ed Clark &            Libertarian &        921128 &   loss &   1.067883 \\
129 &  1980 &            Jimmy Carter &             Democratic &      35480115 &   loss &  41.132848 \\
130 &  1980 &        John B. Anderson &            Independent &       5719850 &   loss &   6.631143 \\
131 &  1980 &           Ronald Reagan &             Republican &      43903230 &    win &  50.897944 \\
132 &  1984 &          David Bergland &            Libertarian &        228111 &   loss &   0.247245 \\
133 &  1984 &           Ronald Reagan &             Republican &      54455472 &    win &  59.023326 \\
134 &  1984 &          Walter Mondale &             Democratic &      37577352 &   loss &  40.729429 \\
135 &  1988 &       George H. W. Bush &             Republican &      48886597 &    win &  53.518845 \\
136 &  1988 &           Lenora Fulani &           New Alliance &        217221 &   loss &   0.237804 \\
137 &  1988 &         Michael Dukakis &             Democratic &      41809074 &   loss &  45.770691 \\
138 &  1988 &                Ron Paul &            Libertarian &        431750 &   loss &   0.472660 \\
139 &  1992 &            Andre Marrou &            Libertarian &        290087 &   loss &   0.278516 \\
140 &  1992 &            Bill Clinton &             Democratic &      44909806 &    win &  43.118485 \\
141 &  1992 &                Bo Gritz &               Populist &        106152 &   loss &   0.101918 \\
142 &  1992 &       George H. W. Bush &             Republican &      39104550 &   loss &  37.544784 \\
143 &  1992 &              Ross Perot &            Independent &      19743821 &   loss &  18.956298 \\
144 &  1996 &            Bill Clinton &             Democratic &      47400125 &    win &  49.296938 \\
145 &  1996 &                Bob Dole &             Republican &      39197469 &   loss &  40.766036 \\
146 &  1996 &            Harry Browne &            Libertarian &        485759 &   loss &   0.505198 \\
147 &  1996 &         Howard Phillips &              Taxpayers &        184656 &   loss &   0.192045 \\
148 &  1996 &            John Hagelin &            Natural Law &        113670 &   loss &   0.118219 \\
149 &  1996 &             Ralph Nader &                  Green &        685297 &   loss &   0.712721 \\
150 &  1996 &              Ross Perot &                 Reform &       8085294 &   loss &   8.408844 \\
151 &  2000 &                 Al Gore &             Democratic &      50999897 &   loss &  48.491813 \\
152 &  2000 &          George W. Bush &             Republican &      50456002 &    win &  47.974666 \\
153 &  2000 &            Harry Browne &            Libertarian &        384431 &   loss &   0.365525 \\
154 &  2000 &            Pat Buchanan &                 Reform &        448895 &   loss &   0.426819 \\
155 &  2000 &             Ralph Nader &                  Green &       2882955 &   loss &   2.741176 \\
156 &  2004 &              David Cobb &                  Green &        119859 &   loss &   0.098088 \\
157 &  2004 &          George W. Bush &             Republican &      62040610 &    win &  50.771824 \\
158 &  2004 &              John Kerry &             Democratic &      59028444 &   loss &  48.306775 \\
159 &  2004 &        Michael Badnarik &            Libertarian &        397265 &   loss &   0.325108 \\
160 &  2004 &        Michael Peroutka &           Constitution &        143630 &   loss &   0.117542 \\
161 &  2004 &             Ralph Nader &            Independent &        465151 &   loss &   0.380663 \\
162 &  2008 &            Barack Obama &             Democratic &      69498516 &    win &  53.023510 \\
163 &  2008 &                Bob Barr &            Libertarian &        523715 &   loss &   0.399565 \\
164 &  2008 &           Chuck Baldwin &           Constitution &        199750 &   loss &   0.152398 \\
165 &  2008 &        Cynthia McKinney &                  Green &        161797 &   loss &   0.123442 \\
166 &  2008 &             John McCain &             Republican &      59948323 &   loss &  45.737243 \\
167 &  2008 &             Ralph Nader &            Independent &        739034 &   loss &   0.563842 \\
168 &  2012 &            Barack Obama &             Democratic &      65915795 &    win &  51.258484 \\
169 &  2012 &            Gary Johnson &            Libertarian &       1275971 &   loss &   0.992241 \\
170 &  2012 &              Jill Stein &                  Green &        469627 &   loss &   0.365199 \\
171 &  2012 &             Mitt Romney &             Republican &      60933504 &   loss &  47.384076 \\
172 &  2016 &          Darrell Castle &           Constitution &        203091 &   loss &   0.149640 \\
173 &  2016 &            Donald Trump &             Republican &      62984828 &    win &  46.407862 \\
174 &  2016 &           Evan McMullin &            Independent &        732273 &   loss &   0.539546 \\
175 &  2016 &            Gary Johnson &            Libertarian &       4489235 &   loss &   3.307714 \\
176 &  2016 &         Hillary Clinton &             Democratic &      65853514 &   loss &  48.521539 \\
177 &  2016 &              Jill Stein &                  Green &       1457226 &   loss &   1.073699 \\
178 &  2020 &            Joseph Biden &             Democratic &      81268924 &    win &  51.311515 \\
179 &  2020 &            Donald Trump &             Republican &      74216154 &   loss &  46.858542 \\
180 &  2020 &            Jo Jorgensen &            Libertarian &       1865724 &   loss &   1.177979 \\
181 &  2020 &          Howard Hawkins &                  Green &        405035 &   loss &   0.255731 \\
\bottomrule
\end{tabular}

Let's dissect the code above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We first import the \texttt{pandas} library into our Python
  environment, using the alias \texttt{pd}.
   \texttt{import\ pandas\ as\ pd}
\item
  There are a number of ways to read data into a DataFrame. In Data 100,
  our data are typically stored in a CSV (comma-seperated values) file
  format. We can import a CSV file into a DataFrame by passing the data
  path as an argument to the following \texttt{pandas} function.
   \texttt{pd.read\_csv("elections.csv")}
\end{enumerate}

This code stores our DataFrame object in the \texttt{elections}
variable. Upon inspection, our \texttt{elections} DataFrame has 182 rows
and 6 columns (\texttt{Year}, \texttt{Candidate}, \texttt{Party},
\texttt{Popular\ Vote}, \texttt{Result}, \texttt{\%}). Each row
represents a single record -- in our example, a presedential candidate
from some particular year. Each column represents a single attribute, or
feature of the record.

In the example above, we constructed a DataFrame object using data from
a CSV file. As we'll explore in the next section, we can create a
DataFrame with data of our own.

\hypertarget{creating-a-dataframe}{%
\subsubsection{Creating a DataFrame}\label{creating-a-dataframe}}

There are many ways to create a DataFrame. Here, we will cover the most
popular approaches.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using a list and column names
\item
  From a dictionary
\item
  From a Series
\end{enumerate}

\hypertarget{using-a-list-and-column-names}{%
\paragraph{Using a List and Column
Names}\label{using-a-list-and-column-names}}

Consider the following examples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_list }\OperatorTok{=}\NormalTok{ pd.DataFrame([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], columns}\OperatorTok{=}\NormalTok{[}\StringTok{"Numbers"}\NormalTok{])}
\NormalTok{df\_list}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Numbers \\
\midrule
0 &        1 \\
1 &        2 \\
2 &        3 \\
\bottomrule
\end{tabular}

The first code cell creates a DataFrame with a single column
\texttt{Numbers}, while the second creates a DataFrame with an
additional column \texttt{Description}. Notice how a 2D list of values
is required to initialize the second DataFrame -- each nested list
represents a single row of data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_list }\OperatorTok{=}\NormalTok{ pd.DataFrame([[}\DecValTok{1}\NormalTok{, }\StringTok{"one"}\NormalTok{], [}\DecValTok{2}\NormalTok{, }\StringTok{"two"}\NormalTok{]], columns }\OperatorTok{=}\NormalTok{ [}\StringTok{"Number"}\NormalTok{, }\StringTok{"Description"}\NormalTok{])}
\NormalTok{df\_list}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrl}
\toprule
{} &  Number & Description \\
\midrule
0 &       1 &         one \\
1 &       2 &         two \\
\bottomrule
\end{tabular}

\hypertarget{from-a-dictionary}{%
\paragraph{From a Dictionary}\label{from-a-dictionary}}

A second (and more common) way to create a DataFrame is with a
dictionary. The dictionary keys represent the column names, and the
dictionary values represent the column values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_dict }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"Fruit"}\NormalTok{: [}\StringTok{"Strawberry"}\NormalTok{, }\StringTok{"Orange"}\NormalTok{], }\StringTok{"Price"}\NormalTok{: [}\FloatTok{5.49}\NormalTok{, }\FloatTok{3.99}\NormalTok{]\})}
\NormalTok{df\_dict}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llr}
\toprule
{} &       Fruit &  Price \\
\midrule
0 &  Strawberry &   5.49 \\
1 &      Orange &   3.99 \\
\bottomrule
\end{tabular}

\hypertarget{from-a-series}{%
\paragraph{From a Series}\label{from-a-series}}

Earlier, we explained how a Series was synonymous to a column in a
DataFrame. It follows then, that a DataFrame is equivalent to a
collection of Series, which all share the same index.

In fact, we can initialize a DataFrame by merging two or more Series.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Notice how our indices, or row labels, are the same}

\NormalTok{s\_a }\OperatorTok{=}\NormalTok{ pd.Series([}\StringTok{"a1"}\NormalTok{, }\StringTok{"a2"}\NormalTok{, }\StringTok{"a3"}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"r1"}\NormalTok{, }\StringTok{"r2"}\NormalTok{, }\StringTok{"r3"}\NormalTok{])}
\NormalTok{s\_b }\OperatorTok{=}\NormalTok{ pd.Series([}\StringTok{"b1"}\NormalTok{, }\StringTok{"b2"}\NormalTok{, }\StringTok{"b3"}\NormalTok{], index }\OperatorTok{=}\NormalTok{ [}\StringTok{"r1"}\NormalTok{, }\StringTok{"r2"}\NormalTok{, }\StringTok{"r3"}\NormalTok{])}

\NormalTok{pd.DataFrame(\{}\StringTok{"A{-}column"}\NormalTok{: s\_a, }\StringTok{"B{-}column"}\NormalTok{: s\_b\})}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lll}
\toprule
{} & A-column & B-column \\
\midrule
r1 &       a1 &       b1 \\
r2 &       a2 &       b2 \\
r3 &       a3 &       b3 \\
\bottomrule
\end{tabular}

\hypertarget{indices}{%
\subsection{Indices}\label{indices}}

The major takeaway: we can think of a \textbf{DataFrame} as a collection
of \textbf{Series} that all share the same \textbf{Index}.

On a more technical note, an Index doesn't have to be an integer, nor
does it have to be unique. For example, we can set the index of the
\texttt{elections} Dataframe to be the name of presedential candidates.
Selecting a new Series from this modified DataFrame yields the
following.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This sets the index to the "Candidate" column}
\NormalTok{elections.set\_index(}\StringTok{"Candidate"}\NormalTok{, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{pandas_1/images/index_comparison_2.png}

To retrieve the indices of a DataFrame, simply use the \texttt{.index}
attribute of the DataFrame class.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.index}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Index(['Andrew Jackson', 'John Quincy Adams', 'Andrew Jackson',
       'John Quincy Adams', 'Andrew Jackson', 'Henry Clay', 'William Wirt',
       'Hugh Lawson White', 'Martin Van Buren', 'William Henry Harrison',
       ...
       'Darrell Castle', 'Donald Trump', 'Evan McMullin', 'Gary Johnson',
       'Hillary Clinton', 'Jill Stein', 'Joseph Biden', 'Donald Trump',
       'Jo Jorgensen', 'Howard Hawkins'],
      dtype='object', name='Candidate', length=182)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This resets the index to be the default list of integers}
\NormalTok{elections.reset\_index(inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\hypertarget{slicing-in-dataframes}{%
\section{Slicing in DataFrames}\label{slicing-in-dataframes}}

Now that we've learned how to create DataFrames, let's dive deeper into
their capabilities.

The API (application programming interface) for the DataFrame class is
enormous. In this section, we'll discuss several methods of the
DataFrame API that allow us to extract subsets of data.

The simplest way to manipulate a DataFrame is to extract a subset of
rows and columns, known as \textbf{slicing}. We will do so with three
primary methods of the DataFrame class:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.loc}
\item
  \texttt{.iloc}
\item
  \texttt{{[}{]}}
\end{enumerate}

\hypertarget{indexing-with-.loc}{%
\subsection{Indexing with .loc}\label{indexing-with-.loc}}

The \texttt{.loc} operator selects rows and columns in a DataFrame by
their row and column label(s), respectively. The \textbf{row labels}
(commonly referred to as the \textbf{indices}) are the bold text on the
far \emph{left} of a DataFrame, while the \textbf{column labels} are the
column names found at the \emph{top} of a DataFrame.

To grab data with \texttt{.loc}, we must specify the row and column
label(s) where the data exists. The row labels are the first argument to
the \texttt{.loc} function; the column labels are the second. For
example, we can select the the row labeled \texttt{0} and the column
labeled \texttt{Candidate} from the \texttt{elections} DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{, }\StringTok{\textquotesingle{}Candidate\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Andrew Jackson'
\end{verbatim}

To select \emph{multiple} rows and columns, we can use Python slice
notation. Here, we select both the first four rows and columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Popular vote\textquotesingle{}}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrlr}
\toprule
{} &  Year &                  Party &  Popular vote \\
\midrule
0 &  1824 &  Democratic-Republican &        151271 \\
1 &  1824 &  Democratic-Republican &        113142 \\
2 &  1828 &             Democratic &        642806 \\
3 &  1828 &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Suppose that instead, we wanted \emph{every} column value for the first
four rows in the \texttt{elections} DataFrame. The shorthand \texttt{:}
is useful for this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[}\DecValTok{0}\NormalTok{:}\DecValTok{3}\NormalTok{, :]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrlrlr}
\toprule
{} &          Candidate &  Year &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &     Andrew Jackson &  1824 &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  John Quincy Adams &  1824 &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &     Andrew Jackson &  1828 &             Democratic &        642806 &    win &  56.203927 \\
3 &  John Quincy Adams &  1828 &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

There are a couple of things we should note. Unlike conventional Python,
Pandas allows us to slice string values (in our example, the column
labels). Secondly, slicing with \texttt{.loc} is \emph{inclusive}.
Notice how our resulting DataFrame includes every row and column between
and including the slice labels we specified.

Equivalently, we can use a list to obtain multiple rows and columns in
our \texttt{elections} DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Candidate\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Popular vote\textquotesingle{}}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Lastly, we can interchange list and slicing notation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.loc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], :]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrlrlr}
\toprule
{} &          Candidate &  Year &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &     Andrew Jackson &  1824 &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  John Quincy Adams &  1824 &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &     Andrew Jackson &  1828 &             Democratic &        642806 &    win &  56.203927 \\
3 &  John Quincy Adams &  1828 &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

\hypertarget{indexing-with-.iloc}{%
\subsection{Indexing with .iloc}\label{indexing-with-.iloc}}

Slicing with \texttt{.iloc} works similarily to \texttt{.loc}, although
\texttt{.iloc} uses the integer positions of rows and columns rather the
labels. The arguments to the \texttt{.iloc} function also behave
similarly - single values, lists, indices, and any combination of these
are permitted.

Let's begin reproducing our results from above. We'll begin by selecting
for the first presedential candidate in our \texttt{elections}
DataFrame:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# elections.loc[0, "Candidate"] {-} Previous approach}
\NormalTok{elections.iloc[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1824
\end{verbatim}

Notice how the first argument to both \texttt{.loc} and \texttt{.iloc}
are the same. This is because the row with a label of 0 is conveniently
in the 0\textsuperscript{th} (or first) position of the
\texttt{elections} DataFrame. Generally, this is true of any DataFrame
where the row labels are incremented in ascending order from 0.

However, when we select the first four rows and columns using
\texttt{.iloc}, we notice something.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# elections.loc[0:3, \textquotesingle{}Year\textquotesingle{}:\textquotesingle{}Popular vote\textquotesingle{}] {-} Previous approach}
\NormalTok{elections.iloc[}\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrlr}
\toprule
{} &          Candidate &  Year &                  Party &  Popular vote \\
\midrule
0 &     Andrew Jackson &  1824 &  Democratic-Republican &        151271 \\
1 &  John Quincy Adams &  1824 &  Democratic-Republican &        113142 \\
2 &     Andrew Jackson &  1828 &             Democratic &        642806 \\
3 &  John Quincy Adams &  1828 &    National Republican &        500897 \\
\bottomrule
\end{tabular}

Slicing is no longer inclusive in \texttt{.iloc} - it's
\emph{exclusive}. This is one of Pandas syntatical subtleties; you'll
get used to with practice.

List behavior works just as expected.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#elections.loc[[0, 1, 2, 3], [\textquotesingle{}Year\textquotesingle{}, \textquotesingle{}Candidate\textquotesingle{}, \textquotesingle{}Party\textquotesingle{}, \textquotesingle{}Popular vote\textquotesingle{}]] {-} Previous Approach}
\NormalTok{elections.iloc[[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], [}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrlr}
\toprule
{} &          Candidate &  Year &                  Party &  Popular vote \\
\midrule
0 &     Andrew Jackson &  1824 &  Democratic-Republican &        151271 \\
1 &  John Quincy Adams &  1824 &  Democratic-Republican &        113142 \\
2 &     Andrew Jackson &  1828 &             Democratic &        642806 \\
3 &  John Quincy Adams &  1828 &    National Republican &        500897 \\
\bottomrule
\end{tabular}

This discussion begs the question: when should we use \texttt{.loc} vs
\texttt{.iloc}? In most cases, \texttt{.loc} is generally safer to use.
You can imagine \texttt{.iloc} may return incorrect values when applied
to a dataset where the ordering of data can change.

\hypertarget{indexing-with}{%
\subsection{Indexing with {[}{]}}\label{indexing-with}}

The \texttt{{[}{]}} selection operator is the most baffling of all, yet
the commonly used. It only takes a single argument, which may be one of
the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A slice of row numbers
\item
  A list of column labels
\item
  A single column label
\end{enumerate}

That is, \texttt{{[}{]}} is \emph{context dependent}. Let's see some
examples.

\hypertarget{a-slice-of-row-numbers}{%
\subsubsection{A slice of row numbers}\label{a-slice-of-row-numbers}}

Say we wanted the first four rows of our \texttt{elections} DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[}\DecValTok{0}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrlrlr}
\toprule
{} &          Candidate &  Year &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &     Andrew Jackson &  1824 &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  John Quincy Adams &  1824 &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &     Andrew Jackson &  1828 &             Democratic &        642806 &    win &  56.203927 \\
3 &  John Quincy Adams &  1828 &    National Republican &        500897 &   loss &  43.796073 \\
\bottomrule
\end{tabular}

\hypertarget{a-list-of-column-labels}{%
\subsubsection{A list of column labels}\label{a-list-of-column-labels}}

Suppose we now want the first four columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[[}\StringTok{"Year"}\NormalTok{, }\StringTok{"Candidate"}\NormalTok{, }\StringTok{"Party"}\NormalTok{, }\StringTok{"Popular vote"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllr}
\toprule
{} &  Year &               Candidate &                  Party &  Popular vote \\
\midrule
0   &  1824 &          Andrew Jackson &  Democratic-Republican &        151271 \\
1   &  1824 &       John Quincy Adams &  Democratic-Republican &        113142 \\
2   &  1828 &          Andrew Jackson &             Democratic &        642806 \\
3   &  1828 &       John Quincy Adams &    National Republican &        500897 \\
4   &  1832 &          Andrew Jackson &             Democratic &        702735 \\
5   &  1832 &              Henry Clay &    National Republican &        484205 \\
6   &  1832 &            William Wirt &           Anti-Masonic &        100715 \\
7   &  1836 &       Hugh Lawson White &                   Whig &        146109 \\
8   &  1836 &        Martin Van Buren &             Democratic &        763291 \\
9   &  1836 &  William Henry Harrison &                   Whig &        550816 \\
10  &  1840 &        Martin Van Buren &             Democratic &       1128854 \\
11  &  1840 &  William Henry Harrison &                   Whig &       1275583 \\
12  &  1844 &              Henry Clay &                   Whig &       1300004 \\
13  &  1844 &              James Polk &             Democratic &       1339570 \\
14  &  1848 &              Lewis Cass &             Democratic &       1223460 \\
15  &  1848 &        Martin Van Buren &              Free Soil &        291501 \\
16  &  1848 &          Zachary Taylor &                   Whig &       1360235 \\
17  &  1852 &         Franklin Pierce &             Democratic &       1605943 \\
18  &  1852 &            John P. Hale &              Free Soil &        155210 \\
19  &  1852 &          Winfield Scott &                   Whig &       1386942 \\
20  &  1856 &          James Buchanan &             Democratic &       1835140 \\
21  &  1856 &         John C. Frémont &             Republican &       1342345 \\
22  &  1856 &        Millard Fillmore &               American &        873053 \\
23  &  1860 &         Abraham Lincoln &             Republican &       1855993 \\
24  &  1860 &               John Bell &   Constitutional Union &        590901 \\
25  &  1860 &    John C. Breckinridge &    Southern Democratic &        848019 \\
26  &  1860 &      Stephen A. Douglas &    Northern Democratic &       1380202 \\
27  &  1864 &         Abraham Lincoln &         National Union &       2211317 \\
28  &  1864 &     George B. McClellan &             Democratic &       1812807 \\
29  &  1868 &         Horatio Seymour &             Democratic &       2708744 \\
30  &  1868 &           Ulysses Grant &             Republican &       3013790 \\
31  &  1872 &          Horace Greeley &     Liberal Republican &       2834761 \\
32  &  1872 &           Ulysses Grant &             Republican &       3597439 \\
33  &  1876 &        Rutherford Hayes &             Republican &       4034142 \\
34  &  1876 &        Samuel J. Tilden &             Democratic &       4288546 \\
35  &  1880 &         James B. Weaver &              Greenback &        308649 \\
36  &  1880 &          James Garfield &             Republican &       4453337 \\
37  &  1880 &  Winfield Scott Hancock &             Democratic &       4444976 \\
38  &  1884 &         Benjamin Butler &          Anti-Monopoly &        134294 \\
39  &  1884 &        Grover Cleveland &             Democratic &       4914482 \\
40  &  1884 &         James G. Blaine &             Republican &       4856905 \\
41  &  1884 &           John St. John &            Prohibition &        147482 \\
42  &  1888 &          Alson Streeter &            Union Labor &        146602 \\
43  &  1888 &       Benjamin Harrison &             Republican &       5443633 \\
44  &  1888 &         Clinton B. Fisk &            Prohibition &        249819 \\
45  &  1888 &        Grover Cleveland &             Democratic &       5534488 \\
46  &  1892 &       Benjamin Harrison &             Republican &       5176108 \\
47  &  1892 &        Grover Cleveland &             Democratic &       5553898 \\
48  &  1892 &         James B. Weaver &               Populist &       1041028 \\
49  &  1892 &            John Bidwell &            Prohibition &        270879 \\
50  &  1896 &          John M. Palmer &    National Democratic &        134645 \\
51  &  1896 &         Joshua Levering &            Prohibition &        131312 \\
52  &  1896 &  William Jennings Bryan &             Democratic &       6509052 \\
53  &  1896 &        William McKinley &             Republican &       7112138 \\
54  &  1900 &         John G. Woolley &            Prohibition &        210864 \\
55  &  1900 &  William Jennings Bryan &             Democratic &       6370932 \\
56  &  1900 &        William McKinley &             Republican &       7228864 \\
57  &  1904 &         Alton B. Parker &             Democratic &       5083880 \\
58  &  1904 &          Eugene V. Debs &              Socialist &        402810 \\
59  &  1904 &        Silas C. Swallow &            Prohibition &        259102 \\
60  &  1904 &      Theodore Roosevelt &             Republican &       7630557 \\
61  &  1904 &        Thomas E. Watson &               Populist &        114070 \\
62  &  1908 &          Eugene V. Debs &              Socialist &        420852 \\
63  &  1908 &        Eugene W. Chafin &            Prohibition &        254087 \\
64  &  1908 &  William Jennings Bryan &             Democratic &       6408979 \\
65  &  1908 &            William Taft &             Republican &       7678335 \\
66  &  1912 &          Eugene V. Debs &              Socialist &        901551 \\
67  &  1912 &        Eugene W. Chafin &            Prohibition &        208156 \\
68  &  1912 &      Theodore Roosevelt &            Progressive &       4122721 \\
69  &  1912 &            William Taft &             Republican &       3486242 \\
70  &  1912 &          Woodrow Wilson &             Democratic &       6296284 \\
71  &  1916 &         Allan L. Benson &              Socialist &        590524 \\
72  &  1916 &    Charles Evans Hughes &             Republican &       8548728 \\
73  &  1916 &             Frank Hanly &            Prohibition &        221302 \\
74  &  1916 &          Woodrow Wilson &             Democratic &       9126868 \\
75  &  1920 &        Aaron S. Watkins &            Prohibition &        188787 \\
76  &  1920 &          Eugene V. Debs &              Socialist &        913693 \\
77  &  1920 &            James M. Cox &             Democratic &       9139661 \\
78  &  1920 &   Parley P. Christensen &           Farmer–Labor &        265398 \\
79  &  1920 &          Warren Harding &             Republican &      16144093 \\
80  &  1924 &         Calvin Coolidge &             Republican &      15723789 \\
81  &  1924 &           John W. Davis &             Democratic &       8386242 \\
82  &  1924 &      Robert La Follette &            Progressive &       4831706 \\
83  &  1928 &                Al Smith &             Democratic &      15015464 \\
84  &  1928 &          Herbert Hoover &             Republican &      21427123 \\
85  &  1928 &           Norman Thomas &              Socialist &        267478 \\
86  &  1932 &      Franklin Roosevelt &             Democratic &      22821277 \\
87  &  1932 &          Herbert Hoover &             Republican &      15761254 \\
88  &  1932 &           Norman Thomas &              Socialist &        884885 \\
89  &  1932 &       William Z. Foster &              Communist &        103307 \\
90  &  1936 &              Alf Landon &             Republican &      16679543 \\
91  &  1936 &      Franklin Roosevelt &             Democratic &      27752648 \\
92  &  1936 &           Norman Thomas &              Socialist &        187910 \\
93  &  1936 &           William Lemke &                  Union &        892378 \\
94  &  1940 &      Franklin Roosevelt &             Democratic &      27313945 \\
95  &  1940 &           Norman Thomas &              Socialist &        116599 \\
96  &  1940 &         Wendell Willkie &             Republican &      22347744 \\
97  &  1944 &      Franklin Roosevelt &             Democratic &      25612916 \\
98  &  1944 &         Thomas E. Dewey &             Republican &      22017929 \\
99  &  1948 &        Claude A. Watson &            Prohibition &        103708 \\
100 &  1948 &            Harry Truman &             Democratic &      24179347 \\
101 &  1948 &        Henry A. Wallace &            Progressive &       1157328 \\
102 &  1948 &           Norman Thomas &              Socialist &        139569 \\
103 &  1948 &          Strom Thurmond &              Dixiecrat &       1175930 \\
104 &  1948 &         Thomas E. Dewey &             Republican &      21991292 \\
105 &  1952 &         Adlai Stevenson &             Democratic &      27375090 \\
106 &  1952 &       Dwight Eisenhower &             Republican &      34075529 \\
107 &  1952 &        Vincent Hallinan &            Progressive &        140746 \\
108 &  1956 &         Adlai Stevenson &             Democratic &      26028028 \\
109 &  1956 &       Dwight Eisenhower &             Republican &      35579180 \\
110 &  1956 &      T. Coleman Andrews &         States' Rights &        107929 \\
111 &  1960 &            John Kennedy &             Democratic &      34220984 \\
112 &  1960 &           Richard Nixon &             Republican &      34108157 \\
113 &  1964 &         Barry Goldwater &             Republican &      27175754 \\
114 &  1964 &          Lyndon Johnson &             Democratic &      43127041 \\
115 &  1968 &          George Wallace &   American Independent &       9901118 \\
116 &  1968 &         Hubert Humphrey &             Democratic &      31271839 \\
117 &  1968 &           Richard Nixon &             Republican &      31783783 \\
118 &  1972 &         George McGovern &             Democratic &      29173222 \\
119 &  1972 &         John G. Schmitz &   American Independent &       1100868 \\
120 &  1972 &           Richard Nixon &             Republican &      47168710 \\
121 &  1976 &         Eugene McCarthy &            Independent &        740460 \\
122 &  1976 &             Gerald Ford &             Republican &      39148634 \\
123 &  1976 &            Jimmy Carter &             Democratic &      40831881 \\
124 &  1976 &           Lester Maddox &   American Independent &        170274 \\
125 &  1976 &          Roger MacBride &            Libertarian &        172557 \\
126 &  1976 &      Thomas J. Anderson &               American &        158271 \\
127 &  1980 &          Barry Commoner &               Citizens &        233052 \\
128 &  1980 &                Ed Clark &            Libertarian &        921128 \\
129 &  1980 &            Jimmy Carter &             Democratic &      35480115 \\
130 &  1980 &        John B. Anderson &            Independent &       5719850 \\
131 &  1980 &           Ronald Reagan &             Republican &      43903230 \\
132 &  1984 &          David Bergland &            Libertarian &        228111 \\
133 &  1984 &           Ronald Reagan &             Republican &      54455472 \\
134 &  1984 &          Walter Mondale &             Democratic &      37577352 \\
135 &  1988 &       George H. W. Bush &             Republican &      48886597 \\
136 &  1988 &           Lenora Fulani &           New Alliance &        217221 \\
137 &  1988 &         Michael Dukakis &             Democratic &      41809074 \\
138 &  1988 &                Ron Paul &            Libertarian &        431750 \\
139 &  1992 &            Andre Marrou &            Libertarian &        290087 \\
140 &  1992 &            Bill Clinton &             Democratic &      44909806 \\
141 &  1992 &                Bo Gritz &               Populist &        106152 \\
142 &  1992 &       George H. W. Bush &             Republican &      39104550 \\
143 &  1992 &              Ross Perot &            Independent &      19743821 \\
144 &  1996 &            Bill Clinton &             Democratic &      47400125 \\
145 &  1996 &                Bob Dole &             Republican &      39197469 \\
146 &  1996 &            Harry Browne &            Libertarian &        485759 \\
147 &  1996 &         Howard Phillips &              Taxpayers &        184656 \\
148 &  1996 &            John Hagelin &            Natural Law &        113670 \\
149 &  1996 &             Ralph Nader &                  Green &        685297 \\
150 &  1996 &              Ross Perot &                 Reform &       8085294 \\
151 &  2000 &                 Al Gore &             Democratic &      50999897 \\
152 &  2000 &          George W. Bush &             Republican &      50456002 \\
153 &  2000 &            Harry Browne &            Libertarian &        384431 \\
154 &  2000 &            Pat Buchanan &                 Reform &        448895 \\
155 &  2000 &             Ralph Nader &                  Green &       2882955 \\
156 &  2004 &              David Cobb &                  Green &        119859 \\
157 &  2004 &          George W. Bush &             Republican &      62040610 \\
158 &  2004 &              John Kerry &             Democratic &      59028444 \\
159 &  2004 &        Michael Badnarik &            Libertarian &        397265 \\
160 &  2004 &        Michael Peroutka &           Constitution &        143630 \\
161 &  2004 &             Ralph Nader &            Independent &        465151 \\
162 &  2008 &            Barack Obama &             Democratic &      69498516 \\
163 &  2008 &                Bob Barr &            Libertarian &        523715 \\
164 &  2008 &           Chuck Baldwin &           Constitution &        199750 \\
165 &  2008 &        Cynthia McKinney &                  Green &        161797 \\
166 &  2008 &             John McCain &             Republican &      59948323 \\
167 &  2008 &             Ralph Nader &            Independent &        739034 \\
168 &  2012 &            Barack Obama &             Democratic &      65915795 \\
169 &  2012 &            Gary Johnson &            Libertarian &       1275971 \\
170 &  2012 &              Jill Stein &                  Green &        469627 \\
171 &  2012 &             Mitt Romney &             Republican &      60933504 \\
172 &  2016 &          Darrell Castle &           Constitution &        203091 \\
173 &  2016 &            Donald Trump &             Republican &      62984828 \\
174 &  2016 &           Evan McMullin &            Independent &        732273 \\
175 &  2016 &            Gary Johnson &            Libertarian &       4489235 \\
176 &  2016 &         Hillary Clinton &             Democratic &      65853514 \\
177 &  2016 &              Jill Stein &                  Green &       1457226 \\
178 &  2020 &            Joseph Biden &             Democratic &      81268924 \\
179 &  2020 &            Donald Trump &             Republican &      74216154 \\
180 &  2020 &            Jo Jorgensen &            Libertarian &       1865724 \\
181 &  2020 &          Howard Hawkins &                  Green &        405035 \\
\bottomrule
\end{tabular}

\hypertarget{a-single-column-label}{%
\subsubsection{A single column label}\label{a-single-column-label}}

Lastly, if we only want the \texttt{Candidate} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections[}\StringTok{"Candidate"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &               Candidate \\
\midrule
0   &          Andrew Jackson \\
1   &       John Quincy Adams \\
2   &          Andrew Jackson \\
3   &       John Quincy Adams \\
4   &          Andrew Jackson \\
5   &              Henry Clay \\
6   &            William Wirt \\
7   &       Hugh Lawson White \\
8   &        Martin Van Buren \\
9   &  William Henry Harrison \\
10  &        Martin Van Buren \\
11  &  William Henry Harrison \\
12  &              Henry Clay \\
13  &              James Polk \\
14  &              Lewis Cass \\
15  &        Martin Van Buren \\
16  &          Zachary Taylor \\
17  &         Franklin Pierce \\
18  &            John P. Hale \\
19  &          Winfield Scott \\
20  &          James Buchanan \\
21  &         John C. Frémont \\
22  &        Millard Fillmore \\
23  &         Abraham Lincoln \\
24  &               John Bell \\
25  &    John C. Breckinridge \\
26  &      Stephen A. Douglas \\
27  &         Abraham Lincoln \\
28  &     George B. McClellan \\
29  &         Horatio Seymour \\
30  &           Ulysses Grant \\
31  &          Horace Greeley \\
32  &           Ulysses Grant \\
33  &        Rutherford Hayes \\
34  &        Samuel J. Tilden \\
35  &         James B. Weaver \\
36  &          James Garfield \\
37  &  Winfield Scott Hancock \\
38  &         Benjamin Butler \\
39  &        Grover Cleveland \\
40  &         James G. Blaine \\
41  &           John St. John \\
42  &          Alson Streeter \\
43  &       Benjamin Harrison \\
44  &         Clinton B. Fisk \\
45  &        Grover Cleveland \\
46  &       Benjamin Harrison \\
47  &        Grover Cleveland \\
48  &         James B. Weaver \\
49  &            John Bidwell \\
50  &          John M. Palmer \\
51  &         Joshua Levering \\
52  &  William Jennings Bryan \\
53  &        William McKinley \\
54  &         John G. Woolley \\
55  &  William Jennings Bryan \\
56  &        William McKinley \\
57  &         Alton B. Parker \\
58  &          Eugene V. Debs \\
59  &        Silas C. Swallow \\
60  &      Theodore Roosevelt \\
61  &        Thomas E. Watson \\
62  &          Eugene V. Debs \\
63  &        Eugene W. Chafin \\
64  &  William Jennings Bryan \\
65  &            William Taft \\
66  &          Eugene V. Debs \\
67  &        Eugene W. Chafin \\
68  &      Theodore Roosevelt \\
69  &            William Taft \\
70  &          Woodrow Wilson \\
71  &         Allan L. Benson \\
72  &    Charles Evans Hughes \\
73  &             Frank Hanly \\
74  &          Woodrow Wilson \\
75  &        Aaron S. Watkins \\
76  &          Eugene V. Debs \\
77  &            James M. Cox \\
78  &   Parley P. Christensen \\
79  &          Warren Harding \\
80  &         Calvin Coolidge \\
81  &           John W. Davis \\
82  &      Robert La Follette \\
83  &                Al Smith \\
84  &          Herbert Hoover \\
85  &           Norman Thomas \\
86  &      Franklin Roosevelt \\
87  &          Herbert Hoover \\
88  &           Norman Thomas \\
89  &       William Z. Foster \\
90  &              Alf Landon \\
91  &      Franklin Roosevelt \\
92  &           Norman Thomas \\
93  &           William Lemke \\
94  &      Franklin Roosevelt \\
95  &           Norman Thomas \\
96  &         Wendell Willkie \\
97  &      Franklin Roosevelt \\
98  &         Thomas E. Dewey \\
99  &        Claude A. Watson \\
100 &            Harry Truman \\
101 &        Henry A. Wallace \\
102 &           Norman Thomas \\
103 &          Strom Thurmond \\
104 &         Thomas E. Dewey \\
105 &         Adlai Stevenson \\
106 &       Dwight Eisenhower \\
107 &        Vincent Hallinan \\
108 &         Adlai Stevenson \\
109 &       Dwight Eisenhower \\
110 &      T. Coleman Andrews \\
111 &            John Kennedy \\
112 &           Richard Nixon \\
113 &         Barry Goldwater \\
114 &          Lyndon Johnson \\
115 &          George Wallace \\
116 &         Hubert Humphrey \\
117 &           Richard Nixon \\
118 &         George McGovern \\
119 &         John G. Schmitz \\
120 &           Richard Nixon \\
121 &         Eugene McCarthy \\
122 &             Gerald Ford \\
123 &            Jimmy Carter \\
124 &           Lester Maddox \\
125 &          Roger MacBride \\
126 &      Thomas J. Anderson \\
127 &          Barry Commoner \\
128 &                Ed Clark \\
129 &            Jimmy Carter \\
130 &        John B. Anderson \\
131 &           Ronald Reagan \\
132 &          David Bergland \\
133 &           Ronald Reagan \\
134 &          Walter Mondale \\
135 &       George H. W. Bush \\
136 &           Lenora Fulani \\
137 &         Michael Dukakis \\
138 &                Ron Paul \\
139 &            Andre Marrou \\
140 &            Bill Clinton \\
141 &                Bo Gritz \\
142 &       George H. W. Bush \\
143 &              Ross Perot \\
144 &            Bill Clinton \\
145 &                Bob Dole \\
146 &            Harry Browne \\
147 &         Howard Phillips \\
148 &            John Hagelin \\
149 &             Ralph Nader \\
150 &              Ross Perot \\
151 &                 Al Gore \\
152 &          George W. Bush \\
153 &            Harry Browne \\
154 &            Pat Buchanan \\
155 &             Ralph Nader \\
156 &              David Cobb \\
157 &          George W. Bush \\
158 &              John Kerry \\
159 &        Michael Badnarik \\
160 &        Michael Peroutka \\
161 &             Ralph Nader \\
162 &            Barack Obama \\
163 &                Bob Barr \\
164 &           Chuck Baldwin \\
165 &        Cynthia McKinney \\
166 &             John McCain \\
167 &             Ralph Nader \\
168 &            Barack Obama \\
169 &            Gary Johnson \\
170 &              Jill Stein \\
171 &             Mitt Romney \\
172 &          Darrell Castle \\
173 &            Donald Trump \\
174 &           Evan McMullin \\
175 &            Gary Johnson \\
176 &         Hillary Clinton \\
177 &              Jill Stein \\
178 &            Joseph Biden \\
179 &            Donald Trump \\
180 &            Jo Jorgensen \\
181 &          Howard Hawkins \\
\bottomrule
\end{tabular}

The output looks like a Series! In this course, we'll become very
comfortable with \texttt{{[}{]}}, especially for selecting columns. In
practice, \texttt{{[}{]}} is much more common than \texttt{.loc}.

\hypertarget{parting-note}{%
\section{Parting Note}\label{parting-note}}

The \texttt{pandas} library is enormous and contains many useful
functions. Here is a link to
\href{https://pandas.pydata.org/docs/}{documentation}.

The introductory \texttt{pandas} lectures will cover important data
structures and methods you should be fluent in. However, we want you to
get familiar with the real world programming practice of
\ldots Googling! Answers to your questions can be found in
documentation, Stack Overflow, etc.

With that, let's move on to Pandas II.

\bookmarksetup{startatroot}

\hypertarget{pandas-ii}{%
\chapter{Pandas II}\label{pandas-ii}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Build familiarity with advanced \texttt{pandas} syntax
\item
  Extract data from a DataFrame using conditional selection
\item
  Recognize situations where aggregation is useful and identify the
  correct technique for performing an aggregation
\end{itemize}

\end{tcolorbox}

Last time, we introduced the \texttt{pandas} library as a toolkit for
processing data. We learned the DataFrame and Series data structures,
familiarized ourselves with the basic syntax for manipulating tabular
data, and began writing our first lines of \texttt{pandas} code.

In this lecture, we'll start to dive into some advanced \texttt{pandas}
syntax. You may find it helpful to follow along with a notebook of your
own as we walk through these new pieces of code.

We'll start by loading the \texttt{babynames} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{import}\NormalTok{ os.path}
\ImportTok{import}\NormalTok{ zipfile}

\NormalTok{data\_url }\OperatorTok{=} \StringTok{"https://www.ssa.gov/oact/babynames/state/namesbystate.zip"}
\NormalTok{local\_filename }\OperatorTok{=} \StringTok{"babynamesbystate.zip"}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ os.path.exists(local\_filename): }\CommentTok{\# if the data exists don\textquotesingle{}t download again}
    \ControlFlowTok{with}\NormalTok{ urllib.request.urlopen(data\_url) }\ImportTok{as}\NormalTok{ resp, }\BuiltInTok{open}\NormalTok{(local\_filename, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(resp.read())}

\NormalTok{zf }\OperatorTok{=}\NormalTok{ zipfile.ZipFile(local\_filename, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{)}

\NormalTok{ca\_name }\OperatorTok{=} \StringTok{\textquotesingle{}CA.TXT\textquotesingle{}}
\NormalTok{field\_names }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Sex\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Year\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Name\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{]}
\ControlFlowTok{with}\NormalTok{ zf.}\BuiltInTok{open}\NormalTok{(ca\_name) }\ImportTok{as}\NormalTok{ fh:}
\NormalTok{    babynames }\OperatorTok{=}\NormalTok{ pd.read\_csv(fh, header}\OperatorTok{=}\VariableTok{None}\NormalTok{, names}\OperatorTok{=}\NormalTok{field\_names)}

\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

\hypertarget{conditional-selection}{%
\section{Conditional Selection}\label{conditional-selection}}

Conditional selection allows us to select a subset of rows in a
DataFrame if they follow some specified condition.

To understand how to use conditional selection, we must look at another
possible input of the \texttt{.loc} and \texttt{{[}{]}} methods -- a
boolean array, which is simply an array where each element is either
\texttt{True} or \texttt{False}. This boolean array must have a length
equal to the number of rows in the DataFrame. It will return all rows in
the position of a corresponding True value in the array.

To see this in action, let's select all even-indexed rows in the first
10 rows of our DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ask yourself: why is :9 is the correct slice to select the first 10 rows?}
\NormalTok{babynames\_first\_10\_rows }\OperatorTok{=}\NormalTok{ babynames.loc[:}\DecValTok{9}\NormalTok{, :]}

\CommentTok{\# Notice how we have exactly 10 elements in our boolean array argument}
\NormalTok{babynames\_first\_10\_rows[[}\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
6 &    CA &   F &  1910 &    Evelyn &    126 \\
8 &    CA &   F &  1910 &  Virginia &    101 \\
\bottomrule
\end{tabular}

We can perform a similar operation using \texttt{.loc}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames\_first\_10\_rows.loc[[}\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{, }\VariableTok{True}\NormalTok{, }\VariableTok{False}\NormalTok{], :]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
6 &    CA &   F &  1910 &    Evelyn &    126 \\
8 &    CA &   F &  1910 &  Virginia &    101 \\
\bottomrule
\end{tabular}

These techniques worked well in this example, but you can imagine how
tedious it might be to list out \texttt{True}s and \texttt{False}s for
every row in a larger DataFrame. To make things easier, we can instead
provide a logical condition as an input to \texttt{.loc} or
\texttt{{[}{]}} that returns a boolean array with the necessary length.

For example, to return all names associated with \texttt{F} sex:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, use a logical condition to generate a boolean array}
\NormalTok{logical\_operator }\OperatorTok{=}\NormalTok{ (babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{)}

\CommentTok{\# Then, use this boolean array to filter the DataFrame}
\NormalTok{babynames[logical\_operator].head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Here, \texttt{logical\_operator} evaluates to a Series of boolean values
with length 400762.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"There are a total of }\SpecialCharTok{\{\}}\StringTok{ values in \textquotesingle{}logical\_operator\textquotesingle{}"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(}\BuiltInTok{len}\NormalTok{(logical\_operator)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
There are a total of 400762 values in 'logical_operator'
\end{verbatim}

Rows starting at row 0 and ending at row 235790 evaluate to
\texttt{True} and are thus returned in the DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 0th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{0}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 235790th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{235790}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The 235791th item in this \textquotesingle{}logical\_operator\textquotesingle{} is: }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(logical\_operator.iloc[}\DecValTok{235791}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The 0th item in this 'logical_operator' is: True
The 235790th item in this 'logical_operator' is: True
The 235791th item in this 'logical_operator' is: False
\end{verbatim}

Passing a Series as an argument to \texttt{babynames{[}{]}} has the same
affect as using a boolean array. In fact, the \texttt{{[}{]}} selection
operator can take a boolean Series, array, and list as arguments. These
three are used interchangeably thoughout the course.

We can also use \texttt{.loc} to achieve similar results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.loc[babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{].head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Boolean conditions can be combined using various operators that allow us
to filter results by multiple conditions. Some examples include the
\texttt{\&} (and) operator and the \texttt{\textbar{}} (or) operator.

\textbf{Note:} When combining multiple conditions with logical
operators, be sure to surround each condition with a set of parenthesis
\texttt{()}. If you forget, your code will throw an error.

For example, if we want to return data on all females born before the
21st century, we can write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[(babynames[}\StringTok{"Sex"}\NormalTok{] }\OperatorTok{==} \StringTok{"F"}\NormalTok{) }\OperatorTok{\&}\NormalTok{ (babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{\textless{}} \DecValTok{2000}\NormalTok{)].head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &      Name &  Count \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 \\
1 &    CA &   F &  1910 &     Helen &    239 \\
2 &    CA &   F &  1910 &   Dorothy &    220 \\
3 &    CA &   F &  1910 &  Margaret &    163 \\
4 &    CA &   F &  1910 &   Frances &    134 \\
\bottomrule
\end{tabular}

Boolean array selection is a useful tool, but can lead to overly verbose
code for complex conditions. \texttt{Pandas} provide many alternatives:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}
\NormalTok{    babynames[(babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Bella"}\NormalTok{) }\OperatorTok{|} 
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Alex"}\NormalTok{) }\OperatorTok{|}
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Ani"}\NormalTok{) }\OperatorTok{|}
\NormalTok{              (babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Lisa"}\NormalTok{)]}
\NormalTok{).head()}
\CommentTok{\# Note: The parentheses surrounding the code make it possible to break the code on to multiple lines for readability}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &   Name &  Count \\
\midrule
6289  &    CA &   F &  1923 &  Bella &      5 \\
7512  &    CA &   F &  1925 &  Bella &      8 \\
12368 &    CA &   F &  1932 &   Lisa &      5 \\
14741 &    CA &   F &  1936 &   Lisa &      8 \\
17084 &    CA &   F &  1939 &   Lisa &      5 \\
\bottomrule
\end{tabular}

The \texttt{.isin} function can be used to filter dataframes. The method
helps in selecting rows with having a particular (or multiple) value in
a particular column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{names }\OperatorTok{=}\NormalTok{ [}\StringTok{"Bella"}\NormalTok{, }\StringTok{"Alex"}\NormalTok{, }\StringTok{"Ani"}\NormalTok{, }\StringTok{"Lisa"}\NormalTok{]}
\NormalTok{babynames[babynames[}\StringTok{"Name"}\NormalTok{].isin(names)].head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &   Name &  Count \\
\midrule
6289  &    CA &   F &  1923 &  Bella &      5 \\
7512  &    CA &   F &  1925 &  Bella &      8 \\
12368 &    CA &   F &  1932 &   Lisa &      5 \\
14741 &    CA &   F &  1936 &   Lisa &      8 \\
17084 &    CA &   F &  1939 &   Lisa &      5 \\
\bottomrule
\end{tabular}

The function \texttt{str.startswith} can be used to define a filter
based on string values in a \texttt{Series} object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.startswith(}\StringTok{"N"}\NormalTok{)].head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &    Name &  Count \\
\midrule
76  &    CA &   F &  1910 &   Norma &     23 \\
83  &    CA &   F &  1910 &  Nellie &     20 \\
127 &    CA &   F &  1910 &    Nina &     11 \\
198 &    CA &   F &  1910 &    Nora &      6 \\
310 &    CA &   F &  1911 &  Nellie &     23 \\
\bottomrule
\end{tabular}

\hypertarget{handy-utility-functions}{%
\section{Handy Utility Functions}\label{handy-utility-functions}}

\texttt{pandas} contains an extensive library of functions that can help
shorten the process of setting and getting information from its data
structures. In the following section, we will give overviews of each of
the main utility functions that will help us in Data 100.

\begin{itemize}
\tightlist
\item
  \texttt{Numpy} and built-in function support
\item
  \texttt{.shape}
\item
  \texttt{.size}
\item
  \texttt{.describe()}
\item
  \texttt{.sample()}
\item
  \texttt{.value\_counts()}
\item
  \texttt{.unique()}
\item
  \texttt{.sort\_values()}
\end{itemize}

\hypertarget{numpy}{%
\subsection{\texorpdfstring{\texttt{Numpy}}{Numpy}}\label{numpy}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bella\_counts }\OperatorTok{=}\NormalTok{ babynames[babynames[}\StringTok{"Name"}\NormalTok{] }\OperatorTok{==} \StringTok{"Bella"}\NormalTok{][}\StringTok{"Count"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Average number of babies named Bella each year}
\NormalTok{np.mean(bella\_counts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
270.1860465116279
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Max number of babies named Bella born on a given year}
\BuiltInTok{max}\NormalTok{(bella\_counts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
902
\end{verbatim}

\hypertarget{shape-and-.size}{%
\subsection{\texorpdfstring{\texttt{.shape} and
\texttt{.size}}{.shape and .size}}\label{shape-and-.size}}

\texttt{.shape} and \texttt{.size} are attributes of Series and
DataFrames that measure the ``amount'' of data stored in the structure.
Calling \texttt{.shape} returns a tuple containing the number of rows
and columns present in the DataFrame or Series. \texttt{.size} is used
to find the total number of elements in a structure, equivalent to the
number of rows times the number of columns.

Many functions strictly require the dimensions of the arguments along
certain axes to match. Calling these dimension-finding functions is much
faster than counting all of the items by hand.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(400762, 5)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.size}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2003810
\end{verbatim}

\hypertarget{describe}{%
\subsection{\texorpdfstring{\texttt{.describe()}}{.describe()}}\label{describe}}

If many statistics are required from a DataFrame (minimum value, maximum
value, mean value, etc.), then
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html}{\texttt{.describe()}}
can be used to compute all of them at once.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.describe()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrr}
\toprule
{} &           Year &          Count \\
\midrule
count &  400762.000000 &  400762.000000 \\
mean  &    1985.131287 &      79.953781 \\
std   &      26.821004 &     295.414618 \\
min   &    1910.000000 &       5.000000 \\
25\%   &    1968.000000 &       7.000000 \\
50\%   &    1991.000000 &      13.000000 \\
75\%   &    2007.000000 &      38.000000 \\
max   &    2021.000000 &    8262.000000 \\
\bottomrule
\end{tabular}

A different set of statistics will be reported if \texttt{.describe()}
is called on a Series.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Sex"}\NormalTok{].describe()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &     Sex \\
\midrule
count  &  400762 \\
unique &       2 \\
top    &       F \\
freq   &  235791 \\
\bottomrule
\end{tabular}

\hypertarget{sample}{%
\subsection{\texorpdfstring{\texttt{.sample()}}{.sample()}}\label{sample}}

As we will see later in the semester, random processes are at the heart
of many data science techniques (for example, train-test splits,
bootstrapping, and cross-validation).
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html}{\texttt{.sample()}}
lets us quickly select random entries (a row if called from a DataFrame,
or a value if called from a Series).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.sample()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &  Name &  Count \\
\midrule
160609 &    CA &   F &  2003 &  Kate &    337 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.sample(}\DecValTok{5}\NormalTok{).iloc[:, }\DecValTok{2}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrlr}
\toprule
{} &  Year &        Name &  Count \\
\midrule
148533 &  1999 &      Sintia &      6 \\
13238  &  1934 &      Muriel &     28 \\
174807 &  2006 &  Brooklynne &     10 \\
301642 &  1984 &     Sheldon &     21 \\
97555  &  1985 &       Karin &     27 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{==} \DecValTok{2000}\NormalTok{].sample(}\DecValTok{4}\NormalTok{, replace }\OperatorTok{=} \VariableTok{True}\NormalTok{).iloc[:, }\DecValTok{2}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrlr}
\toprule
{} &  Year &      Name &  Count \\
\midrule
151841 &  2000 &     Niyah &      7 \\
339307 &  2000 &    Favian &     36 \\
340688 &  2000 &  Enrrique &      6 \\
339053 &  2000 &    Curtis &     95 \\
\bottomrule
\end{tabular}

\hypertarget{value_counts}{%
\subsection{\texorpdfstring{\texttt{.value\_counts()}}{.value\_counts()}}\label{value_counts}}

When we want to know the distribution of the items in a Series (for
example, what items are most/least common), we use
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html}{\texttt{.value-counts()}}
to get a breakdown of the unique \emph{values} and their \emph{counts}.
In the example below, we can determine the name with the most years in
which at least one person has taken that name by counting the number of
times each name appears in the \texttt{"Name"} column of
\texttt{babynames}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].value\_counts().head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Name \\
\midrule
Jean      &   221 \\
Francis   &   219 \\
Guadalupe &   216 \\
Jessie    &   215 \\
Marion    &   213 \\
\bottomrule
\end{tabular}

\hypertarget{unique}{%
\subsection{\texorpdfstring{\texttt{.unique()}}{.unique()}}\label{unique}}

If we have a Series with many repeated values, then
\href{https://pandas.pydata.org/docs/reference/api/pandas.unique.html}{\texttt{.unique()}}
can be used to identify only the \emph{unique} values. Here we can get a
list of all the names in \texttt{babynames}.

\textbf{Exercise:} what function can we call on the Series below to get
the number of unique names?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].unique()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array(['Mary', 'Helen', 'Dorothy', ..., 'Zyire', 'Zylo', 'Zyrus'],
      dtype=object)
\end{verbatim}

\hypertarget{sort_values}{%
\subsection{\texorpdfstring{\texttt{.sort\_values()}}{.sort\_values()}}\label{sort_values}}

Ordering a DataFrame can be useful for isolating extreme values. For
example, the first 5 entries of a row sorted in descending order (that
is, from highest to lowest) are the largest 5 values.
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html}{\texttt{.sort\_values}}
allows us to order a DataFrame or Series by a specified rule. For
DataFrames, we must specify the column by which we want to compare the
rows and the function will return such rows. We can choose to either
receive the rows in \texttt{ascending} order (default) or
\texttt{descending} order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.sort\_values(by }\OperatorTok{=} \StringTok{"Count"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &     Name &  Count \\
\midrule
263272 &    CA &   M &  1956 &  Michael &   8262 \\
264297 &    CA &   M &  1957 &  Michael &   8250 \\
313644 &    CA &   M &  1990 &  Michael &   8247 \\
278109 &    CA &   M &  1969 &  Michael &   8244 \\
279405 &    CA &   M &  1970 &  Michael &   8197 \\
\bottomrule
\end{tabular}

We do not need to explicitly specify the column used for sorting when
calling \texttt{.value\_counts()} on a Series. We can still specify the
ordering paradigm -- that is, whether values are sorted in ascending or
descending order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames[}\StringTok{"Name"}\NormalTok{].sort\_values(ascending}\OperatorTok{=}\VariableTok{True}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &     Name \\
\midrule
380256 &    Aadan \\
362255 &    Aadan \\
365374 &    Aadan \\
394460 &  Aadarsh \\
366561 &    Aaden \\
\bottomrule
\end{tabular}

\hypertarget{sorting-with-a-custom-key}{%
\subsubsection{Sorting With a Custom
Key}\label{sorting-with-a-custom-key}}

Using \texttt{.sort\_values} can be useful in many situations, but it
many not cover all use cases. This is because \texttt{pandas}
automatically sorts values in order according to numeric value (for
number data) or alphabetical order (for string data). The following code
finds the top 5 most popular names in California in 2021.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort names by count in year 2021}
\CommentTok{\# Recall that \textasciigrave{}.head(5)\textasciigrave{} displays the first five rows in the DataFrame}
\NormalTok{babynames[babynames[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{==} \DecValTok{2021}\NormalTok{].sort\_values(}\StringTok{"Count"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &    Name &  Count \\
\midrule
397909 &    CA &   M &  2021 &    Noah &   2591 \\
397910 &    CA &   M &  2021 &    Liam &   2469 \\
232145 &    CA &   F &  2021 &  Olivia &   2395 \\
232146 &    CA &   F &  2021 &    Emma &   2171 \\
397911 &    CA &   M &  2021 &   Mateo &   2108 \\
\bottomrule
\end{tabular}

This offers us a lot of functionality, but what if we need to sort by
some other metric? For example, what if we wanted to find the longest
names in the DataFrame?

We can do this by specifying the \texttt{key} parameter of
\texttt{.sort\_values}. The \texttt{key} parameter is assigned to a
function of our choice. This function is then applied to each value in
the specified column. \texttt{pandas} will, finally, sort the DataFrame
by the values outputted by the function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here, a lambda function is applied to find the length of each value, \textasciigrave{}x\textasciigrave{}, in the "Name" column}
\NormalTok{babynames.sort\_values(}\StringTok{"Name"}\NormalTok{, key}\OperatorTok{=}\KeywordTok{lambda}\NormalTok{ x: x.}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{(), ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &             Name &  Count \\
\midrule
313143 &    CA &   M &  1989 &  Franciscojavier &      6 \\
333732 &    CA &   M &  1997 &  Ryanchristopher &      5 \\
330421 &    CA &   M &  1996 &  Franciscojavier &      8 \\
323615 &    CA &   M &  1993 &  Johnchristopher &      5 \\
310235 &    CA &   M &  1988 &  Franciscojavier &     10 \\
\bottomrule
\end{tabular}

\hypertarget{adding-and-removing-columns}{%
\section{Adding and Removing
Columns}\label{adding-and-removing-columns}}

To add a new column to a DataFrame, we use a syntax similar to that used
when accessing an existing column. Specify the name of the new column by
writing \texttt{dataframe{[}"new\_column"{]}}, then assign this to a
Series or Array containing the values that will populate this column.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add a column named "name\_lengths" that includes the length of each name}
\NormalTok{babynames[}\StringTok{"name\_lengths"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.}\BuiltInTok{len}\NormalTok{()}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  name\_lengths \\
\midrule
0 &    CA &   F &  1910 &      Mary &    295 &             4 \\
1 &    CA &   F &  1910 &     Helen &    239 &             5 \\
2 &    CA &   F &  1910 &   Dorothy &    220 &             7 \\
3 &    CA &   F &  1910 &  Margaret &    163 &             8 \\
4 &    CA &   F &  1910 &   Frances &    134 &             7 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Sort by the temporary column}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.sort\_values(by }\OperatorTok{=} \StringTok{"name\_lengths"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlrr}
\toprule
{} & State & Sex &  Year &             Name &  Count &  name\_lengths \\
\midrule
313143 &    CA &   M &  1989 &  Franciscojavier &      6 &            15 \\
333732 &    CA &   M &  1997 &  Ryanchristopher &      5 &            15 \\
330421 &    CA &   M &  1996 &  Franciscojavier &      8 &            15 \\
323615 &    CA &   M &  1993 &  Johnchristopher &      5 &            15 \\
310235 &    CA &   M &  1988 &  Franciscojavier &     10 &            15 \\
\bottomrule
\end{tabular}

In the example above, we made use of an in-built function given to us by
the \texttt{str} accessor for getting the length of names. Then we used
\texttt{name\_length} column to sort the dataframe. What if we had
wanted to generate the values in our new column using a function of our
own making?

We can do this using the Series
\href{https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html}{\texttt{.map}}
method. \texttt{.map} takes in a function as input, and will apply this
function to each value of a Series.

For example, say we wanted to find the number of occurrences of the
sequence ``dr'' or ``ea'' in each name.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First, define a function to count the number of times "dr" or "ea" appear in each name}
\KeywordTok{def}\NormalTok{ dr\_ea\_count(string):}
    \ControlFlowTok{return}\NormalTok{ string.count(}\StringTok{"dr"}\NormalTok{) }\OperatorTok{+}\NormalTok{ string.count(}\StringTok{"ea"}\NormalTok{)}

\CommentTok{\# Then, use \textasciigrave{}map\textasciigrave{} to apply \textasciigrave{}dr\_ea\_count\textasciigrave{} to each name in the "Name" column}
\NormalTok{babynames[}\StringTok{"dr\_ea\_count"}\NormalTok{] }\OperatorTok{=}\NormalTok{ babynames[}\StringTok{"Name"}\NormalTok{].}\BuiltInTok{map}\NormalTok{(dr\_ea\_count)}

\CommentTok{\# Sort the DataFrame by the new "dr\_ea\_count" column so we can see our handiwork}
\NormalTok{babynames.sort\_values(by }\OperatorTok{=} \StringTok{"dr\_ea\_count"}\NormalTok{, ascending }\OperatorTok{=} \VariableTok{False}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlrrr}
\toprule
{} & State & Sex &  Year &      Name &  Count &  name\_lengths &  dr\_ea\_count \\
\midrule
101969 &    CA &   F &  1986 &  Deandrea &      6 &             8 &            3 \\
304390 &    CA &   M &  1985 &  Deandrea &      6 &             8 &            3 \\
131022 &    CA &   F &  1994 &  Leandrea &      5 &             8 &            3 \\
115950 &    CA &   F &  1990 &  Deandrea &      5 &             8 &            3 \\
108723 &    CA &   F &  1988 &  Deandrea &      5 &             8 &            3 \\
\bottomrule
\end{tabular}

If we want to remove a column or row of a DataFrame, we can call the
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html}{\texttt{.drop}}
method. Use the \texttt{axis} parameter to specify whether a column or
row should be dropped. Unless otherwise specified, \texttt{pandas} will
assume that we are dropping a row by default.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Drop our "dr\_ea\_count" and "length" columns from the DataFrame}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ babynames.drop([}\StringTok{"dr\_ea\_count"}\NormalTok{, }\StringTok{"name\_lengths"}\NormalTok{], axis}\OperatorTok{=}\StringTok{"columns"}\NormalTok{)}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &             Name &  Count \\
\midrule
313143 &    CA &   M &  1989 &  Franciscojavier &      6 \\
333732 &    CA &   M &  1997 &  Ryanchristopher &      5 \\
330421 &    CA &   M &  1996 &  Franciscojavier &      8 \\
323615 &    CA &   M &  1993 &  Johnchristopher &      5 \\
310235 &    CA &   M &  1988 &  Franciscojavier &     10 \\
\bottomrule
\end{tabular}

Notice that we reassigned \texttt{babynames} to the result of
\texttt{babynames.drop(...)}. This is a subtle, but important point:
\texttt{pandas} table operations \textbf{do not occur in-place}. Calling
\texttt{dataframe.drop(...)} will output a \emph{copy} of
\texttt{dataframe} with the row/column of interest removed, without
modifying the original \texttt{dataframe} table.

In other words, if we simply call:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This creates a copy of \textasciigrave{}babynames\textasciigrave{} and removes the row with label 3...}
\NormalTok{babynames.drop(}\DecValTok{3}\NormalTok{, axis}\OperatorTok{=}\StringTok{"rows"}\NormalTok{)}

\CommentTok{\# ...but the original \textasciigrave{}babynames\textasciigrave{} is unchanged! }
\CommentTok{\# Notice that the row with label 3 is still present}
\NormalTok{babynames.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrlr}
\toprule
{} & State & Sex &  Year &             Name &  Count \\
\midrule
313143 &    CA &   M &  1989 &  Franciscojavier &      6 \\
333732 &    CA &   M &  1997 &  Ryanchristopher &      5 \\
330421 &    CA &   M &  1996 &  Franciscojavier &      8 \\
323615 &    CA &   M &  1993 &  Johnchristopher &      5 \\
310235 &    CA &   M &  1988 &  Franciscojavier &     10 \\
\bottomrule
\end{tabular}

\hypertarget{aggregating-data-with-groupby}{%
\section{Aggregating Data with
GroupBy}\label{aggregating-data-with-groupby}}

Up until this point, we have been working with individual rows of
DataFrames. As data scientists, we often wish to investigate trends
across a larger \emph{subset} of our data. For example, we may want to
compute some summary statistic (the mean, median, sum, etc.) for a group
of rows in our DataFrame. To do this, we'll use \texttt{pandas}
\texttt{GroupBy} objects.

Let's say we wanted to aggregate all rows in \texttt{babynames} for a
given year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fcb080a2520>
\end{verbatim}

What does this strange output mean? Calling
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html}{\texttt{.groupby}}
has generated a \texttt{GroupBy} object. You can imagine this as a set
of ``mini'' sub-DataFrames, where each subframe contains all of the rows
from \texttt{babynames} that correspond to a particular year.

The diagram below shows a simplified view of \texttt{babynames} to help
illustrate this idea.

\begin{figure}

{\centering \includegraphics{pandas_2/images/gb.png}

}

\caption{Creating a GroupBy object}

\end{figure}

We can't work with a \texttt{GroupBy} object directly -- that is why you
saw that strange output earlier, rather than a standard view of a
DataFrame. To actually manipulate values within these ``mini''
DataFrames, we'll need to call an \emph{aggregation method}. This is a
method that tells \texttt{pandas} how to aggregate the values within the
\texttt{GroupBy} object. Once the aggregation is applied,
\texttt{pandas} will return a normal (now grouped) DataFrame.

The first aggregation method we'll consider is \texttt{.agg}. The
\texttt{.agg} method takes in a function as its argument; this function
is then applied to each column of a ``mini'' grouped DataFrame. We end
up with a new DataFrame with one aggregated row per subframe. Let's see
this in action by finding the \texttt{sum} of all counts for each year
in \texttt{babynames} -- this is equivalent to finding the number of
babies born in each year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{).agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &   9163 \\
1911 &   9983 \\
1912 &  17946 \\
1913 &  22094 \\
1914 &  26926 \\
\bottomrule
\end{tabular}

We can relate this back to the diagram we used above. Remember that the
diagram uses a simplified version of \texttt{babynames}, which is why we
see smaller values for the summed counts.

\begin{figure}

{\centering \includegraphics{pandas_2/images/agg.png}

}

\caption{Performing an aggregation}

\end{figure}

Calling \texttt{.agg} has condensed each subframe back into a single
row. This gives us our final output: a DataFrame that is now indexed by
\texttt{"Year"}, with a single row for each unique year in the original
\texttt{babynames} DataFrame.

You may be wondering: where did the \texttt{"State"}, \texttt{"Sex"},
and \texttt{"Name"} columns go? Logically, it doesn't make sense to
\texttt{sum} the string data in these columns (how would we add ``Mary''
+ ``Ann''?). Because of this, \texttt{pandas} will simply omit these
columns when it performs the aggregation on the DataFrame. Since this
happens implicitly, without the user specifying that these columns
should be ignored, it's easy to run into troubling situations where
columns are removed without the programmer noticing. It is better coding
practice to select \emph{only} the columns we care about before
performing the aggregation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Same result, but now we explicitly tell Pandas to only consider the "Count" column when summing}
\NormalTok{babynames.groupby(}\StringTok{"Year"}\NormalTok{)[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  Count \\
Year &        \\
\midrule
1910 &   9163 \\
1911 &   9983 \\
1912 &  17946 \\
1913 &  22094 \\
1914 &  26926 \\
\bottomrule
\end{tabular}

\hypertarget{parting-note-1}{%
\subsection{Parting note}\label{parting-note-1}}

Manipulating \texttt{DataFrames} is a skill that is not mastered in just
one day. Due to the flexibility of \texttt{pandas}, there are many
different ways to get from a point A to a point B. We recommend trying
multiple different ways to solve the same problem to gain even more
practice and reach that point of mastery sooner.

Next, we will start digging deeper into the mechanics behind grouping
data.

\bookmarksetup{startatroot}

\hypertarget{pandas-iii}{%
\chapter{Pandas III}\label{pandas-iii}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Perform advanced aggregation using \texttt{.groupby()}
\item
  Use the \texttt{pd.pivot\_table} method to contruct a pivot table
\item
  Perform simple merges between DataFrames using \texttt{pd.merge()}
\end{itemize}

\end{tcolorbox}

\hypertarget{more-on-agg-function}{%
\section{\texorpdfstring{More on \texttt{agg()}
Function}{More on agg() Function}}\label{more-on-agg-function}}

Last time, we introduced the concept of aggregating data -- we
familiarized ourselves with \texttt{GroupBy} objects and used them as
tools to consolidate and summarize a DataFrame. In this lecture, we will
explore some advanced \texttt{.groupby} methods to show just how
powerful of a resource they can be for understanding our data. We will
also introduce other techniques for data aggregation to provide
flexibility in how we manipulate our tables.

\hypertarget{groupby-continued}{%
\section{\texorpdfstring{\texttt{GroupBy()},
Continued}{GroupBy(), Continued}}\label{groupby-continued}}

As we learned last lecture, a \texttt{groupby} operation involves some
combination of \textbf{splitting a DataFrame into grouped subframes},
\textbf{applying a function}, and \textbf{combining the results}.

For some arbitrary DataFrame \texttt{df} below, the code
\texttt{df.groupby("year").agg(sum)} does the following:

\begin{itemize}
\tightlist
\item
  Organizes all rows with the same year into a subframe for that year.
\item
  Creates a new DataFrame with one row representing each subframe year.
\item
  Combines all integer rows in each subframe using the \texttt{sum}
  function.
\end{itemize}

\hypertarget{aggregation-with-lambda-functions}{%
\subsection{\texorpdfstring{Aggregation with \texttt{lambda}
Functions}{Aggregation with lambda Functions}}\label{aggregation-with-lambda-functions}}

Throughout this note, we'll work with the \texttt{elections} DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{elections }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{)}
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

What if we wish to aggregate our DataFrame using a non-standard function
-- for example, a function of our own design? We can do so by combining
\texttt{.agg} with \texttt{lambda} expressions.

Let's first consider a puzzle to jog our memory. We will attempt to find
the \texttt{Candidate} from each \texttt{Party} with the highest
\texttt{\%} of votes.

A naive approach may be to group by the \texttt{Party} column and
aggregate by the maximum.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.groupby(}\StringTok{"Party"}\NormalTok{).agg(}\BuiltInTok{max}\NormalTok{).head(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrlrlr}
\toprule
{} &  Year &           Candidate &  Popular vote & Result &          \% \\
Party                 &       &                     &               &        &            \\
\midrule
American              &  1976 &  Thomas J. Anderson &        873053 &   loss &  21.554001 \\
American Independent  &  1976 &       Lester Maddox &       9901118 &   loss &  13.571218 \\
Anti-Masonic          &  1832 &        William Wirt &        100715 &   loss &   7.821583 \\
Anti-Monopoly         &  1884 &     Benjamin Butler &        134294 &   loss &   1.335838 \\
Citizens              &  1980 &      Barry Commoner &        233052 &   loss &   0.270182 \\
Communist             &  1932 &   William Z. Foster &        103307 &   loss &   0.261069 \\
Constitution          &  2016 &    Michael Peroutka &        203091 &   loss &   0.152398 \\
Constitutional Union  &  1860 &           John Bell &        590901 &   loss &  12.639283 \\
Democratic            &  2020 &      Woodrow Wilson &      81268924 &    win &  61.344703 \\
Democratic-Republican &  1824 &   John Quincy Adams &        151271 &    win &  57.210122 \\
\bottomrule
\end{tabular}

This approach is clearly wrong -- the DataFrame claims that Woodrow
Wilson won the presidency in 2020.

Why is this happening? Here, the \texttt{max} aggregation function is
taken over every column \emph{independently}. Among Democrats,
\texttt{max} is computing:

\begin{itemize}
\tightlist
\item
  The most recent \texttt{Year} a Democratic candidate ran for president
  (2020)
\item
  The \texttt{Candidate} with the alphabetically ``largest'' name
  (``Woodrow Wilson'')
\item
  The \texttt{Result} with the alphabetically ``largest'' outcome
  (``win'')
\end{itemize}

Instead, let's try a different approach. We will:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sort the DataFrame so that rows are in descending order of \texttt{\%}
\item
  Group by \texttt{Party} and select the first row of each groupby
  object
\end{enumerate}

While it may seem unintuitive, sorting \texttt{elections} by descending
order of \texttt{\%} is extremely helpful. If we then group by
\texttt{Party}, the first row of each groupby object will contain
information about the \texttt{Candidate} with the highest voter
\texttt{\%}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections\_sorted\_by\_percent }\OperatorTok{=}\NormalTok{ elections.sort\_values(}\StringTok{"\%"}\NormalTok{, ascending}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{elections\_sorted\_by\_percent.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &           Candidate &       Party &  Popular vote & Result &          \% \\
\midrule
114 &  1964 &      Lyndon Johnson &  Democratic &      43127041 &    win &  61.344703 \\
91  &  1936 &  Franklin Roosevelt &  Democratic &      27752648 &    win &  60.978107 \\
120 &  1972 &       Richard Nixon &  Republican &      47168710 &    win &  60.907806 \\
79  &  1920 &      Warren Harding &  Republican &      16144093 &    win &  60.574501 \\
133 &  1984 &       Ronald Reagan &  Republican &      54455472 &    win &  59.023326 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections\_sorted\_by\_percent.groupby(}\StringTok{"Party"}\NormalTok{).agg(}\KeywordTok{lambda}\NormalTok{ x : x.iloc[}\DecValTok{0}\NormalTok{]).head(}\DecValTok{10}\NormalTok{)}

\CommentTok{\# Equivalent to the below code}
\CommentTok{\# elections\_sorted\_by\_percent.groupby("Party").agg(\textquotesingle{}first\textquotesingle{}).head(10)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrlrlr}
\toprule
{} &  Year &          Candidate &  Popular vote & Result &          \% \\
Party                 &       &                    &               &        &            \\
\midrule
American              &  1856 &   Millard Fillmore &        873053 &   loss &  21.554001 \\
American Independent  &  1968 &     George Wallace &       9901118 &   loss &  13.571218 \\
Anti-Masonic          &  1832 &       William Wirt &        100715 &   loss &   7.821583 \\
Anti-Monopoly         &  1884 &    Benjamin Butler &        134294 &   loss &   1.335838 \\
Citizens              &  1980 &     Barry Commoner &        233052 &   loss &   0.270182 \\
Communist             &  1932 &  William Z. Foster &        103307 &   loss &   0.261069 \\
Constitution          &  2008 &      Chuck Baldwin &        199750 &   loss &   0.152398 \\
Constitutional Union  &  1860 &          John Bell &        590901 &   loss &  12.639283 \\
Democratic            &  1964 &     Lyndon Johnson &      43127041 &    win &  61.344703 \\
Democratic-Republican &  1824 &     Andrew Jackson &        151271 &   loss &  57.210122 \\
\bottomrule
\end{tabular}

Notice how our code correctly determines that Lyndon Johnson from the
Democratic Party has the highest voter \texttt{\%}.

More generally, \texttt{lambda} functions are used to design custom
aggregation functions that aren't pre-defined by Python. The input
parameter \texttt{x} to the \texttt{lambda} function is a
\texttt{GroupBy} object. Therefore, it should make sense why
\texttt{lambda\ x\ :\ x.iloc{[}0{]}} selects the first row in each
groupby object.

In fact, there's a few different ways to approach this problem. Each
approach has different tradeoffs in terms of readability, performance,
memory consumption, complexity, etc. We've given a few examples below.

\textbf{Note}: Understanding these alternative solutions is not
required. They are given to demonstrate the vast number of
problem-solving approaches in \texttt{pandas}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using the idxmax function}
\NormalTok{best\_per\_party }\OperatorTok{=}\NormalTok{ elections.loc[elections.groupby(}\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{)[}\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{].idxmax()]}
\NormalTok{best\_per\_party.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &         Candidate &                 Party &  Popular vote & Result &          \% \\
\midrule
22  &  1856 &  Millard Fillmore &              American &        873053 &   loss &  21.554001 \\
115 &  1968 &    George Wallace &  American Independent &       9901118 &   loss &  13.571218 \\
6   &  1832 &      William Wirt &          Anti-Masonic &        100715 &   loss &   7.821583 \\
38  &  1884 &   Benjamin Butler &         Anti-Monopoly &        134294 &   loss &   1.335838 \\
127 &  1980 &    Barry Commoner &              Citizens &        233052 &   loss &   0.270182 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using the .drop\_duplicates function}
\NormalTok{best\_per\_party2 }\OperatorTok{=}\NormalTok{ elections.sort\_values(}\StringTok{\textquotesingle{}\%\textquotesingle{}}\NormalTok{).drop\_duplicates([}\StringTok{\textquotesingle{}Party\textquotesingle{}}\NormalTok{], keep}\OperatorTok{=}\StringTok{\textquotesingle{}last\textquotesingle{}}\NormalTok{)}
\NormalTok{best\_per\_party2.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &           Candidate &           Party &  Popular vote & Result &         \% \\
\midrule
148 &  1996 &        John Hagelin &     Natural Law &        113670 &   loss &  0.118219 \\
164 &  2008 &       Chuck Baldwin &    Constitution &        199750 &   loss &  0.152398 \\
110 &  1956 &  T. Coleman Andrews &  States' Rights &        107929 &   loss &  0.174883 \\
147 &  1996 &     Howard Phillips &       Taxpayers &        184656 &   loss &  0.192045 \\
136 &  1988 &       Lenora Fulani &    New Alliance &        217221 &   loss &  0.237804 \\
\bottomrule
\end{tabular}

\hypertarget{other-groupby-features}{%
\subsection{\texorpdfstring{Other \texttt{GroupBy}
Features}{Other GroupBy Features}}\label{other-groupby-features}}

There are many aggregation methods we can use with \texttt{.agg}. Some
useful options are:

\begin{itemize}
\tightlist
\item
  \href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.max.html}{\texttt{.max}}:
  creates a new DataFrame with the maximum value of each group
\item
  \href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.mean.html}{\texttt{.mean}}:
  creates a new DataFrame with the mean value of each group
\item
  \href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.size.html}{\texttt{.size}}:
  creates a new Series with the number of entries in each group
\end{itemize}

In fact, these (and other) aggregation functions are so common that
\texttt{pandas} allows for writing shorthand. Instead of explicitly
stating the use of \texttt{.agg}, we can call the function directly on
the \texttt{GroupBy} object.

For example, the following are equivalent:

\begin{itemize}
\tightlist
\item
  \texttt{elections.groupby("Candidate").agg(mean)}
\item
  \texttt{elections.groupby("Candidate").mean()}
\end{itemize}

\hypertarget{the-groupby.filter-function}{%
\subsection{\texorpdfstring{The \texttt{groupby.filter()}
function}{The groupby.filter() function}}\label{the-groupby.filter-function}}

Another common use for \texttt{GroupBy} objects is to filter data by
group.

\texttt{groupby.filter} takes an argument \(\text{f}\), where
\(\text{f}\) is a function that:

\begin{itemize}
\tightlist
\item
  Takes a \texttt{GroupBy} object as input
\item
  Returns a single \texttt{True} or \texttt{False} for the entire
  subframe
\end{itemize}

\texttt{GroupBy} objects that correspond to \texttt{True} are returned
in the final result, whereas those with a \texttt{False} value are not.
Importantly, \texttt{groupby.filter} is different from
\texttt{groupby.agg} in that the \emph{entire} subframe is returned in
the final DataFrame, not just a single row.

To illustrate how this happens, consider the following \texttt{.filter}
function applied on some arbitrary data. Say we want to identify
``tight'' election years -- that is, we want to find all rows that
correspond to elections years where all candidates in that year won a
similar portion of the total vote. Specifically, let's find all rows
corresponding to a year where no candidate won more than 45\% of the
total vote.

An equivalent way of framing this goal is to say:

\begin{itemize}
\tightlist
\item
  Find the years where the maximum \texttt{\%} in that year is less than
  45\%
\item
  Return all DataFrame rows that correspond to these years
\end{itemize}

For each year, we need to find the maximum \texttt{\%} among \emph{all}
rows for that year. If this maximum \texttt{\%} is lower than 45\%, we
will tell \texttt{pandas} to keep all rows corresponding to that year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.groupby(}\StringTok{"Year"}\NormalTok{).}\BuiltInTok{filter}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ sf: sf[}\StringTok{"\%"}\NormalTok{].}\BuiltInTok{max}\NormalTok{() }\OperatorTok{\textless{}} \DecValTok{45}\NormalTok{).head(}\DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &             Candidate &                 Party &  Popular vote & Result &          \% \\
\midrule
23 &  1860 &       Abraham Lincoln &            Republican &       1855993 &    win &  39.699408 \\
24 &  1860 &             John Bell &  Constitutional Union &        590901 &   loss &  12.639283 \\
25 &  1860 &  John C. Breckinridge &   Southern Democratic &        848019 &   loss &  18.138998 \\
26 &  1860 &    Stephen A. Douglas &   Northern Democratic &       1380202 &   loss &  29.522311 \\
66 &  1912 &        Eugene V. Debs &             Socialist &        901551 &   loss &   6.004354 \\
67 &  1912 &      Eugene W. Chafin &           Prohibition &        208156 &   loss &   1.386325 \\
68 &  1912 &    Theodore Roosevelt &           Progressive &       4122721 &   loss &  27.457433 \\
69 &  1912 &          William Taft &            Republican &       3486242 &   loss &  23.218466 \\
70 &  1912 &        Woodrow Wilson &            Democratic &       6296284 &    win &  41.933422 \\
\bottomrule
\end{tabular}

What's going on here? In this example, we've defined our filtering
function, \(\text{f}\), to be
\texttt{lambda\ sf:\ sf{[}"\%"{]}.max()\ \textless{}\ 45}. This
filtering function will find the maximum \texttt{"\%"} value among all
entries in the grouped subframe, which we call \texttt{sf}. If the
maximum value is less than 45, then the filter function will return
\texttt{True} and all rows in that grouped subframe will appear in the
final output DataFrame.

Examine the DataFrame above. Notice how, in this preview of the first 9
rows, all entries from the years 1860 and 1912 appear. This means that
in 1860 and 1912, no candidate in that year won more than 45\% of the
total vote.

You may ask: how is the \texttt{groupby.filter} procedure different to
the boolean filtering we've seen previously? Boolean filtering considers
\emph{individual} rows when applying a boolean condition. For example,
the code \texttt{elections{[}elections{[}"\%"{]}\ \textless{}\ 45{]}}
will check the \texttt{"\%"} value of every single row in
\texttt{elections}; if it is less than 45, then that row will be kept in
the output. \texttt{groupby.filter}, in contrast, applies a boolean
condition \emph{across} all rows in a group. If not all rows in that
group satisfy the condition specified by the filter, the entire group
will be discarded in the output.

\hypertarget{aggregating-data-with-pivot-tables}{%
\section{Aggregating Data with Pivot
Tables}\label{aggregating-data-with-pivot-tables}}

We know now that \texttt{.groupby} gives us the ability to group and
aggregate data across our DataFrame. The examples above formed groups
using just one column in the DataFrame. It's possible to group by
multiple columns at once by passing in a list of columns names to
\texttt{.groupby}.

Let's consider the \texttt{babynames} dataset from last lecture. In this
problem, we will find the total number of baby names associated with
each sex for each year. To do this, we'll group by \emph{both} the
\texttt{"Year"} and \texttt{"Sex"} columns.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ urllib.request}
\ImportTok{import}\NormalTok{ os.path}

\CommentTok{\# Download data from the web directly}
\NormalTok{data\_url }\OperatorTok{=} \StringTok{"https://www.ssa.gov/oact/babynames/names.zip"}
\NormalTok{local\_filename }\OperatorTok{=} \StringTok{"data/babynames.zip"}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ os.path.exists(local\_filename): }\CommentTok{\# if the data exists don\textquotesingle{}t download again}
    \ControlFlowTok{with}\NormalTok{ urllib.request.urlopen(data\_url) }\ImportTok{as}\NormalTok{ resp, }\BuiltInTok{open}\NormalTok{(local\_filename, }\StringTok{\textquotesingle{}wb\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{        f.write(resp.read())}

        
\CommentTok{\# Load data without unzipping the file}
\ImportTok{import}\NormalTok{ zipfile}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ [] }
\ControlFlowTok{with}\NormalTok{ zipfile.ZipFile(local\_filename, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ zf:}
\NormalTok{    data\_files }\OperatorTok{=}\NormalTok{ [f }\ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ zf.filelist }\ControlFlowTok{if}\NormalTok{ f.filename[}\OperatorTok{{-}}\DecValTok{3}\NormalTok{:] }\OperatorTok{==} \StringTok{"txt"}\NormalTok{]}
    \KeywordTok{def}\NormalTok{ extract\_year\_from\_filename(fn):}
        \ControlFlowTok{return} \BuiltInTok{int}\NormalTok{(fn[}\DecValTok{3}\NormalTok{:}\DecValTok{7}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ f }\KeywordTok{in}\NormalTok{ data\_files:}
\NormalTok{        year }\OperatorTok{=}\NormalTok{ extract\_year\_from\_filename(f.filename)}
        \ControlFlowTok{with}\NormalTok{ zf.}\BuiltInTok{open}\NormalTok{(f) }\ImportTok{as}\NormalTok{ fp:}
\NormalTok{            df }\OperatorTok{=}\NormalTok{ pd.read\_csv(fp, names}\OperatorTok{=}\NormalTok{[}\StringTok{"Name"}\NormalTok{, }\StringTok{"Sex"}\NormalTok{, }\StringTok{"Count"}\NormalTok{])}
\NormalTok{            df[}\StringTok{"Year"}\NormalTok{] }\OperatorTok{=}\NormalTok{ year}
\NormalTok{            babynames.append(df)}
\NormalTok{babynames }\OperatorTok{=}\NormalTok{ pd.concat(babynames)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrr}
\toprule
{} &       Name & Sex &  Count &  Year \\
\midrule
0 &       Mary &   F &   7065 &  1880 \\
1 &       Anna &   F &   2604 &  1880 \\
2 &       Emma &   F &   2003 &  1880 \\
3 &  Elizabeth &   F &   1939 &  1880 \\
4 &     Minnie &   F &   1746 &  1880 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Find the total number of baby names associated with each sex for each year in the data}
\NormalTok{babynames.groupby([}\StringTok{"Year"}\NormalTok{, }\StringTok{"Sex"}\NormalTok{])[[}\StringTok{"Count"}\NormalTok{]].agg(}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llr}
\toprule
     &   &   Count \\
Year & Sex &         \\
\midrule
1880 & F &   90994 \\
     & M &  110490 \\
1881 & F &   91953 \\
     & M &  100737 \\
1882 & F &  107847 \\
     & M &  113686 \\
\bottomrule
\end{tabular}

Notice that both \texttt{"Year"} and \texttt{"Sex"} serve as the index
of the DataFrame (they are both rendered in bold). We've created a
\emph{multindex} where two different index values, the year and sex, are
used to uniquely identify each row.

This isn't the most intuitive way of representing this data -- and,
because multindexes have multiple dimensions in their index, they can
often be difficult to use.

Another strategy to aggregate across two columns is to create a pivot
table. You saw these back in
\href{https://inferentialthinking.com/chapters/08/3/Cross-Classifying_by_More_than_One_Variable.html\#pivot-tables-rearranging-the-output-of-group}{Data
8}. One set of values is used to create the index of the table; another
set is used to define the column names. The values contained in each
cell of the table correspond to the aggregated data for each
index-column pair.

The best way to understand pivot tables is to see one in action. Let's
return to our original goal of summing the total number of names
associated with each combination of year and sex. We'll call the
\texttt{pandas}
\href{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html}{\texttt{.pivot\_table}}
method to create a new table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textasciigrave{}pivot\_table\textasciigrave{} method is used to generate a Pandas pivot table}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{babynames.pivot\_table(index }\OperatorTok{=} \StringTok{"Year"}\NormalTok{, columns }\OperatorTok{=} \StringTok{"Sex"}\NormalTok{, values }\OperatorTok{=} \StringTok{"Count"}\NormalTok{, aggfunc }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrr}
\toprule
Sex &       F &       M \\
Year &         &         \\
\midrule
1880 &   90994 &  110490 \\
1881 &   91953 &  100737 \\
1882 &  107847 &  113686 \\
1883 &  112319 &  104625 \\
1884 &  129019 &  114442 \\
\bottomrule
\end{tabular}

Looks a lot better! Now, our DataFrame is structured with clear
index-column combinations. Each entry in the pivot table represents the
summed count of names for a given combination of \texttt{"Year"} and
\texttt{"Sex"}.

Let's take a closer look at the code implemented above.

\begin{itemize}
\tightlist
\item
  \texttt{index\ =\ "Year"} specifies the column name in the original
  DataFrame that should be used as the index of the pivot table
\item
  \texttt{columns\ =\ "Sex"} specifies the column name in the original
  DataFrame that should be used to generate the columns of the pivot
  table
\item
  \texttt{values\ =\ "Count"} indicates what values from the original
  DataFrame should be used to populate the entry for each index-column
  combination
\item
  \texttt{aggfunc\ =\ np.sum} tells \texttt{pandas} what function to use
  when aggregating the data specified by \texttt{values}. Here, we are
  \texttt{sum}ming the name counts for each pair of \texttt{"Year"} and
  \texttt{"Sex"}
\end{itemize}

We can even include multiple values in the index or columns of our pivot
tables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{babynames\_pivot }\OperatorTok{=}\NormalTok{ babynames.pivot\_table(}
\NormalTok{    index}\OperatorTok{=}\StringTok{"Year"}\NormalTok{,     }\CommentTok{\# the rows (turned into index)}
\NormalTok{    columns}\OperatorTok{=}\StringTok{"Sex"}\NormalTok{,    }\CommentTok{\# the column values}
\NormalTok{    values}\OperatorTok{=}\NormalTok{[}\StringTok{"Count"}\NormalTok{, }\StringTok{"Name"}\NormalTok{], }
\NormalTok{    aggfunc}\OperatorTok{=}\BuiltInTok{max}\NormalTok{,   }\CommentTok{\# group operation}
\NormalTok{)}
\NormalTok{babynames\_pivot.head(}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrll}
\toprule
{} & \multicolumn{2}{l}{Count} & \multicolumn{2}{l}{Name} \\
Sex &     F &     M &     F &       M \\
Year &       &       &       &         \\
\midrule
1880 &  7065 &  9655 &  Zula &    Zeke \\
1881 &  6919 &  8769 &  Zula &     Zeb \\
1882 &  8148 &  9557 &  Zula &     Zed \\
1883 &  8012 &  8894 &  Zula &    Zeno \\
1884 &  9217 &  9388 &  Zula &  Zollie \\
1885 &  9128 &  8756 &  Zula &  Zollie \\
\bottomrule
\end{tabular}

\hypertarget{joining-tables}{%
\section{Joining Tables}\label{joining-tables}}

When working on data science projects, we're unlikely to have absolutely
all the data we want contained in a single DataFrame -- a real-world
data scientist needs to grapple with data coming from multiple sources.
If we have access to multiple datasets with related information, we can
join two or more tables into a single DataFrame.

To put this into practice, we'll revisit the \texttt{elections} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

Say we want to understand the 2020 popularity of the names of each
presidential candidate. To do this, we'll need the combined data of
\texttt{babynames} \emph{and} \texttt{elections}.

We'll start by creating a new column containing the first name of each
presidential candidate. This will help us join each name in
\texttt{elections} to the corresponding name data in \texttt{babynames}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This \textasciigrave{}str\textasciigrave{} operation splits each candidate\textquotesingle{}s full name at each }
\CommentTok{\# blank space, then takes just the candidiate\textquotesingle{}s first name}
\NormalTok{elections[}\StringTok{"First Name"}\NormalTok{] }\OperatorTok{=}\NormalTok{ elections[}\StringTok{"Candidate"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.split().}\BuiltInTok{str}\NormalTok{[}\DecValTok{0}\NormalTok{]}
\NormalTok{elections.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlrl}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% & First Name \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 &     Andrew \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 &       John \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 &     Andrew \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 &       John \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 &     Andrew \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here, we\textquotesingle{}ll only consider \textasciigrave{}babynames\textasciigrave{} data from 2020}
\NormalTok{babynames\_2020 }\OperatorTok{=}\NormalTok{ babynames[babynames[}\StringTok{"Year"}\NormalTok{]}\OperatorTok{==}\DecValTok{2020}\NormalTok{]}
\NormalTok{babynames\_2020.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrr}
\toprule
{} &       Name & Sex &  Count &  Year \\
\midrule
0 &     Olivia &   F &  17641 &  2020 \\
1 &       Emma &   F &  15656 &  2020 \\
2 &        Ava &   F &  13160 &  2020 \\
3 &  Charlotte &   F &  13065 &  2020 \\
4 &     Sophia &   F &  13036 &  2020 \\
\bottomrule
\end{tabular}

Now, we're ready to join the two tables.
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{\texttt{pd.merge}}
is the \texttt{pandas} method used to join DataFrames together. The
\texttt{left} and \texttt{right} parameters are used to specify the
DataFrames to be joined. The \texttt{left\_on} and \texttt{right\_on}
parameters are assigned to the string names of the columns to be used
when performing the join. These two \texttt{on} parameters tell
\texttt{pandas} what values should act as pairing keys to determine
which rows to merge across the DataFrames. We'll talk more about this
idea of a pairing key next lecture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged }\OperatorTok{=}\NormalTok{ pd.merge(left }\OperatorTok{=}\NormalTok{ elections, right }\OperatorTok{=}\NormalTok{ babynames\_2020, }\OperatorTok{\textbackslash{}}
\NormalTok{                  left\_on }\OperatorTok{=} \StringTok{"First Name"}\NormalTok{, right\_on }\OperatorTok{=} \StringTok{"Name"}\NormalTok{)}
\NormalTok{merged.head()}
\CommentTok{\# Notice that pandas automatically specifies \textasciigrave{}Year\_x\textasciigrave{} and \textasciigrave{}Year\_y\textasciigrave{} }
\CommentTok{\# when both merged DataFrames have the same column name to avoid confusion}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlrlllrr}
\toprule
{} &  Year\_x &       Candidate &                  Party &  Popular vote & Result &          \% & First Name &    Name & Sex &  Count &  Year\_y \\
\midrule
0 &    1824 &  Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 &     Andrew &  Andrew &   F &     12 &    2020 \\
1 &    1824 &  Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 &     Andrew &  Andrew &   M &   6036 &    2020 \\
2 &    1828 &  Andrew Jackson &             Democratic &        642806 &    win &  56.203927 &     Andrew &  Andrew &   F &     12 &    2020 \\
3 &    1828 &  Andrew Jackson &             Democratic &        642806 &    win &  56.203927 &     Andrew &  Andrew &   M &   6036 &    2020 \\
4 &    1832 &  Andrew Jackson &             Democratic &        702735 &    win &  54.574789 &     Andrew &  Andrew &   F &     12 &    2020 \\
\bottomrule
\end{tabular}

\bookmarksetup{startatroot}

\hypertarget{data-cleaning-and-eda}{%
\chapter{Data Cleaning and EDA}\label{data-cleaning-and-eda}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Recognize common file formats
\item
  Categorize data by its variable type
\item
  Build awareness of issues with data faithfulness and develop targeted
  solutions
\end{itemize}

\end{tcolorbox}

In the past few lectures, we've learned that \texttt{pandas} is a
toolkit to restructure, modify, and explore a dataset. What we haven't
yet touched on is \emph{how} to make these data transformation
decisions. When we receive a new set of data from the ``real world,''
how do we know what processing we should do to convert this data into a
usable form?

\textbf{Data cleaning}, also called \textbf{data wrangling}, is the
process of transforming raw data to facilitate subsequent analysis. It
is often used to address issues like:

\begin{itemize}
\tightlist
\item
  Unclear structure or formatting
\item
  Missing or corrupted values
\item
  Unit conversions
\item
  \ldots and so on
\end{itemize}

\textbf{Exploratory Data Analysis (EDA)} is the process of understanding
a new dataset. It is an open-ended, informal analysis that involves
familiarizing ourselves with the variables present in the data,
discovering potential hypotheses, and identifying potential issues with
the data. This last point can often motivate further data cleaning to
address any problems with the dataset's format; because of this, EDA and
data cleaning are often thought of as an ``infinite loop,'' with each
process driving the other.

In this lecture, we will consider the key properties of data to consider
when performing data cleaning and EDA. In doing so, we'll develop a
``checklist'' of sorts for you to consider when approaching a new
dataset. Throughout this process, we'll build a deeper understanding of
this early (but very important!) stage of the data science lifecycle.

\hypertarget{structure}{%
\section{Structure}\label{structure}}

\hypertarget{file-format}{%
\subsection{File Format}\label{file-format}}

In the past two \texttt{pandas} lectures, we briefly touched on the idea
of file format: the way data is encoded in a file for storage.
Specifically, our \texttt{elections} and \texttt{babynames} datasets
were stored and loaded as CSVs:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{pd.read\_csv(}\StringTok{"data/elections.csv"}\NormalTok{).head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllrlr}
\toprule
{} &  Year &          Candidate &                  Party &  Popular vote & Result &          \% \\
\midrule
0 &  1824 &     Andrew Jackson &  Democratic-Republican &        151271 &   loss &  57.210122 \\
1 &  1824 &  John Quincy Adams &  Democratic-Republican &        113142 &    win &  42.789878 \\
2 &  1828 &     Andrew Jackson &             Democratic &        642806 &    win &  56.203927 \\
3 &  1828 &  John Quincy Adams &    National Republican &        500897 &   loss &  43.796073 \\
4 &  1832 &     Andrew Jackson &             Democratic &        702735 &    win &  54.574789 \\
\bottomrule
\end{tabular}

CSVs, which stand for \textbf{Comma-Separated Values}, are a common
tabular data format. To better understand the properties of a CSV, let's
take a look at the first few rows of the raw data file to see what it
looks like before being loaded into a DataFrame.

\begin{verbatim}
Year,Candidate,Party,Popular vote,Result,%

1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204

1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796

1828,Andrew Jackson,Democratic,642806,win,56.20392707
\end{verbatim}

Each row, or \textbf{record}, in the data is delimited by a newline.
Each column, or \textbf{field}, in the data is delimited by a comma
(hence, comma-separated!).

Another common file type is the \textbf{TSV (Tab-Separated Values)}. In
a TSV, records are still delimited by a newline, while fields are
delimited by \texttt{\textbackslash{}t} tab character. A TSV can be
loaded into \texttt{pandas} using \texttt{pd.read\_csv()} with the
\texttt{delimiter} parameter:
\texttt{pd.read\_csv("file\_name.tsv",\ delimiter="\textbackslash{}t")}.
A raw TSV file is shown below.

\begin{verbatim}
Year   Candidate   Party   Popular vote    Result  %

1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204

1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796

1828    Andrew Jackson  Democratic  642806  win 56.20392707
\end{verbatim}

\textbf{JSON (JavaScript Object Notation)} files behave similarly to
Python dictionaries. They can be loaded into \texttt{pandas} using
\texttt{pd.read\_json}. A raw JSON is shown below.

\begin{verbatim}
[

 {

   "Year": 1824,

   "Candidate": "Andrew Jackson",

   "Party": "Democratic-Republican",

   "Popular vote": 151271,

   "Result": "loss",

   "%": 57.21012204

 },
\end{verbatim}

\hypertarget{variable-types}{%
\subsection{Variable Types}\label{variable-types}}

After loading data into a file, it's a good idea to take the time to
understand what pieces of information are encoded in the dataset. In
particular, we want to identify what variable types are present in our
data. Broadly speaking, we can categorize variables into one of two
overarching types.

\textbf{Quantitative variables} describe some numeric quantity or
amount. We can sub-divide quantitative data into:

\begin{itemize}
\tightlist
\item
  \textbf{Continuous quantitative variables}: numeric data that can be
  measured on a continuous scale to arbitrary precision. Continuous
  variables do not have a strict set of possible values -- they can be
  recorded to any number of decimal places. For example, weights, GPA,
  or CO2 concentrations
\item
  \textbf{Discrete quantitative variables}: numeric data that can only
  take on a finite set of possible values. For example, someone's age or
  number of siblings.
\end{itemize}

\textbf{Qualitative variables}, also known as \textbf{categorical
variables}, describe data that isn't measuring some quantity or amount.
The sub-categories of categorical data are:

\begin{itemize}
\tightlist
\item
  \textbf{Ordinal qualitative variables}: categories with ordered
  levels. Specifically, ordinal variables are those where the difference
  between levels has no consistent, quantifiable meaning. For example, a
  Yelp rating or set of income brackets.
\item
  \textbf{Nominal qualitative variables}: categories with no specific
  order. For example, someone's political affiliation or Cal ID number.
\end{itemize}

\begin{figure}

{\centering \includegraphics{eda/images/variable.png}

}

\caption{Classification of variable types}

\end{figure}

\hypertarget{primary-and-foreign-keys}{%
\subsection{Primary and Foreign Keys}\label{primary-and-foreign-keys}}

Last time, we introduced \texttt{.merge} as the \texttt{pandas} method
for joining multiple DataFrames together. In our discussion of joins, we
touched on the idea of using a ``key'' to determine what rows should be
merged from each table. Let's take a moment to examine this idea more
closely.

The \textbf{primary key} is the column or set of columns in a table that
determine the values of the remaining columns. It can be thought of as
the unique identifier for each individual row in the table. For example,
a table of Data 100 students might use each student's Cal ID as the
primary key.

\begin{tabular}{lrll}
\toprule
{} &      Cal ID &   Name &             Major \\
\midrule
0 &  3034619471 &   Oski &      Data Science \\
1 &  3035619472 &  Ollie &  Computer Science \\
2 &  3025619473 &  Orrie &      Data Science \\
3 &  3046789372 &  Ollie &         Economics \\
\bottomrule
\end{tabular}

The \textbf{foreign key} is the column or set of columns in a table that
reference primary keys in other tables. Knowing a dataset's foreign keys
can be useful when assigning the \texttt{left\_on} and
\texttt{right\_on} parameters of \texttt{.merge}. In the table of office
hour tickets below, \texttt{"Cal\ ID"} is a foreign key referencing the
previous table.

\begin{tabular}{lrrl}
\toprule
{} &  OH Request &      Cal ID &  Question \\
\midrule
0 &           1 &  3034619471 &   HW 2 Q1 \\
1 &           2 &  3035619472 &   HW 2 Q3 \\
2 &           3 &  3025619473 &  Lab 3 Q4 \\
3 &           4 &  3035619472 &   HW 2 Q7 \\
\bottomrule
\end{tabular}

\hypertarget{granularity-scope-and-temporality}{%
\section{Granularity, Scope, and
Temporality}\label{granularity-scope-and-temporality}}

After understanding the structure of the dataset, the next task is to
determine what exactly the data represents. We'll do so by considering
the data's granularity, scope, and temporality.

The \textbf{granularity} of a dataset is what a single row represents.
You can also think of it as the level of detail included in the data. To
determine the data's granularity, ask: what does each row in the dataset
represent? Fine-grained data contains a high level of detail, with a
single row representing a small individual unit. For example, each
record may represent one person. Coarse-grained data is encoded such
that a single row represents a large individual unit -- for example,
each record may represent a group of people.

The \textbf{scope} of a dataset is the subset of the population covered
by the data. If we were investigating student performance in Data
Science courses, a dataset with narrow scope might encompass all
students enrolled in Data 100; a dataset with expansive scope might
encompass all students in California.

The \textbf{temporality} of a dataset describes the time period over
which the data was collected. To fully understand the temporality of the
data, it may be necessary to standardize timezones or inspect recurring
time-based trends in the data (Do patterns recur in 24-hour patterns?
Over the course of a month? Seasonally?).

\hypertarget{faithfulness}{%
\section{Faithfulness}\label{faithfulness}}

At this stage in our data cleaning and EDA workflow, we've achieved
quite a lot: we've identified how our data is structured, come to terms
with what information it encodes, and gained insight as to how it was
generated. Throughout this process, we should always recall the original
intent of our work in Data Science -- to use data to better understand
and model the real world. To achieve this goal, we need to ensure that
the data we use is faithful to reality; that is, that our data
accurately captures the ``real world.''

Data used in research or industry is often ``messy'' -- there may be
errors or inaccuracies that impact the faithfulness of the dataset.
Signs that data may not be faithful include:

\begin{itemize}
\tightlist
\item
  Unrealistic or ``incorrect'' values, such as negative counts,
  locations that don't exist, or dates set in the future
\item
  Violations of obvious dependencies, like an age that does not match a
  birthday
\item
  Clear signs that data was entered by hand, which can lead to spelling
  errors or fields that are incorrectly shifted
\item
  Signs of data falsification, such as fake email addresses or repeated
  use of the same names
\item
  Duplicated records or fields containing the same information
\end{itemize}

A common issue encountered with real-world datasets is that of missing
data. One strategy to resolve this is to simply drop any records with
missing values from the dataset. This does, however, introduce the risk
of inducing biases -- it is possible that the missing or corrupt records
may be systemically related to some feature of interest in the data.

Another method to address missing data is to perform
\textbf{imputation}: infer the missing values using other data available
in the dataset. There is a wide variety of imputation techniques that
can be implemented; some of the most common are listed below.

\begin{itemize}
\tightlist
\item
  Average imputation: replace missing values with the average value for
  that field
\item
  Hot deck imputation: replace missing values with some random value
\item
  Regression imputation: develop a model to predict missing values
\item
  Multiple imputation: replace missing values with multiple random
  values
\end{itemize}

Regardless of the strategy used to deal with missing data, we should
think carefully about \emph{why} particular records or fields may be
missing -- this can help inform whether or not the absence of these
values is signficant in some meaningful way.

\bookmarksetup{startatroot}

\hypertarget{eda-demo-tuberculosis-in-the-united-states}{%
\chapter{EDA Demo: Tuberculosis in the United
States}\label{eda-demo-tuberculosis-in-the-united-states}}

Now, let's follow this data-cleaning and EDA workflow to see what can we
say about the presence of Tuberculosis in the United States!

We will examine the data included in the
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w\#T1_down}{original
CDC article} published in 2021.

\hypertarget{csvs-and-field-names}{%
\section{CSVs and Field Names}\label{csvs-and-field-names}}

Suppose Table 1 was saved as a CSV file located in
\texttt{data/cdc\_tuberculosis.csv}.

We can then explore the CSV (which is a text file, and does not contain
binary-encoded data) in many ways: 1. Using a text editor like emacs,
vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only),
Excel, Google Sheets, etc. 3. The Python file object 4. pandas, using
\texttt{pd.read\_csv()}

1, 2. Let's start with the first two so we really solidify the idea of a
CSV as \textbf{rectangular data (i.e., tabular data) stored as
comma-separated values}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Next, let's try using the Python file object. Let's check out the
  first three lines:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ f:}
        \BuiltInTok{print}\NormalTok{(row)}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
,No. of TB cases,,,TB incidence,,

U.S. jurisdiction,2019,2020,2021,2019,2020,2021

Total,"8,900","7,173","7,860",2.71,2.16,2.37

Alabama,87,72,92,1.77,1.43,1.83
\end{verbatim}

Whoa, why are there blank lines interspaced between the lines of the
CSV?

You may recall that all line breaks in text files are encoded as the
special newline character \texttt{\textbackslash{}n}. Python's
\texttt{print()} prints each string (including the newline), and an
additional newline on top of that.

If you're curious, we can use the \texttt{repr()} function to return the
raw string with all special characters:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    i }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{for}\NormalTok{ row }\KeywordTok{in}\NormalTok{ f:}
        \BuiltInTok{print}\NormalTok{(}\BuiltInTok{repr}\NormalTok{(row)) }\CommentTok{\# print raw strings}
\NormalTok{        i }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{\textgreater{}} \DecValTok{3}\NormalTok{:}
            \ControlFlowTok{break}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
',No. of TB cases,,,TB incidence,,\n'
'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\n'
'Total,"8,900","7,173","7,860",2.71,2.16,2.37\n'
'Alabama,87,72,92,1.77,1.43,1.83\n'
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Finally, let's see the tried-and-true Data 100 approach: pandas.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{)}
\NormalTok{tb\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} &         Unnamed: 0 & No. of TB cases & Unnamed: 2 & Unnamed: 3 &  TB incidence &  Unnamed: 5 &  Unnamed: 6 \\
\midrule
0 &  U.S. jurisdiction &            2019 &       2020 &       2021 &       2019.00 &     2020.00 &     2021.00 \\
1 &              Total &           8,900 &      7,173 &      7,860 &          2.71 &        2.16 &        2.37 \\
2 &            Alabama &              87 &         72 &         92 &          1.77 &        1.43 &        1.83 \\
3 &             Alaska &              58 &         58 &         58 &          7.91 &        7.92 &        7.92 \\
4 &            Arizona &             183 &        136 &        129 &          2.51 &        1.89 &        1.77 \\
\bottomrule
\end{tabular}

Wait, what's up with the ``Unnamed'' column names? And the first row,
for that matter?

Congratulations -- you're ready to wrangle your data. Because of how
things are stored, we'll need to clean the data a bit to name our
columns better.

A reasonable first step is to identify the row with the right header.
The \texttt{pd.read\_csv()} function
(\href{https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html}{documentation})
has the convenient \texttt{header} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\CommentTok{\# row index}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} & U.S. jurisdiction &   2019 &   2020 &   2021 &  2019.1 &  2020.1 &  2021.1 \\
\midrule
0 &             Total &  8,900 &  7,173 &  7,860 &    2.71 &    2.16 &    2.37 \\
1 &           Alabama &     87 &     72 &     92 &    1.77 &    1.43 &    1.83 \\
2 &            Alaska &     58 &     58 &     58 &    7.91 &    7.92 &    7.92 \\
3 &           Arizona &    183 &    136 &    129 &    2.51 &    1.89 &    1.77 \\
4 &          Arkansas &     64 &     59 &     69 &    2.12 &    1.96 &    2.28 \\
\bottomrule
\end{tabular}

Wait\ldots but now we can't differentiate betwen the ``Number of TB
cases'' and ``TB incidence'' year columns. pandas has tried to make our
lives easier by automatically adding ``.1'' to the latter columns, but
this doesn't help us as humans understand the data.

We can do this manually with \texttt{df.rename()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html?highlight=rename\#pandas.DataFrame.rename}{documentation}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rename\_dict }\OperatorTok{=}\NormalTok{ \{}\StringTok{\textquotesingle{}2019\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2019\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2020\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2020\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2021\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB cases 2021\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2019.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2019\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2020.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2020\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}2021.1\textquotesingle{}}\NormalTok{: }\StringTok{\textquotesingle{}TB incidence 2021\textquotesingle{}}\NormalTok{\}}
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ tb\_df.rename(columns}\OperatorTok{=}\NormalTok{rename\_dict)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllllrrr}
\toprule
{} & U.S. jurisdiction & TB cases 2019 & TB cases 2020 & TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
0 &             Total &         8,900 &         7,173 &         7,860 &               2.71 &               2.16 &               2.37 \\
1 &           Alabama &            87 &            72 &            92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &            58 &            58 &            58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &           183 &           136 &           129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &            64 &            59 &            69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\hypertarget{record-granularity}{%
\section{Record Granularity}\label{record-granularity}}

You might already be wondering: What's up with that first record?

Row 0 is what we call a \textbf{rollup record}, or summary record. It's
often useful when displaying tables to humans. The \textbf{granularity}
of record 0 (Totals) vs the rest of the records (States) is different.

Okay, EDA step two. How was the rollup record aggregated?

Let's check if Total TB cases is the sum of all state TB cases. If we
sum over all rows, we should get \textbf{2x} the total cases in each of
our TB cases by year (why?).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.}\BuiltInTok{sum}\NormalTok{(axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                                                  0 \\
\midrule
U.S. jurisdiction &  TotalAlabamaAlaskaArizonaArkansasCaliforniaCol... \\
TB cases 2019     &  8,9008758183642,111666718245583029973261085237... \\
TB cases 2020     &  7,1737258136591,706525417194122219282169239376... \\
TB cases 2021     &  7,8609258129691,750585443194992281064255127494... \\
TB incidence 2019 &                                             109.94 \\
TB incidence 2020 &                                              93.09 \\
TB incidence 2021 &                                             102.94 \\
\bottomrule
\end{tabular}

Whoa, what's going on? Check out the column types:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.dtypes}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &        0 \\
\midrule
U.S. jurisdiction &   object \\
TB cases 2019     &   object \\
TB cases 2020     &   object \\
TB cases 2021     &   object \\
TB incidence 2019 &  float64 \\
TB incidence 2020 &  float64 \\
TB incidence 2021 &  float64 \\
\bottomrule
\end{tabular}

Looks like those commas are causing all TB cases to be read as the
\texttt{object} datatype, or \textbf{storage type} (close to the Python
string datatype), so pandas is concatenating strings instead of adding
integers.

Fortunately \texttt{read\_csv} also has a \texttt{thousands} parameter
(https://pandas.pydata.org/docs/reference/api/pandas.read\_csv.html):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# improve readability: chaining method calls with outer parentheses/line breaks}
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    pd.read\_csv(}\StringTok{"data/cdc\_tuberculosis.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{1}\NormalTok{, thousands}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{rename\_dict)}
\NormalTok{)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
0 &             Total &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
1 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                                                  0 \\
\midrule
U.S. jurisdiction &  TotalAlabamaAlaskaArizonaArkansasCaliforniaCol... \\
TB cases 2019     &                                              17800 \\
TB cases 2020     &                                              14346 \\
TB cases 2021     &                                              15720 \\
TB incidence 2019 &                                             109.94 \\
TB incidence 2020 &                                              93.09 \\
TB incidence 2021 &                                             102.94 \\
\bottomrule
\end{tabular}

The Total TB cases look right. Phew!

Let's just look at the records with \textbf{state-level granularity}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_tb\_df }\OperatorTok{=}\NormalTok{ tb\_df[}\DecValTok{1}\NormalTok{:]}
\NormalTok{state\_tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
\midrule
1 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
2 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
3 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
4 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
5 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 \\
\bottomrule
\end{tabular}

\hypertarget{gather-more-data-census}{%
\section{Gather More Data: Census}\label{gather-more-data-census}}

U.S. Census population estimates
\href{https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html}{source}
(2019),
\href{https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html}{source}
(2020-2021).

Running the below cells cleans the data. There are a few new methods
here: * \texttt{df.convert\_dtypes()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html}{documentation})
conveniently converts all float dtypes into ints and is out of scope for
the class. * \texttt{df.drop\_na()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html}{documentation})
will be explained in more detail next time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2010s census data}
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/nst{-}est2019{-}01.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{3}\NormalTok{, thousands}\OperatorTok{=}\StringTok{","}\NormalTok{)}
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    census\_2010s\_df}
\NormalTok{    .reset\_index()}
\NormalTok{    .drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"index"}\NormalTok{, }\StringTok{"Census"}\NormalTok{, }\StringTok{"Estimates Base"}\NormalTok{])}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"Unnamed: 0"}\NormalTok{: }\StringTok{"Geographic Area"}\NormalTok{\})}
\NormalTok{    .convert\_dtypes()                 }\CommentTok{\# "smart" converting of columns, use at your own risk}
\NormalTok{    .dropna()                         }\CommentTok{\# we\textquotesingle{}ll introduce this next time}
\NormalTok{)}
\NormalTok{census\_2010s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ census\_2010s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.strip(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{)}

\CommentTok{\# with pd.option\_context(\textquotesingle{}display.min\_rows\textquotesingle{}, 30): \# shows more rows}
\CommentTok{\#     display(census\_2010s\_df)}
    
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrr}
\toprule
{} & Geographic Area &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
\midrule
0 &   United States &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
1 &       Northeast &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
2 &         Midwest &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
3 &           South &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
4 &            West &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

Occasionally, you will want to modify code that you have imported. To
reimport those modifications you can either use the python importlib
library:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ importlib }\ImportTok{import} \BuiltInTok{reload}
\BuiltInTok{reload}\NormalTok{(utils)}
\end{Highlighting}
\end{Shaded}

or use iPython magic which will intelligently import code when files
change:

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{\%}\NormalTok{load\_ext autoreload}
\OperatorTok{\%}\NormalTok{autoreload }\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# census 2020s data}
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/NST{-}EST2022{-}POP.csv"}\NormalTok{, header}\OperatorTok{=}\DecValTok{3}\NormalTok{, thousands}\OperatorTok{=}\StringTok{","}\NormalTok{)}
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    census\_2020s\_df}
\NormalTok{    .reset\_index()}
\NormalTok{    .drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"index"}\NormalTok{, }\StringTok{"Unnamed: 1"}\NormalTok{])}
\NormalTok{    .rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"Unnamed: 0"}\NormalTok{: }\StringTok{"Geographic Area"}\NormalTok{\})}
\NormalTok{    .convert\_dtypes()                 }\CommentTok{\# "smart" converting of columns, use at your own risk}
\NormalTok{    .dropna()                         }\CommentTok{\# we\textquotesingle{}ll introduce this next time}
\NormalTok{)}
\NormalTok{census\_2020s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ census\_2020s\_df[}\StringTok{\textquotesingle{}Geographic Area\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.strip(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{)}

\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrr}
\toprule
{} & Geographic Area &       2020 &       2021 &       2022 \\
\midrule
0 &   United States &  331511512 &  332031554 &  333287557 \\
1 &       Northeast &   57448898 &   57259257 &   57040406 \\
2 &         Midwest &   68961043 &   68836505 &   68787595 \\
3 &           South &  126450613 &  127346029 &  128716192 \\
4 &            West &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

\hypertarget{joining-data-on-primary-keys}{%
\section{Joining Data on Primary
Keys}\label{joining-data-on-primary-keys}}

Time to \texttt{merge}! Here we use the DataFrame method
\texttt{df1.merge(right=df2,\ ...)} on DataFrame \texttt{df1}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{documentation}).
Contrast this with the function
\texttt{pd.merge(left=df1,\ right=df2,\ ...)}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.merge.html?highlight=pandas\%20merge\#pandas.merge}{documentation}).
Feel free to use either.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# merge TB dataframe with two US census dataframes}
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df,}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df,}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrlrrrrrrrrrrlrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 & Geographic Area\_x &      2010 &      2011 &      2012 &      2013 &      2014 &      2015 &      2016 &      2017 &      2018 &      2019 & Geographic Area\_y &      2020 &      2021 &      2022 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &           Alabama &   4785437 &   4799069 &   4815588 &   4830081 &   4841799 &   4852347 &   4863525 &   4874486 &   4887681 &   4903185 &           Alabama &   5031362 &   5049846 &   5074296 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &            Alaska &    713910 &    722128 &    730443 &    737068 &    736283 &    737498 &    741456 &    739700 &    735139 &    731545 &            Alaska &    732923 &    734182 &    733583 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &           Arizona &   6407172 &   6472643 &   6554978 &   6632764 &   6730413 &   6829676 &   6941072 &   7044008 &   7158024 &   7278717 &           Arizona &   7179943 &   7264877 &   7359197 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &          Arkansas &   2921964 &   2940667 &   2952164 &   2959400 &   2967392 &   2978048 &   2989918 &   3001345 &   3009733 &   3017804 &          Arkansas &   3014195 &   3028122 &   3045637 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &        California &  37319502 &  37638369 &  37948800 &  38260787 &  38596972 &  38918045 &  39167117 &  39358497 &  39461588 &  39512223 &        California &  39501653 &  39142991 &  39029342 \\
\bottomrule
\end{tabular}

This is a little unwieldy. We could either drop the unneeded columns
now, or just merge on smaller census DataFrames. Let's do the latter.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# try merging again, but cleaner this time}
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df[[}\StringTok{"Geographic Area"}\NormalTok{, }\StringTok{"2019"}\NormalTok{]],}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .drop(columns}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df[[}\StringTok{"Geographic Area"}\NormalTok{, }\StringTok{"2020"}\NormalTok{, }\StringTok{"2021"}\NormalTok{]],}
\NormalTok{           left\_on}\OperatorTok{=}\StringTok{"U.S. jurisdiction"}\NormalTok{, right\_on}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{    .drop(columns}\OperatorTok{=}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 \\
\bottomrule
\end{tabular}

\hypertarget{reproducing-data-compute-incidence}{%
\section{Reproducing Data: Compute
Incidence}\label{reproducing-data-compute-incidence}}

Let's recompute incidence to make sure we know where the original CDC
numbers came from.

From the
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w\#T1_down}{CDC
report}: TB incidence is computed as ``Cases per 100,000 persons using
mid-year population estimates from the U.S. Census Bureau.''

If we define a group as 100,000 people, then we can compute the TB
incidence for a given state population as

\[\text{TB incidence} = \frac{\text{TB cases in population}}{\text{groups in population}} = \frac{\text{TB cases in population}}{\text{population}/100000} \]

\[= \frac{\text{TB cases in population}}{\text{population}} \times 100000\]

Let's try this for 2019:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df[}\StringTok{"recompute incidence 2019"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\StringTok{"TB cases 2019"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\StringTok{"2019"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 &  recompute incidence 2019 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 &                  1.774357 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 &                  7.928425 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 &                  2.514179 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 &                  2.120747 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 &                  5.342651 \\
\bottomrule
\end{tabular}

Awesome!!!

Let's use a for-loop and Python format strings to compute TB incidence
for all years. Python f-strings are just used for the purposes of this
demo, but they're handy to know when you explore data beyond this course
(\href{https://docs.python.org/3/tutorial/inputoutput.html}{Python
documentation}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# recompute incidence for all years}
\ControlFlowTok{for}\NormalTok{ year }\KeywordTok{in}\NormalTok{ [}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{]:}
\NormalTok{    tb\_census\_df[}\SpecialStringTok{f"recompute incidence }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\SpecialStringTok{f"TB cases }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrrrrrrrrrrrr}
\toprule
{} & U.S. jurisdiction &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &      2019 &      2020 &      2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
0 &           Alabama &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &   4903185 &   5031362 &   5049846 &                  1.774357 &                  1.431024 &                  1.821838 \\
1 &            Alaska &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &    731545 &    732923 &    734182 &                  7.928425 &                  7.913519 &                  7.899949 \\
2 &           Arizona &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &   7278717 &   7179943 &   7264877 &                  2.514179 &                  1.894165 &                  1.775667 \\
3 &          Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &   3017804 &   3014195 &   3028122 &                  2.120747 &                  1.957405 &                  2.278640 \\
4 &        California &           2111 &           1706 &           1750 &               5.35 &               4.32 &               4.46 &  39512223 &  39501653 &  39142991 &                  5.342651 &                  4.318807 &                  4.470788 \\
\bottomrule
\end{tabular}

These numbers look pretty close!!! There are a few errors in the
hundredths place, particularly in 2021. It may be useful to further
explore reasons behind this discrepancy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df.describe()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &          2019 &          2020 &          2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
count &      51.000000 &      51.000000 &      51.000000 &          51.000000 &          51.000000 &          51.000000 &  5.100000e+01 &  5.100000e+01 &  5.100000e+01 &                 51.000000 &                 51.000000 &                 51.000000 \\
mean  &     174.509804 &     140.647059 &     154.117647 &           2.102549 &           1.782941 &           1.971961 &  6.436069e+06 &  6.500226e+06 &  6.510423e+06 &                  2.104969 &                  1.784655 &                  1.969928 \\
std   &     341.738752 &     271.055775 &     286.781007 &           1.498745 &           1.337414 &           1.478468 &  7.360660e+06 &  7.408168e+06 &  7.394300e+06 &                  1.500236 &                  1.338263 &                  1.474929 \\
min   &       1.000000 &       0.000000 &       2.000000 &           0.170000 &           0.000000 &           0.210000 &  5.787590e+05 &  5.776050e+05 &  5.794830e+05 &                  0.172783 &                  0.000000 &                  0.210049 \\
25\%   &      25.500000 &      29.000000 &      23.000000 &           1.295000 &           1.210000 &           1.235000 &  1.789606e+06 &  1.820311e+06 &  1.844920e+06 &                  1.297485 &                  1.211433 &                  1.233905 \\
50\%   &      70.000000 &      67.000000 &      69.000000 &           1.800000 &           1.520000 &           1.700000 &  4.467673e+06 &  4.507445e+06 &  4.506589e+06 &                  1.808606 &                  1.521612 &                  1.694502 \\
75\%   &     180.500000 &     139.000000 &     150.000000 &           2.575000 &           1.990000 &           2.220000 &  7.446805e+06 &  7.451987e+06 &  7.502811e+06 &                  2.577577 &                  1.993607 &                  2.219482 \\
max   &    2111.000000 &    1706.000000 &    1750.000000 &           7.910000 &           7.920000 &           7.920000 &  3.951222e+07 &  3.950165e+07 &  3.914299e+07 &                  7.928425 &                  7.913519 &                  7.899949 \\
\bottomrule
\end{tabular}

\hypertarget{bonus-eda-reproducing-the-reported-statistic}{%
\section{Bonus EDA: Reproducing the reported
statistic}\label{bonus-eda-reproducing-the-reported-statistic}}

\textbf{How do we reproduce that reported statistic in the original
\href{https://www.cdc.gov/mmwr/volumes/71/wr/mm7112a1.htm?s_cid=mm7112a1_w}{CDC
report}?}

\begin{quote}
Reported TB incidence (cases per 100,000 persons) increased
\textbf{9.4\%}, from \textbf{2.2} during 2020 to \textbf{2.4} during
2021 but was lower than incidence during 2019 (2.7). Increases occurred
among both U.S.-born and non--U.S.-born persons.
\end{quote}

This is TB incidence computed across the entire U.S. population! How do
we reproduce this * We need to reproduce the ``Total'' TB incidences in
our rolled record. * But our current \texttt{tb\_census\_df} only has 51
entries (50 states plus Washington, D.C.). There is no rolled record. *
What happened\ldots?

Let's get exploring!

Before we keep exploring, we'll set all indexes to more meaningful
values, instead of just numbers that pertained to some row at some
point. This will make our cleaning slightly easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df }\OperatorTok{=}\NormalTok{ tb\_df.set\_index(}\StringTok{"U.S. jurisdiction"}\NormalTok{)}
\NormalTok{tb\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
U.S. jurisdiction &                &                &                &                    &                    &                    \\
\midrule
Total             &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
Alabama           &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
Alaska            &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
Arizona           &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
Arkansas          &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2010s\_df }\OperatorTok{=}\NormalTok{ census\_2010s\_df.set\_index(}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
United States   &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2020s\_df }\OperatorTok{=}\NormalTok{ census\_2020s\_df.set\_index(}\StringTok{"Geographic Area"}\NormalTok{)}
\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrr}
\toprule
{} &       2020 &       2021 &       2022 \\
Geographic Area &            &            &            \\
\midrule
United States   &  331511512 &  332031554 &  333287557 \\
Northeast       &   57448898 &   57259257 &   57040406 \\
Midwest         &   68961043 &   68836505 &   68787595 \\
South           &  126450613 &  127346029 &  128716192 \\
West            &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

It turns out that our merge above only kept state records, even though
our original \texttt{tb\_df} had the ``Total'' rolled record:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_df.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 \\
U.S. jurisdiction &                &                &                &                    &                    &                    \\
\midrule
Total             &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 \\
Alabama           &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 \\
Alaska            &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 \\
Arizona           &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 \\
Arkansas          &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 \\
\bottomrule
\end{tabular}

Recall that merge by default does an \textbf{inner} merge by default,
meaning that it only preserves keys that are present in \textbf{both}
DataFrames.

The rolled records in our census dataframes have different
\texttt{Geographic\ Area} fields, which was the key we merged on:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
United States   &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

The Census DataFrame has several rolled records. The aggregate record we
are looking for actually has the Geographic Area named ``United
States''.

One straightforward way to get the right merge is to rename the value
itself. Because we now have the Geographic Area index, we'll use
\texttt{df.rename()}
(\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html}{documentation}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rename rolled record for 2010s}
\NormalTok{census\_2010s\_df.rename(index}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}United States\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{\}, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{census\_2010s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &       2010 &       2011 &       2012 &       2013 &       2014 &       2015 &       2016 &       2017 &       2018 &       2019 \\
Geographic Area &            &            &            &            &            &            &            &            &            &            \\
\midrule
Total           &  309321666 &  311556874 &  313830990 &  315993715 &  318301008 &  320635163 &  322941311 &  324985539 &  326687501 &  328239523 \\
Northeast       &   55380134 &   55604223 &   55775216 &   55901806 &   56006011 &   56034684 &   56042330 &   56059240 &   56046620 &   55982803 \\
Midwest         &   66974416 &   67157800 &   67336743 &   67560379 &   67745167 &   67860583 &   67987540 &   68126781 &   68236628 &   68329004 \\
South           &  114866680 &  116006522 &  117241208 &  118364400 &  119624037 &  120997341 &  122351760 &  123542189 &  124569433 &  125580448 \\
West            &   72100436 &   72788329 &   73477823 &   74167130 &   74925793 &   75742555 &   76559681 &   77257329 &   77834820 &   78347268 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# same, but for 2020s rename rolled record}
\NormalTok{census\_2020s\_df.rename(index}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}United States\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{\}, inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{census\_2020s\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrr}
\toprule
{} &       2020 &       2021 &       2022 \\
Geographic Area &            &            &            \\
\midrule
Total           &  331511512 &  332031554 &  333287557 \\
Northeast       &   57448898 &   57259257 &   57040406 \\
Midwest         &   68961043 &   68836505 &   68787595 \\
South           &  126450613 &  127346029 &  128716192 \\
West            &   78650958 &   78589763 &   78743364 \\
\bottomrule
\end{tabular}

Next let's rerun our merge. Note the different chaining, because we are
now merging on indexes (\texttt{df.merge()}
\href{https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html}{documentation}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tb\_census\_df }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    tb\_df}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2010s\_df[[}\StringTok{"2019"}\NormalTok{]],}
\NormalTok{           left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    .merge(right}\OperatorTok{=}\NormalTok{census\_2020s\_df[[}\StringTok{"2020"}\NormalTok{, }\StringTok{"2021"}\NormalTok{]],}
\NormalTok{           left\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{, right\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{)}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &       2019 &       2020 &       2021 \\
\midrule
Total    &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 &  328239523 &  331511512 &  332031554 \\
Alabama  &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &    4903185 &    5031362 &    5049846 \\
Alaska   &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &     731545 &     732923 &     734182 \\
Arizona  &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &    7278717 &    7179943 &    7264877 \\
Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &    3017804 &    3014195 &    3028122 \\
\bottomrule
\end{tabular}

Finally, let's recompute our incidences:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# recompute incidence for all years}
\ControlFlowTok{for}\NormalTok{ year }\KeywordTok{in}\NormalTok{ [}\DecValTok{2019}\NormalTok{, }\DecValTok{2020}\NormalTok{, }\DecValTok{2021}\NormalTok{]:}
\NormalTok{    tb\_census\_df[}\SpecialStringTok{f"recompute incidence }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{] }\OperatorTok{=}\NormalTok{ tb\_census\_df[}\SpecialStringTok{f"TB cases }\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{/}\NormalTok{tb\_census\_df[}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{year}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{]}\OperatorTok{*}\DecValTok{100000}
\NormalTok{tb\_census\_df.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrrrr}
\toprule
{} &  TB cases 2019 &  TB cases 2020 &  TB cases 2021 &  TB incidence 2019 &  TB incidence 2020 &  TB incidence 2021 &       2019 &       2020 &       2021 &  recompute incidence 2019 &  recompute incidence 2020 &  recompute incidence 2021 \\
\midrule
Total    &           8900 &           7173 &           7860 &               2.71 &               2.16 &               2.37 &  328239523 &  331511512 &  332031554 &                  2.711435 &                  2.163726 &                  2.367245 \\
Alabama  &             87 &             72 &             92 &               1.77 &               1.43 &               1.83 &    4903185 &    5031362 &    5049846 &                  1.774357 &                  1.431024 &                  1.821838 \\
Alaska   &             58 &             58 &             58 &               7.91 &               7.92 &               7.92 &     731545 &     732923 &     734182 &                  7.928425 &                  7.913519 &                  7.899949 \\
Arizona  &            183 &            136 &            129 &               2.51 &               1.89 &               1.77 &    7278717 &    7179943 &    7264877 &                  2.514179 &                  1.894165 &                  1.775667 \\
Arkansas &             64 &             59 &             69 &               2.12 &               1.96 &               2.28 &    3017804 &    3014195 &    3028122 &                  2.120747 &                  1.957405 &                  2.278640 \\
\bottomrule
\end{tabular}

We reproduced the total U.S. incidences correctly!

We're almost there. Let's revisit the quote:

\begin{quote}
Reported TB incidence (cases per 100,000 persons) increased
\textbf{9.4\%}, from \textbf{2.2} during 2020 to \textbf{2.4} during
2021 but was lower than incidence during 2019 (2.7). Increases occurred
among both U.S.-born and non--U.S.-born persons.
\end{quote}

Recall that percent change from \(A\) to \(B\) is computed as
\(\text{percent change} = \frac{B - A}{A} \times 100\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{incidence\_2020 }\OperatorTok{=}\NormalTok{ tb\_census\_df.loc[}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}recompute incidence 2020\textquotesingle{}}\NormalTok{]}
\NormalTok{incidence\_2020}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.1637257652759883
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{incidence\_2021 }\OperatorTok{=}\NormalTok{ tb\_census\_df.loc[}\StringTok{\textquotesingle{}Total\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}recompute incidence 2021\textquotesingle{}}\NormalTok{]}
\NormalTok{incidence\_2021}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
2.3672448914298068
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{difference }\OperatorTok{=}\NormalTok{ (incidence\_2021 }\OperatorTok{{-}}\NormalTok{ incidence\_2020)}\OperatorTok{/}\NormalTok{incidence\_2020 }\OperatorTok{*} \DecValTok{100}
\NormalTok{difference}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
9.405957511804143
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{regular-expressions}{%
\chapter{Regular Expressions}\label{regular-expressions}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Understand Python string manipulation, Pandas Series methods
\item
  Parse and create regex, with a reference table
\item
  Use vocabulary (closure, metacharater, groups, etc.) to describe regex
  metacharacters
\end{itemize}

\end{tcolorbox}

\hypertarget{why-work-with-text}{%
\section{Why Work with Text?}\label{why-work-with-text}}

Last lecture, we learned of the difference between quantitative and
qualitative variable types. The latter includes string data - the
primary focus of today's lecture. In this note, we'll discuss the
necessary tools to manipulate text: Python string manipulation and
regular expressions.

There are two main reasons for working with text.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Canonicalization: Convert data that has multiple formats into a
  standard form.

  \begin{itemize}
  \tightlist
  \item
    By manipulating text, we can join tables with mismatched string
    labels
  \end{itemize}
\item
  Extract information into a new feature.

  \begin{itemize}
  \tightlist
  \item
    For example, we can extract date and time features from text
  \end{itemize}
\end{enumerate}

\hypertarget{python-string-methods}{%
\section{Python String Methods}\label{python-string-methods}}

First, we'll introduce a few methods useful for string manipulation. The
following table includes a number of string operations supported by
Python and \texttt{pandas}. The Python functions operate on a single
string, while their equivalent in \texttt{pandas} are
\textbf{vectorized} - they operate on a Series of string data.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3889}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Python
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pandas (Series)
\end{minipage} \\
\midrule()
\endhead
Transformation & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.lower(\_)}
\item
  \texttt{s.upper(\_)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.lower(\_)}
\item
  \texttt{ser.str.upper(\_)}
\end{itemize}
\end{minipage} \\
Replacement + Deletion & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.replace(\_)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.replace(\_)}
\end{itemize}
\end{minipage} \\
Split & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s.split(\_)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.split(\_)}
\end{itemize}
\end{minipage} \\
Substring & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{s{[}1:4{]}}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str{[}1:4{]}}
\end{itemize}
\end{minipage} \\
Membership & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{\textquotesingle{}\_\textquotesingle{}\ in\ s}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.contains(\_)}
\end{itemize}
\end{minipage} \\
Length & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{len(s)}
\end{itemize}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{itemize}
\tightlist
\item
  \texttt{ser.str.len()}
\end{itemize}
\end{minipage} \\
\bottomrule()
\end{longtable}

We'll discuss the differences between Python string functions and
\texttt{pandas} Series methods in the following section on
canonicalization.

\hypertarget{canonicalization}{%
\subsection{Canonicalization}\label{canonicalization}}

Assume we want to merge the given tables.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/county\_and\_state.csv\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    county\_and\_state }\OperatorTok{=}\NormalTok{ pd.read\_csv(f)}
    
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/county\_and\_population.csv\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    county\_and\_pop }\OperatorTok{=}\NormalTok{ pd.read\_csv(f)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display(county\_and\_state), display(county\_and\_pop)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lll}
\toprule
{} &                      County & State \\
\midrule
0 &              De Witt County &    IL \\
1 &        Lac qui Parle County &    MN \\
2 &      Lewis and Clark County &    MT \\
3 &  St John the Baptist Parish &    LS \\
\bottomrule
\end{tabular}

\begin{tabular}{llr}
\toprule
{} &                County &  Population \\
\midrule
0 &                DeWitt &       16798 \\
1 &         Lac Qui Parle &        8067 \\
2 &         Lewis \& Clark &       55716 \\
3 &  St. John the Baptist &       43044 \\
\bottomrule
\end{tabular}

Last time, we used a \textbf{primary key} and \textbf{foreign key} to
join two tables. While neither of these keys exist in our DataFrames,
the \texttt{County} columns look similar enough. Can we convert these
columns into one standard, canonical form to merge the two tables?

\hypertarget{canonicalization-with-python-string-manipulation}{%
\subsubsection{Canonicalization with Python String
Manipulation}\label{canonicalization-with-python-string-manipulation}}

The following function uses Python string manipulation to convert a
single county name into canonical form. It does so by eliminating
whitespace, punctuation, and unnecessary text.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ canonicalize\_county(county\_name):}
    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        county\_name}
\NormalTok{            .lower()}
\NormalTok{            .replace(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}\&\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}and\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}county\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .replace(}\StringTok{\textquotesingle{}parish\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}

\NormalTok{canonicalize\_county(}\StringTok{"St. John the Baptist"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'stjohnthebaptist'
\end{verbatim}

We will use the \texttt{pandas} \texttt{map} function to apply the
\texttt{canonicalize\_county} function to every row in both DataFrames.
In doing so, we'll create a new column in each called
\texttt{clean\_county\_python} with the canonical form.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_and\_pop[}\StringTok{\textquotesingle{}clean\_county\_python\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ county\_and\_pop[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(canonicalize\_county)}
\NormalTok{county\_and\_state[}\StringTok{\textquotesingle{}clean\_county\_python\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ county\_and\_state[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{].}\BuiltInTok{map}\NormalTok{(canonicalize\_county)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display(county\_and\_state), display(county\_and\_pop)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llll}
\toprule
{} &                      County & State & clean\_county\_python \\
\midrule
0 &              De Witt County &    IL &              dewitt \\
1 &        Lac qui Parle County &    MN &         lacquiparle \\
2 &      Lewis and Clark County &    MT &       lewisandclark \\
3 &  St John the Baptist Parish &    LS &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\begin{tabular}{llrl}
\toprule
{} &                County &  Population & clean\_county\_python \\
\midrule
0 &                DeWitt &       16798 &              dewitt \\
1 &         Lac Qui Parle &        8067 &         lacquiparle \\
2 &         Lewis \& Clark &       55716 &       lewisandclark \\
3 &  St. John the Baptist &       43044 &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\hypertarget{canonicalization-with-pandas-series-methods}{%
\subsubsection{Canonicalization with Pandas Series
Methods}\label{canonicalization-with-pandas-series-methods}}

Alternatively, we can use \texttt{pandas} Series methods to create this
standardized column. To do so, we must call the \texttt{.str} attribute
of our Series object prior to calling any methods, like \texttt{.lower}
and \texttt{.replace}. Notice how these method names match their
equivalent built-in Python string functions.

Chaining multiple Series methods in this manner eliminates the need to
use the \texttt{map} function (as this code is vectorized).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ canonicalize\_county\_series(county\_series):}
    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        county\_series}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.lower()}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}\&\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}and\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}county\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{            .}\BuiltInTok{str}\NormalTok{.replace(}\StringTok{\textquotesingle{}parish\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}

\NormalTok{county\_and\_pop[}\StringTok{\textquotesingle{}clean\_county\_pandas\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ canonicalize\_county\_series(county\_and\_pop[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{])}
\NormalTok{county\_and\_state[}\StringTok{\textquotesingle{}clean\_county\_pandas\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ canonicalize\_county\_series(county\_and\_state[}\StringTok{\textquotesingle{}County\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{display(county\_and\_pop), display(county\_and\_state)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{llrll}
\toprule
{} &                County &  Population & clean\_county\_python & clean\_county\_pandas \\
\midrule
0 &                DeWitt &       16798 &              dewitt &              dewitt \\
1 &         Lac Qui Parle &        8067 &         lacquiparle &         lacquiparle \\
2 &         Lewis \& Clark &       55716 &       lewisandclark &       lewisandclark \\
3 &  St. John the Baptist &       43044 &    stjohnthebaptist &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\begin{tabular}{lllll}
\toprule
{} &                      County & State & clean\_county\_python & clean\_county\_pandas \\
\midrule
0 &              De Witt County &    IL &              dewitt &              dewitt \\
1 &        Lac qui Parle County &    MN &         lacquiparle &         lacquiparle \\
2 &      Lewis and Clark County &    MT &       lewisandclark &       lewisandclark \\
3 &  St John the Baptist Parish &    LS &    stjohnthebaptist &    stjohnthebaptist \\
\bottomrule
\end{tabular}

\hypertarget{extraction}{%
\subsection{Extraction}\label{extraction}}

Extraction explores the idea of obtaining useful information from text
data. This will be particularily important in model building, which
we'll study in a few weeks.

Say we want to read some data from a \texttt{.txt} file.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}data/log.txt\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
\NormalTok{    log\_lines }\OperatorTok{=}\NormalTok{ f.readlines()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_lines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n',
 '193.205.203.3 - - [2/Feb/2005:17:23:6 -0800] "GET /stat141/Notes/dim.html HTTP/1.0" 404 302 "http://eeyore.ucdavis.edu/stat141/Notes/session.html"\n',
 '169.237.46.240 - "" [3/Feb/2006:10:18:37 -0800] "GET /stat141/homework/Solutions/hw1Sol.pdf HTTP/1.1"\n']
\end{verbatim}

Suppose we want to extract the day, month, year, hour, minutes, seconds,
and timezone. Unfortunately, these items are not in a fixed position
from the beginning of the string, so slicing by some fixed offset won't
work.

Instead, we can use some clever thinking. Notice how the relevant
information is contained within a set of brackets, further seperated by
\texttt{/} and \texttt{:}. We can hone in on this region of text, and
split the data on these characters. Python's built-in \texttt{.split}
function makes this easy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OperatorTok{=}\NormalTok{ log\_lines[}\DecValTok{0}\NormalTok{] }\CommentTok{\# Only considering the first row of data}

\NormalTok{pertinent }\OperatorTok{=}\NormalTok{ first.split(}\StringTok{"["}\NormalTok{)[}\DecValTok{1}\NormalTok{].split(}\StringTok{\textquotesingle{}]\textquotesingle{}}\NormalTok{)[}\DecValTok{0}\NormalTok{]}
\NormalTok{day, month, rest }\OperatorTok{=}\NormalTok{ pertinent.split(}\StringTok{\textquotesingle{}/\textquotesingle{}}\NormalTok{)}
\NormalTok{year, hour, minute, rest }\OperatorTok{=}\NormalTok{ rest.split(}\StringTok{\textquotesingle{}:\textquotesingle{}}\NormalTok{)}
\NormalTok{seconds, time\_zone }\OperatorTok{=}\NormalTok{ rest.split(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{)}
\NormalTok{day, month, year, hour, minute, seconds, time\_zone}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
('26', 'Jan', '2014', '10', '47', '58', '-0800')
\end{verbatim}

There are two problems with this code:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Python's built-in functions limit us to extract data one record at a
  time

  \begin{itemize}
  \tightlist
  \item
    This can be resolved using a map function or Pandas Series methods.
  \end{itemize}
\item
  The code is quite verbose

  \begin{itemize}
  \tightlist
  \item
    This is a larger issue that is trickier to solve
  \end{itemize}
\end{enumerate}

In the next section, we'll introduce regular expressions - a tool that
solves problem 2.

\hypertarget{regex-basics}{%
\section{Regex Basics}\label{regex-basics}}

A \textbf{regular expression (``regex'')} is a sequence of characters
that specifies a search pattern. They are written to extract specific
information from text. Regular expressions are essentially part of a
smaller programming language embedded in Python, made available through
the \texttt{re} module. As such, they have a stand-alone syntax and
methods for various capabilities.

Regular expressions are useful in many applications beyond data science.
For example, Social Security Numbers (SSNs) are often validated with
regular expresions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{r"[0{-}9]\{3\}{-}[0{-}9]\{2\}{-}[0{-}9]\{4\}"} \CommentTok{\# Regular Expression Syntax}

\CommentTok{\# 3 of any digit, then a dash,}
\CommentTok{\# then 2 of any digit, then a dash,}
\CommentTok{\# then 4 of any digit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'[0-9]{3}-[0-9]{2}-[0-9]{4}'
\end{verbatim}

There are a ton of resources to learn and experiment with regular
expressions. A few are provided below:

\begin{itemize}
\tightlist
\item
  \href{https://docs.python.org/3/howto/regex.html}{Official Regex
  Guide}
\item
  \href{https://ds100.org/sp22/resources/assets/hw/regex_reference.pdf}{Data
  100 Reference Sheet}
\item
  \href{https://regex101.com/}{Regex101.com}

  \begin{itemize}
  \tightlist
  \item
    Be sure to check \texttt{Python} under the category on the left.
  \end{itemize}
\end{itemize}

\hypertarget{basics-regex-syntax}{%
\subsection{Basics Regex Syntax}\label{basics-regex-syntax}}

There are four basic operations with regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1875}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1771}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1458}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2083}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Order
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule()
\endhead
\texttt{Or}: \texttt{\textbar{}} & 4 & AA\textbar BAAB & AA BAAB & every
other string \\
\texttt{Concatenation} & 3 & AABAAB & AABAAB & every other string \\
\texttt{Closure}: \texttt{*} (zero or more) & 2 & AB*A & AA ABBBBBBA &
AB ABABA \\
\texttt{Group}: \texttt{()} (parenthesis) & 1 & A(A\textbar B)AAB (AB)*A
& AAAAB ABAAB A ABABABABA & every other string AA ABBA \\
\bottomrule()
\end{longtable}

Notice how these metacharacter operations are ordered. Rather than being
literal characters, these \textbf{metacharacters} manipulate adjacent
characters. \texttt{()} takes precedence, followed by \texttt{*}, and
finally \texttt{\textbar{}}. This allows us to differentiate between
very different regex commands like \texttt{AB*} and \texttt{(AB)*}. The
former reads ``\texttt{A} then zero or more copies of \texttt{B}'',
while the latter specifies ``zero or more copies of \texttt{AB}''.

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\textbf{Question 1}: Give a regular expression that matches
\texttt{moon}, \texttt{moooon}, etc. Your expression should match any
even number of \texttt{o}s except zero (i.e.~don't match \texttt{mn}).

\textbf{Answer 1}: \texttt{moo(oo)*n}

\begin{itemize}
\tightlist
\item
  Hardcoding \texttt{oo} before the capture group ensures that
  \texttt{mn} is not matched.
\item
  A capture group of \texttt{(oo)*} ensures the number of \texttt{o}'s
  is even.
\end{itemize}

\textbf{Question 2}: Using only basic operations, formulate a regex that
matches \texttt{muun}, \texttt{muuuun}, \texttt{moon}, \texttt{moooon},
etc. Your expression should match any even number of \texttt{u}s or
\texttt{o}s except zero (i.e.~don't match \texttt{mn}).

\textbf{Answer 2}: \texttt{m(uu(uu)*\textbar{}oo(oo)*)n}

\begin{itemize}
\tightlist
\item
  The leading \texttt{m} and trailing \texttt{n} ensures that only
  strings beginning with \texttt{m} and ending with \texttt{n} are
  matched.
\item
  Notice how the outer capture group surrounds the \texttt{\textbar{}}.

  \begin{itemize}
  \tightlist
  \item
    Consider the regex \texttt{m(uu(uu)*)\textbar{}(oo(oo)*)n}. This
    incorrectly matches \texttt{muu} and \texttt{oooon}.

    \begin{itemize}
    \tightlist
    \item
      Each OR clause is everything to the left and right of
      \texttt{\textbar{}}. The incorrect solution matches only half of
      the string, and ignores either the beginning \texttt{m} or
      trailing \texttt{n}.
    \item
      A set of paranthesis must surround \texttt{\textbar{}}. That way,
      each OR clause is everything to the left and right of
      \texttt{\textbar{}} \textbf{within} the group. This ensures both
      the beginning \texttt{m} \emph{and} trailing \texttt{n} are
      matched.
    \end{itemize}
  \end{itemize}
\end{itemize}

\hypertarget{regex-expanded}{%
\section{Regex Expanded}\label{regex-expanded}}

Provided below are more complex regular expression functions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1619}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1810}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule()
\endhead
\texttt{Any\ Character}: \texttt{.} (except newline) & .U.U.U. & CUMULUS
JUGULUM & SUCCUBUS TUMULTUOUS \\
\texttt{Character\ Class}: \texttt{{[}{]}} (match one character in
\texttt{{[}{]}}) & {[}A-Za-z{]}{[}a-z{]}* & word Capitalized & camelCase
4illegal \\
\texttt{Repeated\ "a"\ Times}: \texttt{\{a\}} & j{[}aeiou{]}\{3\}hn &
jaoehn jooohn & jhn jaeiouhn \\
\texttt{Repeated\ "from\ a\ to\ b"\ Times}: \texttt{\{a,\ b\}} &
j{[}0u{]}\{1,2\}hn & john juohn & jhn jooohn \\
\texttt{At\ Least\ One}: \texttt{+} & jo+hn & john joooooohn & jhn
jjohn \\
\texttt{Zero\ or\ One}: \texttt{?} & joh?n & jon john & any other
string \\
\bottomrule()
\end{longtable}

A character class matches a single character in it's class. These
characters can be hardcoded -- in the case of \texttt{{[}aeiou{]}} -- or
shorthand can be specified to mean a range of characters. Examples
include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{{[}A-Z{]}}: Any capitalized letter
\item
  \texttt{{[}a-z{]}}: Any lowercase letter
\item
  \texttt{{[}0-9{]}}: Any single digit
\item
  \texttt{{[}A-Za-z{]}}: Any capitalized of lowercase letter
\item
  \texttt{{[}A-Za-z0-9{]}}: Any capitalized or lowercase letter or
  single digit
\end{enumerate}

\hypertarget{examples-1}{%
\subsubsection{Examples}\label{examples-1}}

Let's analyze a few examples of complex regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4722}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Does Not Match
\end{minipage} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.*SPB.*}
\end{enumerate}
\end{minipage} & \\
RASPBERRY SPBOO & SUBSPACE SUBSPECIES \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \texttt{{[}0-9{]}\{3\}-{[}0-9{]}\{2\}-{[}0-9{]}\{4\}}
\end{enumerate}
\end{minipage} & \\
231-41-5121 573-57-1821 & 231415121 57-3571821 \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \texttt{{[}a-z{]}+@({[}a-z{]}+\textbackslash{}.)+(edu\textbar{}com)}
\end{enumerate}
\end{minipage} & \\
horse@pizza.com horse@pizza.food.com & frank\_99@yahoo.com hug@cs \\
\bottomrule()
\end{longtable}

\textbf{Explanations}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{.*SPB.*} only matches strings that contain the substring
  \texttt{SPB}.

  \begin{itemize}
  \tightlist
  \item
    The \texttt{.*} metacharacter matches any amount of non-negative
    characters. Newlines do not count.\\
  \end{itemize}
\item
  This regular expression matches 3 of any digit, then a dash, then 2 of
  any digit, then a dash, then 4 of any digit

  \begin{itemize}
  \tightlist
  \item
    You'll recognize this as the familiar Social Security Number regular
    expression
  \end{itemize}
\item
  Matches any email with a \texttt{com} or \texttt{edu} domain, where
  all characters of the email are letters.

  \begin{itemize}
  \tightlist
  \item
    At least one \texttt{.} must preceed the domain name. Including a
    backslash \texttt{\textbackslash{}} before any metacharacter (in
    this case, the \texttt{.}) tells regex to match that character
    exactly.
  \end{itemize}
\end{enumerate}

\hypertarget{convenient-regex}{%
\section{Convenient Regex}\label{convenient-regex}}

Here are a few more convenient regular expressions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1619}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1810}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Operation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Syntax Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Matches
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doesn't Match
\end{minipage} \\
\midrule()
\endhead
\texttt{built\ in\ character\ class} & \texttt{\textbackslash{}w+}
\texttt{\textbackslash{}d+} \texttt{\textbackslash{}s+} & Fawef\_03
231123 \texttt{whitespace} & this person 423 people
\texttt{non-whitespace} \\
\texttt{character\ class\ negation}: \texttt{{[}\^{}{]}} (everything
except the given characters) & {[}\^{}a-z{]}+. & PEPPERS3982 17211!↑å &
porch CLAmS \\
\texttt{escape\ character}: \texttt{\textbackslash{}} (match the literal
next character) & cow\textbackslash.com & cow.com & cowscom \\
\texttt{beginning\ of\ line}: \texttt{\^{}} & \^{}ark & ark two ark o
ark & dark \\
\texttt{end\ of\ line}: \texttt{\$} & ark\$ & dark ark o ark & ark
two \\
\texttt{lazy\ version\ of\ zero\ or\ more} : \texttt{*?} & 5.*?5 & 5005
55 & 5005005 \\
\bottomrule()
\end{longtable}

\hypertarget{examples-2}{%
\subsubsection{Examples}\label{examples-2}}

Let's revist our earlier problem of extracting date/time data from the
given \texttt{.txt} files. Here is how the data looked.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_lines[}\DecValTok{0}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n'
\end{verbatim}

\textbf{Question}: Give a regular expression that matches everything
contained within and including the brackets - the day, month, year,
hour, minutes, seconds, and timezone.

\textbf{Answer}: \texttt{\textbackslash{}{[}.*\textbackslash{}{]}}

\begin{itemize}
\tightlist
\item
  Notice how matching the literal \texttt{{[}} and \texttt{{]}} is
  necessary. Therefore, an escape character \texttt{\textbackslash{}} is
  required before both \texttt{{[}} and \texttt{{]}} - otherwise these
  metacharacters will match character classes.
\item
  We need to match a particular format between \texttt{{[}} and
  \texttt{{]}}. For this example, \texttt{.*} will suffice.
\end{itemize}

\textbf{Alternative Solution}:
\texttt{\textbackslash{}{[}\textbackslash{}w+/\textbackslash{}w+/\textbackslash{}w+:\textbackslash{}w+:\textbackslash{}w+:\textbackslash{}w+\textbackslash{}s-\textbackslash{}w+\textbackslash{}{]}}

\begin{itemize}
\tightlist
\item
  This solution is much safer.

  \begin{itemize}
  \tightlist
  \item
    Imagine the data between \texttt{{[}} and \texttt{{]}} was garbage -
    \texttt{.*} will still match that.
  \item
    The alternate solution will only match data that follows the correct
    format.
  \end{itemize}
\end{itemize}

\hypertarget{regex-in-python-and-pandas-regex-groups}{%
\section{Regex in Python and Pandas (Regex
Groups)}\label{regex-in-python-and-pandas-regex-groups}}

\hypertarget{canonicalization-1}{%
\subsection{Canonicalization}\label{canonicalization-1}}

\hypertarget{canonicalization-with-regex}{%
\subsubsection{Canonicalization with
Regex}\label{canonicalization-with-regex}}

Earlier in this note, we examined the process of canonicalization using
Python string manipulation and \texttt{pandas} Series methods. However,
we mentioned this approach had a major flaw: our code was unnecessarily
verbose. Equipped with our knowledge of regular expressions, let's fix
this.

To do so, we need to understand a few functions in the \texttt{re}
module. The first of these is the substitute function:
\texttt{re.sub(pattern,\ rep1,\ text)}. It behaves similarily to
Python's built-in \texttt{.replace} function, and returns text with all
instances of \texttt{pattern} replaced by \texttt{rep1}.

The regular expression here removes text surrounded by
\texttt{\textless{}\textgreater{}} (also known as HTML tags).

In order, the pattern matches \ldots{} 1. a single \texttt{\textless{}}
2. any character that is not a \texttt{\textgreater{}} : div, td
valign\ldots, /td, /div 3. a single \texttt{\textgreater{}}

Any substring in \texttt{text} that fulfills all three conditions will
be replaced by \texttt{\textquotesingle{}\textquotesingle{}}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ re}

\NormalTok{text }\OperatorTok{=} \StringTok{"\textless{}div\textgreater{}\textless{}td valign=\textquotesingle{}top\textquotesingle{}\textgreater{}Moo\textless{}/td\textgreater{}\textless{}/div\textgreater{}"}
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"\textless{}[\^{}\textgreater{}]+\textgreater{}"}
\NormalTok{re.sub(pattern, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, text) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'Moo'
\end{verbatim}

Notice the \texttt{r} preceeding the regular expression pattern; this
specifies the regular expression is a raw string. Raw strings do not
recognize escape sequences (ie the Python newline metacharacter
\texttt{\textbackslash{}n}). This makes them useful for regular
expressions, which often contain literal \texttt{\textbackslash{}}
characters.

In other words, don't forget to tag your regex with a \texttt{r}.

\hypertarget{canonicalization-with-pandas}{%
\subsubsection{Canonicalization with
Pandas}\label{canonicalization-with-pandas}}

We can also use regular expressions with Pandas Series methods. This
gives us the benefit of operating on an entire column of data as opposed
to a single value. The code is simple:
\texttt{ser.str.replace(pattern,\ repl,\ regex=True}).

Consider the following DataFrame \texttt{html\_data} with a single
column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{"HTML"}\NormalTok{: [}\StringTok{"\textless{}div\textgreater{}\textless{}td valign=\textquotesingle{}top\textquotesingle{}\textgreater{}Moo\textless{}/td\textgreater{}\textless{}/div\textgreater{}"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                 \StringTok{"\textless{}a href=\textquotesingle{}http://ds100.org\textquotesingle{}\textgreater{}Link\textless{}/a\textgreater{}"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                 \StringTok{"\textless{}b\textgreater{}Bold text\textless{}/b\textgreater{}"}\NormalTok{]\}}
\NormalTok{html\_data }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{html\_data}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                                  HTML \\
\midrule
0 &  <div><td valign='top'>Moo</td></div> \\
1 &   <a href='http://ds100.org'>Link</a> \\
2 &                      <b>Bold text</b> \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"\textless{}[\^{}\textgreater{}]+\textgreater{}"}
\NormalTok{html\_data[}\StringTok{\textquotesingle{}HTML\textquotesingle{}}\NormalTok{].}\BuiltInTok{str}\NormalTok{.replace(pattern, }\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{, regex}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &       HTML \\
\midrule
0 &        Moo \\
1 &       Link \\
2 &  Bold text \\
\bottomrule
\end{tabular}

\hypertarget{extraction-1}{%
\subsection{Extraction}\label{extraction-1}}

\hypertarget{extraction-with-regex}{%
\subsubsection{Extraction with Regex}\label{extraction-with-regex}}

Just like with canonicalization, the \texttt{re} module provides
capability to extract relevant text from a string:
\texttt{re.findall(pattern,\ text)}. This function returns a list of all
matches to \texttt{pattern}.

Using the familiar regular expression for Social Security Numbers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"My social security number is 123{-}45{-}6789 bro, or maybe it’s 321{-}45{-}6789."}
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r"[0{-}9]}\SpecialCharTok{\{3\}}\VerbatimStringTok{{-}[0{-}9]}\SpecialCharTok{\{2\}}\VerbatimStringTok{{-}[0{-}9]}\SpecialCharTok{\{4\}}\VerbatimStringTok{"}
\NormalTok{re.findall(pattern, text)  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
['123-45-6789', '321-45-6789']
\end{verbatim}

\hypertarget{extraction-with-pandas}{%
\subsubsection{Extraction with Pandas}\label{extraction-with-pandas}}

Pandas similarily provides extraction functionality on a Series of data:
\texttt{ser.str.findall(pattern)}

Consider the following DataFrame \texttt{ssn\_data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ \{}\StringTok{"SSN"}\NormalTok{: [}\StringTok{"987{-}65{-}4321"}\NormalTok{, }\StringTok{"forty"}\NormalTok{, }\OperatorTok{\textbackslash{}}
                \StringTok{"123{-}45{-}6789 bro or 321{-}45{-}6789"}\NormalTok{,}
               \StringTok{"999{-}99{-}9999"}\NormalTok{]\}}
\NormalTok{ssn\_data }\OperatorTok{=}\NormalTok{ pd.DataFrame(data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_data}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                             SSN \\
\midrule
0 &                     987-65-4321 \\
1 &                           forty \\
2 &  123-45-6789 bro or 321-45-6789 \\
3 &                     999-99-9999 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ssn\_data[}\StringTok{"SSN"}\NormalTok{].}\BuiltInTok{str}\NormalTok{.findall(pattern)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &                         SSN \\
\midrule
0 &               [987-65-4321] \\
1 &                          [] \\
2 &  [123-45-6789, 321-45-6789] \\
3 &               [999-99-9999] \\
\bottomrule
\end{tabular}

This function returns a list for every row containing the pattern
matches in a given string.

\hypertarget{regular-expression-capture-groups}{%
\subsection{Regular Expression Capture
Groups}\label{regular-expression-capture-groups}}

Earlier we used parentheses \texttt{(} \texttt{)} to specify the highest
order of operation in regular expressions. However, they have another
meaning; paranthesis are often used to represent \textbf{capture
groups}. Capture groups are essentially, a set of smaller regular
expressions that match multiple substrings in text data.

Let's take a look at an example.

\hypertarget{example-1}{%
\subsubsection{Example 1}\label{example-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{text }\OperatorTok{=} \StringTok{"Observations: 03:04:53 {-} Horse awakens. }\CharTok{\textbackslash{}}
\StringTok{        03:05:14 {-} Horse goes back to sleep."}
\end{Highlighting}
\end{Shaded}

Say we want to capture all occurences of time data (hour, minute, and
second) as \emph{seperate entities}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern\_1 }\OperatorTok{=} \VerbatimStringTok{r"(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d)"}
\NormalTok{re.findall(pattern\_1, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[('03', '04', '53'), ('03', '05', '14')]
\end{verbatim}

Notice how the given pattern has 3 capture groups, each specified by the
regular expression \texttt{(\textbackslash{}d\textbackslash{}d)}. We
then use \texttt{re.findall} to return these capture groups, each as
tuples containing 3 matches.

These regular expression capture groups can be different. We can use the
\texttt{(\textbackslash{}d\{2\})} shorthand to extract the same data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern\_2 }\OperatorTok{=} \VerbatimStringTok{r"(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d\textbackslash{}d):(\textbackslash{}d}\SpecialCharTok{\{2\}}\VerbatimStringTok{)"}
\NormalTok{re.findall(pattern\_2, text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[('03', '04', '53'), ('03', '05', '14')]
\end{verbatim}

\hypertarget{example-2}{%
\subsubsection{Example 2}\label{example-2}}

With the notion of capture groups, convince yourself how the following
regular expression works.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OperatorTok{=}\NormalTok{ log\_lines[}\DecValTok{0}\NormalTok{]}
\NormalTok{first}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] "GET /stat141/Winter04/ HTTP/1.1" 200 2585 "http://anson.ucdavis.edu/courses/"\n'
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pattern }\OperatorTok{=} \VerbatimStringTok{r\textquotesingle{}\textbackslash{}[(\textbackslash{}d+)\textbackslash{}/(\textbackslash{}w+)\textbackslash{}/(\textbackslash{}d+):(\textbackslash{}d+):(\textbackslash{}d+):(\textbackslash{}d+) (.+)\textbackslash{}]\textquotesingle{}}
\NormalTok{day, month, year, hour, minute, second, time\_zone }\OperatorTok{=}\NormalTok{ re.findall(pattern, first)[}\DecValTok{0}\NormalTok{]}
\BuiltInTok{print}\NormalTok{(day, month, year, hour, minute, second, time\_zone)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
26 Jan 2014 10 47 58 -0800
\end{verbatim}

\hypertarget{limitations-of-regular-expressions}{%
\section{Limitations of Regular
Expressions}\label{limitations-of-regular-expressions}}

Today, we explored the capabilities of regular expressions in data
wrangling with text data. However, there are a few things to be wary of.

Writing regular expressions is like writing a program.

\begin{itemize}
\tightlist
\item
  Need to know the syntax well.
\item
  Can be easier to write than to read.
\item
  Can be difficult to debug.
\end{itemize}

Regular expressions are terrible at certain types of problems:

\begin{itemize}
\tightlist
\item
  For parsing a hierarchical structure, such as JSON, use the
  \texttt{json.load()} parser, not regex!
\item
  Complex features (e.g.~valid email address).
\item
  Counting (same number of instances of a and b). (impossible)
\item
  Complex properties (palindromes, balanced parentheses). (impossible)
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{visualization-i}{%
\chapter{Visualization I}\label{visualization-i}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Use \texttt{matplotlib}, \texttt{seaborn}, and \texttt{plotly} to
  create data visualization.
\item
  Analyze histogram and identify outliers, mode, and skewness.
\item
  Using \texttt{boxplot} and \texttt{violinplot} to compare two
  distributions.
\end{itemize}

\end{tcolorbox}

In our journey of the data science lifecycle, we have begun to explore
the vast world of exploratory data analysis. More recently, we learned
how to pre-process data using various data manipulation techniques. As
we work towards understanding our data, there is one key component
missing in our arsenal - the ability to visualize and discern
relationships in existing data.

These next two lectures will introduce you to various examples of data
visualizations and their underlying theory. In doing so, we'll motivate
their importace in real-world examples with the use of plotting
libraries.

\hypertarget{visualizations-in-data-8-and-data-100-so-far}{%
\section{Visualizations in Data 8 and Data 100 (so
far)}\label{visualizations-in-data-8-and-data-100-so-far}}

You've likely encountered several forms of data visualizations in your
studies. You may remember two such examples from Data 8: line charts and
histograms. Each of these served a unique purpose. For example, line
charts displayed how numerical quantities changed over time, while
histograms were useful in understanding a variable's distribution.

\textbf{Line Chart}

\textbf{Histogram}

\hypertarget{goals-of-visualization}{%
\section{Goals of Visualization}\label{goals-of-visualization}}

Visualizations are useful for a number of reasons. In Data 100, we
consider two areas in particular:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To broaden your understanding of the data

  \begin{itemize}
  \tightlist
  \item
    Key part in exploratory data analysis.
  \item
    Useful in investigating relationships between variables.
  \end{itemize}
\item
  To communicate results/conclusions to others

  \begin{itemize}
  \tightlist
  \item
    Visualization theory is especially important here.
  \end{itemize}
\end{enumerate}

One of the most common applications of visualizations is in
understanding a distribution of data.

\hypertarget{an-overview-of-distributions}{%
\section{An Overview of
Distributions}\label{an-overview-of-distributions}}

A distribution describes the frequency of unique values in a variable.
Distributions must satisfy two properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each data point must belong to only one category.
\item
  The total frequency of all categories must sum to 100\%. In other
  words, their total count should equal the number of values in
  consideration.
\end{enumerate}

\textbf{Not a Valid Distribution}

\textbf{Valid Distribution}

Left Diagram: This is not a valid distribution since individuals can be
associated to more than one category and the bar values demonstrate
values in minutes and not probability

Right Diagram: This example satisfies the two properties of
distributions, so it is a valid distribution.

\hypertarget{bar-plots}{%
\section{Bar Plots}\label{bar-plots}}

As we saw above, a \textbf{bar plot} is one of the most common ways of
displaying the distribution of a \textbf{qualitative} (categorical)
variable. The length of a bar plot encodes the frequency of a category;
the width encodes no useful information.

Let's contextualize this in an example. We will use the familiar
\texttt{births} dataset from Data 8 in our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{births }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/baby.csv"}\NormalTok{)}
\NormalTok{births.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrl}
\toprule
{} &  Birth Weight &  Gestational Days &  Maternal Age &  Maternal Height &  Maternal Pregnancy Weight &  Maternal Smoker \\
\midrule
0 &           120 &               284 &            27 &               62 &                        100 &            False \\
1 &           113 &               282 &            33 &               64 &                        135 &            False \\
2 &           128 &               279 &            28 &               64 &                        115 &             True \\
3 &           108 &               282 &            23 &               67 &                        125 &             True \\
4 &           136 &               286 &            25 &               62 &                         93 &            False \\
\bottomrule
\end{tabular}

We can visualize the distribution of the \texttt{Maternal\ Smoker}
column using a bar plot. There are a few ways to do this.

\hypertarget{plotting-in-pandas}{%
\subsection{Plotting in Pandas}\label{plotting-in-pandas}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{].value\_counts().plot(kind }\OperatorTok{=} \StringTok{\textquotesingle{}bar\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-3-output-1.png}

}

\end{figure}

Recall that \texttt{.value\_counts()} returns a \texttt{Series} with the
total count of each unique value. We call
\texttt{.plot(kind\ =\ \textquotesingle{}bar\textquotesingle{})} on this
result to visualize these counts as a bar plot.

Plotting methods in \texttt{pandas} are the least preferred and not
supported in Data 100, as their functionality is limited. Instead,
future examples will focus on other libaries built specifically for
visualizing data. The most well-known library here is
\texttt{matplotlib}.

\hypertarget{plotting-in-matplotlib}{%
\subsection{Plotting in Matplotlib}\label{plotting-in-matplotlib}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{ms }\OperatorTok{=}\NormalTok{ births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{].value\_counts()}
\NormalTok{plt.bar(ms.index.astype(}\StringTok{\textquotesingle{}string\textquotesingle{}}\NormalTok{), ms)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Count\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-4-output-1.png}

}

\end{figure}

While more code is required to achieve the same result,
\texttt{matplotlib} is often used over \texttt{pandas} for its ability
to plot more complex visualizations, some of which are discussed
shortly.

However, notice how we need to explicitly specify the type of the value
for the x-axis to \texttt{string}. In absence of conversion, the x-axis
will be a range of integers rather than the two categories,
\texttt{True} and \texttt{False}. This is because \texttt{matplotlib}
coerces \texttt{True} to a value of 1 and \texttt{False} to 0. Also,
note how we needed to label the axes with \texttt{plt.xlabel} and
\texttt{plt.ylabel} - \texttt{matplotlib} does not support automatic
axis labeling. To get around these inconveniences, we can use a more
effecient plotting library, \texttt{seaborn}.

\hypertarget{plotting-in-seaborn}{%
\subsection{Plotting in Seaborn}\label{plotting-in-seaborn}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{sns.countplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-5-output-1.png}

}

\end{figure}

\texttt{seaborn.countplot} both counts and visualizes the number of
unique values in a given column. This column is specified by the
\texttt{x} argument to \texttt{sns.countplot}, while the
\texttt{DataFrame} is specified by the \texttt{data} argument.

For the vast majority of visualizations, \texttt{seaborn} is far more
concise and aesthetically pleasing than \texttt{matplotlib}. However,
the color scheme of this particular bar plot is abritrary - it encodes
no additional information about the categories themselves. This is not
always true; color may signify meaningful detail in other
visualizations. We'll explore this more in-depth during the next
lecture.

\hypertarget{plotting-in-plotly}{%
\subsection{Plotting in Plotly}\label{plotting-in-plotly}}

\texttt{plotly} is one of the most versatile plottling libraries and
widely used in industry. However, \texttt{plotly} has various
dependencies that make it difficult to support in Data 100. Therfore, we
have intentionally excluded the code to generate the plot above.

By now, you'll have noticed that each of these plotting libraries have a
very different syntax. As with \texttt{pandas}, we'll teach you the
important methods in \texttt{matplotlib} and \texttt{seaborn}, but
you'll learn more through documentation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://matplotlib.org/stable/index.html}{Matplotlib
  Documentation}
\item
  \href{https://seaborn.pydata.org/}{Seaborn Documentation}
\end{enumerate}

Example Questions:

\begin{itemize}
\tightlist
\item
  What colors should we use?
\item
  How wide should the bars be?
\item
  Should the legend exist?
\item
  Should the bars and axes have dark borders?
\end{itemize}

To accomplish goal 2, here are some ways we can improve plot:

\begin{itemize}
\tightlist
\item
  Introducing different colors for each bar
\item
  Including a legend
\item
  Including a title
\item
  Labeling the y-axis
\item
  Using color-blind friendly palettes
\item
  Re-orienting the labels
\item
  Increase the font size
\end{itemize}

\hypertarget{histograms}{%
\section{Histograms}\label{histograms}}

\textbf{Histograms} are a natural extension to bar plots; they visualize
the distribution of \textbf{quantitative} (numerical) data.

Revisiting our example with the \texttt{births} DataFrame, let's plot
the distribution of the \texttt{Maternal\ Pregnancy\ Weight} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrl}
\toprule
{} &  Birth Weight &  Gestational Days &  Maternal Age &  Maternal Height &  Maternal Pregnancy Weight &  Maternal Smoker \\
\midrule
0 &           120 &               284 &            27 &               62 &                        100 &            False \\
1 &           113 &               282 &            33 &               64 &                        135 &            False \\
2 &           128 &               279 &            28 &               64 &                        115 &             True \\
3 &           108 &               282 &            23 &               67 &                        125 &             True \\
4 &           136 &               286 &            25 &               62 &                         93 &            False \\
\bottomrule
\end{tabular}

How should we define our categories for this variable? In the previous
example, these were the unique values of the \texttt{Maternal\ Smoker}
column: \texttt{True} and \texttt{False}. If we use similar logic here,
our categories are the different numerical weights contained in the
\texttt{Maternal\ Pregnancy\ Weight} column.

Under this assumption, let's plot this distribution using the
\texttt{seaborn.countplot} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.countplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

This histogram clearly suffers from \textbf{overplotting}. This is
somewhat expected for \texttt{Maternal\ Pregnancy\ Weight} - it is a
quantitative variable that takes on a wide range of values.

To combat this problem, statisticians use bins to categorize numerical
data. Luckily, \texttt{seaborn} provides a helpful plotting function
that automatically bins our data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-8-output-1.png}

}

\end{figure}

This diagram is known as a histogram. While it looks more reasonable,
notice how we lose fine-grain information on the distribution of data
contained within each bin. We can introduce rug plots to minimize this
information loss. An overlaid ``rug plot'' displays the within-bin
distribution of our data, as denoted by the thickness of the colored
line on the x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{sns.rugplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{, color }\OperatorTok{=} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-9-output-1.png}

}

\end{figure}

You may have seen histograms drawn differently - perhaps with an
overlaid \textbf{density curve} and normalized y-axis. We can display
both with a few tweaks to our code.

To visualize a density curve, we can set the the \texttt{kde\ =\ True}
argument of the \texttt{sns.histplot}. Setting the argument
\texttt{stat\ =\ \textquotesingle{}density\textquotesingle{}} normalizes
our histogram and displays densities, instead of counts, on the y-axis.
You'll notice that the area under the density curve is 1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{, kde }\OperatorTok{=} \VariableTok{True}\NormalTok{, }
\NormalTok{             stat }\OperatorTok{=} \StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.rugplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{, color }\OperatorTok{=} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-10-output-1.png}

}

\end{figure}

\hypertarget{evaluating-histograms}{%
\section{Evaluating Histograms}\label{evaluating-histograms}}

Histograms allow us to assess a distribution by their shape. There are a
few properties of histograms we can analyze:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Skewness and Tails

  \begin{itemize}
  \tightlist
  \item
    Skewed left vs skewed right
  \item
    Left tail vs right tail
  \end{itemize}
\item
  Outliers

  \begin{itemize}
  \tightlist
  \item
    Defined arbitrarily for now
  \end{itemize}
\item
  Modes

  \begin{itemize}
  \tightlist
  \item
    Most commonly occuring data
  \end{itemize}
\end{enumerate}

\hypertarget{skewness-and-tails}{%
\subsection{Skewness and Tails}\label{skewness-and-tails}}

If a distribution has a long right tail (such as
\texttt{Maternal\ Pregancy\ Weight}), it is \textbf{skewed right}. In a
right-skewed distribution, the few large outliers ``pull'' the mean to
the \textbf{right} of the median.

If a distribution has a long left tail, it is \textbf{skewed left}. In a
left-skewed distribution, the few small outliers ``pull'' the mean to
the \textbf{left} of the median.

In the case where a distribution has equal-sized right and left tails,
it is \textbf{symmetric}. The mean is approximately \textbf{equal} to
the median. Think of mean as the balancing point of the distribution

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{df\_mean }\OperatorTok{=}\NormalTok{ np.mean(births[}\StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{])}
\NormalTok{df\_median }\OperatorTok{=}\NormalTok{ np.median(births[}\StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The mean is: }\SpecialCharTok{\{\}}\StringTok{ and the median is }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(df\_mean,df\_median))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The mean is: 128.4787052810903 and the median is 125.0
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-11-output-2.png}

}

\end{figure}

\hypertarget{outliers}{%
\subsection{Outliers}\label{outliers}}

Loosely speaking, an \textbf{outlier} is defined as a data point that
lies an abnormally large distance away from other values. We'll define
the statistical measure for this shortly.

Outliers disproportionately influce the mean because their magnitude is
directly involved in computing the average. However, the median is
largely unaffected - the magnitude of an outlier is irrelevant; we only
care that it is some non-zero distance away from the midpoint of the
data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\CommentTok{\#\# Where do we draw the line of outlier? }
\NormalTok{plt.axvline(df\_mean}\OperatorTok{*}\FloatTok{1.75}\NormalTok{, color }\OperatorTok{=} \StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-12-output-1.png}

}

\end{figure}

\hypertarget{modes}{%
\subsection{Modes}\label{modes}}

A \textbf{mode} of a distribution is a local or global maximum. A
distribution with a single clear maximum is \textbf{unimodal},
distributions with two modes are \textbf{bimodal}, and those with 3 or
more are \textbf{multimodal}. You need to distinguish between
\textbf{modes} and \emph{random noise}.

For example, the distribution of birth weights for maternal smokers is
(weakly) multimodal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births\_maternal\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{True}\NormalTok{]}
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{\textbackslash{}}
\NormalTok{            .}\BuiltInTok{set}\NormalTok{(title }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker histogram\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-13-output-1.png}

}

\end{figure}

On the other hand, the distribution of birth weights for maternal
non-smokers is weakly bi-modal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births\_maternal\_non\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{False}\NormalTok{]}
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_non\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{\textbackslash{}}
\NormalTok{            .}\BuiltInTok{set}\NormalTok{(title }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Non{-}Smoker histogram\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-14-output-1.png}

}

\end{figure}

However, changing the bins reveals that the data is not bi-modal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_non\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{,}\OperatorTok{\textbackslash{}}
\NormalTok{             bins }\OperatorTok{=} \DecValTok{20}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-15-output-1.png}

}

\end{figure}

\hypertarget{density-curves}{%
\section{Density Curves}\label{density-curves}}

Instead of a discrete histogram, we can visualize what a continuous
distribution corresponding to that same data could look like using a
curve. - The smooth curve drawn on top of the histogram here is called a
density curve.

In lecture 8, we will study how exactly to compute these density curves
(using a technique is called Kernel Density Estimation).

If we plot \texttt{birth\ weights} of babies of \emph{smoking mothers},
we get a histogram that appears bimodal.

\begin{itemize}
\tightlist
\item
  Density curve reinforces belief in this bimodality.
\end{itemize}

However, if we plot \texttt{birth\ weights} of babies of
\emph{non-smoking mothers}, we get a histogram that appears unimodal.

From a goal 1 perspective, this is EDA which tells us there may be
something interesting here worth pursuing.

\begin{itemize}
\tightlist
\item
  Deeper analysis necessary!
\item
  If we found something truly interesting, we'd have to cautiously write
  up an argument and create goal 2 level visualizations.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births\_non\_maternal\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{False}\NormalTok{]}
\NormalTok{births\_maternal\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{True}\NormalTok{]}
 
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_smoker , x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}\OperatorTok{\textbackslash{}}
\NormalTok{             kde }\OperatorTok{=} \VariableTok{True}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-16-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_non\_maternal\_smoker , x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}\OperatorTok{\textbackslash{}}
\NormalTok{             kde }\OperatorTok{=} \VariableTok{True}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-17-output-1.png}

}

\end{figure}

\hypertarget{histograms-and-density}{%
\subsection{Histograms and Density}\label{histograms-and-density}}

Rather than labeling by counts, we can instead plot the density, as
shown below. Density gives us a measure that is invariant to the total
number of observed units. The numerical values on the Y-axis for a
sample of 100 units would be the same for when we observe a sample of
10000 units instead. We can still calculate the absolute number of
observed units using density.

Example: There are 1174 observations total. - Total area of this bin
should be: 120/1174 = \textasciitilde10\% - Density of this bin is
therefore: 10\% / (115 - 110) = 0.02

\hypertarget{box-plots-and-violin-plots}{%
\section{Box Plots and Violin Plots}\label{box-plots-and-violin-plots}}

\hypertarget{boxplots}{%
\subsection{Boxplots}\label{boxplots}}

\textbf{Boxplots} are an alternative to histograms that visualize
numerical distributions. They are especially useful in graphicaly
summarizing several characteristics of a distribution. These include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lower Quartile (\(1^{st}\) Quartile)
\item
  Median (\(2^{nd}\) Quartile)
\item
  Upper Quartile (\(3^{rd}\) Quartile)
\item
  Interquartile Range (IQR)
\item
  Whiskers
\item
  Outliers
\end{enumerate}

The \textbf{lower quartile}, \textbf{median}, and \textbf{uper quartile}
are the \(25^{th}\), \(50^{th}\), and \(75^{th}\) percentiles of data,
respectively. The \textbf{interquartile range} measures the spread of
the middle \(50\)\% of the distribution, calculated as the (\(3^{rd}\)
Quartile \(-\) \(1^{st}\) Quartile).

The \textbf{whiskers} of a box-plot are the two points that lie at the
{[}\(1^{st}\) Quartile \(-\) (\(1.5\times\) IQR){]}, and the
{[}\(3^{rd}\) Quartile \(+\) (\(1.5\times\) IQR){]}. They are the lower
and upper ranges of ``normal'' data (the points excluding outliers).
Subsequently, the \textbf{outliers} are the data points that fall beyond
the whiskers, or further than (\(1.5 \times\) IQR) from the extreme
quartiles.

Let's visualize a box-plot of the \texttt{Birth\ Weight} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.boxplot(data }\OperatorTok{=}\NormalTok{ births, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}

\NormalTok{bweights }\OperatorTok{=}\NormalTok{ births[}\StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{]}
\NormalTok{q1 }\OperatorTok{=}\NormalTok{ np.percentile(bweights, }\DecValTok{25}\NormalTok{)}
\NormalTok{q2 }\OperatorTok{=}\NormalTok{ np.percentile(bweights, }\DecValTok{50}\NormalTok{)}
\NormalTok{q3 }\OperatorTok{=}\NormalTok{ np.percentile(bweights, }\DecValTok{75}\NormalTok{)}
\NormalTok{iqr }\OperatorTok{=}\NormalTok{ q3 }\OperatorTok{{-}}\NormalTok{ q1}
\NormalTok{whisk1 }\OperatorTok{=}\NormalTok{ q1 }\OperatorTok{{-}}\NormalTok{ (}\FloatTok{1.5} \OperatorTok{*}\NormalTok{ iqr)}
\NormalTok{whisk2 }\OperatorTok{=}\NormalTok{ q3 }\OperatorTok{+}\NormalTok{ (}\FloatTok{1.5} \OperatorTok{*}\NormalTok{ iqr)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"The first quartile is }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(q1))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The second quartile is }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(q2))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The third quartile is }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(q3))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The interquartile range is }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(iqr))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"The whiskers are }\SpecialCharTok{\{\}}\StringTok{ and }\SpecialCharTok{\{\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(whisk1, whisk2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The first quartile is 108.0
The second quartile is 120.0
The third quartile is 131.0
The interquartile range is 23.0
The whiskers are 73.5 and 165.5
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-18-output-2.png}

}

\end{figure}

Here is a helpful visual that summarizes our discussion above.

\hypertarget{violin-plots}{%
\subsection{Violin Plots}\label{violin-plots}}

Another diagram that is useful in visualizing a variable's distribution
is the violin plot. A \textbf{violin plot} supplements a box-plot with a
smoothed density curve on either side of the plot. These density curves
highlight the relative frequency of variable's possible values. If you
look closely, you'll be able to discern the quartiles, whiskers, and
other hallmark features of the box-plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.violinplot(data }\OperatorTok{=}\NormalTok{ births, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-19-output-1.png}

}

\end{figure}

\hypertarget{comparing-quantitative-distributions}{%
\section{Comparing Quantitative
Distributions}\label{comparing-quantitative-distributions}}

Earlier in our discussion of the mode, we visualized two histograms that
described the distribution of birth weights for maternal smokers and
non-smokers. However, comparing these histograms was difficult because
they were displayed on seperate plots. Can we overlay the two to tell a
more compelling story?

In \texttt{seaborn}, multiple calls to a plotting library in the same
code cell will overlay the plots. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births\_maternal\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{False}\NormalTok{]}
\NormalTok{births\_non\_maternal\_smoker }\OperatorTok{=}\NormalTok{ births[births[}\StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \VariableTok{True}\NormalTok{]}

\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}
\NormalTok{             color }\OperatorTok{=} \StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, label }\OperatorTok{=} \StringTok{\textquotesingle{}smoker\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.histplot(data }\OperatorTok{=}\NormalTok{ births\_non\_maternal\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}
\NormalTok{             color }\OperatorTok{=} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, label }\OperatorTok{=} \StringTok{\textquotesingle{}nonsmoker\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-20-output-1.png}

}

\end{figure}

However, notice how this diagram suffers from overplotting. We can fix
this with a call to \texttt{sns.kdeplot}. This will remove the bins and
overlay the histogram with a density curve that better summarizes the
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.kdeplot(data }\OperatorTok{=}\NormalTok{ births\_maternal\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{, color }\OperatorTok{=} \StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, label }\OperatorTok{=} \StringTok{\textquotesingle{}smoker\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.kdeplot(data }\OperatorTok{=}\NormalTok{ births\_non\_maternal\_smoker, x }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{, color }\OperatorTok{=} \StringTok{\textquotesingle{}blue\textquotesingle{}}\NormalTok{, label }\OperatorTok{=} \StringTok{\textquotesingle{}nonsmoker\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.legend()}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-21-output-1.png}

}

\end{figure}

Unfortunately, we lose critical information in our distribution by
removing small details. Therefore, we typically prefer to use box-plots
and violin plots when comparing distributions. These are more concise
and allow us to compare summary statistics across many distributions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.violinplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-22-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.boxplot(data}\OperatorTok{=}\NormalTok{births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_1/visualization_1_files/figure-pdf/cell-23-output-1.png}

}

\end{figure}

\hypertarget{ridge-plots}{%
\section{Ridge Plots}\label{ridge-plots}}

Ridge plots show many density curves offset from one another with
minimal overlap. They are useful when the specific shape of each curve
is important.

\bookmarksetup{startatroot}

\hypertarget{visualization-ii}{%
\chapter{Visualization II}\label{visualization-ii}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Use KDE for estimating density curve.
\item
  Using transformations to analyze the relationship between two
  variables.
\item
  Evalauting quality of a visualization based on visualization theory
  concepts.
\end{itemize}

\end{tcolorbox}

\hypertarget{kernel-density-functions}{%
\section{Kernel Density Functions}\label{kernel-density-functions}}

\hypertarget{kde-mechanics}{%
\subsection{KDE Mechanics}\label{kde-mechanics}}

In the last lecture, we learned that density curves are smooth,
continuous functions that represent a distribution of values. In this
section, we'll learn how to construct density curves using Kernel
Density Estimation (KDE).

\hypertarget{smoothing}{%
\subsubsection{Smoothing}\label{smoothing}}

Kernel Density Estimation involves a technique called \textbf{smoothing}
- a process applied to a distribution of values that allows us to
analyze the more general structure of the dataset.

Many of the visualizations we learned during the last lecture are
examples of smoothing. Histograms are smoothed versions of
one-dimensional rug plots, and hex plots are smoother alternatives to
two-dimensional scatter plots. They remove the detail from individual
observations so we can visualize the larger properties of our
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{titanic }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{\textquotesingle{}titanic\textquotesingle{}}\NormalTok{)}
\NormalTok{sns.rugplot(titanic[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{],height }\OperatorTok{=} \FloatTok{0.5}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-2-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.histplot(titanic[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-3-output-1.png}

}

\end{figure}

\hypertarget{kernel-density-estimation}{%
\subsubsection{Kernel Density
Estimation}\label{kernel-density-estimation}}

\textbf{Kernel Density Estimation} is a smoothing technique that allows
us to estimate a density curve (also known as a probability density
function) from a set of observations. There are a few steps in this
process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Place a kernel at each data point
\item
  Normalize kernels to have total area of 1 (across all kernels)
\item
  Sum kernels together
\end{enumerate}

Suppose we have 5 data points: \([2.2, 2.8, 3.7, 5.3, 5.7]\). We wish to
recreate the following Kernel Density Estimate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ [}\FloatTok{2.2}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{3.7}\NormalTok{, }\FloatTok{5.3}\NormalTok{, }\FloatTok{5.7}\NormalTok{]}
\NormalTok{sns.kdeplot(data)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-4-output-1.png}

}

\end{figure}

Let's walk through each step to construct this density curve.

\hypertarget{step-1---place-a-kernel-at-each-data-point}{%
\paragraph{Step 1 - Place a Kernel at Each Data
Point}\label{step-1---place-a-kernel-at-each-data-point}}

To begin generating a density curve, we need to choose a \textbf{kernel}
and \textbf{bandwidth value (\(\alpha\))}. What are these exactly? A
\textbf{kernel} is a density curve itself, and the \textbf{bandwidth
(\(\alpha\))} is a measure of the kernel's width. Recall that a valid
density has an area of 1.

At each of our 5 points (depicted in the rug plot on the left), we've
placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We'll
explore what these are in the next section.

\textbf{Rugplot of Data}

\textbf{Kernelized Data}

\hypertarget{step-2---normalize-kernels-to-have-total-area-of-1}{%
\paragraph{Step 2 - Normalize Kernels to Have Total Area of
1}\label{step-2---normalize-kernels-to-have-total-area-of-1}}

Notice how these 5 kernels are density curves - meaning they each have
an area of 1. In Step 3, we will be summing each these kernels, and we
want the result to be a valid density that has an area of 1. Therefore,
it makes sense to normalize our current set of kernels by multiplying
each by \(\frac{1}{5}\).

\textbf{Kernelized Data}

\textbf{Normalized Kernels}

\hypertarget{step-3---sum-kernels-together}{%
\paragraph{Step 3 - Sum Kernels
Together}\label{step-3---sum-kernels-together}}

Our kernel density estimate (KDE) is the sum of the normalized kernels
along the x-axis. It is depicted below on the right.

\textbf{Normalized Kernels}

\textbf{Kernel Density Estimate}

\hypertarget{kernel-functions-and-bandwidth}{%
\subsection{Kernel Functions and
Bandwidth}\label{kernel-functions-and-bandwidth}}

\hypertarget{kernels}{%
\subsubsection{Kernels}\label{kernels}}

A \textbf{kernel} (for our purposes) is a valid density function. This
means it:

\begin{itemize}
\tightlist
\item
  Must be non-negative for all inputs.
\item
  Must integrate to 1.
\end{itemize}

A general ``KDE formula'' function is given above.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(K_{\alpha}(x, xi)\) is the kernel centered on the observation
  \texttt{i}.

  \begin{itemize}
  \tightlist
  \item
    Each kernel individually has area 1.
  \item
    x represents any number on the number line. It is the input to our
    function.
  \end{itemize}
\item
  \(n\) is the number of observed data points that we have.

  \begin{itemize}
  \tightlist
  \item
    We multiply by \(\frac{1}{n}\) so that the total area of the KDE is
    still 1.
  \end{itemize}
\item
  Each \(x_i \in \{x_1, x_2, \dots, x_n\}\) represents an observed data
  point.

  \begin{itemize}
  \tightlist
  \item
    These are what we use to create our KDE by summing multiple shifted
    kernels centered at these points.
  \end{itemize}
\end{enumerate}

*\(\alpha\) (alpha) is the bandwidth or smoothing parameter.

\hypertarget{gaussian-kernel}{%
\paragraph{Gaussian Kernel}\label{gaussian-kernel}}

The most common kernel is the \textbf{Gaussian kernel}. The Gaussian
kernel is equivalent to the Gaussian probability density function (the
Normal distribution), centered at the observed value \(x_i\) with a
standard deviation of \(\alpha\) (this is known as the
\textbf{bandwidth} parameter).

\(K_a(x, x_i) = \frac{1}{\sqrt{2\pi\alpha^{2}}}e^{-\frac{(x-x_i)^{2}}{2\alpha^{2}}}\)

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt }

\KeywordTok{def}\NormalTok{ gaussian\_kernel(alpha, x, z):}
    \ControlFlowTok{return} \FloatTok{1.0}\OperatorTok{/}\NormalTok{np.sqrt(}\FloatTok{2.} \OperatorTok{*}\NormalTok{ np.pi }\OperatorTok{*}\NormalTok{ alpha}\OperatorTok{**}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{(x }\OperatorTok{{-}}\NormalTok{ z) }\OperatorTok{**} \DecValTok{2} \OperatorTok{/}\NormalTok{ (}\FloatTok{2.0} \OperatorTok{*}\NormalTok{ alpha}\OperatorTok{**}\DecValTok{2}\NormalTok{))}

\NormalTok{xs }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{alpha }\OperatorTok{=} \DecValTok{1}
\NormalTok{kde\_curve }\OperatorTok{=}\NormalTok{ [gaussian\_kernel(alpha, x, }\DecValTok{0}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]}
\NormalTok{plt.plot(xs, kde\_curve)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-5-output-1.png}

}

\caption{The Gaussian kernel centered at 0 with bandwidth \(\alpha\) =
1.}

\end{figure}

If you've taken a probability class, you'll recognize that the mean of
this Gaussian kernel is \(x_i\) and the standard deviation is
\(\alpha\). Increasing \(\alpha\) - equivalently, the bandwidth -
smoothens the density curve. Larger values of \(\alpha\) are typically
easier to understand; however, we begin to lose important distributional
information.

Here is how adjusting \(\alpha\) affects a distribution in some variable
from an arbitrary dataset.

\textbf{Gaussian Kernel, \(\alpha\) = 0.1}

\textbf{Gaussian Kernel, \(\alpha\) = 1}

\textbf{Gaussian Kernel, \(\alpha\) = 2}

\textbf{Gaussian Kernel, \(\alpha\) = 10}

\hypertarget{boxcar-kernel}{%
\paragraph{Boxcar Kernel}\label{boxcar-kernel}}

Another example of a kernel is the \textbf{Boxcar kernel}. The boxcar
kernel assigns a uniform density to points within a ``window'' of the
observation, and a density of 0 elsewhere. The equation below is a
Boxcar kernel with the center at \(x_i\) and the bandwidth of
\(\alpha\).

\(K_a(x, x_i) = \begin{cases}  \frac{1}{\alpha}, & |x - x_i| \le \frac{\alpha}{2}\\  0, & \text{else }  \end{cases}\)

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ boxcar\_kernel(alpha, x, z):}
    \ControlFlowTok{return}\NormalTok{ (((x}\OperatorTok{{-}}\NormalTok{z)}\OperatorTok{\textgreater{}={-}}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{)}\OperatorTok{\&}\NormalTok{((x}\OperatorTok{{-}}\NormalTok{z)}\OperatorTok{\textless{}=}\NormalTok{alpha}\OperatorTok{/}\DecValTok{2}\NormalTok{))}\OperatorTok{/}\NormalTok{alpha}

\NormalTok{xs }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{alpha}\OperatorTok{=}\DecValTok{1}
\NormalTok{kde\_curve }\OperatorTok{=}\NormalTok{ [boxcar\_kernel(alpha, x, }\DecValTok{0}\NormalTok{) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]}
\NormalTok{plt.plot(xs, kde\_curve)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-6-output-1.png}

}

\caption{The Boxcar kernel centered at 0 with bandwidth \(\alpha\) = 1.}

\end{figure}

The diagram on the right is how the density curve for our 5 point
dataset would have looked had we used the Boxcar kernel with bandwidth
\(\alpha\) = 1.

\hypertarget{relationships-between-quantitative-variables}{%
\subsection{Relationships Between Quantitative
Variables}\label{relationships-between-quantitative-variables}}

Up until now, we've discussed how to visualize single-variable
distributions. Going beyond this, we want to understand the relationship
between pairs of numerical variables.

\hypertarget{scatter-plots}{%
\subsubsection{Scatter Plots}\label{scatter-plots}}

\textbf{Scatter plots} are one of the most useful tools in representing
the relationship between two quantitative variables. They are
particularly important in gauging the strength, or correlation between
variables. Knowledge of these relationships can then motivate decisions
in our modeling process.

For example, let's plot a scatter plot comparing the
\texttt{Maternal\ Height} and \texttt{Birth\ Weight} colums, using both
\texttt{matplotlib} and \texttt{seaborn}.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{births }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/baby.csv"}\NormalTok{)}
\NormalTok{births.head(}\DecValTok{5}\NormalTok{)}

\CommentTok{\# Matplotlib Example}
\NormalTok{plt.scatter(births[}\StringTok{\textquotesingle{}Maternal Height\textquotesingle{}}\NormalTok{], births[}\StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{])}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}Maternal Height\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Seaborn Example}
\NormalTok{sns.scatterplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Height\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}
\NormalTok{                hue }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<AxesSubplot: xlabel='Maternal Height', ylabel='Birth Weight'>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-8-output-2.png}

}

\end{figure}

This is an example where color is used to add a third dimension to our
plot. This is possible with the \texttt{hue} paramater in
\texttt{seaborn}, which adds a categorical column encoding to an
existing visualization. This way, we can look for relationships in
\texttt{Maternal\ Height} and \texttt{Birth\ Weight} in both maternal
smokers and non-smokers. If we wish to see the relationship's strength
more clearly, we can use \texttt{sns.lmplot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.lmplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Height\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{, }
\NormalTok{           hue }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{, ci }\OperatorTok{=} \VariableTok{False}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-9-output-1.png}

}

\end{figure}

We can make out a weak, positive relationship in the mother's height and
birth weight for both maternal smokers and non-smokers (the baseline is
slightly lower in maternal smokers).

\hypertarget{overplotting}{%
\subsection{Overplotting}\label{overplotting}}

As you may have noticed, the scatterplots of \texttt{Maternal\ Height}
vs.~\texttt{Birth\ Weight} have many densely plotted areas. Many of the
points are on top of one other! This makes it difficult to tell exactly
how many babies are plotted in each the more densely populated regions
of the graph. This can arise when the tools used for measuring data have
low granularity, many different values are rounded to the same value, or
if the ranges of the two variables differ greatly in scale.

We can overcome this by introducing a small amount of uniform random
noise to our data. This is called \emph{jittering}. Let's see what
happens when we introduce noise to the \texttt{Maternal\ Height}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{births[}\StringTok{"Maternal Height (jittered)"}\NormalTok{] }\OperatorTok{=}\NormalTok{ births[}\StringTok{"Maternal Height"}\NormalTok{] }\OperatorTok{+}\NormalTok{ np.random.uniform(}\OperatorTok{{-}}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\BuiltInTok{len}\NormalTok{(births))}
\NormalTok{sns.lmplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Height (jittered)\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{, }
\NormalTok{           hue }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Smoker\textquotesingle{}}\NormalTok{, ci }\OperatorTok{=} \VariableTok{False}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-10-output-1.png}

}

\end{figure}

This plot more clearly shows that most of the data is clustered tightly
around the point (62.5,120) and gradually becomes more loose further
away from the center. It is much easier for us and others to see how the
data is distributed. In conclusion, \emph{jittering} helps us better
understand our own data (Goal 1) and communicate results to others (Goal
2).

\hypertarget{hex-plots-and-contour-plots}{%
\subsubsection{Hex Plots and Contour
Plots}\label{hex-plots-and-contour-plots}}

Unfortunately, our scatter plots above suffered from overplotting, which
made them hard to interpret. And with a large number of points,
jittering is unlikely to resolve the issue. Instead, we can look to hex
plots and contour plots.

\textbf{Hex Plots} can be thought of as a two dimensional histogram that
shows the joint distribution between two variables. This is particularly
useful working with very dense data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.jointplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Pregnancy Weight\textquotesingle{}}\NormalTok{, }
\NormalTok{              y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{, kind }\OperatorTok{=} \StringTok{\textquotesingle{}hex\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<seaborn.axisgrid.JointGrid at 0x7f988c881ee0>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-11-output-2.png}

}

\end{figure}

The axes are evidently binned into hexagons, which makes the linear
relationship easier to decipher. Darker regions generally indicate a
higher density of points.

On the other hand, \textbf{contour plots} are two dimensional versions
of density curves with marginal distributions of each variable on the
axes. We've used very similar code here to generate our contour plots,
with the addition of the
\texttt{kind\ =\ \textquotesingle{}kde\textquotesingle{}} and
\texttt{fill\ =\ True} arguments.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.jointplot(data }\OperatorTok{=}\NormalTok{ births, x }\OperatorTok{=} \StringTok{\textquotesingle{}Maternal Height\textquotesingle{}}\NormalTok{, y }\OperatorTok{=} \StringTok{\textquotesingle{}Birth Weight\textquotesingle{}}\NormalTok{,}\OperatorTok{\textbackslash{}}
\NormalTok{              kind }\OperatorTok{=} \StringTok{\textquotesingle{}kde\textquotesingle{}}\NormalTok{, fill }\OperatorTok{=} \VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
<seaborn.axisgrid.JointGrid at 0x7f988c018e50>
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{visualization_2/visualization_2_files/figure-pdf/cell-12-output-2.png}

}

\end{figure}

\hypertarget{transformations}{%
\section{Transformations}\label{transformations}}

These last two lectures have covered visualizations in great depth. We
looked at various forms of visualizations, plotting libraries, and
high-level theory.

Much of this was done to uncover insights in data, which will prove
necessary for the modeling process. A strong graphical correlation
between two variables hinted an underlying relationship that has reason
for further study. However, relying on visual relationships alone is
limiting - not all plots show association. The presence of outliers and
other statistical anomalies make it hard to interpret data.

\textbf{Transformations} are the process of manipulating data to find
significant relationships between variables. These are often found by
applying mathematical functions to variables that ``transform'' their
range of possible values and highlight some previously hidden
associations between data.

\hypertarget{transforming-a-distribution}{%
\subsubsection{Transforming a
Distribution}\label{transforming-a-distribution}}

When a distribution has a large dynamic range, it can be useful to take
the logarithm of the data. For example, computing the logarithm of the
ticket prices on the Titanic reduces skeweness and yields a distribution
that is more ``spread'' across the x-axis. While it makes individual
observations harder to interpret, the distribution is more favorable for
subsequent analysis.

\hypertarget{linearizing-a-relationship}{%
\subsubsection{Linearizing a
Relationship}\label{linearizing-a-relationship}}

Transformations are perhaps most useful to \textbf{linearize a
relationship} between variables. If we find a transformation to make a
scatter plot of two variables linear, we can ``backtrack'' to find the
exact relationship between the variables. Linear relationships are
particularly simple to interpret, and we'll be doing a lot of linear
modeling in Data 100 - starting next week!

Say we want to understand the relationship between healthcare and life
expectancy. Intuitively there should be a positive correlation, but upon
plotting values from a dataset, we find a non-linear relationship that
is somewhat hard to understand. However, applying a logarithmic
transformation to both variables - healthcare and life expectancy -
results in a scatter plot with a linear trend that we can interpret.

How can we find the relationship between the original variables? We know
that taking a log of both axes gave us a linear relationship, so we can
say (roughly) that

\[\log y= a\times\log x + b\]

Solving for \(y\) implies a \textbf{power} relationship in the original
plot.

\[y= e^{a\times\log x + b}\] \[y= Ce^{a\times\log x}\] \[y= Cx^{a}\]

How did we know that taking the logarithm of both sides would result in
a linear relationship? The \textbf{Tukey-Mosteller Bulge Diagram} is
helpful here. We can use the direction of the buldge in our original
data to find the appropriate transformations that will linearize the
relationship. These transformations are found on axes that are nearest
to the buldge. The buldge in our earlier example lay in Quadrant 2, so
the transformations \(\log x\), \(\sqrt x\), \(y^{2}\), or \(y^{3}\) are
possible contenders. It's important to note that this diagram is not
perfect, and some transformations will work better than others. In our
case, \(\log x\) and \(\log y\) (found in Quadrant 3) were the best.

\hypertarget{additional-remarks}{%
\subsubsection{Additional Remarks}\label{additional-remarks}}

Visualization requires a lot of thought! - There are many tools for
visualizing distributions. - Distribution of a single variable: 1. rug
plot 2. histogram 3. density plot 4. box plot 5. violin plot - Joint
distribution of two quantitative variables: 1. scatter plot 2. hex plot
3. contour plot.

This class primarily uses \texttt{seaborn} and \texttt{matplotlib}, but
\texttt{Pandas} also has basic built-in plotting methods. Many other
visualization libraries exist, and \texttt{plotly} is one of them. -
\texttt{plotly} creates very easily creates interactive plots. -
\texttt{plotly} will occasionally appear in lecture code, labs, and
assignments!

Next, we'll go deeper into the theory behind visualization.

\hypertarget{visualization-theory}{%
\section{Visualization Theory}\label{visualization-theory}}

This section marks a pivot to the second major topic of this lecture -
visualization theory. We'll discuss the abstract nature of
visualizations and analyze how they convey information.

Remember, we had two goals for visualizing data. This section is
particularly important in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Helping us understand the data and results
\item
  Communicating our results and conclusions with others
\end{enumerate}

\hypertarget{information-channels}{%
\subsection{Information Channels}\label{information-channels}}

Visualizations are able to convey information through various encodings.
In the remainder of this lecture, we'll look at the use of color, scale,
and depth, to name a few.

\hypertarget{encodings-in-rugplots}{%
\subsubsection{Encodings in Rugplots}\label{encodings-in-rugplots}}

One detail that we may have overlooked in our earlier discussion of
rugplots is the importance of encodings. Rugplots are effective visuals
because they utilize line thickness to encode frequency. Consider the
following diagram:

\hypertarget{multi-dimensional-encodings}{%
\subsubsection{Multi-Dimensional
Encodings}\label{multi-dimensional-encodings}}

Encodings are also useful for representing multi-dimensional data.
Notice how the following visual highlights four distinct ``dimensions''
of data:

\begin{itemize}
\tightlist
\item
  X-axis
\item
  Y-axis
\item
  Area
\item
  Color
\end{itemize}

The human visual perception sytem is only capable of visualizing data in
a three-dimensional plane, but as you've seen, we can encode many more
channels of information.

\hypertarget{harnessing-the-axes}{%
\subsection{Harnessing the Axes}\label{harnessing-the-axes}}

\hypertarget{consider-scale-of-the-data}{%
\subsubsection{Consider Scale of the
Data}\label{consider-scale-of-the-data}}

However, we should be careful to not misrepresent relationships in our
data by manipulating the scale or axes. The visualization below
improperly portrays two seemingly independent relationships on the same
plot. The authors have clearly changed the scale of the y-axis to
mislead their audience.

Notice how the downwards-facing line segment contains values in the
millions, while the upwards-trending segment only contains values near
three hundred thousand. These lines should not be intersecting.

When there is a large difference in the magnitude of the data, it's
advised to analyze percentages instead of counts. The following diagrams
correctly display the trends in cancer screening and abortion rates.

\hypertarget{reveal-the-data}{%
\subsubsection{Reveal the Data}\label{reveal-the-data}}

Great visualizations not only consider the scale of the data, but also
utilize the axes in a way that best conveys information. For example,
data scientists commonly set certain axes limits to highlight parts of
the visualization they are most interested in.

The visualization on the right captures the trend in coronavirus cases
during the month March in 2020. From only looking at the visualization
on the left, a viewer may incorrectly believe that coronavirus began to
skyrocket on March 4\textsuperscript{th}, 2020. However, the second
illustration tells a different story - cases rose closer to March
21\textsuperscript{th}, 2020.

\hypertarget{harnessing-color}{%
\subsection{Harnessing Color}\label{harnessing-color}}

Color is another important feature in visualizations that does more than
what meets the eye.

Last lecture, we used color to encode a categorical variable in our
scatter plot. In this section, we will discuss uses of color in novel
visualizations like colormaps and heatmaps.

5-8\% of the world is red-green color blind, so we have to be very
particular about our color scheme. We want to make these as accessible
as possible. Choosing a set of colors which work together is evidently a
challenging task!

\hypertarget{colormaps}{%
\subsubsection{Colormaps}\label{colormaps}}

Colormaps are mappings from pixel data to color values, and they're
often used to highlight distinct parts of an image. Let's investigate a
few properties of colormaps.

\textbf{Jet Colormap}

\textbf{Viridis Colormap}

The jet colormap is infamous for being misleading. While it seems more
vibrant than viridis, the aggressive colors poorly encode numerical
data. To understand why, let's analyze the following images.

The diagram on the left compares how a variety of colormaps represent
pixel data that transitions from a high to low intensity. These include
the jet colormap (row a) and grayscale (row b). Notice how the grayscale
images do the best job in smoothly transitioning between pixel data. The
jet colormap is the worst at this - the four images in row (a) look like
a conglomeration of individual colors.

The difference is also evident in the images labeled (a) and (b) on the
left side. The grayscale image is better at preserving finer detail in
the vertical line strokes. Additionally, grayscale is preferred in x-ray
scans for being more neutral. The intensity of dark red color in the jet
colormap is frightening and indicates something is wrong.

Why is the jet colormap so much worse? The answer lies in how its color
composition is percieved to the human eye.

\textbf{Jet Colormap Perception}

\textbf{Viridis Colormap Perception}

The jet colormap is largely misleading because it is not perceptually
uniform. \textbf{Perceptually uniform colormaps} have the property that
if the pixel data goes from 0.1 to 0.2, the perceptual change is the
same as when the data goes from 0.8 to 0.9.

Notice how the said uniformity is present within the linear trend
displayed in the viridis colormap. On the other hand, the jet colormap
is largely non-linear - this is precisely why it's considered a worse
colormap.

\hypertarget{harnessing-markings}{%
\subsection{Harnessing Markings}\label{harnessing-markings}}

In our earlier discussion of multi-dimensional encodings, we analyzed a
scatter plot with four pseudo-dimensions: the two axes, area, and color.
Were these appropriate to use? The following diagram analyzes how well
the human eye can distinguish between these ``markings''.

There are a few key takeaways from this diagram

\begin{itemize}
\tightlist
\item
  Lengths are easy to discern. Don't use plots with jiggled baselines -
  keep everything axis-aligned.
\item
  Avoid pie charts! Angle judgements are inaccurate.
\item
  Areas and volumes are hard to distinguish (area charts, word clouds,
  etc)
\end{itemize}

\hypertarget{harnessing-conditioning}{%
\subsection{Harnessing Conditioning}\label{harnessing-conditioning}}

Conditioning is the process of comparing data that belong to seperate
groups. We've seen this before in overlayed distributions, side-by-side
box-plots, and scatter plots with categorical encodings. Here, we'll
introduce terminology that formalizes these examples.

Consider an example where we want to analyze income earnings for male
and females with varying levels of education. There are multiple ways to
compare this data.

The barplot is an example of \textbf{juxtaposition}: placing multiple
plots side by side, with the same scale. The scatter plot is an example
of \textbf{superposition}: placing multiple density curves, scatter
plots on top of each other.

Which is better depends on the problem at hand. Here, superposition
makes the precise wage difference very clear from a quick glance. But
many sophisticated plots convey information that favors the use of
juxtaposition. Below is one example.

\hypertarget{harnessing-context}{%
\subsection{Harnessing Context}\label{harnessing-context}}

The last component to a great visualization is perhaps the most critical
- the use of context. Adding informative titles, axis labels, and
descriptive captions are all best practices that we've heard repeatedly
in Data 8.

A publication-ready plot (and every Data 100 plot) needs:

\begin{itemize}
\tightlist
\item
  Informative title (takeaway, not description)
\item
  Axis labels
\item
  Reference lines, markers, etc
\item
  Legends, if appropriate
\item
  Captions that describe data
\end{itemize}

Captions should be:

\begin{itemize}
\tightlist
\item
  Comprehensive and self-contained
\item
  Describe what has been graphed
\item
  Draw attention to important features
\item
  Describe conclusions drawn from graphs
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Understand how to appropriately collect data to help answer a
  question.
\end{itemize}

\end{tcolorbox}

In Data Science, understanding characteristics of a population starts
with having quality data to investigate. While it is often impossible to
collect all the data describing a population, we can overcome this by
properly sampling from the population. In this note, we will discuss
appropriate techniques for sampling from populations.

\begin{figure}

{\centering \includegraphics{sampling/images/data_life_cycle_sampling.png}

}

\caption{Lifecycle diagram}

\end{figure}

\hypertarget{censuses-and-surveys}{%
\section{Censuses and Surveys}\label{censuses-and-surveys}}

In general: a \textbf{census} is ``an official count or survey of a
population, typically recording various details of individuals.''

\begin{itemize}
\tightlist
\item
  Example: The U.S. Decennial Census was held in April 2020, and it
  counts \textbf{every person} living in all 50 states, DC, and US
  territories. (Not just citizens.) Participation is required by law (it
  is mandated by the U.S. Constitution). Important uses include the
  allocation of Federal funds, congressional representation, and drawing
  congressional and state legislative districts. The census is composed
  of a \textbf{survey} mailed to different housing addresses in the
  United States.
\item
  \textbf{Individuals} in a population are not always people. Other
  populations include: bacteria in your gut (sampled using DNA
  sequencing); trees of a certain species; small businesses receiving a
  microloan; or published results in an academic journal / field.
\end{itemize}

A \textbf{survey} is a set of questions. An example is workers sampling
individuals and households. What is asked, and how it is asked, can
affect how the respondent answers, or even whether the respondent
answers in the first place.

While censuses are great, it is often difficult and expensive to survey
everyone in a population. Thus, we usually survey a subset of the
population instead.

A \textbf{sample} is often used to make inferences about the population.
That being said, how the sample is drawn will affect the reliability of
such inferences. Two common source of error in sampling are
\textbf{chance error}, where random samples can vary from what is
expected, in any direction; and \textbf{bias}, which is a a systematic
error in one direction.

Because of how surveys and samples are drawn, it turns out that samples
are usually---but not always---a subset of the population: *
\textbf{Population}: The group that you want to learn something about. *
\textbf{Sampling Frame}: The list from which the sample is drawn. For
example, if sampling people, then the sampling frame is the set of all
people that could possibly end up in your sample. * \textbf{Sample}: Who
you actually end up sampling. The sample is therefore a subset of your
\emph{sampling frame}.

While ideally these three sets would be exactly the same, in practice
they usually aren't. For example, there may be individuals in your
sampling frame (and hence, your sample) that are not in your population.
And generally, sample sizes are much smaller than population sizes.

\begin{figure}

{\centering \includegraphics{sampling/images/samplingframe.png}

}

\caption{Sampling\_Frames}

\end{figure}

\hypertarget{bias-a-case-study}{%
\section{Bias: A Case Study}\label{bias-a-case-study}}

The following case study is adapted from \emph{Statistics} by Freedman,
Pisani, and Purves, W.W. Norton NY, 1978.

In 1936, President Franklin D. Roosevelt (D) went up for re-election
against Alf Landon (R) . As is usual, \textbf{polls} were conducted in
the months leading up to the election to try and predict the outcome.
The \emph{Literary Digest} was a magazine that had successfully
predicted the outcome of 5 general elections coming into 1936. In their
polling for the 1936 election, they sent out their survey to 10 million
individuals, who they found from phone books, lists of magazine
subscribers, and lists of country club members. Of the roughly 2.4
million people who filled out the survey, only 43\% reported they would
vote for Roosevelt; thus the \emph{Digest} predicted that Landon would
win.

On election day, Roosevelt won in a landslide, winning 61\% of the
popular vote of about 45 million voters. How could the \emph{Digest}
have been so wrong with their polling?

It turns out that the \emph{Literary Digest} sample was not
representative of the population. Their sampling frame inherently skewed
towards more affluent voters, who tended to vote Republican, and they
completely overlooked the lion's share of voters who were still
suffering through the Great Depression. Furthermore, they had a dismal
response rate (about 24\%); who knows how the other non-respondents
would have polled? The \emph{Digest} folded just 18 months after this
disaster.

At the same time, George Gallup, a rising statistician, also made
predictions about the 1936 elections. His estimate (56\% Roosevelt) was
much closer despite having a smaller sample size of ``only'' 50,000
(still more than necessary; more when we cover the Central Limit
Theorem). Gallup also predicted the \emph{Digest}'s prediction within
1\%, with a sample size of only 3000 people. He did so by anticipating
the \emph{Digest}'s affluent sampling frame and subsampled those
individuals. The \textbf{Gallup Poll} today is one of the leading polls
for election results.

So what's the moral of the story? Samples, while convenient, are subject
to chance error and \textbf{bias}. Election polling, in particular, can
involve many sources of bias. To name a few: * \textbf{Selection bias}
systematically excludes (or favors) particular groups. *
\textbf{Response bias} occurs because people don't always respond
truthfully. Survey designers pay special detail to the nature and
wording of questions to avoid this type of bias. * \textbf{Non-response
bias} occurs because people don't always respond to survey requests,
which can skew responses. For example, the Gallup poll is conducted
through landline phone calls, but many different populations in the U.S.
do not pay for a landline, and still more do not always answer the
phone. Surveyers address this bias by staying persistent and keeping
surveys short.

\hypertarget{probability-samples}{%
\section{Probability Samples}\label{probability-samples}}

When sampling, it is essential to focus on the quality of the sample
rather than the quantity of the sample. A huge sample size does not fix
a bad sampling method. Our main goal is to gather a sample that is
representative of the population it came from. The most common way to
accomplish this is by randomly sampling from the population.

\begin{itemize}
\tightlist
\item
  A \textbf{convenience sample} is whatever you can get ahold of. Note
  that haphazard sampling is not necessarily random sampling; there are
  many potential sources of bias.
\item
  In a \textbf{probability sample}, we know the chance any given set of
  individuals will be in the sample.

  \begin{itemize}
  \tightlist
  \item
    Probability samples allow us to estimate the bias and chance error,
    which helps us quantify uncertainty (more in a future lecture).
  \item
    Note that this does not imply that all individuals in the population
    need have the same chance of being selected (see: stratified random
    samples).
  \item
    Further note that the real world is usually more complicated. For
    example, we do not generally know the probability that a given
    bacterium is in a microbiome sample, or whether people will answer
    when Gallup calls landlines. That being said, we try to model
    probability sampling where possible if the sampling or measurement
    process is not fully under our control.
  \end{itemize}
\end{itemize}

A few common random sampling schemes: * A \textbf{random sample with
replacement} is a sample drawn \textbf{uniformly} at random
\textbf{with} replacement. * Random doesn't always mean ``uniformly at
random,'' but in this specific context, it does. * Some individuals in
the population might get picked more than once

\begin{itemize}
\tightlist
\item
  A \textbf{simple random sample (SRS)} is a sample drawn uniformly at
  random without replacement.

  \begin{itemize}
  \tightlist
  \item
    Every individual (and subset of individuals) has the same chance of
    being selected.
  \item
    Every pair has the same chance as every other pair.
  \item
    Every triple has the same chance as every other triple.
  \item
    And so on.
  \end{itemize}
\item
  A \textbf{stratified random sample}, where random sampling is
  performed on strata (specific groups), and the groups together compose
  a sample.
\end{itemize}

\hypertarget{example-stratified-random-sample}{%
\subsection{Example: Stratified random
sample}\label{example-stratified-random-sample}}

Suppose that we are trying to run a poll to predict the mayoral election
in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose
we try a \textbf{stratified random sample} to select 100 voters as
follows: 1. First, we take a simple random sample and obtain 50 voters
that are above the median city income (``above-median-income''), i.e.,
in the upper 50-th percentile of income in the city. 2. We then take a
simple random sample of the other 50 from voters that are below the
median city income.

This is a \textbf{probability sample}: For any group of 100 people, if
there are not exactly 50 ``above-median-income'' voters, then that group
has zero probability of being chosen. For any other group (which has
exactly 50 ``above-median-income'' voters), then the chance of it being
chosen is 1/ \# of such groups.

Note that even if we replace the group counts with 80/20 (80
``above-median-income'' voters, 20 others), then it is still a
probability sample, because we can compute the precise probability of
each group being chosen. However, the sampling scheme (and thus the
modeling of voter preferences) becomes biased towards voters with income
above the median.

\hypertarget{approximating-simple-random-sampling}{%
\section{Approximating Simple Random
Sampling}\label{approximating-simple-random-sampling}}

The following is a very common situation in data science: - We have an
enormous population. - We can only afford to sample a relatively small
number of individuals. If the population is huge compared to the sample,
then random sampling with and without replacement are pretty much the
same.

\textbf{Example} : Suppose there are 10,000 people in a population.
Exactly 7,500 of them like Snack 1; the other 2,500 like Snack 2. What
is the probability that in a random sample of 20, all people like Snack
1?

\begin{itemize}
\tightlist
\item
  Method 1: SRS (Random Sample Without Replacement):
  \(\prod\limits_{k=0}^{19} \dfrac{7500 - k}{10000 - k} \approx 0.003151\)
\item
  Method 2: Random Sample with Replacement:
  \((0.75)^{20} \approx 0.003171\)
\end{itemize}

As seen here, when the population size is large, probabilities of
sampling with replacement are much easier to compute and lead to a
reasonable approximation.

\hypertarget{multinomial-probabilities}{%
\subsection{Multinomial Probabilities}\label{multinomial-probabilities}}

The approximation discussed above suggests the convenience of
\textbf{multinomial probabilities}, which arise from sampling a
categorical distribution at random **with replacement*.

Suppose that we have a bag of marbles with the following distribution:
60\% are blue, 30\% are green, and 10\% are red. If we then proceed to
draw 100 marbles from this bag, at random with replacement, then the
resulting 100-size sample is modeled as a multinomial distribution using
\texttt{np.random.multinomial}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{np.random.multinomial(}\DecValTok{100}\NormalTok{, [}\FloatTok{0.60}\NormalTok{, }\FloatTok{0.30}\NormalTok{, }\FloatTok{0.10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([61, 25, 14])
\end{verbatim}

This method allows us to generate, say, 10 samples of size 100 using the
\texttt{size} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.multinomial(}\DecValTok{100}\NormalTok{, [}\FloatTok{0.60}\NormalTok{, }\FloatTok{0.30}\NormalTok{, }\FloatTok{0.10}\NormalTok{], size}\OperatorTok{=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[56, 36,  8],
       [58, 34,  8],
       [56, 37,  7],
       [67, 24,  9],
       [53, 36, 11],
       [60, 29, 11],
       [65, 25, 10],
       [56, 35,  9],
       [53, 36, 11],
       [73, 20,  7]])
\end{verbatim}

\hypertarget{comparing-convenience-sample-and-srs}{%
\section{Comparing Convenience Sample and
SRS}\label{comparing-convenience-sample-and-srs}}

Suppose that we are trying to run a poll to predict the mayoral election
in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose
we took a sample to predict the election outcome by polling all
retirees. Even if they answer truthfully, we have a \textbf{convenience
sample}. How biased would this sample be in predicting the results?
While we will not numerically quantify the bias, in this demo we'll
visually show that because of the voter population distribution, any
error in our prediction from a retiree sample cannot be simply due to
chance:

First, let's grab a data set that has every single voter in the
Bearkeley (again, this is a fake dataset) and how they \textbf{actually}
voted in the election. For the purposes of this example, assume: *
``high income'' indicates a voter is above the median household income,
which is \$97,834 (actual Berkeley number). * There are only two mayoral
candidates: one Democrat and one Republican. * Every registered voter
votes in the election for the candidate under their registered party
(Dem or Rep).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{bearkeley }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/bearkeley.csv"}\NormalTok{)}

\CommentTok{\# create a 1/0 int that indicates democratic vote}
\NormalTok{bearkeley[}\StringTok{\textquotesingle{}vote.dem\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ (bearkeley[}\StringTok{\textquotesingle{}vote\textquotesingle{}}\NormalTok{] }\OperatorTok{==} \StringTok{\textquotesingle{}Dem\textquotesingle{}}\NormalTok{).astype(}\BuiltInTok{int}\NormalTok{)}
\NormalTok{bearkeley.head()}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrllr}
\toprule
{} &  age &  high\_income & vote &  vote.dem \\
\midrule
0 &   35 &        False &  Dem &         1 \\
1 &   42 &         True &  Rep &         0 \\
2 &   55 &        False &  Dem &         1 \\
3 &   77 &         True &  Rep &         0 \\
4 &   31 &        False &  Dem &         1 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bearkeley.shape}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(1300000, 4)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{actual\_vote }\OperatorTok{=}\NormalTok{ np.mean(bearkeley[}\StringTok{"vote.dem"}\NormalTok{])}
\NormalTok{actual\_vote}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.5302792307692308
\end{verbatim}

This is the \textbf{actual outcome} of the election. Based on this
result, the Democratic candidate would win. However, if we were to only
consider retiree voters (a retired person is anyone age 65 and up):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{convenience\_sample }\OperatorTok{=}\NormalTok{ bearkeley[bearkeley[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{] }\OperatorTok{\textgreater{}=} \DecValTok{65}\NormalTok{]}
\NormalTok{np.mean(convenience\_sample[}\StringTok{"vote.dem"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.3744755089093924
\end{verbatim}

Based on this result, we would have predicted that the Republican
candidate would win! This error is not due to the sample being too small
to yield accurate predictions, because there are 359,396 retirees (about
27\% of the 1.3 million Bearkeley voters). Instead, there seems to be
something larger happening. Let's visualize the voter preferences of the
entire population to see how retirees trend:

Let us aggregate all voters by age and visualize the fraction of
Democratic voters, split by income.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ matplotlib.ticker }\ImportTok{as}\NormalTok{ ticker}

\NormalTok{sns.set\_theme(style}\OperatorTok{=}\StringTok{\textquotesingle{}darkgrid\textquotesingle{}}\NormalTok{, font\_scale }\OperatorTok{=} \FloatTok{1.5}\NormalTok{,}
\NormalTok{              rc}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}figure.figsize\textquotesingle{}}\NormalTok{:(}\DecValTok{7}\NormalTok{,}\DecValTok{5}\NormalTok{)\})}

\CommentTok{\# aggregate all voters by age}
\NormalTok{votes\_by\_demo }\OperatorTok{=}\NormalTok{ bearkeley.groupby([}\StringTok{"age"}\NormalTok{,}\StringTok{"high\_income"}\NormalTok{]).agg(}\StringTok{"mean"}\NormalTok{).reset\_index()}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}\OperatorTok{;}
\NormalTok{red\_blue }\OperatorTok{=}\NormalTok{ [}\StringTok{"\#bf1518"}\NormalTok{, }\StringTok{"\#397eb7"}\NormalTok{]}
\ControlFlowTok{with}\NormalTok{ sns.color\_palette(sns.color\_palette(red\_blue)):}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ sns.pointplot(data}\OperatorTok{=}\NormalTok{votes\_by\_demo, x }\OperatorTok{=} \StringTok{"age"}\NormalTok{, y }\OperatorTok{=} \StringTok{"vote.dem"}\NormalTok{, hue }\OperatorTok{=} \StringTok{"high\_income"}\NormalTok{)}

\NormalTok{ax.set\_title(}\StringTok{"Voting preferences by demographics"}\NormalTok{)}
\NormalTok{fig.canvas.draw()}
\NormalTok{new\_ticks }\OperatorTok{=}\NormalTok{ [i.get\_text() }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ ax.get\_xticklabels()]}\OperatorTok{;}
\NormalTok{plt.xticks(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\BuiltInTok{len}\NormalTok{(new\_ticks), }\DecValTok{10}\NormalTok{), new\_ticks[::}\DecValTok{10}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{sampling/sampling_files/figure-pdf/cell-8-output-1.png}

}

\end{figure}

From the plot above, we see that retirees in the imaginary city of
Bearkeley tend to vote less Democrat, which skewed our predictions from
our sample. We also note that high-income voters tend to vote less
Democrat (and more Republican).

Let's compare our biased convenience sample to a simple random sample.
Supposing we took an SRS the same size as our retiree sample, we see
that we get a result very close to the actual vote:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# By default, replace = False}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(convenience\_sample)}
\NormalTok{random\_sample }\OperatorTok{=}\NormalTok{ bearkeley.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{)}

\NormalTok{np.mean(random\_sample[}\StringTok{"vote.dem"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.5295189707175372
\end{verbatim}

This is very close to the actual vote!

We could even get pretty close with a \emph{much smaller sample size},
say 800:

It turns out that we are pretty close, \textbf{much smaller sample
size}, say, 800 (we'll learn how to choose this number when we introduce
the Central Limit Theorem):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OperatorTok{=} \DecValTok{800}
\NormalTok{random\_sample }\OperatorTok{=}\NormalTok{ bearkeley.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{np.mean(random\_sample[}\StringTok{"vote.dem"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.56
\end{verbatim}

To visualize the chance error in an SRS, let's simulate 1000 samples of
the 800-size Simple Random Sample:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{poll\_result }\OperatorTok{=}\NormalTok{ []}
\NormalTok{nrep }\OperatorTok{=} \DecValTok{1000}   \CommentTok{\# number of simulations}
\NormalTok{n }\OperatorTok{=} \DecValTok{800}       \CommentTok{\# size of our sample}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,nrep):}
\NormalTok{    random\_sample }\OperatorTok{=}\NormalTok{ bearkeley.sample(n, replace }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{    poll\_result.append(np.mean(random\_sample[}\StringTok{"vote.dem"}\NormalTok{]))}
\NormalTok{sns.histplot(poll\_result, stat}\OperatorTok{=}\StringTok{\textquotesingle{}density\textquotesingle{}}\NormalTok{, kde}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# What fraction of these simulated samples would have predicted Democrat?}
\NormalTok{poll\_result }\OperatorTok{=}\NormalTok{ pd.Series(poll\_result)}
\NormalTok{np.}\BuiltInTok{sum}\NormalTok{(poll\_result }\OperatorTok{\textgreater{}=} \FloatTok{0.5}\NormalTok{)}\OperatorTok{/}\DecValTok{1000}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.963
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{sampling/sampling_files/figure-pdf/cell-11-output-2.png}

}

\end{figure}

A few observations: First, the KDE looks roughly Gaussian. Second,
supposing that we predicted a Democratic winner if 50\% of our sample
voted Democrat, then just about 4\% of our simulated samples would have
predicted the election result incorrectly. This visualization further
justifies why our convenience sample had error that was not entirely
just due to chance. We'll revisit this notion later in the course.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

Understanding the sampling process is what lets us go from describing
the data to understanding the world. Without knowing / assuming
something about how the data were collected, there is no connection
between the sample and the population. Ultimately, the dataset doesn't
tell us about the world behind the data.

\bookmarksetup{startatroot}

\hypertarget{introduction-to-modeling}{%
\chapter{Introduction to Modeling}\label{introduction-to-modeling}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Understand what models are and how to carry out the four-step modeling
  process
\item
  Define the concept of loss and gain familiarity with L1 and L2 loss
\item
  Fit a model using minimization techniques
\end{itemize}

\end{tcolorbox}

Up until this point in the semester, we've focused on analyzing
datasets. We've looked into the early stages of the data science
lifecycle, focusing on the programming tools, visualization techniques,
and data cleaning methods needed for data analysis.

This lecture marks a shift in focus. We will move away from examining
datasets to actually \emph{using} our data to better understand the
world. Specifically, the next sequence of lectures will explore
predictive modeling: generating models to make some prediction about the
world around us. In this lecture, we'll introduce the conceptual
framework for setting up a modeling task. In the next few lectures,
we'll put this framework into practice by implementing several kinds of
models.

\hypertarget{what-is-a-model}{%
\section{What is a Model?}\label{what-is-a-model}}

A model is an \textbf{idealized representation} of a system. A system is
a set of principles or procedures according to which something
functions. We live in a world full of systems: the procedure of turning
on a light happens according to a specific set of rules dictating the
flow of electricity. The truth behind how any event occurs are usually
complex, and many times the specifics are unknown. The workings of the
world can be viewed is its own giant procedure. Models seek to simplify
the world and distill them it into workable pieces.

Example: We model the fall of an object on Earth as subject to a
constant acceleration of \(9.81 \frac{m}{s^2}\) due to gravity.

\begin{itemize}
\tightlist
\item
  While this describes the behavior of our system, it is merely an
  approximation.
\item
  It doesn't account for the effects of air resistance, local variations
  in gravity, etc.
\item
  In practice, it's accurate enough to be useful!
\end{itemize}

\hypertarget{reasons-for-building-models}{%
\subsection{Reasons for building
models}\label{reasons-for-building-models}}

Often times, (1) we care about creating models that are simple and
interpretable, allowing us to understand what the relationships between
our variables are. Other times, (2) we care more about making extremely
accurate predictions, at the cost of having an uninterpretable model.
These are sometimes called black-box models, and are common in fields
like deep learning.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To understand complex phenomena occurring in the world we live in.

  \begin{itemize}
  \tightlist
  \item
    What factors play a role in the growth of COVID-19?
  \item
    How do an object's velocity and acceleration impact how far it
    travels? (Physics: \(d = d_0 + vt + \frac{1}{2}at^2\))
  \end{itemize}
\item
  To make accurate predictions about unseen data.

  \begin{itemize}
  \tightlist
  \item
    Can we predict if an email is spam or not?
  \item
    Can we generate a one-sentence summary of this 10-page long article?
  \end{itemize}
\end{enumerate}

\hypertarget{common-types-of-models}{%
\subsection{Common Types of Models}\label{common-types-of-models}}

In general, models can be split into two categories:

Note: These specific models are not in the scope of Data 100 and exist
to serve as motivation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Deterministic physical (mechanistic) models: Laws that govern how the
  world works.

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Kepler\%27s_laws_of_planetary_motion\#Third_law}{Kepler's
    Third Law of Planetary Motion (1619)}: The ratio of the square of an
    object's orbital period with the cube of the semi-major axis of its
    orbit is the same for all objects orbiting the same primary.

    \begin{itemize}
    \tightlist
    \item
      \(T^2 \propto R^3\)
    \end{itemize}
  \item
    \href{https://en.wikipedia.org/wiki/Newton\%27s_laws_of_motion}{Newton's
    Laws: motion and gravitation (1687)}: Newton's second law of motion
    models the relationship between the mass of an object and the force
    required to accelerate it.

    \begin{itemize}
    \tightlist
    \item
      \(F = ma\)
    \item
      \(F_g = G \frac{m_1 m_2}{r^2}\)
    \end{itemize}
  \end{itemize}
\item
  Probabilistic models: models that attempt to understand how random
  processes evolve. These are more general and can be used describe many
  phenomena in the real world. These models commonly make simplifying
  assumption about the nature of the world.

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Poisson_point_process}{Poisson
    Process models}: Used to model random events that can happen with
    some probability at any point in time and are strictly increasing in
    count, such as the arrival of customers at a store.
  \end{itemize}
\end{enumerate}

\hypertarget{simple-linear-regression}{%
\section{Simple Linear Regression}\label{simple-linear-regression}}

The \textbf{regression line} is the unique straight line that minimizes
the \textbf{mean squared error} of estimation among all straight lines.
As with any straight line, it can be defined by a slope and a
y-intercept:

\begin{itemize}
\tightlist
\item
  slope:
  \(r \cdot \frac{\text{Standard Deviation of y}}{\text{Standard Deviation of x}}\)
\item
  y-intercept:
  \(\text{average of y} - \text{slope}\cdot\text{average of x}\)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\CommentTok{\# Set random seed for consistency }
\NormalTok{np.random.seed(}\DecValTok{43}\NormalTok{)}
\NormalTok{plt.style.use(}\StringTok{\textquotesingle{}default\textquotesingle{}}\NormalTok{) }

\CommentTok{\#Generate random noise for plotting}
\NormalTok{x }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \FloatTok{0.5} \OperatorTok{{-}} \DecValTok{1} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}

\CommentTok{\#plot regression line}
\NormalTok{sns.regplot(x}\OperatorTok{=}\NormalTok{x,y}\OperatorTok{=}\NormalTok{y)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{intro_to_modeling/intro_to_modeling_files/figure-pdf/cell-2-output-1.png}

}

\end{figure}

\hypertarget{definitions}{%
\subsection{Definitions}\label{definitions}}

For a random variable x:

\begin{itemize}
\tightlist
\item
  Mean: \(\bar{x}\)
\item
  Standard Deviation: \(\sigma_x\)
\item
  Predicted value: \(\hat{x}\)
\end{itemize}

\hypertarget{standard-units}{%
\subsubsection{Standard Units}\label{standard-units}}

A random variable is represented in standard units if the following are
true:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  0 in standard units is the mean (\(\bar{x}\)) in the original
  variable's units.
\item
  An increase of 1 standard unit is an increase of 1 standard
  deviation(\(\sigma_x\)) in the original variable's units
\end{enumerate}

\hypertarget{correlation}{%
\subsubsection{Correlation}\label{correlation}}

The correlation (\(r\)) is the average of the product of \(x\) and
\(y\), both measured in \emph{standard units}. Correlation measures the
strength of a linear association between two variables.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(r = \frac{1}{n} \sum_1^n (\frac{x_i - \bar{x}}{\sigma_x})(\frac{y_i - \bar{y}}{\sigma_y})\)
\item
  Correlations are between -1 and 1: \(|r| < 1\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_and\_get\_corr(ax, x, y, title):}
\NormalTok{    ax.set\_xlim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{    ax.set\_ylim(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{    ax.set\_xticks([])}
\NormalTok{    ax.set\_yticks([])}
\NormalTok{    ax.scatter(x, y, alpha }\OperatorTok{=} \FloatTok{0.73}\NormalTok{)}
\NormalTok{    r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, y)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{    ax.set\_title(title }\OperatorTok{+} \StringTok{" (corr: }\SpecialCharTok{\{\}}\StringTok{)"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(r.}\BuiltInTok{round}\NormalTok{(}\DecValTok{2}\NormalTok{)))}
    \ControlFlowTok{return}\NormalTok{ r}

\NormalTok{fig, axs }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, figsize }\OperatorTok{=}\NormalTok{ (}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}

\CommentTok{\# Just noise}
\NormalTok{x1, y1 }\OperatorTok{=}\NormalTok{ np.random.randn(}\DecValTok{2}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{corr1 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{], x1, y1, title }\OperatorTok{=} \StringTok{"noise"}\NormalTok{)}

\CommentTok{\# Strong linear}
\NormalTok{x2 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y2 }\OperatorTok{=}\NormalTok{ x2 }\OperatorTok{*} \FloatTok{0.5} \OperatorTok{{-}} \DecValTok{1} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}
\NormalTok{corr2 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{], x2, y2, title }\OperatorTok{=} \StringTok{"strong linear"}\NormalTok{)}

\CommentTok{\# Unequal spread}
\NormalTok{x3 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y3 }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{ x3}\OperatorTok{/}\DecValTok{3} \OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{)}\OperatorTok{*}\NormalTok{(x3)}\OperatorTok{/}\FloatTok{2.5}
\NormalTok{corr3 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{], x3, y3, title }\OperatorTok{=} \StringTok{"strong linear"}\NormalTok{)}
\NormalTok{extent }\OperatorTok{=}\NormalTok{ axs[}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{].get\_window\_extent().transformed(fig.dpi\_scale\_trans.inverted())}

\CommentTok{\# Strong non{-}linear}
\NormalTok{x4 }\OperatorTok{=}\NormalTok{ np.linspace(}\OperatorTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{y4 }\OperatorTok{=} \DecValTok{2}\OperatorTok{*}\NormalTok{np.sin(x3 }\OperatorTok{{-}} \FloatTok{1.5}\NormalTok{) }\OperatorTok{+}\NormalTok{ np.random.randn(}\DecValTok{100}\NormalTok{) }\OperatorTok{*} \FloatTok{0.3}
\NormalTok{corr4 }\OperatorTok{=}\NormalTok{ plot\_and\_get\_corr(axs[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{], x4, y4, title }\OperatorTok{=} \StringTok{"strong non{-}linear"}\NormalTok{)}

\NormalTok{plt.show()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{intro_to_modeling/intro_to_modeling_files/figure-pdf/cell-3-output-1.png}

}

\end{figure}

\hypertarget{alternate-form}{%
\subsection{Alternate Form}\label{alternate-form}}

When the variables \(y\) and \(x\) are measured in \emph{standard
units}, the regression line for predicting \(y\) based on \(x\) has
slope \(r\) and passes through the origin.

\[\hat{y}_{su} = r \cdot x_{su}\]

\begin{itemize}
\tightlist
\item
  In the original units, this becomes
\end{itemize}

\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]

\hypertarget{derivation}{%
\subsection{Derivation}\label{derivation}}

Starting from the top, we have our claimed form of the regression line
and we want to show that its equivalent to the optimal linear regression
line: \(\hat{y} = \hat{a} + \hat{b}x\)

Recall:

\begin{itemize}
\tightlist
\item
  \(\hat{b}\):
  \(r \cdot \frac{\text{Standard Deviation of y}}{\text{Standard Deviation of x}}\)
\item
  \(\hat{a}\):
  \(\text{average of y} - \text{slope}\cdot\text{average of x}\)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, rightrule=.15mm, colback=white, colframe=quarto-callout-color-frame, toprule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, left=2mm]

Proof:

\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]

Multiply by \(\sigma_y\) and add \(\bar{y}\) on both sides.

\[\hat{y} = \sigma_y \cdot r \cdot \frac{x - \bar{x}}{\sigma_x} + \bar{y}\]

Distribute coefficient \(\sigma_{y}\cdot r\) to the
\(\frac{x - \bar{x}}{\sigma_x}\) term

\[\hat{y} = (\frac{r\sigma_y}{\sigma_x} ) \cdot x + (\bar{y} - (\frac{r\sigma_y}{\sigma_x} ) \bar{x})\]

We now see that we have a line that matches our claim:

\begin{itemize}
\tightlist
\item
  slope:
  \(r\cdot\frac{\text{SD of x}}{\text{SD of y}} = r\cdot\frac{\sigma_x}{\sigma_y}\)
\item
  intercept: \(\bar{y} - \text{slope}\cdot x\)
\end{itemize}

\end{tcolorbox}

\hypertarget{the-modeling-process}{%
\section{The Modeling Process}\label{the-modeling-process}}

At a high level, a model is some way of representing a system. In Data
100, we'll treat a model as some mathematical rule we use to describe
the relationship between variables.

What variables are we modeling? Typically, we use a subset of the
variables in our sample of collected data to model another variable in
this data. To put this more formally, say we have the following dataset
\(\mathbb{D}\):

\[\mathbb{D} = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\]

Each pair of values \((x_i, y_i)\) represents a datapoint. In a modeling
setting, we call these \textbf{observations}. \(y_i\) is the dependent
variable we are trying to model, also called an \textbf{output} or
\textbf{response}. \(x_i\) is the independent variable inputted into the
model to make predictions, also known as a \textbf{feature}.

Our goal in modeling is to use the observed data \(\mathbb{D}\) to
predict the output variable \(y_i\). We denote each prediction as
\(\hat{y}_i\) (read: ``y hat sub i'').

How do we generate these predictions? Some examples of models we'll
encounter in the next few lectures are given below:

\[\hat{y}_i = \theta\] \[\hat{y}_i = \theta_0 + \theta_1 x_i\]

The examples above are known as \textbf{parametric models}. They relate
the collected data, \(x_i\), to the prediction we make, \(\hat{y}_i\). A
few parameters (\(\theta\), \(\theta_0\), \(\theta_1\)) are used to
describe the relationship between \(x_i\) and \(\hat{y}_i\).

Notice that we don't immediately know the values of these parameters.
While the features, \(x_i\), are taken from our observed data, we need
to decide what values to give \(\theta\), \(\theta_0\), and \(\theta_1\)
ourselves. This is the heart of parametric modeling: \emph{what
parameter values should we choose so our model makes the best possible
predictions?}

To choose our model parameters, we'll work through the \textbf{modeling
process}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: how should we represent the world?
\item
  Choose a loss function: how do we quantify prediction error?
\item
  Fit the model: how do we choose the best parameters of our model given
  our data?
\item
  Evaluate model performance: how do we evaluate whether this process
  gave rise to a good model?
\end{enumerate}

\hypertarget{choosing-a-model}{%
\section{Choosing a Model}\label{choosing-a-model}}

Our first step is choosing a model: defining the mathematical rule that
describes the relationship between the features, \(x_i\), and
predictions \(\hat{y}_i\).

In
\href{https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html}{Data
8}, you learned about the \textbf{Simple Linear Regression (SLR) model}.
You learned that the model takes the form: \[\hat{y}_i = a + bx_i\]

In Data 100, we'll use slightly different notation: we will replace
\(a\) with \(\theta_0\) and \(b\) with \(\theta_1\). This will allow us
to use the same notation when we explore more complex models later on in
the course.

\[\hat{y}_i = \theta_0 + \theta_1 x_i\]

The parameters of the SLR model are \(\theta_0\), also called the
intercept term, and \(\theta_1\), also called the slope term. To create
an effective model, we want to choose values for \(\theta_0\) and
\(\theta_1\) that most accurately predict the output variable. The
``best'' fitting model parameters are given the special names
\(\hat{\theta}_0\) and \(\hat{\theta}_1\) -- they are the specific
parameter values that allow our model to generate the best possible
predictions.

In Data 8, you learned that the best SLR model parameters are:
\[\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} \qquad \qquad \hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]

A quick reminder on notation:

\begin{itemize}
\tightlist
\item
  \(\bar{y}\) and \(\bar{x}\) indicate the mean value of \(y\) and
  \(x\), respectively
\item
  \(\sigma_y\) and \(\sigma_x\) indicate the standard deviations of
  \(y\) and \(x\)
\item
  \(r\) is the
  \href{https://inferentialthinking.com/chapters/15/1/Correlation.html\#the-correlation-coefficient}{correlation
  coefficient}, defined as the average of the product of \(x\) and \(y\)
  measured in standard units:
  \(\frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y})\)
\end{itemize}

In Data 100, we want to understand \emph{how} to derive these best model
coefficients. To do so, we'll introduce the concept of a loss function.

\hypertarget{choosing-a-loss-function}{%
\section{Choosing a Loss Function}\label{choosing-a-loss-function}}

We've talked about the idea of creating the ``best'' possible
predictions. This begs the question: how do we decide how ``good'' or
``bad'' our model's predictions are?

A \textbf{loss function} characterizes the cost, error, or fit resulting
from a particular choice of model or model parameters. This function,
\(L(y, \hat{y})\), quantifies how ``far off'' a single prediction by our
model is from a true, observed value in our collected data.

The choice of loss function for a particular model depends on the
modeling task at hand. Regardless of the specific function used, a loss
function should follow two basic principles:

\begin{itemize}
\tightlist
\item
  If the prediction \(\hat{y}_i\) is \emph{close} to the actual value
  \(y_i\), loss should be low
\item
  If the prediction \(\hat{y}_i\) is \emph{far} from the actual value
  \(y_i\), loss should be high
\end{itemize}

Two common choices of loss function are squared loss and absolute loss.

\textbf{Squared loss}, also known as \textbf{L2 loss}, computes loss as
the square of the difference between the observed \(y_i\) and predicted
\(\hat{y}_i\): \[L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2\]

\textbf{Absolute loss}, also known as \textbf{L1 loss}, computes loss as
the absolute difference between the observed \(y_i\) and predicted
\(\hat{y}_i\): \[L(y_i, \hat{y}_i) = |y_i - \hat{y}_i|\]

L1 and L2 loss give us a tool for quantifying our model's performance on
a single datapoint. This is a good start, but ideally we want to
understand how our model performs across our \emph{entire} dataset. A
natural way to do this is to compute the average loss across all
datapoints in the dataset. This is known as the \textbf{cost function},
\(\hat{R}(\theta)\):
\[\hat{R}(\theta) = \frac{1}{n} \sum^n_{i=1} L(y_i, \hat{y}_i)\]

The cost function has many names in statistics literature. You may also
encounter the terms:

\begin{itemize}
\tightlist
\item
  Empirical risk (this is why we give the cost function the name \(R\))
\item
  Error function
\item
  Average loss
\end{itemize}

We can substitute our L1 and L2 loss into the cost function definition.
The \textbf{Mean Squared Error (MSE)} is the average squared loss across
a dataset: \[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

The \textbf{Mean Absolute Error (MAE)} is the average absolute loss
across a dataset:
\[\text{MAE}= \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\]

\hypertarget{fitting-the-model}{%
\section{Fitting the Model}\label{fitting-the-model}}

Now that we've established the concept of a loss function, we can return
to our original goal of choosing model parameters. Specifically, we want
to choose the best set of model parameters that will minimize the
model's cost on our dataset. This process is called fitting the model.

We know from calculus that a function is minimized when (1) its first
derivative is equal to zero and (2) its second derivative is positive.
We often call the function being minimized the \textbf{objective
function} (our objective is to find its minimum).

To find the optimal model parameter, we:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take the derivative of the cost function with respect to that
  parameter
\item
  Set the derivative equal to 0
\item
  Solve for the parameter
\end{enumerate}

We repeat this process for each parameter present in the model. For now,
we'll disregard the second derivative condition.

To help us make sense of this process, let's put it into action by
deriving the optimal model parameters for simple linear regression using
the mean squared error as our cost function. Remember: although the
notation may look tricky, all we are doing is following the three steps
above!

Step 1: take the derivative of the cost function with respect to each
model parameter. We substitute the SLR model,
\(\hat{y}_i = \theta_0+\theta_1 x_i\), into the definition of MSE above
and differentiate with respect to \(\theta_0\) and \(\theta_1\).
\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)^2\]

\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} y_i - \theta_0 - \theta_1 x_i\]

\[\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)x_i\]

Step 2: set the derivatives equal to 0. After simplifying terms, this
produces two \textbf{estimating equations}. The best set of model
parameters \((\theta_0, \theta_1)\) \emph{must} satisfy these two
optimality conditions.
\[0 = \frac{-2}{n} \sum_{i=1}^{n} y_i - \theta_0 - \theta_1 x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} y_i - \hat{y}_i = 0\]
\[0 = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)x_i = 0\]

Step 3: solve the estimating equations to compute estimates for
\(\hat{\theta}_0\) and \(\hat{\theta}_1\).

Taking the first equation gives the estimate of \(\hat{\theta}_0\): \[
\frac{1}{n} \sum_{i=1}^n y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i = 0 \\
\left(\frac{1}{n} \sum_{i=1}^n y_i \right) - \hat{\theta}_0 - \hat{\theta}_1\left(\frac{1}{n} \sum_{i=1}^n x_i \right) = 0 \\
\hat{\theta}_0 = \bar{y} - \hat{\theta}_1 \bar{x}
\]

With a bit more maneuvering, the second equation gives the estimate of
\(\hat{\theta}_1\). Start by multiplying the first estimating equation
by \(\bar{x}\), then subtracting the result from the second estimating
equation. \[
\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)x_i - \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)\bar{x} = 0 \\
\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)(x_i - \bar{x}) = 0
\]

Next, plug in
\(\hat{y}_i = \hat{\theta}_0 + \hat{\theta}_1 x_i = \bar{y} + \hat{\theta}_1(x_i - \bar{x})\):
\[
\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y} - \hat{\theta}_1(x - \bar{x}))(x_i - \bar{x}) = 0 \\
\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x}) = \hat{\theta}_1 \times \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]

By using the definition of correlation
\(\left(r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y}) \right)\)
and standard deviation
\(\left(\sigma_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \right)\),
we can conclude:
\[r \sigma_x \sigma_y = \hat{\theta}_1 \times \sigma_x^2\]
\[\hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]

Just as was given in Data 8!

Remember, this derivation found the optimal model parameters for SLR
when using the MSE cost function. If we had used a different model or
different loss function, we likely would have found different values for
the best model parameters. However, regardless of the model and loss
used, we can \emph{always} follow these three steps to fit the model.

\hypertarget{evaluating-performance}{%
\section{Evaluating Performance}\label{evaluating-performance}}

At this point, we've:

\begin{itemize}
\tightlist
\item
  Defined our model
\item
  Defined our loss function
\item
  Fit the model to identify the best model parameters
\end{itemize}

Now, what are some ways to determine if our model was a good fit to our
data? We will delve into this more in the next chapter, but there are
three main ways for evaluating a model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Statistics:

  \begin{itemize}
  \tightlist
  \item
    Plot original data
  \item
    Compute column means
  \item
    Compute standard deviations
  \item
    If we want to fit a linear model, compute correlation (r)
  \end{itemize}
\item
  Performance metrics:

  \begin{itemize}
  \tightlist
  \item
    Root Mean Square Error (RMSE). It is the square root of MSE, which
    is the average loss that we've been minimizing to determine optimal
    model parameters.
  \item
    RMSE is in the same units as \(y\).
  \item
    A lower RMSE indicates more ``accurate'' predictions (lower
    ``average loss'' across data)
  \end{itemize}
\item
  Visualization:

  \begin{itemize}
  \tightlist
  \item
    Look at a residual plot of \(e_i = y_i - \hat{y_i}\) to visualize
    the difference between actual and predicted \(y\) values.
  \end{itemize}
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{constant-model-loss-and-transformations}{%
\chapter{Constant Model, Loss, and
Transformations}\label{constant-model-loss-and-transformations}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Derive the optimal model parameters for the constant model under MSE
  and MAE cost functions
\item
  Evaluate the differences between MSE and MAE risk
\item
  Understand the need for linearization of variables and apply the
  Tukey-Mosteller bulge diagram for transformations
\end{itemize}

\end{tcolorbox}

Last time, we introduced the modeling process. We set up a framework to
predict target variables as functions of our features, following a set
workflow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model
\item
  Choose a loss function
\item
  Fit the model
\item
  Evaluate model performance
\end{enumerate}

To illustrate this process, we derived the optimal model parameters
under simple linear regression with mean squared error as the cost
function. In this lecture, we'll continue familiarizing ourselves with
the modeling process by finding the best model parameters under a new
model. We'll also test out two different loss functions to understand
how our choice of loss influences model design. Later on, we'll consider
what happens when a linear model isn't the best choice to capture trends
in our data -- and what solutions there are to create better models.

\hypertarget{constant-model-mse}{%
\section{Constant Model + MSE}\label{constant-model-mse}}

In today's lecture, our focus will be on the \textbf{constant model}.
The constant model is slightly different from the simple linear
regression model we've explored previously. Rather than generate
predictions from an inputted feature variable, the constant model
\emph{predicts the same constant number every time.} We call this
constant \(\theta\).

\[\hat{y}_i = \theta\]

\(\theta\) is the parameter of the constant model, just as \(\theta_0\)
and \(\theta_1\) were the parameters in SLR. Our task now is to
determine what value of \(\theta\) represents the optimal model -- in
other words, what number should we guess each time to have the lowest
possible average loss on our data?

Consider the case where L2 (squared) loss is used as the loss function
and mean squared error is used as the cost function. At this stage,
we're well into the modeling process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: constant model
\item
  Choose a loss function: L2 loss
\item
  Fit the model
\item
  Evaluate model performance
\end{enumerate}

In Homework 5, you will fit the constant model under MSE cost to find
that the best choice of \(\theta\) is the \textbf{mean of the observed
\(y\) values}. In other words, \(\hat{\theta} = \bar{y}\).

Let's take a moment to interpret this result. Our optimal model
parameter is the value of the parameter that minimizes the cost
function. This minimum value of the cost function can be expressed:

\[R(\hat{\theta}) = \min_{\theta} R(\theta)\]

To restate the above in plain English: we are looking at the value of
the cost function when it takes the best parameter as input. This
optimal model parameter, \(\hat{\theta}\), is the value of \(\theta\)
that minimizes the cost \(R\).

For modeling purposes, we care less about the minimum value of cost,
\(R(\hat{\theta})\), and more about the \emph{value of \(\theta\)} that
results in this lowest average loss. In other words, we concern
ourselves with finding the best parameter value such that:

\[\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)\]

That is, we want to find the \textbf{arg}ument \(\theta\) that
\textbf{min}imizes the cost function.

\hypertarget{constant-model-mae}{%
\section{Constant Model + MAE}\label{constant-model-mae}}

We see now that changing the model used for prediction leads to a wildly
different result for the optimal model parameter. What happens if we
instead change the loss function used in model evaluation?

This time, we will consider the constant model with L1 (absolute loss)
as the loss function. This means that the average loss will be expressed
as the mean absolute error.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Choose a model: constant model
\item
  Choose a loss function: L1 loss
\item
  Fit the model
\item
  Evaluate model performance
\end{enumerate}

To fit the model and find the optimal parameter value \(\hat{\theta}\),
follow the usual process of differentiating the cost function with
respect to \(\theta\), setting the derivative equal to zero, and solving
for \(\theta\). Writing this out in longhand:

\[
R(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta| \\
\frac{d}{d\theta} R(\theta) = \frac{d}{d\theta} \left(\frac{1}{n} \sum^{n}_{i=1} |y_i - \theta| \right) \\
\frac{d}{d\theta} R(\theta) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta} |y_i - \theta|
\]

Here, we seem to have run into a problem: the derivative of an absolute
value is undefined when the argument is 0 (i.e.~when \(y_i = \theta\)).
For now, we'll ignore this issue. It turns out that disregarding this
case doesn't influence our final result.

To perform the derivative, consider two cases. When \(\theta\) is
\emph{less than} \(y_i\), the term \(y_i - \theta\) will be positive and
the absolute value has no impact. When \(\theta\) is \emph{greater than}
\(y_i\), the term \(y_i - \theta\) will be negative. Applying the
absolute value will convert this to a positive value, which we can
express by saying \(-(y_i - \theta) = \theta - y_i\).

\[|y_i - \theta| = \begin{cases} y_i - \theta \quad \text{ if: } \theta < y_i \\ \theta - y_i \quad \text{if: }\theta > y_i \end{cases}\]

Taking derivatives:

\[\frac{d}{d\theta} |y_i - \theta| = \begin{cases} \frac{d}{d\theta} (y_i - \theta) = -1 \quad \text{if: }\theta < y_i \\ \frac{d}{d\theta} (\theta - y_i) = 1 \quad \text{if: }\theta > y_i \end{cases}\]

This means that we obtain a different value for the derivative for
datapoints where \(\theta < y_i\) and where \(\theta > y_i\). We can
summarize this by saying:

\[\frac{d}{d\theta} R(\theta) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta} |y_i - \theta| \\
= \frac{1}{n} \left[\sum_{\theta < y_i} (-1) + \sum_{\theta > y_i} (+1) \right]
\]

To finish finding the best value of \(\theta\), set this derivative
equal to zero and solve for \(\theta\). You'll do this in Homework 5 to
show that \(\hat{\theta} = \text{median}(y)\).

\hypertarget{comparing-loss-functions}{%
\section{Comparing Loss Functions}\label{comparing-loss-functions}}

Now, we've tried our hand at fitting a model under both MSE and MAE cost
functions. How do the two results compare?

Let's consider a dataset where each entry represents the number of
drinks sold at a bubble tea store each day. We'll fit a constant model
to predict the number of drinks that will be sold tomorrow.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{drinks }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{20}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{22}\NormalTok{, }\DecValTok{29}\NormalTok{, }\DecValTok{33}\NormalTok{])}
\NormalTok{drinks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([20, 21, 22, 29, 33])
\end{verbatim}

From our derivations above, we know that the optimal model parameter
under MSE cost is the mean of the dataset. Under MAE cost, the optimal
parameter is the median of the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.mean(drinks), np.median(drinks)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(25.0, 22.0)
\end{verbatim}

If we plot each empirical risk function across several possible values
of \(\theta\), we find that each \(\hat{\theta}\) does indeed correspond
to the lowest value of error:

Notice that the MSE above is a \textbf{smooth} function -- it is
differentiable at all points, making it easy to minimize using numerical
methods. The MAE, in contrast, is not differentiable at each of its
``kinks.'' We'll explore how the smoothness of the cost function can
impact our ability to apply numerical optimization in a few weeks.

How do outliers affect each cost function? Imagine we replace the
largest value in the dataset with 1000. The mean of the data increases
substantially, while the median is nearly unaffected.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_with\_outlier }\OperatorTok{=}\NormalTok{ np.append(drinks, }\DecValTok{1000}\NormalTok{)}
\NormalTok{display(drinks\_with\_outlier)}
\NormalTok{np.mean(drinks\_with\_outlier), np.median(drinks\_with\_outlier)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([  20,   21,   22,   29,   33, 1000])
\end{verbatim}

\begin{verbatim}
(187.5, 25.5)
\end{verbatim}

This means that under the MSE, the optimal model parameter
\(\hat{\theta}\) is strongly affected by the presence of outliers. Under
the MAE, the optimal parameter is not as influenced by outlying data. We
can generalize this by saying that the MSE is \textbf{sensitive} to
outliers, while the MAE is \textbf{robust} to outliers.

Let's try another experiment. This time, we'll add an additional,
non-outlying datapoint to the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_with\_additional\_observation }\OperatorTok{=}\NormalTok{ np.append(drinks, }\DecValTok{35}\NormalTok{)}
\NormalTok{drinks\_with\_additional\_observation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([20, 21, 22, 29, 33, 35])
\end{verbatim}

When we again visualize the cost functions, we find that the MAE now
plots a horizontal line between 22 and 29. This means that there are
\emph{infinitely} many optimal values for the model parameter: any value
\(\hat{\theta} \in [22, 29]\) will minimize the MAE. In contrast, the
MSE still has a single best value for \(\hat{\theta}\). In other words,
the MSE has a \textbf{unique} solution for \(\hat{\theta}\); the MAE is
not guaranteed to have a single unique solution.

\hypertarget{evaluating-models}{%
\section{Evaluating Models}\label{evaluating-models}}

This leaves us with one final question -- how ``good'' are the
predictions made by this ``best'' fitted model?

One way we might want to evaluate our model's performance is by
computing summary statistics. If the mean and standard deviation of our
predictions are close to those of the original observed \(y_i\)s, we
might be inclined to say that our model has done well. A large magnitude
for the correlation coefficient between the feature and response
variables might also support this conclusion. However, we should be
cautious with this approach. To see why, we'll consider a classic
dataset called \textbf{Anscombe's quartet.}

It turns out that the four sets of points shown here all have identical
means, standard deviations, and correlation coefficients. However, it
only makes sense to model the first of these four sets of data using
SLR! It is important to visualize your data \emph{before} starting to
model to confirm that your choice of model makes sense for the data.

Another way of evaluating model performance is by using performance
metrics. A common choice of metric is the \textbf{Root Mean Squared
Error}, or RMSE. The RMSE is simply the square root of MSE. Taking the
square root converts the value back into the original, non-squared units
of \(y_i\), which is useful for understanding the model's performance. A
low RMSE indicates more ``accurate'' predictions -- that there is lower
average loss across the dataset.
\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]

We may also wish to visualize the model's \textbf{residuals}, defined as
the difference between the observed and predicted \(y_i\) value
(\(e_i = y_i - \hat{y}_i\)). This gives a high-level view of how ``off''
each prediction is from the true observed value. Recall that you
explored this concept in
\href{https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity\#detecting-heteroscedasticity}{Data
8}: a good regression fit should display no clear pattern in its plot of
residuals. The residual plots for Anscombe's quartet are displayed
below. Note how only the first plot shows no clear pattern to the
magnitude of residuals. This is an indication that SLR is not the best
choice of model for the remaining three sets of points.

\hypertarget{linear-transformations}{%
\section{Linear Transformations}\label{linear-transformations}}

At this point, we have an effective method of fitting models to predict
linear relationships. Given a feature variable and target, we can apply
our four-step process to find the optimal model parameters.

A key word above is \emph{linear}. When we computed parameter estimates
earlier, we assumed that \(x_i\) and \(y_i\) shared roughly a linear
relationship.

Data in the real world isn't always so straightforward. Consider the
dataset below, which contains information about the ages and lengths of
dugongs.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{dugong }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/dugongs.txt"}\NormalTok{, delimiter}\OperatorTok{=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{).sort\_values(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{x, y }\OperatorTok{=}\NormalTok{ dugong[}\StringTok{"Length"}\NormalTok{], dugong[}\StringTok{"Age"}\NormalTok{]}

\CommentTok{\# \textasciigrave{}corrcoef\textasciigrave{} computes the correlation coefficient between two variables}
\CommentTok{\# \textasciigrave{}std\textasciigrave{} finds the standard deviation}
\NormalTok{r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, y)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{theta\_1 }\OperatorTok{=}\NormalTok{ r}\OperatorTok{*}\NormalTok{np.std(y)}\OperatorTok{/}\NormalTok{np.std(x)}
\NormalTok{theta\_0 }\OperatorTok{=}\NormalTok{ np.mean(y) }\OperatorTok{{-}}\NormalTok{ theta\_1}\OperatorTok{*}\NormalTok{np.mean(x)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{200}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].scatter(x, y)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_ylabel(}\StringTok{"Age"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].scatter(x, y)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x, theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1}\OperatorTok{*}\NormalTok{x, }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_ylabel(}\StringTok{"Age"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-6-output-1.png}

}

\end{figure}

Looking at the plot on the left, we see that there is a slight curvature
to the data points. Plotting the SLR curve on the right results in a
poor fit.

For SLR to perform well, we'd like there to be a rough linear trend
relating \texttt{"Age"} and \texttt{"Length"}. What is making the raw
data deviate from a linear relationship? Notice that the data points
with \texttt{"Length"} greater than 2.6 have disproportionately high
values of \texttt{"Age"} relative to the rest of the data. If we could
manipulate these data points to have lower \texttt{"Age"} values, we'd
``shift'' these points downwards and reduce the curvature in the data.
Applying a logarithmic transformation to \(y_i\) (that is, taking
\(\log(\) \texttt{"Age"} \()\) ) would achieve just that.

An important word on \(\log\): in Data 100 (and most upper-division STEM
courses), \(\log\) denotes the natural logarithm with base \(e\). The
base-10 logarithm, where relevant, is indicated by \(\log_{10}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z }\OperatorTok{=}\NormalTok{ np.log(y)}

\NormalTok{r }\OperatorTok{=}\NormalTok{ np.corrcoef(x, z)[}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{theta\_1 }\OperatorTok{=}\NormalTok{ r}\OperatorTok{*}\NormalTok{np.std(z)}\OperatorTok{/}\NormalTok{np.std(x)}
\NormalTok{theta\_0 }\OperatorTok{=}\NormalTok{ np.mean(z) }\OperatorTok{{-}}\NormalTok{ theta\_1}\OperatorTok{*}\NormalTok{np.mean(x)}

\NormalTok{fig, ax }\OperatorTok{=}\NormalTok{ plt.subplots(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, dpi}\OperatorTok{=}\DecValTok{200}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].scatter(x, z)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{0}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}log\{(Age)\}$"}\NormalTok{)}

\NormalTok{ax[}\DecValTok{1}\NormalTok{].scatter(x, z)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].plot(x, theta\_0 }\OperatorTok{+}\NormalTok{ theta\_1}\OperatorTok{*}\NormalTok{x, }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{ax[}\DecValTok{1}\NormalTok{].set\_ylabel(}\VerbatimStringTok{r"$\textbackslash{}log\{(Age)\}$"}\NormalTok{)}

\NormalTok{plt.subplots\_adjust(wspace}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

Our SLR fit looks a lot better! We now have a new target variable: the
SLR model is now trying to predict the \emph{log} of \texttt{"Age"},
rather than the untransformed \texttt{"Age"}. In other words, we are
applying the transformation \(z_i = \log{(y_i)}\). The SLR model
becomes:

\[\log{\hat{(y_i)}} = \theta_0 + \theta_1 x_i\]
\[\hat{z}_i = \theta_0 + \theta_1 x_i\]

It turns out that this linearized relationship can help us understand
the underlying relationship between \(x_i\) and \(y_i\). If we rearrange
the relationship above, we find: \[
\log{(y_i)} = \theta_0 + \theta_1 x_i \\
y_i = e^{\theta_0 + \theta_1 x_i} \\
y_i = (e^{\theta_0})e^{\theta_1 x_i} \\
y_i = C e^{k x_i}
\]

For some constants \(C\) and \(k\).

\(y_i\) is an \emph{exponential} function of \(x_i\). Applying an
exponential fit to the untransformed variables corroborates this
finding.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt.figure(dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{, figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{))}

\NormalTok{plt.scatter(x, y)}
\NormalTok{plt.plot(x, np.exp(theta\_0)}\OperatorTok{*}\NormalTok{np.exp(theta\_1}\OperatorTok{*}\NormalTok{x), }\StringTok{"tab:red"}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{"Length"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"Age"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{constant_model_loss_transformations/loss_transformations_files/figure-pdf/cell-8-output-1.png}

}

\end{figure}

You may wonder: why did we choose to apply a log transformation
specifically? Why not some other function to linearize the data?

Practically, many other mathematical operations that modify the relative
scales of \texttt{"Age"} and \texttt{"Length"} could have worked here.
The \textbf{Tukey-Mosteller Bulge Diagram} is a useful tool for
summarizing what transformations can linearize the relationship between
two variables. To determine what transformations might be appropriate,
trace the shape of the ``bulge'' made by your data. Find the quadrant of
the diagram that matches this bulge. The transformations shown on the
vertical and horizontal axes of this quadrant can help improve the fit
between the variables.

\bookmarksetup{startatroot}

\hypertarget{ordinary-least-squares}{%
\chapter{Ordinary Least Squares}\label{ordinary-least-squares}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Define linearity with respect to a vector of parameters \(\theta\)
\item
  Understand the use of matrix notation to express multiple linear
  regression
\item
  Interpret ordinary least squares as the minimization of the norm of
  the residual vector
\item
  Compute performance metrics for multiple linear regression
\end{itemize}

\end{tcolorbox}

We've now spent a number of lectures exploring how to build effective
models -- we introduced the SLR and constant models, selected cost
functions to suit our modeling task, and applied transformations to
improve the linear fit.

Throughout all of this, we considered models of one feature
(\(\hat{y}_i = \theta_0 + \theta_1 x_i\)) or zero features
(\(\hat{y}_i = \theta\)). As data scientists, we usually have access to
datasets containing \emph{many} features. To make the best models we
can, it will be beneficial to consider all of the variables available to
us as inputs to a model, rather than just one. In today's lecture, we'll
introduce \textbf{multiple Linear regression} as a framework to
incorporate multiple features into a model. We will also learn how to
accelerate the modeling process -- specifically, we'll see how linear
algebra offers us a powerful set of tools for understanding model
performance.

\hypertarget{linearity}{%
\section{Linearity}\label{linearity}}

An expression is \textbf{linear in \(\theta\)} (a set of parameters) if
it is a linear combination of the elements of the set. Checking if an
expression can separate into a matrix product of two terms: a
\textbf{vector of \(\theta\)} s, and a matrix/vector \textbf{not
involving \(\theta\)}.

Example: \(\theta = [\theta_1, \theta_2, ... \theta_p]\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear in theta: \(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\)
\end{enumerate}

\[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Not linear in theta:
  \(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\)
\end{enumerate}

\hypertarget{multiple-linear-regression}{%
\section{Multiple Linear Regression}\label{multiple-linear-regression}}

Multiple Linear regression is an extension of simple linear regression
that adds additional features into the model. Say we collect information
on several variables when making an observation. For example, we may
record the age, height, and weekly hours of sleep for a student in Data
100. This single observation now contains data for multiple features. To
accommodate for the fact that we now consider several feature variables,
we'll adjust our notation slightly. Each observation can now be thought
of as a row vector with an entry for each of \(p\) features.

The multiple Linear regression model takes the form:

\[\hat{y}_i = \theta_0\:+\:\theta_1x_{i1}\:+\:\theta_2 x_{i2}\:+\:...\:+\:\theta_p x_{ip}\]

Our \(i\)th prediction, \(\hat{y}_i\), is a linear combination of the
parameters, \(\theta_i\). Because we are now dealing with many parameter
values, we'll collect them all into a \textbf{parameter vector} with
dimensions \((p+1) \times 1\) to keep things tidy.

\[\theta = \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix}\]

We are now working with two vectors: a row vector representing the
observed data, and a column vector containing the model parameters. The
multiple Linear regression model given above is \textbf{equivalent to
the dot (scalar) product of the observation vector and parameter
vector}.

\[[1,\:x_{i1},\:x_{i2},\:x_{i3},\:...,\:x_{ip}] \theta = [1,\:x_{i1},\:x_{i2},\:x_{i3},\:...,\:x_{ip}] \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix} = \theta_0\:+\:\theta_1x_{i1}\:+\:\theta_2 x_{i2}\:+\:...\:+\:\theta_p x_{ip}\]

Notice that we have inserted 1 as the first value in the observation
vector. When the dot product is computed, this 1 will be multiplied with
\(\theta_0\) to give the intercept of the regression model. We call this
1 entry the \textbf{intercept} or \textbf{bias} term.

\hypertarget{linear-algebra-approach}{%
\section{Linear Algebra Approach}\label{linear-algebra-approach}}

We now know how to generate a single prediction from multiple observed
features. Data scientists usually work at scale -- that is, they want to
build models that can produce many predictions, all at once. The vector
notation we introduced above gives us a hint on how we can expedite
multiple Linear regression. We want to use the tools of linear algebra.

Let's think carefully about what we did to generate the single
prediction above. To make a prediction from the first observation in the
data, we took the scalar product of the parameter vector and first
observation vector. To make a prediction from the \emph{second}
observation, we would repeat this process to find the scalar product of
the parameter vector and the \emph{second} observation vector. If we
wanted to find the model predictions for each observation in the
dataset, we'd repeat this process for all \(n\) observations in the
data.

\[\hat{y}_1 = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\]
\[\hat{y}_2 = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}] \theta\]
\[\vdots\]
\[\hat{y}_n = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]

Our observed data is represented by \(n\) row vectors, each with
dimension \((p+1)\). We can collect them all into a single matrix, which
we call \(\mathbb{X}\).

The matrix \(\mathbb{X}\) is known as the \textbf{design matrix}. It
contains all observed data for each of our \(p\) features. It often (but
not always) contains an additional column of all ones to represent the
\textbf{intercept} or \textbf{bias column}.

To review what is happening in the design matrix: each row represents a
single observation. For example, a student in Data 100. Each column
represents a feature. For example, the ages of students in Data 100.
This convention allows us to easily transfer our previous work in
DataFrames over to this new linear algebra perspective.

The multiple Linear regression model can then be restated in terms of
matrices: \[\mathbb{\hat{Y}} = \mathbb{X} \theta\]

Here, \(\mathbb{\hat{Y}}\) is the \textbf{prediction vector} with
dimensions \((n \times 1)\). It contains the prediction made by the
model for each of \(n\) input observations.

We now have a new approach to understanding models in terms of vectors
and matrices. To accompany this new convention, we should update our
understanding of cost functions and model fitting.

Recall our definition of MSE:
\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

At its heart, the MSE is a measure of \emph{distance} -- it gives an
indication of how ``far away'' the predictions are from the true values,
on average.

When working with vectors, this idea of ``distance'' is represented by
the \textbf{norm}. More precisely, the distance between two vectors
\(\vec{a}\) and \(\vec{b}\) can be expressed as:
\[||\vec{a} - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\]

The double bars are mathematical notation for the norm. The subscript 2
indicates that we are computing the L2, or squared norm.

Looks pretty familiar! We can rewrite the MSE to express it as a squared
L2 norm in terms of the prediction vector, \(\hat{\mathbb{Y}}\), and
true target vector, \(\mathbb{Y}\):

\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y} - \hat{\mathbb{Y}}||_2^2\]

Here, the superscript 2 outside of the norm double bars means that we
are \emph{squaring} the norm. If we plug in our linear model
\(\hat{\mathbb{Y}} = \mathbb{X} \theta\), we find the MSE cost function
in vector notation:

\[R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2\]

Under the linear algebra perspective, our new task is to fit the optimal
parameter vector \(\theta\) such that the cost function is minimized.
Equivalently, we wish to minimize the norm
\[||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2\].

We can restate this goal in two ways:

\begin{itemize}
\tightlist
\item
  Minimize the \emph{distance} between the vector of true values,
  \(\mathbb{Y}\), and the vector of predicted values,
  \(\mathbb{\hat{Y}}\)
\item
  Minimize the \emph{length} of the \textbf{residual vector}, defined
  as: \[e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}
           y_1 - \hat{y}_1 \\
           y_2 - \hat{y}_2 \\
           \vdots \\
           y_n - \hat{y}_n
         \end{bmatrix}\]
\end{itemize}

\hypertarget{geometric-perspective}{%
\section{Geometric Perspective}\label{geometric-perspective}}

To derive the best parameter vector to meet this goal, we can turn to
the geometric properties of our modeling set-up.

Up until now, we've mostly thought of our model as a scalar product
between horizontally-stacked observations and the parameter vector. We
can also think of \(\hat{\mathbb{Y}}\) as a \textbf{linear combination
of feature vectors}, scaled by the parameters. We use the notation
\(\mathbb{X}_{:, i}\) to denote the \(i\)th column of the design matrix.
You can think of this as following the same convention as used when
calling \texttt{.iloc} and \texttt{.loc}. ``:'' means that we are taking
all entries in the \(i\)th column.

\[
\hat{\mathbb{Y}} = 
\theta_0 \begin{bmatrix}
           1 \\
           1 \\
           \vdots \\
           1
         \end{bmatrix} + \theta_1 \begin{bmatrix}
           x_{11} \\
           x_{21} \\
           \vdots \\
           x_{n1}
         \end{bmatrix} + \ldots + \theta_p \begin{bmatrix}
           x_{1p} \\
           x_{2p} \\
           \vdots \\
           x_{np}
         \end{bmatrix}
         = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p \mathbb{X}_{:,\:p+1}\]

This new approach is useful because it allows us to take advantage of
the properties of linear combinations.

Recall that the \textbf{span} or \textbf{column space} of a matrix is
the set of all possible linear combinations of the matrix's columns. In
other words, the span represents every point in space that could
possibly be reached by adding and scaling some combination of the matrix
columns.

Because the prediction vector, \(\hat{\mathbb{Y}} = \mathbb{X} \theta\),
is a linear combination of the columns of \(\mathbb{X}\), we know that
the \textbf{predictions are contained in the span of \(\mathbb{X}\)}.
That is, we know that \(\mathbb{\hat{Y}} \in \text{Span}(\mathbb{X})\).

The diagram below is a simplified view of \(\text{Span}(\mathbb{X})\),
assuminh that each column of \(\mathbb{X}\) has length \(n\). Notice
that the columns of \(\mathbb{X}\) define a subspace of
\(\mathbb{R}^n\), where each point in the subspace can be reached by a
linear combination of \(\mathbb{X}\)'s columns. The prediction vector
\(\mathbb{\hat{Y}}\) lies somewhere in this subspace.

Examining this diagram, we find a problem. The vector of true values,
\(\mathbb{Y}\), could theoretically lie \emph{anywhere} in
\(\mathbb{R}^n\) space -- its exact location depends on the data we
collect out in the real world. However, our multiple Linear regression
model can only make predictions in the subspace of \(\mathbb{R}^n\)
spanned by \(\mathbb{X}\). Remember the model fitting goal we
established in the previous section: we want to generate predictions
such that the distance between the vector of true values,
\(\mathbb{Y}\), and the vector of predicted values,
\(\mathbb{\hat{Y}}\), is minimized. This means that \textbf{we want
\(\mathbb{\hat{Y}}\) to be the vector in \(\text{Span}(\mathbb{X})\)
that is closest to \(\mathbb{Y}\)}.

Another way of rephrasing this goal is to say that we wish to minimize
the length of the residual vector \(e\), as measured by its \(L_2\)
norm.

The vector in \(\text{Span}(\mathbb{X})\) that is closest to
\(\mathbb{Y}\) is always the \textbf{orthogonal projection} of
\(\mathbb{Y}\) onto \(\text{Span}(\mathbb{X})\). Thus, we should choose
the parameter vector \(\theta\) that makes the \textbf{residual vector
orthogonal to any vector in \(\text{Span}(\mathbb{X})\)}. You can
visualize this as the vector created by dropping a perpendicular line
from \(\mathbb{Y}\) onto the span of \(\mathbb{X}\).

How does this help us identify the optimal parameter vector,
\(\hat{\theta}\)? Recall that two vectors are orthogonal if their dot
product is zero. A vector \(\vec{v}\) is orthogonal to the span of a
matrix \(M\) if \(\vec{v}\) is orthogonal to each column in \(M\). Put
together, a vector \(\vec{v}\) is orthogonal to \(\text{Span}(M)\) if:

\[M^T \vec{v} = \vec{0}\]

Because our goal is to find \(\hat{\theta}\) such that the residual
vector \(e = \mathbb{Y} - \mathbb{X} \theta\) is orthogonal to
\(\text{Span}(\mathbb{X})\), we can write:

\[\mathbb{X}^T e = \vec{0}\]
\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]
\[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} = \vec{0}\]
\[\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]

This last line is known as the \textbf{normal equation}. Any vector
\(\theta\) that minimizes MSE on a dataset must satisfy this equation.

If \(\mathbb{X}^T \mathbb{X}\) is invertible, we can conclude:
\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}\]

This is called the \textbf{least squares estimate} of \(\theta\): it is
the value of \(\theta\) that minimizes the squared loss.

Note that the least squares estimate was derived under the assumption
that \(\mathbb{X}^T \mathbb{X}\) is \emph{invertible}. This condition
holds true when \(\mathbb{X}^T \mathbb{X}\) is full column rank, which,
in turn, happens when \(\mathbb{X}\) is full column rank. We will
explore the consequences of this fact in lab and homework.

\hypertarget{evaluating-model-performance}{%
\section{Evaluating Model
Performance}\label{evaluating-model-performance}}

Our geometric view of multiple Linear regression has taken us far! We
have identified the optimal set of parameter values to minimize MSE in a
model of multiple features.

Now, we want to understand how well our fitted model performs. One
measure of model performance is the \textbf{Root Mean Squared Error}, or
RMSE. The RMSE is simply the square root of MSE. Taking the square root
converts the value back into the original, non-squared units of \(y_i\),
which is useful for understanding the model's performance. A low RMSE
indicates more ``accurate'' predictions -- that there is lower average
loss across the dataset.

\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]

When working with SLR, we generated plots of the residuals against a
single feature to understand the behavior of residuals. When working
with several features in multiple Linear regression, it no longer makes
sense to consider a single feature in our residual plots. Instead,
multiple Linear regression is evaluated by making plots of the residuals
against the predicted values. As was the case with SLR, a multiple
Linear model performs well if its residual plot shows no patterns.

For SLR, we used the correlation coefficient to capture the association
between the target variable and a single feature variable. In a multiple
Linear setting, we will need a performance metric that can account for
multiple features at once. \textbf{Multiple \(R^2\)}, also called the
\textbf{coefficient of determination}, is the ratio of the variance of
the predicted values \(\hat{y}_i\) to the variance of the true values
\(y_i\). It can be interpreted as the \emph{proportion} of variance in
the observations that is explained by the model.

\[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]

As we add more features, our fitted values tend to become closer and
closer to our actual values. Thus, \(\mathbb{R}^2\) increases.

\hypertarget{ols-properties}{%
\section{OLS Properties}\label{ols-properties}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When using the optimal parameter vector, our residuals
  \(e = \mathbb{Y} - \hat{\mathbb{Y}}\) are orthogonal to
  \(span(\mathbb{X})\)
\end{enumerate}

\[\mathbb{X}^Te = 0 \]

\begin{tcolorbox}[enhanced jigsaw, breakable, rightrule=.15mm, colback=white, colframe=quarto-callout-color-frame, toprule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, left=2mm]

Proof:

The optimal parameter vector, \(\hat{\theta}\), solves the normal
equations
\(\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)

\[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]

\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{X}\hat{\theta}\]

Any matrix multiplied with its own inverse is the identity matrix
\(\mathbb{I}\)

\[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y} = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  For all linear models \emph{with an intercept term}, the sum of
  residuals is zero.
\end{enumerate}

\[\sum_i^n e_i = 0\]

\begin{tcolorbox}[enhanced jigsaw, breakable, rightrule=.15mm, colback=white, colframe=quarto-callout-color-frame, toprule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, left=2mm]

Proof:

For all linear models \emph{with an intercept term}, the average of the
predicted \(y\) values is equal to the average of the true \(y\) values.

\[\bar{y} = \bar{\hat{y}}\]

Rewriting the sum of residuals as two separate sums,

\[\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i\]

Each respective sum is a multiple of the average of the sum.

\[\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0\]

\end{tcolorbox}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The Least Squares estimate \(\hat{\theta}\) is unique if and only if
  \(\mathbb{X}\) is full column rank.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, breakable, rightrule=.15mm, colback=white, colframe=quarto-callout-color-frame, toprule=.15mm, opacityback=0, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, left=2mm]

Proof:

We know the solution to the normal equation
\(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{Y}\) is the least square
estimate that fulfills the prior equality.

\(\hat{\theta}\) has a unique solution \(\iff\) the square matrix
\(\mathbb{X}^T\mathbb{X}\) is invertible.

The rank of a square matrix is the maximum number of of linearly
independent columns it contains. \(\mathbb{X}^T\mathbb{X}\) has shape
\((p + 1) \times (p + 1)\), and therefore has max rank p + 1.

\(rank(\mathbb{X}^T\mathbb{X})\) = \(rank(\mathbb{X})\) (proof out of
scope).

Therefore \(\mathbb{X}^T\mathbb{X}\) has rank p + 1 \(\iff\)
\(\mathbb{X}\) has rank p + 1 \(\iff \mathbb{X}\) is full column rank.

\end{tcolorbox}

\bookmarksetup{startatroot}

\hypertarget{gradient-descent}{%
\chapter{Gradient Descent}\label{gradient-descent}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Understand the standard workflow for fitting models in
  \texttt{sklearn}
\item
  Describe the conceptual basis for gradient descent
\item
  Compute the gradient descent update on a provided dataset
\end{itemize}

\end{tcolorbox}

At this point, we've grown quite familiar with the modeling process.
We've introduced the concept of loss, used it to fit several types of
models, and, most recently, extended our analysis to multiple
regression. Along the way, we've forged our way through the mathematics
of deriving the optimal model parameters in all of its gory detail. It's
time to make our lives a little easier -- let's implement the modeling
process in code!

In this lecture, we'll explore three techniques for model fitting:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Translating our derived formulas for regression to Python
\item
  Using the \texttt{sklearn} Python package
\item
  Applying gradient descent for numerical optimization
\end{enumerate}

\hypertarget{sklearn-implementing-derived-formulas-in-code}{%
\section{\texorpdfstring{\texttt{sklearn}: Implementing Derived Formulas
in
Code}{sklearn: Implementing Derived Formulas in Code}}\label{sklearn-implementing-derived-formulas-in-code}}

Throughout this lecture, we'll refer to the \texttt{penguins} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{penguins }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"penguins"}\NormalTok{)}
\NormalTok{penguins }\OperatorTok{=}\NormalTok{ penguins[penguins[}\StringTok{"species"}\NormalTok{] }\OperatorTok{==} \StringTok{"Adelie"}\NormalTok{].dropna()}
\NormalTok{penguins.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lllrrrrl}
\toprule
{} & species &     island &  bill\_length\_mm &  bill\_depth\_mm &  flipper\_length\_mm &  body\_mass\_g &     sex \\
\midrule
0 &  Adelie &  Torgersen &            39.1 &           18.7 &              181.0 &       3750.0 &    Male \\
1 &  Adelie &  Torgersen &            39.5 &           17.4 &              186.0 &       3800.0 &  Female \\
2 &  Adelie &  Torgersen &            40.3 &           18.0 &              195.0 &       3250.0 &  Female \\
4 &  Adelie &  Torgersen &            36.7 &           19.3 &              193.0 &       3450.0 &  Female \\
5 &  Adelie &  Torgersen &            39.3 &           20.6 &              190.0 &       3650.0 &    Male \\
\bottomrule
\end{tabular}

Suppose our goal is to predict the value of the
\texttt{\textquotesingle{}bill\ depth\textquotesingle{}} for a
particular penguin given its
\texttt{\textquotesingle{}flipper\ length\textquotesingle{}}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define the design matrix, X...}
\NormalTok{X }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{]]}

\CommentTok{\# ...as well as the target variable, y}
\NormalTok{y }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"bill\_depth\_mm"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\hypertarget{simple-linear-regression-slr}{%
\subsection{Simple Linear Regression
(SLR)}\label{simple-linear-regression-slr}}

In the SLR framework we learned last week, this means we are saying our
model for \texttt{bill\ depth}, \(y\), is a linear function of
\texttt{flipper\ length}, \(x\):

\[\hat{y} = \theta_0 + \theta_1 x\]

Let's do some EDA first.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}

\NormalTok{plt.xlabel(}\StringTok{"flipper length (mm)"}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{"bill depth (mm)"}\NormalTok{)}
\NormalTok{plt.scatter(data }\OperatorTok{=}\NormalTok{ penguins, x }\OperatorTok{=} \StringTok{"flipper\_length\_mm"}\NormalTok{, y }\OperatorTok{=} \StringTok{"bill\_depth\_mm"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-4-output-1.png}

}

\end{figure}

Based on our EDA, there is a linear relationship, though it is somewhat
weak.

\hypertarget{slr-w-derived-analytical-formulas}{%
\subsubsection{SLR w/ Derived Analytical
Formulas}\label{slr-w-derived-analytical-formulas}}

Let \(\hat{\theta}_0\) and \(\hat{\theta}_1\) be the choices that
minimize the Mean Squared Error.

One approach to compute \(\hat{\theta}_0\) and \(\hat{\theta}_1\) is
analytically, using the equations we derived in a previous lecture:

\[\hat{\theta}_0 = \bar{y} - \hat{\theta}_1 \bar{x}\]

\[\hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]

\[r = \frac{1}{n} \sum_{i=1}^{n}\left( \frac{x_i - x}{\sigma_x} \right) \left( \frac{y_i - y}{\sigma_y} \right) \]

Let's implement these using the base numpy library, which provides many
important functions such as \texttt{.mean} and \texttt{.std} .

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"flipper\_length\_mm"}\NormalTok{]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{]}

\NormalTok{x\_bar, sigma\_x }\OperatorTok{=}\NormalTok{ np.mean(x), np.std(x)}
\NormalTok{y\_bar, sigma\_y }\OperatorTok{=}\NormalTok{ np.mean(y), np.std(y)}
\NormalTok{r }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{((x }\OperatorTok{{-}}\NormalTok{ x\_bar) }\OperatorTok{/}\NormalTok{ sigma\_x }\OperatorTok{*}\NormalTok{ (y }\OperatorTok{{-}}\NormalTok{ y\_bar) }\OperatorTok{/}\NormalTok{ sigma\_y) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(x)}

\NormalTok{theta1\_hat }\OperatorTok{=}\NormalTok{ r }\OperatorTok{*}\NormalTok{ sigma\_y }\OperatorTok{/}\NormalTok{ sigma\_x}

\NormalTok{theta0\_hat }\OperatorTok{=}\NormalTok{ y\_bar }\OperatorTok{{-}}\NormalTok{ theta1\_hat }\OperatorTok{*}\NormalTok{ x\_bar}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"bias parameter: }\SpecialCharTok{\{}\NormalTok{theta0\_hat}\SpecialCharTok{\}}\SpecialStringTok{, }\CharTok{\textbackslash{}n}\SpecialStringTok{slope parameter: }\SpecialCharTok{\{}\NormalTok{theta1\_hat}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(theta0\_hat,theta1\_hat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
bias parameter: 7.297305899612297, 
slope parameter: 0.058126223695067675
\end{verbatim}

\hypertarget{slr-analytical-approach-performance}{%
\subsubsection{SLR Analytical Approach
Performance}\label{slr-analytical-approach-performance}}

Let's first assess how ``good'' this model is using a performance
metric. For this exercise, let's use the MSE. As a review:

\textbf{Mean Squared Error}: We can compute this explicitly by averaging
the square of the residuals \(e_i\):

\[\large MSE  = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n}\sum_{i=1}^n (e_i)^2 = \frac{1}{n}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# using our estimated parameter values to create a column containing our }
\CommentTok{\# SLR predictions and errors}
\NormalTok{penguins[}\StringTok{"analytical\_preds\_slr"}\NormalTok{] }\OperatorTok{=}\NormalTok{ theta0\_hat }\OperatorTok{+}\NormalTok{ theta1\_hat }\OperatorTok{*}\NormalTok{ penguins[}\StringTok{"flipper\_length\_mm"}\NormalTok{]}

\NormalTok{penguins[}\StringTok{"residual"}\NormalTok{] }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{] }\OperatorTok{{-}}\NormalTok{ penguins[}\StringTok{"analytical\_preds\_slr"}\NormalTok{]}
                                          
\NormalTok{penguins}

\BuiltInTok{print}\NormalTok{(}\StringTok{"MSE: "}\NormalTok{, np.mean(penguins[}\StringTok{"residual"}\NormalTok{]}\OperatorTok{**}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
MSE:  1.3338778799806363
\end{verbatim}

Let's plot plot our results.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.scatterplot(data }\OperatorTok{=}\NormalTok{ penguins, x }\OperatorTok{=} \StringTok{"flipper\_length\_mm"}\NormalTok{, y }\OperatorTok{=} \StringTok{"bill\_depth\_mm"}\NormalTok{)}
\NormalTok{plt.plot(penguins[}\StringTok{"flipper\_length\_mm"}\NormalTok{], penguins[}\StringTok{"analytical\_preds\_slr"}\NormalTok{], }\StringTok{\textquotesingle{}r\textquotesingle{}}\NormalTok{) }\CommentTok{\# a line}
\NormalTok{plt.legend([}\VerbatimStringTok{r\textquotesingle{}$\textbackslash{}hat}\SpecialCharTok{\{y\}}\VerbatimStringTok{$\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}$y$\textquotesingle{}}\NormalTok{])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-7-output-1.png}

}

\end{figure}

\hypertarget{slr-w-sklearn}{%
\subsubsection{\texorpdfstring{SLR w/
\texttt{sklearn}}{SLR w/ sklearn}}\label{slr-w-sklearn}}

We've already saved a lot of time (and avoided tedious calculations) by
translating our derived formulas into code. However, we still had to go
through the process of writing out the linear algebra ourselves.

To make life \emph{even easier}, we can turn to the \texttt{sklearn}
Python library. \texttt{sklearn} is a robust library of machine learning
tools used extensively in research and industry. It gives us a wide
variety of in-built modeling frameworks and methods, so we'll keep
returning to \texttt{sklearn} techniques as we progress through Data
100.

Regardless of the specific type of model being implemented,
\texttt{sklearn} follows a standard set of steps for creating a model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create a model object. This generates a new instance of the model
  class. You can think of it as making a new copy of a standard
  ``template'' for a model. In pseudocode, this looks like:
  \texttt{my\_model\ =\ ModelName()}
\item
  Fit the model to the \texttt{X} design matrix and \texttt{Y} target
  vector. This calculates the optimal model parameters ``behind the
  scenes'' without us explicitly working through the calculations
  ourselves. The fitted parameters are then stored within the model for
  use in future predictions: \texttt{my\_model.fit(X,\ Y)}
\item
  Analyze the fitted parameters using \texttt{.coef\_} or
  \texttt{.intercept\_}, or use the fitted model to make predictions on
  the \texttt{X} input data using \texttt{.predict}.

\begin{verbatim}
my_model.coef_

my_model.intercept_

my_model.predict(X)
\end{verbatim}
\end{enumerate}

Let's put this into action with our multiple regression task. First,
initialize an instance of the \texttt{LinearRegression} class.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\end{Highlighting}
\end{Shaded}

Next, fit the model instance to the design matrix \texttt{X} and target
vector \texttt{Y} by calling \texttt{.fit}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model.fit(X, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LinearRegression()
\end{verbatim}

And, lastly, generate predictions for \(\hat{Y}\) using the
\texttt{.predict} method. Here are the first 5 penguins in our dataset.
How close are our analytical solution's predictions to our
\texttt{sklearn} model's predictions?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Like before, show just the first 5 predictions. The output of predict is usually a np.array}
\NormalTok{penguins[}\StringTok{"sklearn\_preds\_slr"}\NormalTok{] }\OperatorTok{=}\NormalTok{ model.predict(X)}
\NormalTok{sklearn\_5 }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"sklearn\_preds\_slr"}\NormalTok{][:}\DecValTok{5}\NormalTok{].to\_numpy()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Sklearn solution: "}\NormalTok{, sklearn\_5)}
\NormalTok{analytical\_5 }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"analytical\_preds\_slr"}\NormalTok{][:}\DecValTok{5}\NormalTok{].to\_numpy()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Analytical solution: "}\NormalTok{, analytical\_5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Sklearn solution:  [17.81815239 18.10878351 18.63191952 18.51566707 18.3412884 ]
Analytical solution:  [17.81815239 18.10878351 18.63191952 18.51566707 18.3412884 ]
\end{verbatim}

You can also use the model to predict what the \texttt{bill\ depth} of a
hypothetical penguin with \texttt{flipper\ length} of 185mm would have.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this produces a warning since we}
\CommentTok{\# did not specify what X this refers}
\CommentTok{\# to, but since we only }
\CommentTok{\# have one input it is negligible}

\NormalTok{model.predict([[}\DecValTok{185}\NormalTok{]]) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([18.05065728])
\end{verbatim}

We can also check if the fitted
parameters,\(\hat{\theta}_0,\hat{\theta}_1\), themselves are similar to
our analytical solution. Note that since we can have at most 1 intercept
in a SLR or OLS model, so we always get back a \texttt{scalar} value
from \texttt{.intercept}. However, when OLS can have multiple
coefficient values, so \texttt{.coef} returns an array.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta0 }\OperatorTok{=}\NormalTok{ model.intercept\_      }\CommentTok{\# this a scalar}
\BuiltInTok{print}\NormalTok{(}\StringTok{"analytical bias term: "}\NormalTok{, theta0\_hat)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"sklearn bias term: "}\NormalTok{, theta0)}


\NormalTok{theta1 }\OperatorTok{=}\NormalTok{ model.coef\_           }\CommentTok{\# this an array}
\BuiltInTok{print}\NormalTok{(}\StringTok{"analytical coefficient terms: "}\NormalTok{, theta1\_hat)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"sklearn coefficient terms: "}\NormalTok{, theta1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
analytical bias term:  7.297305899612297
sklearn bias term:  7.29730589961231
analytical coefficient terms:  0.058126223695067675
sklearn coefficient terms:  [0.05812622]
\end{verbatim}

\hypertarget{slr-sklearn-performance}{%
\subsubsection{\texorpdfstring{SLR \texttt{sklearn}
Performance}{SLR sklearn Performance}}\label{slr-sklearn-performance}}

The \texttt{sklearn} package also provides a function that computes the
MSE from a list of observations and predictions. This avoids us having
to manually compute MSE by first computing residuals.

\href{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html}{documentation}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}
\NormalTok{MSE\_sklearn }\OperatorTok{=}\NormalTok{ mean\_squared\_error(penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{], penguins[}\StringTok{"sklearn\_preds\_slr"}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"MSE: "}\NormalTok{, MSE\_sklearn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
MSE:  1.333877879980637
\end{verbatim}

We've generated the exact same predictions and error as before, but
without any need for manipulating matrices ourselves!

\hypertarget{multiple-linear-regression-1}{%
\subsection{Multiple Linear
Regression}\label{multiple-linear-regression-1}}

In the previous lecture, we expressed multiple linear regression using
matrix notation.

\[\hat{\mathbb{Y}} = \mathbb{X}\theta\]

\hypertarget{ols-w-derived-analytical-formulas}{%
\subsubsection{OLS w/ Derived Analytical
Formulas}\label{ols-w-derived-analytical-formulas}}

We used a geometric approach to derive the following expression for the
optimal model parameters under MSE error, also called Ordinary Least
Squares (OLS):

\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\]

That's a whole lot of matrix manipulation. How do we implement it in
Python?

There are three operations we need to perform here: multiplying
matrices, taking transposes, and finding inverses.

\begin{itemize}
\tightlist
\item
  To perform matrix multiplication, use the \texttt{@} operator
\item
  To take a transpose, call the \texttt{.T} attribute of an array or
  DataFrame
\item
  To compute an inverse, use \texttt{numpy}'s in-built method
  \texttt{np.linalg.inv}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{, }\StringTok{"body\_mass\_g"}\NormalTok{]].copy()}

\NormalTok{X[}\StringTok{"bias"}\NormalTok{] }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(X))}
\NormalTok{y }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{]}


\NormalTok{theta\_hat }\OperatorTok{=}\NormalTok{ np.linalg.inv(X.T }\OperatorTok{@}\NormalTok{ X) }\OperatorTok{@}\NormalTok{ X.T }\OperatorTok{@}\NormalTok{ y}
\NormalTok{theta\_hat  }
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &          0 \\
\midrule
0 &   0.009828 \\
1 &   0.001477 \\
2 &  11.002995 \\
\bottomrule
\end{tabular}

Note that since we added ``bias'' last, \texttt{theta\_hat{[}2{]}} is
our estimated value for the \(\theta_0\). To make predictions using our
newly-fitted model coefficients, matrix-multiply \texttt{X} and
\texttt{theta\_hat}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_hat }\OperatorTok{=}\NormalTok{ X.to\_numpy() }\OperatorTok{@}\NormalTok{ theta\_hat}

\CommentTok{\# Show just the first 5 predictions to save space on the page}
\NormalTok{y\_hat[:}\DecValTok{5}\NormalTok{]}
\NormalTok{penguins[}\StringTok{"analytical\_preds\_ols"}\NormalTok{] }\OperatorTok{=}\NormalTok{ y\_hat}
\end{Highlighting}
\end{Shaded}

Note, this technique doesn't work if our X is \textbf{not invertible}.

\hypertarget{ols-w-sklearn}{%
\subsubsection{\texorpdfstring{OLS w/
\texttt{sklearn}}{OLS w/ sklearn}}\label{ols-w-sklearn}}

We can actually compute the optimal parameters very easily using
sklearn, using the exact code that we wrote earlier. Note: sklearn does
NOT use the normal equations. Instead it uses gradient descent, a
technique we will learn about soon, which can minimize ANY function, not
just the MSE.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creating our design matrix.}
\CommentTok{\# Note:}
\CommentTok{\#  {-} no bias term needed here bc of sklearn automatically includes one}
\CommentTok{\#  {-} to remove the intercept term, set the fit\_intercept = true in LinearRegression constructor.}

\NormalTok{X\_2d }\OperatorTok{=}\NormalTok{ penguins[[}\StringTok{"flipper\_length\_mm"}\NormalTok{, }\StringTok{"body\_mass\_g"}\NormalTok{]]}
\NormalTok{y }\OperatorTok{=}\NormalTok{ penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{]}
\NormalTok{model\_2d }\OperatorTok{=}\NormalTok{ LinearRegression() }\CommentTok{\# note fit\_intercept=True by default}
\NormalTok{model\_2d.fit(X\_2d, y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
LinearRegression()
\end{verbatim}

Now we again have a model with which we can use to make predictions. For
example, we can ask our model about a penguin's bill depth if they have
185-mm flipper length and 3750 g body mass.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{penguins[}\StringTok{"sklearn\_predictions\_ols"}\NormalTok{] }\OperatorTok{=}\NormalTok{ model\_2d.predict(X\_2d)}
\NormalTok{model\_2d.predict([[}\DecValTok{185}\NormalTok{, }\DecValTok{3750}\NormalTok{]]) }
\CommentTok{\# since we have a 2d data matrix, we maintain the same }
\CommentTok{\# row{-}column expectation for our inputs. }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([18.36187501])
\end{verbatim}

Just like with SLR, we cam also extract the coefficient estimates using
\texttt{.coef\_} and \texttt{.intercept}. The reason why
\texttt{.intercept} returns an array should now be more clear.

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(sklearn) theta0: }\SpecialCharTok{\{}\NormalTok{model\_2d}\SpecialCharTok{.}\NormalTok{intercept\_}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(analytical) theta0 }\SpecialCharTok{\{}\NormalTok{theta\_hat[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(sklearn) theta1: }\SpecialCharTok{\{}\NormalTok{model\_2d}\SpecialCharTok{.}\NormalTok{coef\_[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(analytical) theta1 }\SpecialCharTok{\{}\NormalTok{theta\_hat[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(sklearn) theta2: }\SpecialCharTok{\{}\NormalTok{model\_2d}\SpecialCharTok{.}\NormalTok{coef\_[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"(analytical) theta2 }\SpecialCharTok{\{}\NormalTok{theta\_hat[}\DecValTok{1}\NormalTok{]}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(sklearn) theta0: 11.002995277447072
(analytical) theta0 11.002995277447535
(sklearn) theta1: 0.009828486885248698
(analytical) theta1 0.0098284868852512
(sklearn) theta2: 0.0014774959083212894
(analytical) theta2 0.0014774959083213644
\end{verbatim}

\hypertarget{visualizing-our-2d-linear-model-predictions}{%
\subsubsection{Visualizing Our 2D Linear Model
Predictions}\label{visualizing-our-2d-linear-model-predictions}}

When we have two axis with which we can change an input, moving along
this input plan creates a 2d plane with which we can get model outputs
for. For example, for every single penguin with a
\texttt{flipper\ length} , we also must specify a \texttt{body\ mass}.
These two values in combination will help us predict the
\texttt{bill\ depth}. Thus, we see that the predictions all lie in a
2d-plane. In higher dimensions, they all lie in a ``hyperplane''.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ mpl\_toolkits.mplot3d }\ImportTok{import}\NormalTok{ Axes3D  }\CommentTok{\# noqa: F401 unused import}
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.scatter(penguins[}\StringTok{"flipper\_length\_mm"}\NormalTok{], penguins[}\StringTok{"body\_mass\_g"}\NormalTok{], penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{])}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}flipper\_length\_mm\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}body\_mass\_g\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Text(0.5, 0.5, 'body_mass_g')
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-19-output-2.png}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure()}
\NormalTok{ax }\OperatorTok{=}\NormalTok{ fig.add\_subplot(}\DecValTok{111}\NormalTok{, projection}\OperatorTok{=}\StringTok{\textquotesingle{}3d\textquotesingle{}}\NormalTok{)}
\NormalTok{ax.scatter(penguins[}\StringTok{"flipper\_length\_mm"}\NormalTok{], penguins[}\StringTok{"body\_mass\_g"}\NormalTok{], penguins[}\StringTok{"bill\_depth\_mm"}\NormalTok{])}
\NormalTok{xx, yy }\OperatorTok{=}\NormalTok{ np.meshgrid(}\BuiltInTok{range}\NormalTok{(}\DecValTok{170}\NormalTok{, }\DecValTok{220}\NormalTok{, }\DecValTok{10}\NormalTok{), }\BuiltInTok{range}\NormalTok{(}\DecValTok{2500}\NormalTok{, }\DecValTok{4500}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\NormalTok{zz }\OperatorTok{=}\NormalTok{ ( }\FloatTok{11.0029} \OperatorTok{+} \FloatTok{0.00982} \OperatorTok{*}\NormalTok{ xx }\OperatorTok{+} \FloatTok{0.001477} \OperatorTok{*}\NormalTok{ yy) }\CommentTok{\# thetas\_using\_sklearn}
\NormalTok{ax.plot\_surface(xx, yy, zz, alpha}\OperatorTok{=}\FloatTok{0.2}\NormalTok{)}
\NormalTok{plt.xlabel(}\StringTok{\textquotesingle{}flipper\_length\_mm\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.ylabel(}\StringTok{\textquotesingle{}body\_mass\_g\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.gcf().savefig(}\StringTok{"plane.png"}\NormalTok{, dpi }\OperatorTok{=} \DecValTok{300}\NormalTok{, bbox\_inches }\OperatorTok{=} \StringTok{"tight"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-20-output-1.png}

}

\end{figure}

\hypertarget{loss-terminology}{%
\subsection{Loss Terminology}\label{loss-terminology}}

We use the word ``loss'' in two different (but very related) contexts in
this course.

\begin{itemize}
\item
  In general, loss is the cost function that measures how far off
  model's prediction(s) is(are) from the actual value(s).

  \begin{itemize}
  \tightlist
  \item
    Per-datapoint loss is a cost function that measures the cost of
    \(y\) vs \(\hat{y}\) for a particular datapoint.
  \item
    Loss (without any adjectives) is generally a cost function measured
    across all datapoints. Often times, \emph{empirical risk} is
    \emph{average per-datapoint loss}.
  \end{itemize}
\item
  We prioritize using the latter term, because we don't particularly
  look at a given datapoint's loss when optimizing a model.

  \begin{itemize}
  \tightlist
  \item
    In other words, the dataset-level loss is the objective function
    that we'd like to minimize using gradient descent.
  \item
    We achieve this minimization by using \textbf{per-datapoint} loss
    values.
  \end{itemize}
\end{itemize}

\hypertarget{gradient-descent-1}{%
\section{Gradient Descent}\label{gradient-descent-1}}

At this point, we're fairly comfortable with fitting a regression model
under MSE risk (indeed, we've done it three times now!). It's important
to remember, however, that the results we've found previously apply to
one very specific case: the equations we used above are only relevant to
a linear regression model using MSE as the cost function. In reality,
we'll be working with a wide range of model types and objective
functions, not all of which are as straightforward as the scenario we've
discussed previously. This means that we need some more generalizable
way of fitting a model to minimize loss.

To do this, we'll introduce the technique of \textbf{gradient descent}.

\hypertarget{minimizing-a-1d-function}{%
\subsection{Minimizing a 1D Function}\label{minimizing-a-1d-function}}

Let's shift our focus away from MSE to consider some new, arbitrary cost
function. You can think of this function as outputting the empirical
risk associated with some parameter \texttt{theta}.

\hypertarget{the-naive-approach-guess-and-check}{%
\subsubsection{The Naive Approach: Guess and
Check}\label{the-naive-approach-guess-and-check}}

Above, we saw that the minimum is somewhere around 5.3ish. Let's see if
we can figure out how to find the exact minimum algorithmically from
scratch. One way very slow and terrible way would be manual
guess-and-check.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ arbitrary(x):}
    \ControlFlowTok{return}\NormalTok{ (x}\OperatorTok{**}\DecValTok{4} \OperatorTok{{-}} \DecValTok{15}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{3} \OperatorTok{+} \DecValTok{80}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{2} \OperatorTok{{-}} \DecValTok{180}\OperatorTok{*}\NormalTok{x }\OperatorTok{+} \DecValTok{144}\NormalTok{)}\OperatorTok{/}\DecValTok{10}

\KeywordTok{def}\NormalTok{ simple\_minimize(f, xs):}
    \CommentTok{\# Takes in a function f and a set of values xs. }
    \CommentTok{\# Calculates the value of the function f at all values x in xs}
    \CommentTok{\# Takes the minimum value of f(x) and returns the corresponding value x }
\NormalTok{    y }\OperatorTok{=}\NormalTok{ [f(x) }\ControlFlowTok{for}\NormalTok{ x }\KeywordTok{in}\NormalTok{ xs]  }
    \ControlFlowTok{return}\NormalTok{ xs[np.argmin(y)]}

\NormalTok{simple\_minimize(arbitrary, np.linspace(}\DecValTok{1}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5.421052631578947
\end{verbatim}

\hypertarget{scipy.optimize.minimize}{%
\subsubsection{Scipy.optimize.minimize}\label{scipy.optimize.minimize}}

One way to minimize this mathematical function is to use the
\texttt{scipy.optimize.minimize} function. It takes a function and a
starting guess and tries to find the minimum.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}

\CommentTok{\# takes a function f and a starting point x0 and returns a readout }
\CommentTok{\# with the optimal input value of x which minimizes f}
\NormalTok{minimize(arbitrary, x0 }\OperatorTok{=} \FloatTok{3.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      fun: -0.13827491292966557
 hess_inv: array([[0.73848255]])
      jac: array([6.48573041e-06])
  message: 'Optimization terminated successfully.'
     nfev: 20
      nit: 3
     njev: 10
   status: 0
  success: True
        x: array([2.39275266])
\end{verbatim}

Our choice of start point can affect the outcome. For example if we
start to the left, we get stuck in the local minimum on the left side.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{minimize(arbitrary, x0 }\OperatorTok{=} \DecValTok{1}\NormalTok{)}

\CommentTok{\#here we see the optimal value is different from before}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      fun: -0.13827491294422317
 hess_inv: array([[0.74751575]])
      jac: array([-3.7997961e-07])
  message: 'Optimization terminated successfully.'
     nfev: 16
      nit: 7
     njev: 8
   status: 0
  success: True
        x: array([2.3927478])
\end{verbatim}

\texttt{scipy.optimize.minimize} is great. It may also seem a bit
magical. How could you write a function that can find the minimum of any
mathematical function? There are a number of ways to do this, which
we'll explore in today's lecture, eventually arriving at the important
idea of \textbf{gradient descent}, which is the principle that
\texttt{scipy.optimize.minimize} uses.

It turns out that under the hood, the \texttt{fit} method for
\texttt{LinearRegression} models uses gradient descent. Gradient descent
is also how much of machine learning works, including even advanced
neural network models.

In Data 100, the gradient descent process will usually be invisible to
us, hidden beneath an abstraction layer. However, to be good data
scientists, it's important that we know the basic principles beyond the
optimization functions that harness to find optimal parmaeters.

\hypertarget{digging-into-gradient-descent}{%
\subsection{Digging into Gradient
Descent}\label{digging-into-gradient-descent}}

Looking at the function across this domain, it is clear that the
function's minimum value occurs around \(\theta = 5.3\). Let's pretend
for a moment that we \emph{couldn't} see the full view of the cost
function. How would we guess the value of \(\theta\) that minimizes the
function?

It turns out that the first derivative of the function can give us a
clue. In the plots below, the line indicates the value of the derivative
of each value of \(\theta\). The derivative is negative where it is red
and positive where it is green.

Say we make a guess for the minimizing value of \(\theta\). Remember
that we read plots from left to right, and assume that our starting
\(\theta\) value is to the left of the optimal \(\hat{\theta}\). If the
guess ``undershoots'' the true minimizing value -- our guess for
\(\theta\) is not quite at the value of the \(\hat{\theta}\) that truly
minimizes the function -- the derivative will be \textbf{negative} in
value. This means that if we increase \(\theta\) (move further to the
right), then we \textbf{can decrease} our loss function further. If this
guess ``overshoots'' the true minimizing value, the derivative will be
positive in value, implying the converse.

We can use this pattern to help formulate our next guess for the optimal
\(\hat{\theta}\). Consider the case where we've undershot \(\theta\) by
guessing too low of a value. We'll want our next guess to be greater in
value than the previous guess -- that is, we want to shift our guess to
the right. You can think of this as following the slope ``downhill'' to
the function's minimum value.

If we've overshot \(\hat{\theta}\) by guessing too high of a value,
we'll want our next guess to be lower in value -- we want to shift our
guess for \(\hat{\theta}\) to the left.

\hypertarget{gradient-descent-in-1-dimension}{%
\section{Gradient Descent in 1
Dimension}\label{gradient-descent-in-1-dimension}}

These observations lead us to the \textbf{gradient descent update rule}:
\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]

Begin with our guess for \(\hat{\theta}\) at timestep \(t\). To find our
guess for \(\hat{\theta}\) at the next timestep, \(t+1\), subtract the
objective function's derivative evaluted at \(\theta^{(t)}\),
\(\frac{d}{d\theta} L(\theta^{(t)})\), scaled by a positive value
\(\alpha\). We've replaced the generic function \(f\) with \(L\) to
indicate that we are minimizing loss.

Supposing that any local minima is a global minimum (see
\textbf{convexity} at the end of this page):

\begin{itemize}
\tightlist
\item
  If our guess \(\theta^{(t)}\) is to the left of \(\hat{\theta}\)
  (undershooting), the first derivative will be negative. Subtracting a
  negative number from \(\theta^{(t)}\) will \emph{increase} the value
  of the next guess, \(\theta^{(t+1)}\) and move our loss function down.
  The guess will shift to the right.
\item
  If our guess \(\theta^{(t)}\) was too high (overshooting
  \(\hat{\theta}\)), the first derivative will be positive. Subtracting
  a positive number from \(\theta^{(t)}\) will \emph{decrease} the value
  of the next guess, \(\theta^{(t+1)}\) and move our loss function down.
  The guess will shift to the left.
\end{itemize}

Put together, this captures the same behavior we reasoned through above.
We repeatedly update our guess for the optimal \(\theta\) until we've
completed a set number of updates, or until each additional update
iteration does not change the value of \(\theta\). In this second case,
we say that gradient descent has \textbf{converged} on a solution.

The \(\alpha\) term in the update rule is known as the \textbf{learning
rate}. It is a positive value represents the size of each gradient
descent update step -- in other words, how ``far'' should we step to the
left or right with each updated guess? A high value of \(\alpha\) will
lead to large differences in value between consecutive guesses for
\(\hat{\theta}\); a low value of \(\alpha\) will result in smaller
differences in value between consecutive guesses. This is the first
example of a \textbf{hyperparameter}, a parameter that is hand picked by
the data scientist that changes the model's behavior, in the course.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define the derivative of the arbitrary function we want to minimize}
\KeywordTok{def}\NormalTok{ derivative\_arbitrary(x):}
    \ControlFlowTok{return}\NormalTok{ (}\DecValTok{4}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{3} \OperatorTok{{-}} \DecValTok{45}\OperatorTok{*}\NormalTok{x}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{160}\OperatorTok{*}\NormalTok{x }\OperatorTok{{-}} \DecValTok{180}\NormalTok{)}\OperatorTok{/}\DecValTok{10}

\KeywordTok{def}\NormalTok{ gradient\_descent(df, initial\_guess, alpha, n):}
    \CommentTok{"""Performs n steps of gradient descent on df using learning rate alpha starting}
\CommentTok{       from initial\_guess. Returns a numpy array of all guesses over time."""}
\NormalTok{    guesses }\OperatorTok{=}\NormalTok{ [initial\_guess]}
\NormalTok{    current\_guess }\OperatorTok{=}\NormalTok{ initial\_guess}
    \ControlFlowTok{while} \BuiltInTok{len}\NormalTok{(guesses) }\OperatorTok{\textless{}}\NormalTok{ n:}
\NormalTok{        current\_guess }\OperatorTok{=}\NormalTok{ current\_guess }\OperatorTok{{-}}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ df(current\_guess)}
\NormalTok{        guesses.append(current\_guess)}
        
    \ControlFlowTok{return}\NormalTok{ np.array(guesses)}

\CommentTok{\# calling our function gives us the path that gradient descent takes for 20 steps }
\CommentTok{\# with a learning rate of 0.3 starting at theta = 4}
\NormalTok{trajectory }\OperatorTok{=}\NormalTok{ gradient\_descent(derivative\_arbitrary, }\DecValTok{4}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{trajectory}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([4.        , 4.12      , 4.26729664, 4.44272584, 4.64092624,
       4.8461837 , 5.03211854, 5.17201478, 5.25648449, 5.29791149,
       5.31542718, 5.3222606 , 5.32483298, 5.32578765, 5.32614004,
       5.32626985, 5.32631764, 5.32633523, 5.3263417 , 5.32634408])
\end{verbatim}

Above, we've simply run our algorithm a fixed number of times. More
sophisticated implementations will stop based on a variety of different
stopping criteria, e.g.~error getting too small, error getting too
large, etc. We will not discuss these in our course.

\hypertarget{application-of-1d-gradient-descent}{%
\subsection{Application of 1D Gradient
Descent}\label{application-of-1d-gradient-descent}}

We've seen how to find the optimal parameters for a 1D linear model for
the penguin dataset:

\begin{itemize}
\tightlist
\item
  Using the derived equations from Data 8.
\item
  Using \texttt{sklearn}.

  \begin{itemize}
  \tightlist
  \item
    Uses gradient descent under the hood!
  \end{itemize}
\end{itemize}

In real practice in this course, we'll usually use \texttt{sklearn}. But
for now, let's see how we can do the gradient descent ourselves.

Let's consider a case where we have a linear model with no offset.

\[\hat{y} = \theta_1 x\]

We want to find the parameter \(\theta_1\) such that the L2 loss is
minimized. In sklearn, this is easy. To avoid fitting an intercept, we
set \texttt{fit\_intercept} to false.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression(fit\_intercept }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"tips"}\NormalTok{)}
\NormalTok{model.fit(df[[}\StringTok{"total\_bill"}\NormalTok{]], df[}\StringTok{"tip"}\NormalTok{])}
\NormalTok{model.coef\_ }\CommentTok{\# the optimal tip percentage is 14.37\%}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.1437319])
\end{verbatim}

\hypertarget{creating-an-explicit-mse-function}{%
\subsection{Creating an Explicit MSE
Function}\label{creating-an-explicit-mse-function}}

To employ gradient descent and do this ourselves, we need to define a
function upon which we can use gradient descent. Suppose we select the
L2 loss as our loss function. In this case, our goal will be to minimize
the mean squared error.

Let's start by writing a function that computes the MSE for a given
choice of \(\theta_1\) on our dataset.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mse\_single\_arg(theta1):}
    \CommentTok{"""Returns the MSE on our data for the given theta1"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ df[}\StringTok{"total\_bill"}\NormalTok{]}
\NormalTok{    y\_obs }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{]}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ theta1 }\OperatorTok{*}\NormalTok{ x}
    \ControlFlowTok{return}\NormalTok{ np.mean((y\_hat }\OperatorTok{{-}}\NormalTok{ y\_obs) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}

\NormalTok{mse\_single\_arg(}\FloatTok{0.1437}\NormalTok{)}
\CommentTok{\# The minimum loss value that we can achieve is 1.178 dollars on average away from the truth}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
1.1781165940051928
\end{verbatim}

\hypertarget{plotting-the-mse-function}{%
\subsection{Plotting the MSE Function}\label{plotting-the-mse-function}}

Since we only have 1 parameter, we can simply cross reference our
results with a simnple plot. We do not want to always do this since some
functions can have thousands of inputs, making them difficult to plot.
We can plot the MSE as a function of \texttt{theta1}. It turns out to
look pretty smooth, and quite similar to a parabola.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta1s }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ df[}\StringTok{"total\_bill"}\NormalTok{]}
\NormalTok{y\_obs }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{]}

\NormalTok{MSEs }\OperatorTok{=}\NormalTok{ [mse\_single\_arg(theta1) }\ControlFlowTok{for}\NormalTok{ theta1 }\KeywordTok{in}\NormalTok{ theta1s]}

\NormalTok{plt.plot(theta1s, MSEs)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"Choice for $\textbackslash{}theta\_1$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r"MSE"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-27-output-1.png}

}

\end{figure}

The minimum appears to be around \(\theta_1 = 0.14\). We can once again
check this naively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simple\_minimize(mse\_single\_arg, np.linspace(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{21}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
0.14
\end{verbatim}

As before, what we're doing is computing all the starred values below
and then returning the \(\theta_1\) that goes with the minimum value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta1s }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{200}\NormalTok{)}
\NormalTok{sparse\_theta1s }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{21}\NormalTok{)}

\NormalTok{loss }\OperatorTok{=}\NormalTok{ [mse\_single\_arg(theta1) }\ControlFlowTok{for}\NormalTok{ theta1 }\KeywordTok{in}\NormalTok{ theta1s]}
\NormalTok{sparse\_loss }\OperatorTok{=}\NormalTok{ [mse\_single\_arg(theta1) }\ControlFlowTok{for}\NormalTok{ theta1 }\KeywordTok{in}\NormalTok{ sparse\_theta1s]}

\NormalTok{plt.plot(theta1s, loss)}
\NormalTok{plt.plot(sparse\_theta1s, sparse\_loss, }\StringTok{\textquotesingle{}r*\textquotesingle{}}\NormalTok{)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"Choice for $\textbackslash{}theta\_1$"}\NormalTok{)}
\NormalTok{plt.ylabel(}\VerbatimStringTok{r"MSE"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{gradient_descent/gradient_descent_files/figure-pdf/cell-29-output-1.png}

}

\end{figure}

\hypertarget{using-scipy.optimize.minimize}{%
\subsubsection{Using
Scipy.Optimize.minimize}\label{using-scipy.optimize.minimize}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ scipy.optimize}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}
\NormalTok{minimize(mse\_single\_arg, x0 }\OperatorTok{=} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      fun: 1.1781161154513213
 hess_inv: array([[1]])
      jac: array([4.24683094e-06])
  message: 'Optimization terminated successfully.'
     nfev: 6
      nit: 1
     njev: 3
   status: 0
  success: True
        x: array([0.14373189])
\end{verbatim}

\hypertarget{using-our-gradient-descent-function}{%
\subsubsection{Using Our Gradient Descent
Function}\label{using-our-gradient-descent-function}}

Another approach is to use our 1D gradient descent algorithm from
earlier. This is the exact same function as earlier. We can run it for
100 steps and see where it ultimately ends up.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mse\_loss\_derivative\_single\_arg(theta\_1):}
    \CommentTok{"""Returns the derivative of the MSE on our data for the given theta1"""}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ df[}\StringTok{"total\_bill"}\NormalTok{]}
\NormalTok{    y\_obs }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{]}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ theta\_1 }\OperatorTok{*}\NormalTok{ x}
    
    \ControlFlowTok{return}\NormalTok{ np.mean(}\DecValTok{2} \OperatorTok{*}\NormalTok{ (y\_hat }\OperatorTok{{-}}\NormalTok{ y\_obs) }\OperatorTok{*}\NormalTok{ x)}

\NormalTok{gradient\_descent(mse\_loss\_derivative\_single\_arg, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.0001}\NormalTok{, }\DecValTok{100}\NormalTok{)[}\OperatorTok{{-}}\DecValTok{5}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([0.14372404, 0.14372478, 0.14372545, 0.14372605, 0.1437266 ])
\end{verbatim}

\hypertarget{multidimensional-gradient-descent}{%
\section{Multidimensional Gradient
Descent}\label{multidimensional-gradient-descent}}

We're in good shape now: we've developed a technique to find the minimum
value of a more complex objective function.

The function we worked with above was one-dimensional -- we were only
minimizing the function with respect to a single parameter, \(\theta\).
However, as we've seen before, we often need to optimize a cost function
with respect to several parameters (for example, when selecting the best
model parameters for multiple linear regression). We'll need to extend
our gradient descent rule to \emph{multidimensional} objective
functions.

Now suppose we improve our model so that we want to predict the tip from
the total\_bill plus a constant offset, in other words:

\[\textrm{tip} = \theta_0 + \theta_1 \textrm{bill}\]

To put this in more concrete terms what this means, let's return to the
familiar case of simple linear regression with MSE loss.
\[\text{MSE}(\theta_0,\:\theta_1) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x)^2\]

Now, loss is expressed in terms of \emph{two} parameters, \(\theta_0\)
and \(\theta_1\). Rather than a one-dimensional loss function as we had
above, we are now dealing with a two-dimensional \textbf{loss surface}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This code is for illustration purposes only}
\CommentTok{\# It contains a lot of syntax you have not seen}
\ImportTok{import}\NormalTok{ plotly.graph\_objects }\ImportTok{as}\NormalTok{ go}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression(fit\_intercept }\OperatorTok{=} \VariableTok{False}\NormalTok{)}
\NormalTok{df }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"tips"}\NormalTok{)}
\NormalTok{df[}\StringTok{"bias"}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\NormalTok{model.fit(df[[}\StringTok{"bias"}\NormalTok{,}\StringTok{"total\_bill"}\NormalTok{]], df[}\StringTok{"tip"}\NormalTok{])}
\NormalTok{model.coef\_}

\NormalTok{uvalues }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{vvalues }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{(u,v) }\OperatorTok{=}\NormalTok{ np.meshgrid(uvalues, vvalues)}
\NormalTok{thetas }\OperatorTok{=}\NormalTok{ np.vstack((u.flatten(),v.flatten()))}

\NormalTok{X }\OperatorTok{=}\NormalTok{ df[[}\StringTok{"bias"}\NormalTok{,}\StringTok{"total\_bill"}\NormalTok{]].to\_numpy()}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ df[}\StringTok{"tip"}\NormalTok{].to\_numpy()}

\KeywordTok{def}\NormalTok{ mse\_loss\_single\_arg(theta):}
    \ControlFlowTok{return}\NormalTok{ mse\_loss(theta, X, Y)}

\KeywordTok{def}\NormalTok{ mse\_loss(theta, X, y\_obs):}
\NormalTok{    y\_hat }\OperatorTok{=}\NormalTok{ X }\OperatorTok{@}\NormalTok{ theta}
    \ControlFlowTok{return}\NormalTok{ np.mean((y\_hat }\OperatorTok{{-}}\NormalTok{ Y) }\OperatorTok{**} \DecValTok{2}\NormalTok{)    }

\NormalTok{MSE }\OperatorTok{=}\NormalTok{ np.array([mse\_loss\_single\_arg(t) }\ControlFlowTok{for}\NormalTok{ t }\KeywordTok{in}\NormalTok{ thetas.T])}

\NormalTok{loss\_surface }\OperatorTok{=}\NormalTok{ go.Surface(x}\OperatorTok{=}\NormalTok{u, y}\OperatorTok{=}\NormalTok{v, z}\OperatorTok{=}\NormalTok{np.reshape(MSE, u.shape))}

\NormalTok{ind }\OperatorTok{=}\NormalTok{ np.argmin(MSE)}
\NormalTok{optimal\_point }\OperatorTok{=}\NormalTok{ go.Scatter3d(name }\OperatorTok{=} \StringTok{"Optimal Point"}\NormalTok{,}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ [thetas.T[ind,}\DecValTok{0}\NormalTok{]], y }\OperatorTok{=}\NormalTok{ [thetas.T[ind,}\DecValTok{1}\NormalTok{]], }
\NormalTok{    z }\OperatorTok{=}\NormalTok{ [MSE[ind]],}
\NormalTok{    marker}\OperatorTok{=}\BuiltInTok{dict}\NormalTok{(size}\OperatorTok{=}\DecValTok{10}\NormalTok{, color}\OperatorTok{=}\StringTok{"red"}\NormalTok{))}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ go.Figure(data}\OperatorTok{=}\NormalTok{[loss\_surface, optimal\_point])}
\NormalTok{fig.update\_layout(scene }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{(}
\NormalTok{    xaxis\_title }\OperatorTok{=} \StringTok{"theta0"}\NormalTok{,}
\NormalTok{    yaxis\_title }\OperatorTok{=} \StringTok{"theta1"}\NormalTok{,}
\NormalTok{    zaxis\_title }\OperatorTok{=} \StringTok{"MSE"}\NormalTok{))}
\NormalTok{fig.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

\begin{verbatim}
Unable to display output for mime type(s): text/html
\end{verbatim}

Though our objective function looks a little different, we can use the
same principles as we did earlier to locate the optimal model
parameters. Notice how the minimum value of MSE, marked by the red dot
in the plot above, occurs in the ``valley'' of the loss surface. Like
before, we want our guesses for the best pair of
\((\theta_0,\:\theta_1)\) to move ``downhill'' towards this minimum
point.

The difference now is that we need to update guesses for \emph{both}
\(\theta_0\) and \(\theta_1\) that minimize a loss function
\(L(\theta, \mathbb{X}, \mathbb{Y})\):
\[\theta_0^{(t+1)} = \theta_0^{(t)} - \alpha \frac{\partial L}{\partial \theta_0}\Bigm\vert_{\theta=\theta^{(t)}} \qquad \qquad \theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{\partial L}{\partial \theta_1}\Bigm\vert_{\theta=\theta^{(t)}}\]

We can tidy this statement up by using vector notation:
\[\begin{bmatrix}
           \theta_{0}^{(t+1)} \\
           \theta_{1}^{(t+1)} \\
         \end{bmatrix}
=
\begin{bmatrix}
           \theta_{0}^{(t)} \\
           \theta_{1}^{(t)} \\
         \end{bmatrix}
- \alpha
\begin{bmatrix}
           \frac{\partial L}{\partial \theta_{0}}\vert_{\theta=\theta^{(t)}} \\
           \frac{\partial L}{\partial \theta_{1}}\vert_{\theta=\theta^{(t)}} \\
         \end{bmatrix}
\]

To save ourselves from writing out long column vectors, we'll introduce
some new notation. \(\vec{\theta}^{(t)}\) is a column vector of guesses
for each model parameter \(\theta_i\) at timestep \(t\). We call
\(\nabla_{\vec{\theta}} L\) the \textbf{gradient vector.} In plain
English, it means ``take the derivative of loss,
\(L(\theta, \mathbb{X}, \mathbb{Y})\), with respect to each model
parameter in \(\vec{\theta}\), and evaluate it at the given
\(\theta = \theta^{(t)}\).''

\[\vec{\theta}^{(t+1)}
= \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)}, \mathbb{X}, \mathbb{Y})
\]

\hypertarget{gradient-notation}{%
\subsection{Gradient Notation}\label{gradient-notation}}

Consider the 2D function:
\[f(\theta_0, \theta_1) = 8\theta_0^2 + 3\theta_0\theta_1\]

For a function of 2 variables, \(f(\theta_0, \theta_1)\) we define the
gradient as
\(\nabla_\theta f = \frac{\partial f}{\partial \theta_0} \vec{i} + \frac{\partial f}{\partial \theta_1} \vec{j}\),
where \(\vec{i}\) and \(\vec{j}\) are the unit vectors in the
\(\theta_0\) and \(\theta_1\) directions.

\[\frac{\partial f}{\partial \theta_0} = 16\theta_0 + 3\theta_1\]

\[\frac{\partial f}{\partial \theta_1} = 3\theta_0\]

\[\nabla_\theta f =  (16\theta_0 + 3\theta_1)\vec{i} + (3\theta_0)\vec{j}\]

We can also write it in column vector notation.

\[\nabla_\theta f =  \begin{bmatrix} \frac{\partial f}{\partial \theta_0} \\ \frac{\partial f}{\partial \theta_1} \\... \end{bmatrix}\]

EX:
\(\nabla_\theta f = \begin{bmatrix}16\theta_0 + 3\theta_1 \\ 3\theta_0\end{bmatrix}\)

You should read these gradients as:

\begin{itemize}
\tightlist
\item
  \(\frac{\partial f}{\partial \theta_0}\) : If I nudge the 1st model
  weight, what happens to loss?
\item
  \(\frac{\partial f}{\partial \theta_1}\) :If I nudge the 2nd model
  weight, what happens to loss?
\item
  etc.
\end{itemize}

\hypertarget{visualizing-gradient-descent}{%
\subsection{Visualizing Gradient
Descent}\label{visualizing-gradient-descent}}

First, we need to be able to easily determine the gradient for any pair
of values, \(\theta_0, \theta_1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tips\_with\_bias }\OperatorTok{=}\NormalTok{ df.copy()}
\NormalTok{tips\_with\_bias[}\StringTok{"bias"}\NormalTok{] }\OperatorTok{=} \DecValTok{1}
\NormalTok{X }\OperatorTok{=}\NormalTok{ tips\_with\_bias[[}\StringTok{"bias"}\NormalTok{, }\StringTok{"total\_bill"}\NormalTok{]]}
\NormalTok{X.head(}\DecValTok{5}\NormalTok{)}

\KeywordTok{def}\NormalTok{ mse\_gradient(theta, X, y\_obs):}
    \CommentTok{"""Returns the gradient of the MSE on our data for the given theta"""}    
\NormalTok{    x0 }\OperatorTok{=}\NormalTok{ X.iloc[:, }\DecValTok{0}\NormalTok{]}
\NormalTok{    x1 }\OperatorTok{=}\NormalTok{ X.iloc[:, }\DecValTok{1}\NormalTok{]}
\NormalTok{    dth0 }\OperatorTok{=}\NormalTok{ np.mean(}\OperatorTok{{-}}\DecValTok{2} \OperatorTok{*}\NormalTok{ (y\_obs }\OperatorTok{{-}}\NormalTok{ theta[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\NormalTok{x0 }\OperatorTok{{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\NormalTok{x1) }\OperatorTok{*}\NormalTok{ x0)}
\NormalTok{    dth1 }\OperatorTok{=}\NormalTok{ np.mean(}\OperatorTok{{-}}\DecValTok{2} \OperatorTok{*}\NormalTok{ (y\_obs }\OperatorTok{{-}}\NormalTok{ theta[}\DecValTok{0}\NormalTok{]}\OperatorTok{*}\NormalTok{x0 }\OperatorTok{{-}}\NormalTok{ theta[}\DecValTok{1}\NormalTok{]}\OperatorTok{*}\NormalTok{x1) }\OperatorTok{*}\NormalTok{ x1)}
    \ControlFlowTok{return}\NormalTok{ np.array([dth0, dth1])}

\KeywordTok{def}\NormalTok{ mse\_gradient\_single\_arg(theta):}
    \CommentTok{"""Returns the gradient of the MSE on our data for the given theta"""}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ tips\_with\_bias[[}\StringTok{"bias"}\NormalTok{, }\StringTok{"total\_bill"}\NormalTok{]]}
\NormalTok{    y\_obs }\OperatorTok{=}\NormalTok{ tips\_with\_bias[}\StringTok{"tip"}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ mse\_gradient(theta, X, y\_obs)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ tips\_with\_bias[[}\StringTok{"bias"}\NormalTok{, }\StringTok{"total\_bill"}\NormalTok{]]}
\NormalTok{y\_obs }\OperatorTok{=}\NormalTok{ tips\_with\_bias[}\StringTok{"tip"}\NormalTok{]}
\NormalTok{ex1\_mse }\OperatorTok{=}\NormalTok{ mse\_gradient(np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]), X, y\_obs)}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Gradient for values theta0 = 0 and theta1 = 0 : }\SpecialCharTok{\{}\NormalTok{ex1\_mse}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Gradient for values theta0 = 0 and theta1 = 0 : [  -5.99655738 -135.22631803]
\end{verbatim}

Using our previously previously defined \texttt{gradient\_descent}
function, we can see if our intuition extends to higher dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#print out the last 10 guesses our algorithm outputs to save space}
\NormalTok{guesses }\OperatorTok{=}\NormalTok{ gradient\_descent(mse\_gradient\_single\_arg, np.array([}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{]), }\FloatTok{0.001}\NormalTok{, }\DecValTok{10000}\NormalTok{)[}\OperatorTok{{-}}\DecValTok{10}\NormalTok{:]}
\end{Highlighting}
\end{Shaded}

\hypertarget{mini-batch-gradient-decsent-and-stochastic-gradient-descent}{%
\section{Mini-Batch Gradient Decsent and Stochastic Gradient
Descent}\label{mini-batch-gradient-decsent-and-stochastic-gradient-descent}}

Formally, the algorithm we derived above is called \textbf{batch
gradient descent.} For each iteration of the algorithm, the derivative
of loss is computed across the entire batch of available data. While
this update rule works well in theory, it is not practical in all
circumstances. For large datasets (with perhaps billions of data
points), finding the gradient across all the data is incredibly
computationally taxing.

\textbf{Mini-batch gradient descent} tries to address this issue. In
mini-batch descent, only a subset of the data is used to compute an
estimate of the gradient. For example, we might consider only 10\% of
the total data at each gradient descent update step. At the next
iteration, a different 10\% of the data is sampled to perform the
following update. Once the entire dataset has been used, the process is
repeated. Each complete ``pass'' through the data is known as a
\textbf{training epoch}. In practice, we choose the mini-batch size to
be \(32\).

In the extreme case, we might choose a batch size of only 1 data point
-- that is, a single data point is used to estimate the gradient of loss
with each update step. This is known as \textbf{stochastic gradient
descent}.

Batch gradient descent is a deterministic technique -- because the
entire dataset is used at each update iteration, the algorithm will
always advance towards the minimum of the loss surface. In contrast,
both mini-batch and stochastic gradient descent involve an element of
randomness. Since only a subset of the full data is used to update the
guess for \(\vec{\theta}\) at each iteration, there's a chance the
algorithm will not progress towards the true minimum of loss with each
update. Over the longer term, these stochastic techniques should still
converge towards the optimal solution.

The diagrams below represent a ``bird's eye view'' of a loss surface
from above. Notice that batch gradient descent takes a direct path
towards the optimal \(\hat{\theta}\). Stochastic gradient descent, in
contrast, ``hops around'' on its path to the minimum point on the loss
surface. This reflects the randomness of the sampling process at each
update step.

\hypertarget{convexity}{%
\section{Convexity}\label{convexity}}

In our analysis above, we focused our attention on the global minimum of
the loss function. You may be wondering: what about the local minimum
just to the left?

If we had chosen a different starting guess for \(\theta\), or a
different value for the learning rate \(\alpha\), we may have converged
on the local minimum, rather than on the true optimum value of loss.

If the loss function is \textbf{convex}, gradient descent is guaranteed
to find the global minimum of the objective function. Formally, a
function \(f\) is convex if: \[tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)\]

To put this into words: if you drew a line between any two points on the
curve, all values on the curve must be \emph{on or below} the line.
Importantly, any local minimum of a convex function is also its global
minimum.

In summary, non-convex loss functions can cause problems with
optimization. This means that our choice of loss function is an key
factor in our modeling process. It turns out that MSE \emph{is} convex,
which is a major reason why it is such a popular choice of loss
function.

\bookmarksetup{startatroot}

\hypertarget{feature-engineering}{%
\chapter{Feature Engineering}\label{feature-engineering}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Recognize the value of feature engineering as a tool to improve model
  performance
\item
  Implement polynominal feature generation and one hot encoding
\item
  Understand the interactions between model complexity, model variance,
  and training error
\end{itemize}

\end{tcolorbox}

At this point in the course, we've equipped ourselves with some powerful
techniques to build and optimize models. We've explored how to develop
models of multiple variables, as well as how to fit these models to
maximize their performance.

All of this was done with one major caveat: the regression models we've
worked with so far are all \textbf{linear in the input variables}. We've
assumed that our predictions should be some combination of linear
variables. While this works well in some cases, the real world isn't
always so straightforward. In today's lecture, we'll learn an important
method to address this issue -- and consider some new problems that can
arise when we do so.

\hypertarget{feature-engineering-1}{%
\section{Feature Engineering}\label{feature-engineering-1}}

Feature Engineering is the process of \emph{transforming} the raw
features into \emph{more informative features} that can be used in
modeling or EDA tasks.

Feature engineering allows you to: Capture domain knowledge
(e.g.~periodicity or relationships between features). Express non-linear
relationships using simple linear models. Encode non-numeric features to
be used as inputs to models. Example: Using the country of origin of a
car as an input to modeling its efficiency.

Why doesn't sklearn doesn't have SquareRegression /PolynomialRegression.

\begin{itemize}
\tightlist
\item
  We can translate these into linear models with features that are
  polynomials of x.
\item
  Feature engineering saves \texttt{sklearn} a lot of redundancy in
  their library.
\item
  Linear models have really nice properties.
\end{itemize}

\hypertarget{feature-functions}{%
\section{Feature Functions}\label{feature-functions}}

A feature function takes our original d dimensional input,
\(\mathbb{X}\), and transforms it into a \(d'\) dimensional input
\(\Phi\).

For example, when we add the squared term of an existing column, we are
effectively using kind of feature function, taking a \(n \times 1\)
matrix, \([hp]\), and turning it into an \(n \times 2\) matrix
\([hp,hp^2]\)

As number of features grows, we can capture arbitrarily complex
relationships.

Let's take a moment to dig further in to remind ourselves where this
linearity comes from. Consider the following dataset on vehicles:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}
\NormalTok{vehicles }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"mpg"}\NormalTok{).rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{"horsepower"}\NormalTok{:}\StringTok{"hp"}\NormalTok{\}).dropna()}
\NormalTok{vehicles.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrll}
\toprule
{} &   mpg &  cylinders &  displacement &     hp &  weight &  acceleration &  model\_year & origin &                       name \\
\midrule
0 &  18.0 &          8 &         307.0 &  130.0 &    3504 &          12.0 &          70 &    usa &  chevrolet chevelle malibu \\
1 &  15.0 &          8 &         350.0 &  165.0 &    3693 &          11.5 &          70 &    usa &          buick skylark 320 \\
2 &  18.0 &          8 &         318.0 &  150.0 &    3436 &          11.0 &          70 &    usa &         plymouth satellite \\
3 &  16.0 &          8 &         304.0 &  150.0 &    3433 &          12.0 &          70 &    usa &              amc rebel sst \\
4 &  17.0 &          8 &         302.0 &  140.0 &    3449 &          10.5 &          70 &    usa &                ford torino \\
\bottomrule
\end{tabular}

Suppose we wish to develop a model to predict a vehicle's fuel
efficiency (\texttt{"mpg"}) as a function of its horsepower
(\texttt{"hp"}). Glancing at the plot below, we see that the
relationship between \texttt{"mpg"} and \texttt{"hp"} is non-linear --
an SLR fit doesn't capture the relationship between the two variables.

Recall our standard multiple linear regression model. In its current
form, it is linear in terms of both \(\theta_i\) and \(x\):

\[\hat{y} = \theta_0 + \theta_1 x + \theta_2 x\:+\:...\]

Just by eyeballing the \texttt{vehicle} data plotted above, it seems
that a \emph{quadratic} model might be more appropriate. In other words,
a model of the form below would likely do a better job of capturing the
non-linear relationship between the two variables:

\[\hat{y} = \theta_0 + \theta_1 x + \theta_2 x^2\]

This looks fairly similar to our original multiple regression framework!
Importantly, it is \textbf{still linear in \(\theta_i\)} -- the
prediction \(\hat{y}\) is a linear combination of the model parameters.
This means that we can use the same linear algebra methods as before to
derive the optimal model parameters when fitting the model.

You may be wondering: how can this be a linear model if there is now a
\(x^2\) term? Although the model contains non-linear \(x\) terms, it is
linear with respect to the \emph{model parameters}, \(\theta_i\).
Because our OLS derivation relied on assuming a linear model of
\(\theta_i\), the method is still valid to fit this new model.

If we refit the model with \texttt{"hp"} squared as its own feature, we
see that the model follows the data much more closely.

\[\hat{\text{mpg}} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp})^2\]

Looks much better! What we've done here is called \textbf{feature
engineering}: the process of transforming the raw features of a dataset
into more informative features for modeling. By squaring the
\texttt{"hp"} feature, we were able to create a new feature that
signficantly improved the quality of our model.

We perform feature engineering by defining a \textbf{feature function}.
A feature function is some function applied to the original variables in
the data to generate one or more new features. More formally, a feature
function is said to take a \(d\) dimensional input and transform it to a
\(p\) dimensional input. This results in a new, feature-engineered
design matrix that we rename \(\Phi\).

\[\mathbb{X} \in \mathbb{R}^{n \times d} \longrightarrow \Phi \in \mathbb{R}^{n \times p}\]

In the \texttt{vehicles} example above, we applied a feature function to
transform the original input with \(d=1\) features into an engineered
design matrix with \(p=2\) features.

\hypertarget{one-hot-encoding}{%
\section{One Hot Encoding}\label{one-hot-encoding}}

Feature engineering opens up a whole new set of possibilities for
designing better performing models. As you will see in lab and homework,
feature engineering is one of the most important parts of the entire
modeling process.

A particularly powerful use of feature engineering is to allow us to
perform regression on non-numeric features. \textbf{One hot encoding} is
a feature engineering technique that generates numeric features from
categorical data, allowing us to use our usual methods to fit a
regression model on the data.

To illustrate how this works, we'll refer back to the \texttt{tips} data
from last lecture. Consider the \texttt{"day"} column of the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}
\NormalTok{tips\_df }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"tips"}\NormalTok{).sample(}\DecValTok{100}\NormalTok{)}
\NormalTok{tips\_df[[}\StringTok{"day"}\NormalTok{]].head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{ll}
\toprule
{} &   day \\
\midrule
54  &   Sun \\
46  &   Sun \\
86  &  Thur \\
199 &  Thur \\
106 &   Sat \\
\bottomrule
\end{tabular}

At first glance, it doesn't seem possible to fit a regression model to
this data -- we can't directly perform any mathematical operations on
the entry ``Thur''.

To resolve this, we instead create a new table with a feature for each
unique value in the original \texttt{"day"} column. We then iterate
through the \texttt{"day"} column. For each entry in \texttt{"day"} we
fill the corresponding feature in the new table with 1. All other
features are set to 0.

This can be implemented in code using \texttt{sklearn}'s
\texttt{OneHotEncoder()} to generate the one hot encoding, then calling
\texttt{pd.concat} to combine these new features with the original
DataFrame.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OneHotEncoder}

\CommentTok{\# Perform the one hot encoding}
\NormalTok{oh\_enc }\OperatorTok{=}\NormalTok{ OneHotEncoder()}
\NormalTok{oh\_enc.fit(tips\_df[[}\StringTok{\textquotesingle{}day\textquotesingle{}}\NormalTok{]])}
\NormalTok{ohe\_data }\OperatorTok{=}\NormalTok{ oh\_enc.transform(tips\_df[[}\StringTok{\textquotesingle{}day\textquotesingle{}}\NormalTok{]]).toarray()}


\CommentTok{\# Combine with original features}
\NormalTok{data\_w\_ohe }\OperatorTok{=}\NormalTok{ (tips\_df}
\NormalTok{              .join(}
\NormalTok{                  pd.DataFrame(ohe\_data, columns}\OperatorTok{=}\NormalTok{oh\_enc.get\_feature\_names(), index}\OperatorTok{=}\NormalTok{tips\_df.index))}
\NormalTok{             )}
\NormalTok{data\_w\_ohe}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrllllrrrrr}
\toprule
{} &  total\_bill &   tip &     sex & smoker &   day &    time &  size &  x0\_Fri &  x0\_Sat &  x0\_Sun &  x0\_Thur \\
\midrule
54  &       25.56 &  4.34 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
46  &       22.23 &  5.00 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
86  &       13.03 &  2.00 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
199 &       13.51 &  2.00 &    Male &    Yes &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
106 &       20.49 &  4.06 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
87  &       18.28 &  4.00 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
110 &       14.00 &  3.00 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
166 &       20.76 &  2.24 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
237 &       32.83 &  1.17 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
103 &       22.42 &  3.48 &  Female &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
185 &       20.69 &  5.00 &    Male &     No &   Sun &  Dinner &     5 &     0.0 &     0.0 &     1.0 &      0.0 \\
69  &       15.01 &  2.09 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
163 &       13.81 &  2.00 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
157 &       25.00 &  3.75 &  Female &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
187 &       30.46 &  2.00 &    Male &    Yes &   Sun &  Dinner &     5 &     0.0 &     0.0 &     1.0 &      0.0 \\
67  &        3.07 &  1.00 &  Female &    Yes &   Sat &  Dinner &     1 &     0.0 &     1.0 &     0.0 &      0.0 \\
211 &       25.89 &  5.16 &    Male &    Yes &   Sat &  Dinner &     4 &     0.0 &     1.0 &     0.0 &      0.0 \\
238 &       35.83 &  4.67 &  Female &     No &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
126 &        8.52 &  1.48 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
68  &       20.23 &  2.01 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
121 &       13.42 &  1.68 &  Female &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
56  &       38.01 &  3.00 &    Male &    Yes &   Sat &  Dinner &     4 &     0.0 &     1.0 &     0.0 &      0.0 \\
119 &       24.08 &  2.92 &  Female &     No &  Thur &   Lunch &     4 &     0.0 &     0.0 &     0.0 &      1.0 \\
180 &       34.65 &  3.68 &    Male &    Yes &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
74  &       14.73 &  2.20 &  Female &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
85  &       34.83 &  5.17 &  Female &     No &  Thur &   Lunch &     4 &     0.0 &     0.0 &     0.0 &      1.0 \\
101 &       15.38 &  3.00 &  Female &    Yes &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
53  &        9.94 &  1.56 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
159 &       16.49 &  2.00 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
41  &       17.46 &  2.54 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
178 &        9.60 &  4.00 &  Female &    Yes &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
11  &       35.26 &  5.00 &  Female &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
35  &       24.06 &  3.60 &    Male &     No &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
175 &       32.90 &  3.11 &    Male &    Yes &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
173 &       31.85 &  3.18 &    Male &    Yes &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
25  &       17.81 &  2.34 &    Male &     No &   Sat &  Dinner &     4 &     0.0 &     1.0 &     0.0 &      0.0 \\
206 &       26.59 &  3.41 &    Male &    Yes &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
98  &       21.01 &  3.00 &    Male &    Yes &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
89  &       21.16 &  3.00 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
36  &       16.31 &  2.00 &    Male &     No &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
94  &       22.75 &  3.25 &  Female &     No &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
63  &       18.29 &  3.76 &    Male &    Yes &   Sat &  Dinner &     4 &     0.0 &     1.0 &     0.0 &      0.0 \\
28  &       21.70 &  4.30 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
24  &       19.82 &  3.18 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
124 &       12.48 &  2.52 &  Female &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
168 &       10.59 &  1.61 &  Female &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
107 &       25.21 &  4.29 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
116 &       29.93 &  5.07 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
160 &       21.50 &  3.50 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
43  &        9.68 &  1.32 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
42  &       13.94 &  3.06 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
30  &        9.55 &  1.45 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
208 &       24.27 &  2.03 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
226 &       10.09 &  2.00 &  Female &    Yes &   Fri &   Lunch &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
48  &       28.55 &  2.05 &    Male &     No &   Sun &  Dinner &     3 &     0.0 &     0.0 &     1.0 &      0.0 \\
213 &       13.27 &  2.50 &  Female &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
96  &       27.28 &  4.00 &    Male &    Yes &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
55  &       19.49 &  3.51 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
97  &       12.03 &  1.50 &    Male &    Yes &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
99  &       12.46 &  1.50 &    Male &     No &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
242 &       17.82 &  1.75 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
15  &       21.58 &  3.92 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
117 &       10.65 &  1.50 &  Female &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
115 &       17.31 &  3.50 &  Female &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
135 &        8.51 &  1.25 &  Female &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
75  &       10.51 &  1.25 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
32  &       15.06 &  3.00 &  Female &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
225 &       16.27 &  2.50 &  Female &    Yes &   Fri &   Lunch &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
14  &       14.83 &  3.02 &  Female &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
154 &       19.77 &  2.00 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
214 &       28.17 &  6.50 &  Female &    Yes &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
52  &       34.81 &  5.20 &  Female &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
224 &       13.42 &  1.58 &    Male &    Yes &   Fri &   Lunch &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
228 &       13.28 &  2.72 &    Male &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
150 &       14.07 &  2.50 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
184 &       40.55 &  3.00 &    Male &    Yes &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
21  &       20.29 &  2.75 &  Female &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
39  &       31.27 &  5.00 &    Male &     No &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
113 &       23.95 &  2.55 &    Male &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
146 &       18.64 &  1.36 &  Female &     No &  Thur &   Lunch &     3 &     0.0 &     0.0 &     0.0 &      1.0 \\
123 &       15.95 &  2.00 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
120 &       11.69 &  2.31 &    Male &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
191 &       19.81 &  4.19 &  Female &    Yes &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
19  &       20.65 &  3.35 &    Male &     No &   Sat &  Dinner &     3 &     0.0 &     1.0 &     0.0 &      0.0 \\
29  &       19.65 &  3.00 &  Female &     No &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
204 &       20.53 &  4.00 &    Male &    Yes &  Thur &   Lunch &     4 &     0.0 &     0.0 &     0.0 &      1.0 \\
158 &       13.39 &  2.61 &  Female &     No &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
58  &       11.24 &  1.76 &    Male &    Yes &   Sat &  Dinner &     2 &     0.0 &     1.0 &     0.0 &      0.0 \\
80  &       19.44 &  3.00 &    Male &    Yes &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
13  &       18.43 &  3.00 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
1   &       10.34 &  1.66 &    Male &     No &   Sun &  Dinner &     3 &     0.0 &     0.0 &     1.0 &      0.0 \\
125 &       29.80 &  4.20 &  Female &     No &  Thur &   Lunch &     6 &     0.0 &     0.0 &     0.0 &      1.0 \\
181 &       23.33 &  5.65 &    Male &    Yes &   Sun &  Dinner &     2 &     0.0 &     0.0 &     1.0 &      0.0 \\
2   &       21.01 &  3.50 &    Male &     No &   Sun &  Dinner &     3 &     0.0 &     0.0 &     1.0 &      0.0 \\
222 &        8.58 &  1.92 &    Male &    Yes &   Fri &   Lunch &     1 &     1.0 &     0.0 &     0.0 &      0.0 \\
44  &       30.40 &  5.60 &    Male &     No &   Sun &  Dinner &     4 &     0.0 &     0.0 &     1.0 &      0.0 \\
221 &       13.42 &  3.48 &  Female &    Yes &   Fri &   Lunch &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
59  &       48.27 &  6.73 &    Male &     No &   Sat &  Dinner &     4 &     0.0 &     1.0 &     0.0 &      0.0 \\
100 &       11.35 &  2.50 &  Female &    Yes &   Fri &  Dinner &     2 &     1.0 &     0.0 &     0.0 &      0.0 \\
127 &       14.52 &  2.00 &  Female &     No &  Thur &   Lunch &     2 &     0.0 &     0.0 &     0.0 &      1.0 \\
\bottomrule
\end{tabular}

Now, the ``day'' feature (or rather, the four new boolean features that
represent day) can be used to fit a model.

\hypertarget{higher-order-polynomial-example}{%
\section{Higher-order Polynomial
Example}\label{higher-order-polynomial-example}}

Let's return to where we started today: Creating higher-order polynomial
features for the mpg dataset.

What happens if we add a feature corresponding to the horsepower,
\emph{cubed}? or to the fourth power? the fifth power?

\begin{itemize}
\tightlist
\item
  Will we get better results?
\item
  What will the model look like?
\end{itemize}

Let's try it out. The below code plots polynomial models fit to the mpg
dataset, from order 0 (the constant model) to order 5 (polynomial
features through horsepower to the fifth power).

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.pipeline }\ImportTok{import}\NormalTok{ Pipeline}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ PolynomialFeatures}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}


\NormalTok{vehicle\_data }\OperatorTok{=}\NormalTok{ sns.load\_dataset(}\StringTok{"mpg"}\NormalTok{)}
\NormalTok{vehicle\_data }\OperatorTok{=}\NormalTok{ vehicle\_data.rename(columns }\OperatorTok{=}\NormalTok{ \{}\StringTok{"horsepower"}\NormalTok{: }\StringTok{"hp"}\NormalTok{\})}
\NormalTok{vehicle\_data }\OperatorTok{=}\NormalTok{ vehicle\_data.dropna()}


\KeywordTok{def}\NormalTok{ get\_MSE\_for\_degree\_k\_model(k):}
\NormalTok{    pipelined\_model }\OperatorTok{=}\NormalTok{ Pipeline([}
\NormalTok{        (}\StringTok{\textquotesingle{}poly\_transform\textquotesingle{}}\NormalTok{, PolynomialFeatures(degree }\OperatorTok{=}\NormalTok{ k)),}
\NormalTok{        (}\StringTok{\textquotesingle{}regression\textquotesingle{}}\NormalTok{, LinearRegression(fit\_intercept }\OperatorTok{=} \VariableTok{True}\NormalTok{))    }
\NormalTok{    ])}
\NormalTok{    pipelined\_model.fit(vehicle\_data[[}\StringTok{"hp"}\NormalTok{]], vehicle\_data[}\StringTok{"mpg"}\NormalTok{])}
    \ControlFlowTok{return}\NormalTok{ mean\_squared\_error(pipelined\_model.predict(vehicle\_data[[}\StringTok{"hp"}\NormalTok{]]), vehicle\_data[}\StringTok{"mpg"}\NormalTok{])}

\NormalTok{ks }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{))}
\NormalTok{MSEs }\OperatorTok{=}\NormalTok{ [get\_MSE\_for\_degree\_k\_model(k) }\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ ks]}
\NormalTok{MSEs\_and\_k }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}\StringTok{"k"}\NormalTok{: ks, }\StringTok{"MSE"}\NormalTok{: MSEs\})}
\NormalTok{MSEs\_and\_k.set\_index(}\StringTok{"k"}\NormalTok{)}

\KeywordTok{def}\NormalTok{ plot\_degree\_k\_model(k, MSEs\_and\_k, axs):}
\NormalTok{    pipelined\_model }\OperatorTok{=}\NormalTok{ Pipeline([}
\NormalTok{        (}\StringTok{\textquotesingle{}poly\_transform\textquotesingle{}}\NormalTok{, PolynomialFeatures(degree }\OperatorTok{=}\NormalTok{ k)),}
\NormalTok{        (}\StringTok{\textquotesingle{}regression\textquotesingle{}}\NormalTok{, LinearRegression(fit\_intercept }\OperatorTok{=} \VariableTok{True}\NormalTok{))    }
\NormalTok{    ])}
\NormalTok{    pipelined\_model.fit(vehicle\_data[[}\StringTok{"hp"}\NormalTok{]], vehicle\_data[}\StringTok{"mpg"}\NormalTok{])}
    
\NormalTok{    row }\OperatorTok{=}\NormalTok{ k }\OperatorTok{//} \DecValTok{3}
\NormalTok{    col }\OperatorTok{=}\NormalTok{ k }\OperatorTok{\%} \DecValTok{3}
\NormalTok{    ax }\OperatorTok{=}\NormalTok{ axs[row, col]}
    
\NormalTok{    sns.scatterplot(data}\OperatorTok{=}\NormalTok{vehicle\_data, x}\OperatorTok{=}\StringTok{\textquotesingle{}hp\textquotesingle{}}\NormalTok{, y}\OperatorTok{=}\StringTok{\textquotesingle{}mpg\textquotesingle{}}\NormalTok{, ax}\OperatorTok{=}\NormalTok{ax)}
    
\NormalTok{    x\_range }\OperatorTok{=}\NormalTok{ np.linspace(}\DecValTok{45}\NormalTok{, }\DecValTok{210}\NormalTok{, }\DecValTok{100}\NormalTok{).reshape(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ax.plot(x\_range, pipelined\_model.predict(pd.DataFrame(x\_range, columns}\OperatorTok{=}\NormalTok{[}\StringTok{\textquotesingle{}hp\textquotesingle{}}\NormalTok{])), c}\OperatorTok{=}\StringTok{\textquotesingle{}orange\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
    
\NormalTok{    ax.set\_ylim((}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{))}
\NormalTok{    mse\_str }\OperatorTok{=} \SpecialStringTok{f"MSE: }\SpecialCharTok{\{}\NormalTok{MSEs\_and\_k}\SpecialCharTok{.}\NormalTok{loc[k, }\StringTok{\textquotesingle{}MSE\textquotesingle{}}\NormalTok{]}\SpecialCharTok{:.4\}}\CharTok{\textbackslash{}n}\SpecialStringTok{order: }\SpecialCharTok{\{}\NormalTok{k}\SpecialCharTok{\}}\SpecialStringTok{"}
\NormalTok{    ax.text(}\DecValTok{150}\NormalTok{, }\DecValTok{30}\NormalTok{, mse\_str, }\BuiltInTok{dict}\NormalTok{(size}\OperatorTok{=}\DecValTok{13}\NormalTok{))}

\NormalTok{fig }\OperatorTok{=}\NormalTok{ plt.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\NormalTok{axs }\OperatorTok{=}\NormalTok{ fig.subplots(nrows}\OperatorTok{=}\DecValTok{2}\NormalTok{, ncols}\OperatorTok{=}\DecValTok{3}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{6}\NormalTok{):}
\NormalTok{    plot\_degree\_k\_model(k, MSEs\_and\_k, axs)}
\NormalTok{fig.tight\_layout()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{feature_engineering/feature_engineering_files/figure-pdf/cell-5-output-1.png}

}

\end{figure}

With constant and linear models, there seems to be a clear pattern in
prediction error. With a quadratic model (order 2), the plot seems to
match our data much more consistently across different \texttt{hp}
values. For higher order polynomials, we observe a small improvement in
MSE, but not much beyond 18.98. The MSE will continue to marginally
decrease as we add more and more terms to our model. However, there
ain't no free lunch. This decreasing of the MSE is coming at a major
cost!

\hypertarget{variance-and-training-error}{%
\section{Variance and Training
Error}\label{variance-and-training-error}}

We've seen now that feature engineering allows us to build all sorts of
features to improve the performance of the model. In particular, we saw
that designing a more complex feature (squaring \texttt{"hp"} in the
\texttt{vehicles} data previously) substantially improved the model's
ability to capture non-linear relationships. To take full advantage of
this, we might be inclined to design increasingly complex features.
Consider the following three models, each of different order (the
maximum exponent power of each model):

\begin{itemize}
\tightlist
\item
  Model with order 1:
  \(\hat{\text{mpg}} = \theta_0 + \theta_1 (\text{hp})\)
\item
  Model with order 2:
  \(\hat{\text{mpg}} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp})^2\)
\item
  Model with order 4:
  \(\hat{\text{mpg}} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp})^2 + \theta_3 (\text{hp})^3 + \theta_4 (\text{hp})^4\)
\end{itemize}

When we use our model to make predictions on the same data that was used
to fit the model, we find that the MSE decreases with increasingly
complex models. The \textbf{training error} is the model's error when
generating predictions from the same data that was used for training
purposes. We can conclude that the training error goes down as the
complexity of the model increases.

This seems like good news -- when working on the \textbf{training data},
we can improve model performance by designing increasingly complex
models.

However, high model complexity comes with its own set of issues. When a
model has many complicated features, it becomes increasingly sensitive
to the data used to fit it. Even a small variation in the data points
used to train the model may result in wildly different results for the
fitted model. The plots below illustrate this idea. In each case, we've
fit a model to two very similar sets of data (in fact, they only differ
by two data points!). Notice that the model with order 2 appears roughly
the same across the two sets of data; in contrast, the model with order
4 changes erratically across the two datasets.

The sensitivity of the model to the data used to train it is called the
\textbf{model variance}. As we saw above, model variance tends to
increase with model complexity.

We will further explore this tradeoff (and more precisely define model
variance) in future lectures.

\hypertarget{overfitting}{%
\section{Overfitting}\label{overfitting}}

We can see that there is a clear ``trade-off'' that comes from the
complexity of our model. As model complexity increases, the model's
error on the training data decreases. At the same time, the model's
variance tends to increase.

Why does this matter? To answer this question, let's take a moment to
review our modeling workflow when making predictions on new data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample a dataset of training data from the real world
\item
  Use this training data to fit a model
\item
  Apply this fitted model to generate predictions on unseen data
\end{enumerate}

This first step -- sampling training data -- is important to remember in
our analysis. As we saw above, a highly complex model may produce
results that vary wildly across different samples of training data. If
we happen to sample a set of training data that is a poor representation
of the population we are trying to model, our model may perform poorly
on any new set of data it has not seen before.

To see why, consider a model fit using the training data shown on the
left. Because the model is so complex, it achieves zero error on the
training set -- it perfectly predicts each value in the training data!
When we go to use this model to make predictions on a new sample of
data, however, things aren't so good. The model now has enormous error
on the unseen data.

The phenomenon above is called \textbf{overfitting}. The model
effectively just memorized the training data it encountered when it was
fitted, leaving it unable to handle new situations.

The takeaway here: we need to strike a balance in the complexity of our
models. A model that is too simple won't be able to capture the key
relationships between our variables of interest; a model that is too
complex runs the risk of overfitting.

This begs the question: how do we control the complexity of a model?
Stay tuned for the next lecture.

\bookmarksetup{startatroot}

\hypertarget{cross-validation-and-regularization}{%
\chapter{Cross Validation and
Regularization}\label{cross-validation-and-regularization}}

Consider the question: how do we control the complexity of our model? To
answer this, we must know precisely when our model begins to overfit.
The key to this lies in evaluating the model on unseen data using a
process called Cross-Validation. A second point this note will address
is how to combat overfitting -- namely, through a technique known as
regularization.

From the last lecture, we learned that \emph{increasing} model
complexity \emph{decreased} our model's training error but increased
variance. This makes intuitive sense; adding more features causes our
model to better fit the given data, but generalize worse to new data.
For this reason, a low training error is not representative of our
model's underlying performance -- this may be a side effect of
overfitting.

Truly, the only way to know when our model overfits is by evaluating it
on unseen data. Unfortunately, that means we need to wait for more data.
This may be very expensive and time consuming.

How should we proceed? In this section, we will build up a viable
solution to this problem.

\hypertarget{the-holdout-method}{%
\subsection{The Holdout Method}\label{the-holdout-method}}

The simplest approach to avoid overfitting is to keep some of our data
secret from ourselves. This is known as the \textbf{holdout method}. We
will train our models on \emph{most} of the available data points --
known as the \textbf{training data} . We'll then evaluate the models'
performance on the unseen data points (called the \textbf{validation
set}) to measure overfitting.

Imagine the following example where we wish to train a model on 35 data
points. We choose the training set to be a random sample of 25 of these
points, and the validation set to be the remaining 10 points. Using this
data, we train 7 models, each with a higher polynomial degree than the
last.

We get the following mean squared error (MSE) on the training data.

Using these same models, we compute the MSE on our 10 validation points
and observe the following.

Notice how the training error monotonically \emph{decreases} as
polynomial degree \emph{increases}. This is consistent with our
knowledge. However, validation error first \emph{decreases}, then
\emph{increases} (from k = 3). These higher degree models are performing
worse on unseen data -- this indicates they are overfitting. As such,
the best choice of polynomial degree in this example is \textbf{k = 2}.

More generally, we can represent this relationship with the following
diagram.

Our goal is to train a model with complexity near the red line. Note
that this relationship is a simplification of the real-world. But for
purposes of Data 100, this is good enough.

\hypertarget{hyperparameters}{%
\subsubsection{Hyperparameters}\label{hyperparameters}}

In machine learning, a \textbf{hyperparameter} is a value that controls
the learning process. In our example, we built seven models, each of
which had a hyperparameter \texttt{k} that controlled the polynomial
degree of the model.

To choose between hyperparameters, we use the validation set. This was
evident in our example above; we found that \texttt{k} = 2 had the
lowest validation error. However, this holdout method is a bit naive.
Imagine our random sample of 10 validation points coincidentally favored
a higher degree polynomial. This would have led us to incorrectly favor
a more complex model. In other words, our small amount of validation
data may be different from real-world data.

To minimize this possiblity, we need to evaluate our model on more data.
Decreasing the size of the training set is not an option -- doing so
will worsen our model. How should we proceed?

\hypertarget{k-fold-cross-validation}{%
\subsection{K-Fold Cross Validation}\label{k-fold-cross-validation}}

In the holdout method, we train a model on \emph{only} the training set,
and assess the quality \emph{only} on the validation set. On the other
hand, \textbf{K-Fold Cross Validation} is a technique that determines
the quality of a hyperparameter by evaluating a model-hyperparameter
combination on \texttt{k} independent ``folds'' of data, which together
make up the \emph{entire} dataset. This is a more robust alternative to
the holdout method. Let's break down each step.

\textbf{Note}: The \texttt{k} in k-fold cross validation is different
from the \texttt{k} polynomial degree discussed in the earlier example.

In the k-fold cross-validation approach, we split our data into
\texttt{k} equally sized groups (often called folds). In the example
where \texttt{k} = 5:

\includegraphics{cv_regularization/images/cvfolds.png}

To determine the ``quality'' of a particular hyperparameter:

\begin{itemize}
\tightlist
\item
  Pick a fold, which we'll call the validation fold. Train a model on
  all other \texttt{k-1} folds. Compute an error on the validation fold.
\item
  Repeat the step above for all \texttt{k} possible choices of
  validation fold, each time training a new model.
\item
  Average the \texttt{k} validation fold errors. This will give a single
  error for the hyperparameter.
\end{itemize}

For \texttt{k} = 5, we have the following partitions. At each iteration,
we train a model on the blue data and validate on the orange data. This
gives us a total of 5 errors which we average to obtain a single
representative error for the hyperparameter.

\includegraphics{cv_regularization/images/kfold.png}

Note that the value of the hyperparameter is fixed during this process.
By doing so, we can be confident in the hyperparameter's performance on
the \emph{entire} dataset. To compare multiple choices of a
hyperparameter --say \texttt{m} choices of hyperparameter-- we run
k-fold cross validation \texttt{m} times. The smallest of the \texttt{m}
resulting errors corresponds to the best hyperparameter value.

\hypertarget{hyperparameter-selection-example}{%
\subsubsection{Hyperparameter Selection
Example}\label{hyperparameter-selection-example}}

K-fold cross validation can aid in choosing the best hyperparameter
values in respect to our model and loss function.

Consider an example where we run k-fold cross validation with \texttt{k}
= 3. We are implementing a model that depends on a hyperparameter
\(\alpha\), and we are searching for an \(\alpha\) that minimizes our
loss function. We have narrowed down our hyperparameters such that
\(\alpha = [0.01, 0.1, 1, 10]\).

\includegraphics{cv_regularization/images/cv_hyperparam_selection.png}

The losses of the model are shown per \texttt{k} fold of training data
(arrows are pointing to the loss value) for each value of \(\alpha\).
The average loss over the k-fold cross validation is displayed to the
right of all the k-fold losses.

To determine the best \(\alpha\) value, we must compare the average loss
over the k-folds of training data. \(\alpha = 0.01\) yields us an
average loss of \(5\), \(\alpha = 0.1\) yields us an average loss of
\(4.67\), \(\alpha = 1\) yields us an average loss of \(7\), and
\(\alpha = 10\) yields us an average loss of \(10.67\).

Thus, we would select \(\alpha = 0.1\) as our hyperparameter value as it
results in the lowest average loss over the k-folds of training data out
of our possible \(\alpha\) values.

\hypertarget{picking-k}{%
\subsubsection{Picking K}\label{picking-k}}

Typical choices of \texttt{k} are 5, 10, and N, where N is the number of
data points.

\texttt{k} = N is known as ``leave one out cross validation'', and will
typically give you the best results.

\begin{itemize}
\tightlist
\item
  In this approach, each validation set is only one point.
\item
  Every point gets a chance to get used as the validation set.
\end{itemize}

However, \texttt{k} = N is very expensive and requires you to fit a
large number of models.

\hypertarget{test-sets}{%
\subsection{Test Sets}\label{test-sets}}

Suppose we're researchers building a state of the art regression model.
We choose a model with the lowest validation error and want to report
this out to the world. Unfortunately, our validation error may be
biased; that is, not representative of error on real-world data. This is
because during our hyperparameter search, we were implicitly
``learning'' from our validation data by tuning our model to achieve
better results. Before reporting our results, we should run our model on
a special \textbf{test set} that we've never seen or used for any
purpose whatsoever.

A test set can be something that we generate ourselves, or it can be a
common dataset whose true values are unknown. In the case of the former,
we can split our our available data into 3 partitions: the training set,
testing set, and validation set. The exact amount of data allocated to
each partition varies, but a common split is 80\% train, 10\% test, 10\%
validation.

Below is an implementation of extracting a training, testing and
validation set using \texttt{sklearn.train\_test\_split} on a data
matrix \texttt{X} and an array of observations \texttt{y}.

\texttt{X\_train\_valid,\ X\_test,\ y\_train\_valid,\ y\_test\ =\ train\_test\_split(X,\ y,\ test\_size=0.1)}

\texttt{X\_train,\ X\_val,\ y\_train,\ y\_val\ =\ train\_test\_split(X\_train\_valid,\ y\_train\_valid,\ test\_size=0.11)}

As a recap:

\begin{itemize}
\tightlist
\item
  Training set used to pick parameters.
\item
  Validation set used to pick hyperparameters (or pick between different
  models).
\item
  Test set used to provide an unbiased error at the end.
\end{itemize}

Here is an idealized relationship between training error, test error,
and validation error.

Notice how the test error behaves similarily to the validation error.
Both come from data that is unseen during the model training process, so
both are fairly good estimates of real-world data. Of the two, the test
error is more unbiased for the reasons mentioned above.

As before, the optimal complexity level exists where validation error is
minimized. Logically, we can't design a model that minimizes test error
because we don't use the test set until final evaluation.

\hypertarget{regularization}{%
\section{Regularization}\label{regularization}}

Earlier, we found an optimal model complexity by choosing the
hyperparameter that minimized validation error. This was the polynomial
degree \texttt{k}= 2. Tweaking the ``complexity'' was simple; it was
only a matter of adjusting the polynomial degree.

However, in most machine learning problems, complexity is defined
differently. Today, we'll explore two different definitions of
complexity: the \emph{squared} and \emph{absolute} magnitude of
\(\theta_i\) coefficients.

\hypertarget{constraining-gradient-descent}{%
\subsubsection{Constraining Gradient
Descent}\label{constraining-gradient-descent}}

Before we discuss these definitions, let's first familiarize ourselves
with the concept of \textbf{constrained gradient descent}. Imagine we
have a two feature model with coeffiecient weights of \(\theta_1\) and
\(\theta_2\). Below we've plotted a two dimensional contour plot of the
OLS loss surface -- darker areas indicate regions of lower loss.
Gradient descent will find the optimal paramaters during training
\((\theta_1, \theta_2) = \hat\theta_{No Reg.}\).

Suppose we arbitrarily decide that gradient descent can never land
outside of the green ball.

Gradient descent finds a new solution at \(\hat\theta_{Reg.}\). This is
far from the global optimal solution \(\hat\theta_{No Reg.}\) --
however, it is the closest point that lives in the green ball. In other
words, \(\hat\theta_{Reg.}\) is the \textbf{constrained optimal
solution}.

The size of this ball is completely arbitrary. Increasing its size
allows for a constrained solution closer to the global optimum, and vice
versa.

In fact, the size of this ball is inherently linked to model complexity.
A smaller ball constrains (\(\theta_1\), \(\theta_2\)) more than a
larger ball. This is synonymous with the behavior of a \emph{less}
complex model, which finds a solution farther from the optimum. A
\emph{larger} ball, on the other hand, is synonymous to a \emph{more}
complex model that can achieve a near optimal solution.

Consider the extreme case where the radius is infinitely small. The
solution to every constrained modeling problem would lie on the origin,
at \((\theta_1, \theta_2) = (0, 0)\). This is equivalent to the constant
model we studied -- the least complex of all models. In the case where
the radius is infinitely large, the optimal constrained solution exists
at \(\hat\theta_{No Reg.}\) itself! This is the solution obtained from
OLS with no limitations on complexity.

The intercept coefficient is typically \emph{not} constrained;
\(\theta_0\) can be any value. This way, if all \(\theta_i = 0\) except
\(\theta_0\), the resulting model is a constant model (and \(\theta_0\)
is the mean of all observations).

\hypertarget{l2-regularization}{%
\subsection{L2 Regularization}\label{l2-regularization}}

\hypertarget{the-constrained-form}{%
\subsubsection{The Constrained Form}\label{the-constrained-form}}

\textbf{Regularization} is the formal term that describes the process of
limiting a model's complexity. This is done by constraining the solution
of a cost function, much like how we constrained the set of permissible
(\(\theta_1\), \(\theta_2\)) above. \textbf{L2 Regularization}, commonly
referred to as \textbf{Ridge Regression}, is the technique of
constraining our model's parameters to lie within a \emph{ball} around
the origin. Formally, it is defined as

\[\min_{\theta} \frac{1}{n} || Y - X\theta ||\]

such that \(\sum_{j=1}^{d} \theta_j^{2} \le Q\)

The mathematical definition of complexity in Ridge Regression is
\(\sum_{j=1}^{d} \theta_j^{2} \le Q\). This formulation of complexity
limits the total squared magnitude of the coefficients to some constant
\(Q\). In two dimensional space, this is
\(\theta_{1}^{2} + \theta_{2}^{2} \le Q\). You'll recognize this as the
equation of a circle with axes \(\theta_{1}, \theta_{2}\) and radius
\(Q\). In higher dimensions, this circle becomes a hypersphere and is
conventionally referred to as the \textbf{L2 norm ball}. Decreasing
\(Q\) shrinks the norm ball, and limits the complexity of the model
(discussed in the previous section). Likewise, expanding the norm ball
increases the allowable model complexity.

Without the constraint \(\sum_{j=1}^{d} \theta_j^{2} \le Q\), the
optimal solution is \(\hat{\theta} = \hat\theta_{No Reg.}\). With an
appropriate value of \(Q\) applied to the constraint, the solution
\(\hat{\theta} = \hat\theta_{Reg.}\) is sub-optimal on the training data
but generalizes better to new data.

\hypertarget{the-functional-form}{%
\subsubsection{The Functional Form}\label{the-functional-form}}

Unfortunately, the function above requires some work. It's not easy to
mathematically optimize over a constraint. Instead, in most machine
learning text, you'll see a different formulation of Ridge Regression.

\[\min_{\theta} \frac{1}{n} || Y - X\theta || + \alpha \sum_{j=1}^{d} \theta_j^{2}\]

These two equations are equivalent by Lagrangian Duality (not in scope).

Notice that we've replaced the constraint with a second term in our cost
function. We're now minimizing a function with a regularization term
that penalizes large coefficients. The \(\alpha\) factor controls the
degree of regularization. In fact, \(\alpha \approx \frac{1}{Q}\). To
understand why, let's consider these 2 extreme examples:

\begin{itemize}
\item
  Assume \(\alpha \rightarrow \infty\). Then,
  \(\alpha \sum_{j=1}^{d} \theta_j^{2}\) dominates the cost function. To
  minimize this term, we set \(\theta_j = 0\) for all \(j \ge 1\). This
  is a very constrained model that is mathematically equivalent to the
  constant model. Earlier, we explained the constant model also arises
  when the L2 norm ball radius \(Q \rightarrow 0\).
\item
  Assume \(\alpha \rightarrow 0\). Then,
  \(\alpha \sum_{j=1}^{d} \theta_j^{2}\) is infinitely small. Minimizing
  the cost function is equivalent to
  \(\min_{\theta} \frac{1}{n} || Y - X\theta ||\). This is just OLS, and
  the optimal solution is the global minimum
  \(\hat{\theta} = \hat\theta_{No Reg.}\). We showed that the global
  optimum is achieved when the L2 norm ball radius
  \(Q \rightarrow \infty\).
\end{itemize}

\hypertarget{closed-form-solution}{%
\subsubsection{Closed Form Solution}\label{closed-form-solution}}

An additional benefit to Ridge Regression is that it has a closed form
solution.

\[\hat\theta_{ridge} = (X^TX + n\alpha I)^{-1}X^TY\]

This solution exists even if there is linear dependence in the columns
of the data matrix. We will not derive this result in Data 100, as it
involves a fair bit of matrix calculus.

\hypertarget{implementation-of-ridge-regression}{%
\subsubsection{Implementation of Ridge
Regression}\label{implementation-of-ridge-regression}}

Of course, \texttt{sklearn} has a built-in implementation of Ridge
Regression. Simply import the \texttt{Ridge} class of the
\texttt{sklearn.linear\_model} library.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ Ridge}
\end{Highlighting}
\end{Shaded}

We will various Ridge Regression models on the familiar
\texttt{vehicles} DataFrame from last lecture. This will help solidfy
some of the theoretical concepts discussed earlier.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{vehicles }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/vehicle\_data.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{vehicles\_mpg }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/vehicle\_mpg.csv"}\NormalTok{, index\_col}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vehicles.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrrrrrrrr}
\toprule
{} &  cylinders &  displacement &  horsepower &  weight &  acceleration &  cylinders\textasciicircum 2 &  displacement\textasciicircum 2 &  horsepower\textasciicircum 2 &  weight\textasciicircum 2 &  acceleration\textasciicircum 2 \\
\midrule
0 &          8 &         307.0 &       130.0 &    3504 &          12.0 &           64 &         94249.0 &       16900.0 &  12278016 &          144.00 \\
1 &          8 &         350.0 &       165.0 &    3693 &          11.5 &           64 &        122500.0 &       27225.0 &  13638249 &          132.25 \\
2 &          8 &         318.0 &       150.0 &    3436 &          11.0 &           64 &        101124.0 &       22500.0 &  11806096 &          121.00 \\
3 &          8 &         304.0 &       150.0 &    3433 &          12.0 &           64 &         92416.0 &       22500.0 &  11785489 &          144.00 \\
4 &          8 &         302.0 &       140.0 &    3449 &          10.5 &           64 &         91204.0 &       19600.0 &  11895601 &          110.25 \\
\bottomrule
\end{tabular}

Here, we fit an extremeley regularized model without an intercept. Note
the small coefficient values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge\_model\_large\_reg }\OperatorTok{=}\NormalTok{ Ridge(alpha }\OperatorTok{=} \DecValTok{10000}\NormalTok{)}
\NormalTok{ridge\_model\_large\_reg.fit(vehicles, vehicles\_mpg)}
\NormalTok{ridge\_model\_large\_reg.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[ 8.56292915e-04, -5.92399474e-02, -9.81013894e-02,
        -9.66985253e-03, -5.08353226e-03,  1.49576895e-02,
         1.04959034e-04,  1.14786826e-04,  9.07086742e-07,
        -4.60397349e-04]])
\end{verbatim}

Note how Ridge Regression effectively \textbf{spreads a small weight
across many features}.

When we apply very little regularization, our coefficients increase in
size. Notice how they are identical to the coefficients retrieved from
the \texttt{LinearRegression} model. This indicates the radius \(Q\) of
the L2 norm ball is massive and encompasses the unregularized optimal
solution. Once again, we see that \(\alpha\) and \(Q\) are inversely
related.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge\_model\_small\_reg }\OperatorTok{=}\NormalTok{ Ridge(alpha }\OperatorTok{=} \DecValTok{10}\OperatorTok{**{-}}\DecValTok{5}\NormalTok{)}
\NormalTok{ridge\_model\_small\_reg.fit(vehicles, vehicles\_mpg)}
\NormalTok{ridge\_model\_small\_reg.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[-8.06754383e-01, -6.32025048e-02, -2.92851012e-01,
        -3.41032156e-03, -1.43877512e+00,  1.25829303e-01,
         7.72841216e-05,  6.99398090e-04,  3.11031744e-07,
         3.16084838e-02]])
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\NormalTok{linear\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{linear\_model.fit(vehicles, vehicles\_mpg)}
\NormalTok{linear\_model.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([[-8.06756280e-01, -6.32024872e-02, -2.92851021e-01,
        -3.41032211e-03, -1.43877559e+00,  1.25829450e-01,
         7.72840884e-05,  6.99398114e-04,  3.11031832e-07,
         3.16084973e-02]])
\end{verbatim}

\hypertarget{scaling-data-for-regularization}{%
\subsection{Scaling Data for
Regularization}\label{scaling-data-for-regularization}}

One issue with our approach is that our features are on vastly different
scales. For example, \texttt{weight\^{}2} is in the millions, while the
number of \texttt{cylinders} are under 10. Intuitively, the coefficient
value for \texttt{weight\^{}2} must be very small to offset the large
magnitude of the feature. On the other hand, the coefficient of the
\texttt{cylinders} feature is likely quite large in comparison. We see
these claims are true in the \texttt{LinearRegression} model above.

However, a problem arises in Ridge Regression. If we constrain our
coefficients to a small region around the origin, we are unfairly
restricting larger coefficients -- like that of the \texttt{cylinders}
feature. A smaller coefficient -- that of the \texttt{weight\^{}2}
feature -- likely lies within this region, so the value changes very
little. Compare the coefficients of the regularized and unregularized
\texttt{Ridge} models above, and you'll see this is true.

Therefore, it's imperative to standardize your data. We can do so using
z-scores.

\[z_k = \frac{x_k - u_k}{\sigma_k}\]

You'll do this on lab 8 using the ``StandardScaler'' transformer. The
resulting model coefficients will be all on the same scale.

\hypertarget{l1-regularization}{%
\subsection{L1 Regularization}\label{l1-regularization}}

\textbf{L1 Regularization}, commonly referred to as \textbf{Lasso
Regression}, is an alternate regularization technique that limits the
sum of \emph{absolute} \(\theta_i\) coefficients.

\hypertarget{the-constrained-form-1}{%
\subsubsection{The Constrained Form}\label{the-constrained-form-1}}

\[\min_{\theta} \frac{1}{n} || Y - X\theta ||\]

such that \(\sum_{j=1}^{d} |\theta_j| \le Q\)

In two dimensions, our constraint equation is
\(|\theta_1| + |\theta_2| \le Q\). This is the graph of a diamond
centered on the origin with endpoints \(Q\) units away on each axis.

\hypertarget{the-functional-form-1}{%
\subsubsection{The Functional Form}\label{the-functional-form-1}}

A more convenient way to express Lasso Regression is as follows:

\[\min_{\theta} \frac{1}{n} || Y - X\theta || + \alpha \sum_{j=1}^{d} |\theta_j|\]

As with Ridge Regression, the hyperparameter \(\alpha\) has the same
effect on Lasso Regression. That is, \emph{increasing} ~\(\alpha\)
(equivalently, \emph{decreasing} ~\(Q\)) \emph{increases} the amount of
regularization, and vice versa.

Unfortunately, Lasso Regression does not have a closed form solution --
the cost function is not differentiable everywhere. Specifically, the
sum \(\sum_{j=1}^{d} |\theta_j|\) is problematic because it is composed
of absolute value functions, each of which are non-differentiable at the
origin.

So why use Lasso Regression? As we'll see shortly, it is great at
implicit \textbf{feature selection}.

\hypertarget{implementation-of-lasso-regression}{%
\subsubsection{Implementation of Lasso
Regression}\label{implementation-of-lasso-regression}}

Lasso Regression is great at reducing complexity by eliminating the
least important features in a model. It does so by setting their
respective feature weights to \(0\). See the following example.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ Lasso}
\NormalTok{lasso\_model }\OperatorTok{=}\NormalTok{ Lasso(alpha }\OperatorTok{=} \DecValTok{1}\NormalTok{)}

\NormalTok{standardized\_vehicles}\OperatorTok{=}\NormalTok{(vehicles}\OperatorTok{{-}}\NormalTok{vehicles.mean())}\OperatorTok{/}\NormalTok{vehicles.std()}
\NormalTok{lasso\_model.fit(standardized\_vehicles, vehicles\_mpg)}
\NormalTok{lasso\_model.coef\_}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
array([-0.14009981, -0.28452369, -1.14351999, -4.11329618,  0.        ,
       -0.        , -0.        , -0.        , -0.        ,  0.        ])
\end{verbatim}

Notice how we standardized our data first. Lasso Regression then set the
coefficients of our squared features to \(0\) -- presumably, these are
the least important predictors of \texttt{mpg}.

\hypertarget{summary-of-regularization-methods}{%
\subsection{Summary of Regularization
Methods}\label{summary-of-regularization-methods}}

A summary of our regression models is shwon below:

\includegraphics{cv_regularization/images/reg_models_sum.png}

Understanding the distinction between Ridge Regression and Lasso
Regression is important. We've provided a helpful visual that summarizes
the key differences.

\includegraphics{https://people.eecs.berkeley.edu/~jrs/189/ridgelassoItayEvron.gif}

This diagram displays the L1 and L2 constrained solution for various
orientations of the OLS loss surface. Notice how the L1 (Lasso) solution
almost always lies on some axis, or edge of the diamond. Graphically,
this makes sense; the edges of the diamond are the farthest from the
origin, and usually closest to the global optimum. When this happens,
only one feature has a non-zero coefficient; this ``feature selection''
argument extends quite nicely to multiple features in higher dimensional
space.

The L2 (Ridge) solution, however, typically has an optimal solution in
some quadrant of the graph. Every point on the circumference of the L2
norm ball is equidistant from the origin, and thus similar in distance
to the global optimum. As such, this technique of regularization is
great at distributing a small weight across both features.

\bookmarksetup{startatroot}

\hypertarget{probability-i}{%
\chapter{Probability I}\label{probability-i}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Define a random variable in terms of its distribution
\item
  Compute the expectation and variance of a random variable
\item
  Gain familiarity with the Bernoulli and binomial random variables
\end{itemize}

\end{tcolorbox}

In the past few lectures, we've examined the role of complexity in
influencing model performance. We've considered model complexity in the
context of a tradeoff between two competing factors: model variance and
training error.

Thus far, our analysis has been mostly qualitative. We've acknowledged
that our choice of model complexity needs to strike a balance between
model variance and training error, but we haven't yet discussed
\emph{why} exactly this tradeoff exists.

To better understand the origin of this tradeoff, we will need to
introduce the language of \textbf{random variables}. The next two
lectures of probability will be a brief digression from our work on
modeling so we can build up the concepts needed to understand this
so-called \textbf{bias-variance tradeoff}. Our roadmap for the next few
lectures will be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Probability I: introduce random variables, considering the concepts of
  expectation, variance, and covariance
\item
  Probability II: re-express the ideas of model variance and training
  error in terms of random variables and use this new pespective to
  investigate our choice of model complexity
\end{enumerate}

Let's get to it.

\hypertarget{random-variables-and-distributions}{%
\section{Random Variables and
Distributions}\label{random-variables-and-distributions}}

Suppose we generate a set of random data, like a random sample from some
population. A random variable is a numerical function of the randomness
in the data. It is \emph{random} from the randomness of the sample; it
is \emph{variable} because its exact value depends on how this random
sample came out. We typically denote random variables with uppercase
letters, such as \(X\) or \(Y\).

To give a concrete example: say we draw a random sample \(s\) of size 3
from all students enrolled in Data 100. We might then define the random
variable \(X\) to be the number of Data Science majors in this sample.

The \textbf{distribution} of a random variable \(X\) describes how the
total probability of 100\% is split over all possible values that \(X\)
could take. If \(X\) is a \textbf{discrete random variable} with a
finite number of possible values, define its distribution by stating the
probability of \(X\) taking on some specific value, \(x\), for all
possible values of \(x\).

The distribution of a discrete variable can also be represented using a
histogram. If a variable is \textbf{continuous} -- it can take on
infinitely many values -- we can illustrate its distribution using a
density curve.

Often, we will work with multiple random variables at the same time. In
our example above, we could have defined the random variable \(X\) as
the number of Data Science majors in our sample of students, and the
variable \(Y\) as the number of Statistics majors in the sample. For any
two random variables \(X\) and \(Y\):

\begin{itemize}
\tightlist
\item
  \(X\) and \(Y\) are \textbf{equal} if \(X(s) = Y(s)\) for every sample
  \(s\). Regardless of the exact sample drawn, \(X\) is always equal to
  \(Y\).
\item
  \(X\) and \(Y\) are \textbf{identically distributed} if the
  distribution of \(X\) is equal to the distribution of \(Y\). That is,
  \(X\) and \(Y\) take on the same set of possible values, and each of
  these possible values is taken with the same probability. On any
  specific sample \(s\), identically distributed variables do \emph{not}
  necessarily share the same value.
\item
  \(X\) and \(Y\) are \textbf{independent and identically distributed
  (IID)} if 1) the variables are identically distributed and 2) knowing
  the outcome of one variable does not influence our belief of the
  outcome of the other.
\end{itemize}

\hypertarget{expectation-and-variance}{%
\section{Expectation and Variance}\label{expectation-and-variance}}

Often, it is easier to describe a random variable using some numerical
summary, rather than fully defining its distribution. These numerical
summaries are numbers that characterize some properties of the random
variable. Because they give a ``summary'' of how the variable tends to
behave, they are \emph{not} random -- think of them as a static number
that describes a certain property of the random variable. In Data 100,
we will focus our attention on the expectation and variance of a random
variable.

\hypertarget{expectation}{%
\subsection{Expectation}\label{expectation}}

The \textbf{expectation} of a random variable \(X\) is the weighted
average of the values of \(X\), where the weights are the probabilities
of each value occurring. To compute the expectation, we find each value
\(x\) that the variable could possibly take, weight by the probability
of the variable taking on each specific value, and sum across all
possible values of \(x\).

\[\mathbb{E}[X] = \sum_{\text{all possible } x} x P(X=x)\]

An important property in probability is the \textbf{linearity of
expectation}. The expectation of the linear transformation \(aX+b\),
where \(a\) and \(b\) are constants, is:

\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]

Expectation is also linear in \emph{sums} of random variables.

\[\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\]

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

The \textbf{variance} of a random variable is a measure of its chance
error. It is defined as the expected squared deviation from the
expectation of \(X\). Put more simply, variance asks: how far does \(X\)
typically vary from its average value? What is the spread of \(X\)'s
distribution?

\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]

If we expand the square and use properties of expectation, we can
re-express this statement as the \textbf{computational formula for
variance}. This form is often more convenient to use when computing the
variance of a variable by hand.

\[\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]

How do we compute the expectation of \(X^2\)? Any function of a random
variable is \emph{also} a random variable -- that means that by squaring
\(X\), we've created a new random variable. To compute
\(\mathbb{E}[X^2]\), we can simply apply our definition of expectation
to the random variable \(X^2\).

\[\mathbb{E}[X^2] = \sum_{\text{all possible } x} x^2 P(X^2 = x^2)\]

Unlike expectation, variance is \emph{non-linear}. The variance of the
linear transformation \(aX+b\) is:

\[\text{Var}(aX+b) = a^2 \text{Var}(X)\]

The full proof of this fact can be found using the definition of
variance. As general intuition, consider that \(aX+b\) scales the
variable \(X\) by a factor of \(a\), then shifts the distribution of
\(X\) by \(b\) units.

\begin{itemize}
\tightlist
\item
  Shifting the distribution by \(b\) \emph{does not} impact the
  \emph{spread} of the distribution. Thus,
  \(\text{Var}(aX+b) = \text{Var}(aX)\).
\item
  Scaling the distribution by \(a\) \emph{does} impact the spread of the
  distribution.
\end{itemize}

If we wish to understand the spread in the distribution of the
\emph{summed} random variables \(X + Y\), we can manipulate the
definition of variance to find:

\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]\]

This last term is of special significance. We define the
\textbf{covariance} of two random variables as the expected product of
deviations from expectation. Put more simply, covariance is a
generalization of variance to \emph{two} random variables:
\(\text{Cov}(X, X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X)\).

\[\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\]

We can treat the covariance as a measure of association. Remember the
definition of correlation given when we first established SLR?

\[r(X, Y) = \mathbb{E}\left[\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right] = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}\]

It turns out we've been quietly using covariance for some time now! If
\(X\) and \(Y\) are independent, then \(\text{Cov}(X, Y) =0\) and
\(r(X, Y) = 0\). Note, however, that the converse is not always true:
\(X\) and \(Y\) could have \(\text{Cov}(X, Y) = r(X, Y) = 0\) but not be
independent. This means that the variance of a sum of independent random
variables is the sum of their variances:
\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}\]

\hypertarget{standard-deviation}{%
\subsection{Standard Deviation}\label{standard-deviation}}

Notice that the units of variance are the \emph{square} of the units of
\(X\). For example, if the random variable \(X\) was measured in meters,
its variance would be measured in meters\(^2\). The \textbf{standard
deviation} of a random variable converts things back to the correct
scale by taking the square root of variance.

\[\text{SD}(X)  = \sqrt{\text{Var}(X)}\]

To find the standard deviation of a linear transformation \(aX+b\), take
the square root of the variance:

\[\text{SD}(aX+b) = \sqrt{\text{Var}(aX+b)} = \sqrt{a^2 \text{Var}(X)} = |a|\text{SD}(X)\]

\bookmarksetup{startatroot}

\hypertarget{probability-ii}{%
\chapter{Probability II}\label{probability-ii}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Apply the Central Limit Theorem to approximate parameters of a
  population
\item
  Compute the bias, variance, and MSE of an estimator for a parameter
\item
  Qualitiatively describe the bias-variance tradeoff and decomposition
  of model risk
\item
  Construct confidence intervals for hypothesis testing
\end{itemize}

\end{tcolorbox}

Last time, we introduced the idea of random variables: numerical
functions of a sample. Most of our work in the last lecture was done to
build a background in probability and statistics. Now that we've
established some key ideas, we're in a good place to apply what we've
learned to our original goal -- understanding how the randomness of a
sample impacts the model design process.

In this lecture, we will delve more deeply into this idea of fitting a
model to a sample. We'll explore how to re-express our modeling process
in terms of random variables and use this new understanding to steer
model complexity.

\hypertarget{sample-statistics}{%
\section{Sample Statistics}\label{sample-statistics}}

In the last lecture, we talked at length about the concept of a
distribution -- a statement of all possible values that a random
variable can take, as well as the probability of the variable taking on
each value. Let's take a moment to refine this definition.

\begin{itemize}
\tightlist
\item
  The distribution of a \emph{population} describes how a random
  variable behaves across \emph{all} individuals of interest.
\item
  The distribution of a \emph{sample} describes how a random variable
  behaves in a specific sample from the population.
\end{itemize}

In data science, we seldom have access to the entire population we wish
to investigate. If we want to understand the distribution of a random
variable across the population, we often need to use the distribution of
collected samples to \emph{infer} the properties of the population. For
example, say we wish to understand the distribution of heights of people
across the US population. We can't directly survey every single person
in the country, so we might instead take smaller samples of heights and
use these samples to estimate the population's distribution.

A common situation is wishing to know the mean of a population (eg the
average height of all people in the US). In this case, we can take
several samples of size \(n\) from the population, and compute the mean
of each \emph{sample}.

In
\href{https://inferentialthinking.com/chapters/14/4/Central_Limit_Theorem.html?}{Data
8}, you encountered the \textbf{Central Limit Theorem (CLT)}. This is a
powerful theorem for estimating the distribution of a population with
mean \(\mu\) and standard deviation \(\sigma\) from a collection of
smaller samples. The CLT tells us that if an IID sample of size \(n\) is
large, then the probability distribution of the sample mean is
\textbf{roughly normal with mean \(\mu\) and SD \(\sigma/\sqrt{n}\)}. In
simpler terms, this means:

\begin{itemize}
\tightlist
\item
  Draw a sample of size \(n\) from the population
\item
  Compute the mean of this sample; call it \(\bar{X}_n\)
\item
  Repeat this process: draw many more samples and compute the mean of
  each
\item
  The distribution of these sample means is normal with standard
  deviation \(\sigma/\sqrt{n}\) and mean equal to the population mean,
  \(\mu\)
\end{itemize}

Importantly, the CLT assumes that each observation in our samples is
drawn IID from the distribution of the population. In addition, the CLT
is accurate only when \(n\) is ``large.'' What counts as a ``large''
sample size depends on the specific distribution. If a population is
highly symmetric and unimodal, we could need as few as \(n=20\); if a
population is very skewed, we need a larger \(n\). Classes like Data 140
investigate this idea in great detail.

Why is this helpful? Consider what might happen if we estimated the
population distribution from just \emph{one} sample. If we happened, by
random chance, to draw a sample with a different mean or spread than
that of the population, we might get a skewed view of how the population
behaves (consider the extreme case where we happen to sample the exact
same value \(n\) times!). By drawing many samples, we can consider how
the sample distribution varies across multiple subsets of the data. This
allows us to approximate the properties of the population without the
need to survey every single member.

Notice the difference in variation between the two distributions that
are different in sample size. The distribution with bigger sample size
(\(n=800\)) is tighter around the mean than the distribution with
smaller sample size (\(n=200\)). Try plugging in these values into the
standard deviation equation for the normal distribution to make sense of
this!

\hypertarget{prediction-and-inference}{%
\section{Prediction and Inference}\label{prediction-and-inference}}

At this point in the course, we've spent a great deal of time working
with models. When we first introduced the idea of modeling a few weeks
ago, we did so in the context of \textbf{prediction}: using models to
make predictions about unseen data.

Another reason we might build models is to better understand complex
phenomena in the world around us. \textbf{Inference} is the task of
using a model to infer the true underlying relationships between the
feature and response variables. If we are working with a set of housing
data, \emph{prediction} might ask: given the attributes of a house, how
much is it worth? \emph{Inference} might ask: how much does having a
local park impact the value of a house?

A major goal of inference is to draw conclusions about the full
population of data, given only a random sample. To do this, we aim to
estimate the value of a \textbf{parameter}, which is a numerical
function of the \emph{population} (for example, the population mean
\(\mu\)). We use a collected sample to construct a \textbf{statistic},
which is a numerical function of the random \emph{sample} (for example,
the sample mean \(\bar{X}_n\)). It's helpful to think ``p'' for
``parameter'' and ``population,'' and ``s'' for ``sample'' and
``statistic.''

Since the sample represents a random subset of the population, any
statistic we generate will likely deviate from the true population
parameter. We say that the sample statistic is an \textbf{estimator} of
the true population parameter. Notationally, the population parameter is
typically called \(\theta\), while its estimator is denoted by
\(\hat{\theta}\).

To address our inference question, we aim to construct estimators that
closely estimate the value of the population parameter. We evaluate how
``good'' an estimator is by answering three questions:

\begin{itemize}
\tightlist
\item
  Do we get the right answer for the parameter, on average?
\item
  How variable is the answer?
\item
  How close is our answer to the parameter?
\end{itemize}

The \textbf{bias} of an estimator is how far off it is from the
parameter, on average.

\[\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \theta] = \mathbb{E}[\hat{\theta}] - \theta\]

For example, the bias of the sample mean as an estimator of the
population mean is:

\[\begin{align}\mathbb{E}[\bar{X}_n - \mu]
&= \mathbb{E}[\frac{1}{n}\sum_{i=1}^n (X_i)] - \mu \\
&= \frac{1}{n}\sum_{i=1}^n \mathbb{E}[X_i] - \mu \\
&= \frac{1}{n} (n\mu) - \mu \\
&= 0\end{align}\]

Because its bias is equal to 0, the sample mean is said to be an
\textbf{unbiased} estimator of the population mean.

The \textbf{variance} of an estimator is a measure of how much the
estimator tends to vary from its mean value.

\[\text{Var}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2 \right]\]

The \textbf{mean squared error} measures the ``goodness'' of an
estimator by incorporating both the bias and variance. Formally, it is
defined:

\[\text{MSE}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \theta)^2
\right]\]

If we denote the bias as \(b = \mathbb{E}[\hat{\theta}] - \theta\), the
MSE can be re-expressed to show its relationship to bias and variance.

\[\begin{align}
\text{MSE}(\hat{\theta}) &= \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}] + b)^2 \right] \\
&= \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2\right] + b^2 + 2b\mathbb{E}\left[\hat{\theta} - \mathbb{E}[\hat{\theta}]\right] \\
&= \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2\right] + b^2 + 2b\mathbb{E}[\hat{\theta}] - 2b\mathbb{E}[\hat{\theta}] \\
&= \text{Var}(\hat{\theta}) + b^2
\end{align}\]

\hypertarget{modeling-as-estimation}{%
\section{Modeling as Estimation}\label{modeling-as-estimation}}

Now that we've established the idea of an estimator, let's see how we
can apply this learning to the modeling process. To do so, we'll take a
moment to formalize our data collection and models in the language of
random variables.

Say we are working with an input variable, \(x\), and a response
variable, \(Y\). We assume that \(Y\) and \(x\) are linked by some
relationship \(g\) -- in other words, \(Y = g(x)\). \(g\) represents
some ``universal truth'' or ``law of nature'' that defines the
underlying relationship between \(x\) and \(Y\).

As data scientists, we have no way of directly ``seeing'' the underlying
relationship \(g\). The best we can do is collect observed data out in
the real world to try to understand this relationship. Unfortunately,
the data collection process will always have some inherent error (think
of the randomness you might encounter when taking measurements in a
scientific experiment). We say that each observation comes with some
random error or \textbf{noise} term, \(\epsilon\). This error is assumed
to be a random variable with expectation 0, variance \(\sigma^2\), and
be IID across each observation. The existence of this random noise means
that our observations, \(Y(x)\), are \emph{random variables}.

\[\text{True relationship: }Y = g(x)\]

\[\text{Observed relationship: }Y = g(x) + \epsilon\]

We can only observe our random sample of data, represented by the blue
points above. From this sample, we want to estimate the true
relationship \(g\). We do this by constructing the model \(\hat{Y}(x)\)
to estimate \(g\).

If we assume that the true relationship \(g\) is linear, we can
re-express this goal in a slightly different way. The observed data is
generated by the relationship:

\[Y(x) = g(x) + \epsilon = \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p + \epsilon\]

We aim to train a model to obtain estimates for each \(\theta_i\), which
we refer to as \(\hat{\theta}_i\). Because \(\hat{Y}\) is a fit to
random data, we say that it is also a random variable.

\[\hat{Y}(x) = \hat{\theta}_0 + \hat{\theta}_1 x_1 + \ldots + \hat{\theta}_p x_p\]

Notice that \(Y\) is dependent on \(\epsilon\), which means that \(Y\)
is a random variable itself. Additionally, the parameters of our model,
\(\hat{Y}\), is also dependent on this randomnness, which means our
predictor is random itself.

\bookmarksetup{startatroot}

\hypertarget{case-study-in-human-contexts-and-ethics}{%
\chapter{Case Study in Human Contexts and
Ethics}\label{case-study-in-human-contexts-and-ethics}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Learn about the ethical dilemmas that data scientists face.
\item
  Know how critique models using contextual knowledge about data.
\end{itemize}

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-color-frame, title={DISCLAIMER}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

The following chapter discusses issues of structural racism. Some of the
items in the chapter may be sensitive and may or may not be the
opinions, ideas, and beliefs of students who collected the materials.
The Data 100 course staff tries its best to only present information
that is relevant for teaching the lessons at hand.

\end{tcolorbox}

Let's immerse ourselves in the real world story of data scientists
working for an organization called the Cook County Assessor's Office
(CCAO). Their job is to \textbf{estimate the values of houses} in order
to \textbf{assign property taxes}. This is because tax burden in this
area is determined by the estimated \textbf{value} of a house, which is
different from its price. Since values change over time and there is no
obvious indicators of value, they created a \textbf{model} to estimate
the values of houses. In this chapter, we will dig deep into what
problems that biased the models, the consequences to human lives, and
how we can learn from this example to do better.

\hypertarget{the-problem}{%
\section{The Problem}\label{the-problem}}

In a
\href{https://apps.chicagotribune.com/news/watchdog/cook-county-property-tax-divide/assessments.html}{report}
by the Chicago Tribune uncovered a major scandal. The team showed that
the model perpetuated a highly regressive tax system which
disproportionately burdened African American and Latinx homeowners in
Cook County. How did they know?

In the field of housing assessment, there are standard metrics that
assessors use across the world to estimate the fairness of assessments:
\href{https://www.realestateagent.com/real-estate-glossary/real-estate/coefficient-of-dispersion.html}{coefficient
of dispersion} and
\href{https://leg.wa.gov/House/Committees/FIN/Documents/2009/RatioText.pdf}{price-related
differential}. These metrics have been rigorously tested by experts in
the field, and are out of scope for our class. Calculating these metrics
for the Cook County prices and found that the pricing created by the
CCAO did not fall in acceptable ranges (See figure above). This by
itself is \textbf{not the end} of the story, but a good indicator that
\textbf{something fishy was going on}.

This then prompted them to investigate if the model itself was producing
fair tax rates. Evidently, when accounting for the home owner's income
they found that the model actually produced a \textbf{regressive} tax
rate (See figure above).

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-color-frame, title={Definition}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

Regressive (tax): A tax rate is regressive if the percentage tax rate is
higher for individuals with lower net income.

Progressive (tax): A tax rate is progressive if the percentage tax rate
is higher for individuals with higher net income.

\end{tcolorbox}

Digging further shows that not only was the system unfair to people
across the axis of income, it was also unfair across the axis of
race(See figure above). The likelihood that your home was under or over
assessed was highly dependent on your race did not sit will with many
home owners.

\hypertarget{spotlight-appeals}{%
\subsection{Spotlight: Appeals}\label{spotlight-appeals}}

What actually caused this to come about? The whole answer is a lot
bigger than just models. At the end of the day, these are real systems
that have a lot of moving parts. One of those moving parts was the
\textbf{appeals system}. Homeowners are mailed the value their home is
assess for by CCAO, and the homeowner can choose to appeal to a board of
elected officials to try and change the listed value of their home, and
thus how much they are taxed. In theory, this sounds like a very fair
system: there is a human that oversees the final pricing of houses
rather than just an algorithm. However, it ended up exacerbating the
problems.

``Appeals are a good thing,'' Thomas Jaconetty, deputy assessor for
valuation and appeals, said in an interview. ``The goal here is
fairness. We made the numbers. We can change them.''

Here we can borrow lessons from
\href{https://www.britannica.com/topic/critical-race-theory}{Critical
Race Theory}. On the surface, everyone having the legal right to try and
appeal is undeniable. However, not everyone has an equal ability to.
Those who have the money to hire tax lawyers to appeal for them have a
drastically higher chance trying and succeeding (See above figure). This
model is part of a deeper institutional pattern rife with potential
corruption.

Homeowners who appealed were generally underassessed relative to other
homeowners (See above figure). Those with higher incomes pay less in
property tax, tax lawyers are able to grow their business due to their
role in appeals, and politicians are commonly socially connected to the
aforementioned tax lawyers and wealthy homeowners. All these
stakeholders have reasons to advertise the model as a integral part of a
fair system. Here lies the value in asking questions. A system that
seems fair on the surface may in actuality be unfair upon taking a
closer look.

\hypertarget{human-impacts}{%
\subsection{Human Impacts}\label{human-impacts}}

The impact of the housing model extends beyond the realm of home
ownership and taxation. Discriminatory practices have a long history
within the United States, and the model served to perpetuate this fact.
To this day, Chicago is one of the most segregated cities in the United
States
(\href{https://fivethirtyeight.com/features/the-most-diverse-cities-are-often-the-most-segregated/}{source}).
These factors are central to informing us, as data scientists, about
what is at stake.

\hypertarget{spotlight-intersection-of-real-estate-and-race}{%
\subsection{Spotlight: Intersection of Real Estate and
Race}\label{spotlight-intersection-of-real-estate-and-race}}

Housing has been a persistent source of racial inequality throughout US
History, amongst other factors. It is one of the main areas where
inequalities are created and reproduced. In the beginning,
\href{https://www.history.com/topics/early-20th-century-us/jim-crow-laws}{Jim
Crow} laws were explicit in forbidding Persons of Color from Schools,
public utilities, etc.

Today, while advancements in Civil Rights have been made, the spirit of
the laws are alive in many parts of the US. The real estate industry was
``professionalized'' in the 1920's and 1930's by aspiring to become a
science guided by strict methods and principles:

\begin{itemize}
\tightlist
\item
  Redlining: making it difficult or impossible to get a federally-backed
  mortgage to buy a house in specific neighborhoods coded as ``risky''
  (red).

  \begin{itemize}
  \tightlist
  \item
    What made them ``risky'' according to the makers of these was racial
    composition.
  \item
    Segregation was not only a result of federal policy, but developed
    by real estate professionals.
  \end{itemize}
\item
  The methods centered on creating objective rating systems (information
  technologies) for the appraisal of property values which encoded
  \textbf{race} as a factor of valuation (See figure below)

  \begin{itemize}
  \tightlist
  \item
    This, in turn, influenced federal policy and practice
  \end{itemize}
\end{itemize}

Source: Colin Koopman, How We Became Our Data (2019) p.~137

\hypertarget{the-response-cook-county-open-data-initiative}{%
\section{The Response: Cook County Open Data
Initiative}\label{the-response-cook-county-open-data-initiative}}

The response started in politics. A new assessor, Fritz Kaegi, was
elected and created a new mandate with two goals:

\begin{itemize}
\tightlist
\item
  Distributional equity in property taxation = properties of same value
  treated alike during assessments
\item
  Creates new Office of Data Science
\end{itemize}

\hypertarget{questionproblem-formulation}{%
\subsection{Question/Problem
Formulation}\label{questionproblem-formulation}}

\begin{itemize}
\tightlist
\item
  What do we want to know?
\item
  What problems are we trying to solve?
\item
  What are the hypotheses we want to test?
\item
  What are our metrics for success?
\end{itemize}

The new Office of Data Science started by redefining their goals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Accurately, uniformly, and impartially assess the value of a home.

  \begin{itemize}
  \tightlist
  \item
    Following international standards (coefficient of dispersion)
  \item
    Predicting value of all homes with as little total error as
    possible.
  \end{itemize}
\item
  Create a system that assesses house values that is fair to all people,
  across perceived racial and income differences.

  \begin{itemize}
  \tightlist
  \item
    Disrupts the circuit of corruption (Board of Review appeals process)
  \item
    Eliminates regressivity
  \item
    Engenders trust in the system among all stakeholders
  \end{itemize}
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-color-frame, title={Definitions}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\textbf{Definitions}: Fairness and Transparency

Fairness: The ability of our pipeline to accurately assess property
values, accounting for disparities in geography, information, etc.

Transparency: The ability of the data science department to share and
explain pipeline results and decisions to both internal and external
stakeholders

\end{tcolorbox}

\hypertarget{data-acquisition-and-cleaning}{%
\subsection{Data Acquisition and
Cleaning}\label{data-acquisition-and-cleaning}}

\begin{itemize}
\tightlist
\item
  What data do we have and what data do we need?
\item
  How will we sample more data?
\item
  Is our data representative of the population we want to study?
\end{itemize}

Example: Sales data

They also critically examined their original sales data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How was this data collected?
\item
  When was this data collected?
\item
  Who collected this data?
\item
  For what purposes was the data collected?
\item
  How and why were particular categories created?
\end{enumerate}

For example, attributes can affect likelihood of appearing in the data.
For example, housing data in the floodplains geographic region of
Chicago were less represented than other regions.

The features can even be reported at different rates. Improvements in
homes, which tend to increase property value, were unlikely to be
reported by the homeowners.

Additionally, they found that there was simply more missing data in
lower income neighborhoods.

\hypertarget{exploratory-data-analysis}{%
\subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}}

\begin{itemize}
\tightlist
\item
  How is our data organized and what does it contain?
\item
  Do we already have relevant data?
\item
  What are the biases, anomalies, or other issues with the data?
\item
  How do we transform the data to enable effective analysis?
\end{itemize}

Before the modeling step, they investigated a multitude of crucial
questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Which attributes are most predictive of sales price?
\item
  Is the data uniformly distributed?
\item
  Do all neighborhoods have up to date data? Do all neighborhoods have
  the same granularity?\\
\item
  Do some neighborhoods have missing or outdated data?
\end{enumerate}

Firstly, they found that the impact of certain features, such as bedroom
number, were much more impactful in determining house value inside
certain neighborhoods more than others. This informed them that
different models should be used depending on the neighborhood.

They also noticed that low income neighborhoods had disproportionately
spottier data. This informed them that they needed to develop new data
collection practices--including finding new sources of data.

\hypertarget{prediction-and-inference-1}{%
\subsection{Prediction and Inference}\label{prediction-and-inference-1}}

\begin{itemize}
\tightlist
\item
  What does the data say about the world?
\item
  Does it answer our questions or accurately solve the problem?
\item
  How robust are our conclusions and can we trust the predictions?
\end{itemize}

Rather than using a singular model to predict sale prices (``fair market
value'') of unsold properties, the CCAO fit machine learning models that
discover patterns using known sale prices and characteristics of
\textbf{similar and nearby properties}. It uses different model weights
for each township.

Compared to traditional mass appraisal, the CCAO's new approach is more
granular and more sensitive to neighborhood variations.

\hypertarget{reports-decisions-and-conclusions}{%
\subsection{Reports Decisions, and
Conclusions}\label{reports-decisions-and-conclusions}}

\begin{itemize}
\tightlist
\item
  How successful is the system for each goal?

  \begin{itemize}
  \tightlist
  \item
    accuracy/uniformity of the model.
  \item
    fairness and transparency that eliminates regressivity and engenders
    trust.
  \end{itemize}
\item
  How do you know?
\end{itemize}

The model is not the end of the road. The new Office still sends
homeowners their house evaluations, but now the data that they get sent
back from the homeowners is taken into account. More detailed reports
are being written by the office itself to democratize the information.
Town halls and other public facing outreach helps involves the whole
community in the process of housing evaluations, rather than limiting
participation to a select few.

\hypertarget{key-takeaways}{%
\section{Key Takeaways}\label{key-takeaways}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Accuracy is a necessary, but not sufficient, condition of a fair
  system.
\item
  Fairness and transparency are context-dependent and sociotechnical
  concepts
\item
  Learn to work with contexts, and consider how your data analysis will
  reshape them
\item
  Keep in mind the power, and limits, of data analysis
\end{enumerate}

\hypertarget{lessons-for-data-science-practice}{%
\section{Lessons for Data Science
Practice}\label{lessons-for-data-science-practice}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Question/Problem formulation

  \begin{itemize}
  \tightlist
  \item
    Who is responsible for framing the problem?
  \item
    Who are the stakeholders? How are they involved in the problem
    framing?
  \item
    What do you bring to the table? How does your positionality affect
    your understanding of the problem?
  \item
    What are the narratives that you're tapping into?
  \end{itemize}
\item
  Data Acquisition and Cleaning

  \begin{itemize}
  \tightlist
  \item
    Where does the data come from?
  \item
    Who collected it? For what purpose?
  \item
    What kinds of collecting and recording systems and techniques were
    used?
  \item
    How has this data been used in the past?
  \item
    What restrictions are there on access to the data, and what enables
    you to have access?
  \end{itemize}
\item
  Exploratory Data Analysis \& Visualization

  \begin{itemize}
  \tightlist
  \item
    What kind of personal or group identities have become salient in
    this data?
  \item
    Which variables became salient, and what kinds of relationship
    obtain between them?
  \item
    Do any of the relationships made visible lend themselves to
    arguments that might be potentially harmful to a particular
    community?
  \end{itemize}
\item
  Prediction and Inference

  \begin{itemize}
  \tightlist
  \item
    What does the prediction or inference do in the world?
  \item
    Are the results useful for the intended purposes?
  \item
    Are there benchmarks to compare the results?
  \item
    How are your predictions and inferences dependent upon the larger
    system in which your model works?
  \end{itemize}
\item
  Reports, Decisions, and Solutions

  \begin{itemize}
  \tightlist
  \item
    How do we know if we have accomplished our goals?
  \item
    How does your work fit in the broader literature?
  \item
    Where does your work agree or disagree with the status quo?
  \item
    Do your conclusions make sense?
  \end{itemize}
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{inference-causality}{%
\chapter{Inference \& Causality}\label{inference-causality}}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, colback=white, opacitybacktitle=0.6, opacityback=0, bottomrule=.15mm, titlerule=0mm, toptitle=1mm, bottomtitle=1mm, left=2mm, colframe=quarto-callout-note-color-frame, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, rightrule=.15mm, toprule=.15mm, leftrule=.75mm, arc=.35mm, coltitle=black]

\begin{itemize}
\tightlist
\item
  Introduction to model risk of fitted models
\item
  Decompose the model risk into bias and variance terms
\item
  Construct confidence intervals for hypothesis testing
\item
  Understand the assumptions we make and its impact on our regression
  inference
\item
  Compare regression and causation
\item
  Experment setup, confounding variables, average treatment effect, and
  covariate adjustment
\end{itemize}

\end{tcolorbox}

Last time, we introduced the idea of random variables and its affect on
the observed relationship we use to fit models.

In this lecture, we will explore the decomposition of model risk from a
fitted model, regression inference via hypothesis testing and
considering the assumptions we make, and the environment of
understanding causality in theory and in practice.

\hypertarget{bias-variance-tradeoff}{%
\section{Bias-Variance Tradeoff}\label{bias-variance-tradeoff}}

Recall the model and the data we generated from that model from the last
section:

\[\text{True relationship: }Y = g(x)\]

\[\text{Observed relationship: }Y = g(x) + \epsilon\]

Remember we can only observe our random sample of data, represented by
the blue points above. From this sample, we want to estimate the true
relationship \(g\). We do this by constructing the model \(\hat{Y}(x)\)
to estimate \(g\).

Recall that the observed relationship and our fitted model are random.
With this random variables framework in hand, we're now ready to
understand how model complexity can finetune our model's performance.

To evaluate our model's performance, we've previously considered the MSE
of our model across a single sample of observed data. Now that we've
reframed our model \(\hat{Y}\) and observations \(Y\) as random
variables, we'll use a theoretical approach to better understand the
model's error. \textbf{Model risk} is defined as the mean square
prediction error of the random variable \(\hat{Y}\). It is an
expectation across \emph{all} samples we could have possibly gotten when
fitting the model. Model risk considers the model's performance on any
sample that is theoretically possible, rather than the specific data
that we have collected.

\[\text{Model risk }=\mathbb{E}\left[(Y-\hat{Y})^2\right]\]

What is the origin of the error encoded by model risk? Recall the data
generation process we established earlier. There is the true underlying
relationship \(g\), observed data (with random noise) \(Y\), and model
\(\hat{Y}\).

To better understand model risk, we'll zoom in on a single data point in
the plot above.

Remember that \(\hat{Y}(x)\) is a random variable -- it is the
prediction made for \(x\) after being fit on the specific sample used
for training. If we had used a different sample for training, a
different prediction might have been made for this value of \(x\). To
capture this, the diagram above considers both the prediction
\(\hat{Y}(x)\) made for a particular random training sample, and the
\emph{expected} prediction across all possible training samples,
\(\mathbb{E}[\hat{Y}(x)]\).

We can use this simplified diagram to break down the model risk into
smaller components. First, start by considering the error on a single
prediction, \(Y(x)-\hat{Y}(x)\).

We can identify three components to this error.

That is, the error can be written as:

\[Y(x)-\hat{Y}(x) = \epsilon + \left(g(x)-\mathbb{E}\left[\hat{Y}(x)\right]\right) + \left(\mathbb{E}\left[\hat{Y}(x)\right] - \hat{Y}(x)\right)\]
\[\newline   \] The model risk is the expected square of the expression
above, \(\mathbb{E}\left[(Y(x)-\hat{Y}(x))^2\right]\). If we work
through the algebra, we find the following relationship:

\[\mathbb{E}\left[(Y(x)-\hat{Y}(x))^2\right]=\sigma^2 + \left(\mathbb{E}\left[\hat{Y}(x)\right]-g(x)\right)^2 + \text{Var}(\hat{Y}(x))\]

This expression may look complicated at first blush, but we've actually
already defined each term earlier this lecture! Let's break things down.

\begin{itemize}
\tightlist
\item
  The \textbf{model risk},
  \(\mathbb{E}\left[(Y(x)-\hat{Y}(x))^2\right]\), is the mean squared
  prediction error of the model.
\item
  The \textbf{observation variance}, \(\sigma^2\), is the variance of
  the random noise in the observations. It describes how variable the
  random error \(\epsilon\) is for each observation.
\item
  The \textbf{model bias}, \(\mathbb{E}\left[\hat{Y}(x)\right]-g(x)\),
  is how ``off'' the \(\hat{Y}(x)\) is as an estimator of the true
  underlying relationship \(g(x)\).
\item
  The \textbf{model variance}, \(\text{Var}(\hat{Y}(x))\), describes how
  much the prediction \(\hat{Y}(x)\) tends to vary when we fit the model
  on different samples.
\end{itemize}

This means that we can simplify the statement above.

\[\text{Model risk = observation variance + (model bias)}^2 \text{+ model variance}\]

This is known as the \textbf{bias-variance tradeoff}. What does it mean?
Remember that the model risk is a measure of the model's performance.
Our goal in building models is to keep model risk low; this means that
we will want to ensure that each component of model risk is kept at a
low value.

Observation variance is an inherent, random part of the data collection
process. We aren't able to reduce the observation variance, so we'll
focus our attention to the model bias and model variance.

In the Feature Engineering lecture, we considered the issue of
overfitting. We saw that the model's error or bias tends to decrease as
model complexity increases -- if we design a highly complex model, it
will tend to make predictions that are closer to the true relationship
\(g\). At the same time, model variance tends to \emph{increase} as
model complexity increases: a complex model may overfit to the training
data, meaning that small differences in the random samples used for
training lead to large differences in the fitted model. We have a
problem. To reduce model risk, we could decrease model bias by
increasing the model's complexity, which would lead to an increase in
model variance. Alternatively, we could decrease model variance by
decreasing the model's complexity -- at the cost of increased bias.

We need to strike a balance. Our goal in model creation is to use a
complexity level that is high enough to keep bias low, but not so high
that model variance is large. We'll explore how to do this using
\textbf{regularization} in the next lecture.

\hypertarget{inference-for-regression}{%
\section{Inference for Regression}\label{inference-for-regression}}

We've spent a great deal of time now using random variables to explore
model performance. It turns out that our work in probability can help us
understand another aspect of model design -- interpreting what exactly
our fitted models can tell us about the world around us.

Recall the framework we established earlier this lecture. If we assume
that the underlying relationship between our observations and input
features is linear, we can express this relationship in terms of the
unknown, true model parameters \(\theta\).

\[Y(x) = g(x) + \epsilon = \theta_0 + \theta_1 x_1 + \ldots + \theta_p x_p + \epsilon\]

Our model attempts to estimate each true parameter \(\theta_i\) using
the estimates \(\hat{\theta}_i\).

\[\hat{Y}(x) = \hat{\theta}_0 + \hat{\theta}_1 x_1 + \ldots + \hat{\theta}_p x_p\]

Let's pause for a moment. At this point, we're very used to working with
the idea of a model parameter. But what exactly does each coefficient
\(\theta_i\) actually \emph{mean}? We can think of each \(\theta_i\) as
a \emph{slope} of the linear model -- if all other variables are held
constant, a unit change in \(x_i\) will result in a \(\theta_i\) change
in \(Y(x)\). Broadly speaking, a large value of \(\theta_i\) means that
the feature \(x_i\) has a large effect on the response; conversely, a
small value of \(\theta_i\) means that \(x_i\) has little effect on the
response. In the extreme case, if the true parameter \(\theta_i\) is 0,
then the feature \(x_i\) has \textbf{no effect} on \(Y(x)\).

If the true parameter \(\theta_i\) for a particular feature is 0, this
tells us something pretty significant about the world -- there is no
underlying relationship between \(x_i\) and \(Y(x)\)! How then, can we
test if a parameter is 0? As a baseline, we go through our usual process
of drawing a sample, using this data to fit a model, and computing an
estimate \(\hat{\theta}_i\). However, we need to also consider the fact
that if our random sample had come out differently, we may have found a
different result for \(\hat{\theta}_i\). To infer if the true parameter
\(\theta_i\) is 0, we want to draw our conclusion from the distribution
of \(\hat{\theta}_i\) estimates we could have drawn across all other
random samples.

To do this, we'll use an inference technique called \textbf{hypothesis
testing}. This concept was introduced back in
\href{https://inferentialthinking.com/chapters/11/Testing_Hypotheses.html}{Data
8}. You may find it helpful to review the hypothesis testing method to
refresh your memory.

We'll work with the
\href{https://www.audubon.org/field-guide/bird/snowy-plover}{snowy
plover} dataset throughout this section.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\NormalTok{eggs }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"data/snowy\_plover.csv"}\NormalTok{)}
\NormalTok{eggs.head(}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrrrr}
\toprule
{} &  egg\_weight &  egg\_length &  egg\_breadth &  bird\_weight \\
\midrule
0 &         7.4 &       28.80 &        21.84 &          5.2 \\
1 &         7.7 &       29.04 &        22.45 &          5.4 \\
2 &         7.9 &       29.36 &        22.48 &          5.6 \\
3 &         7.5 &       30.10 &        21.71 &          5.3 \\
4 &         8.3 &       30.17 &        22.75 &          5.9 \\
\bottomrule
\end{tabular}

Our goal will be to predict the weight of a newborn plover chick, which
we assume follows the true relationship below.

\[\text{bird_weight} = \theta_0 + \theta_1 \text{egg_weight} + \theta_2 \text{egg_length} + \theta_3 \text{egg_breadth} + \epsilon\]

Say we wish to determine if the \texttt{egg\_weight} impacts the
\texttt{bird\_weight} of a chick -- we want to infer if \(\theta_1\) is
equal to 0.

First, we define our hypotheses:

\begin{itemize}
\tightlist
\item
  Null hypothesis: the true parameter \(\theta_1\) is 0
\item
  Alternative hypothesis: the true parameter \(\theta_1\) is not 0
\end{itemize}

Next, we use our data to fit a model that approximates the relationship
above. This gives us the \textbf{observed value} of \(\hat{\theta}_1\)
found from our data.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}

\NormalTok{X }\OperatorTok{=}\NormalTok{ eggs[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ eggs[}\StringTok{"bird\_weight"}\NormalTok{]}

\NormalTok{model }\OperatorTok{=}\NormalTok{ LinearRegression()}

\NormalTok{model.fit(X, Y)}

\CommentTok{\# This gives an array containing the fitted model parameter estimates}
\NormalTok{thetas }\OperatorTok{=}\NormalTok{ model.coef\_}

\CommentTok{\# Put the parameter estimates in a nice table for viewing}
\NormalTok{pd.DataFrame(\{}\StringTok{"theta\_hat"}\NormalTok{:[model.intercept\_, thetas[}\DecValTok{0}\NormalTok{], thetas[}\DecValTok{1}\NormalTok{], thetas[}\DecValTok{2}\NormalTok{]]\}, index}\OperatorTok{=}\NormalTok{[}\StringTok{"theta\_0"}\NormalTok{, }\StringTok{"theta\_1"}\NormalTok{, }\StringTok{"theta\_2"}\NormalTok{, }\StringTok{"theta\_3"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  theta\_hat \\
\midrule
theta\_0 &  -4.605670 \\
theta\_1 &   0.431229 \\
theta\_2 &   0.066570 \\
theta\_3 &   0.215914 \\
\bottomrule
\end{tabular}

We now have the value of \(\hat{\theta}_1\) when considering the single
sample of data that we have. To get a sense of how this estimate might
vary if we were to draw different random samples, we will use
\textbf{\href{https://inferentialthinking.com/chapters/13/2/Bootstrap.html?}{bootstrapping}}.
To construct a bootstrap sample, we will draw a resample from the
collected data that:

\begin{itemize}
\tightlist
\item
  Has the same sample size as the collected data, and
\item
  Is drawn with replacement (this ensures that we don't draw the exact
  same sample every time!)
\end{itemize}

We draw a bootstrap sample, use this sample to fit a model, and record
the result for \(\hat{\theta}_1\) on this bootstrapped sample. We then
repeat this process many times to generate a \textbf{bootstrapped
distribution} of \(\hat{\theta}_1\). This gives us an estimate of what
the true distribution of \(\hat{\theta}_1\) across all possible samples
might look like.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set a random seed so you generate the same random sample as staff}
\CommentTok{\# In the "real world", we wouldn\textquotesingle{}t do this}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\CommentTok{\# Set the sample size of each bootstrap sample}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(eggs)}

\CommentTok{\# Create a list to store all the bootstrapped estimates}
\NormalTok{estimates }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Generate a bootstrap resample from \textasciigrave{}eggs\textasciigrave{} and find an estimate for theta\_1 using this sample. }
\CommentTok{\# Repeat 10000 times.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
\NormalTok{    bootstrap\_resample }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[}\StringTok{"bird\_weight"}\NormalTok{]}
    
\NormalTok{    bootstrap\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model.fit(X\_bootstrap, Y\_bootstrap)}
\NormalTok{    bootstrap\_thetas }\OperatorTok{=}\NormalTok{ bootstrap\_model.coef\_}
    
\NormalTok{    estimates.append(bootstrap\_thetas[}\DecValTok{0}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\ImportTok{import}\NormalTok{ seaborn }\ImportTok{as}\NormalTok{ sns}

\NormalTok{plt.figure(dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{sns.histplot(estimates, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}hat\{\textbackslash{}theta\}\_1$"}\NormalTok{)}
\NormalTok{plt.title(}\VerbatimStringTok{r"Bootstrapped estimates $\textbackslash{}hat\{\textbackslash{}theta\}\_1$"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{inference_causality/inference_causality_files/figure-pdf/cell-5-output-1.png}

}

\end{figure}

Now, we have a sense of how \(\hat{\theta}_1\) might vary across
different random samples. How does this help us understand if the true
parameter \(\theta_1\) is 0 or not? We'll use an approximate
\textbf{confidence interval}. Recall from
\href{https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html?}{Data
8} that a x\% confidence interval is a range of values where we are x\%
confident that the interval contains the true parameter. In other words,
if we repeated the bootstrapping process above many times, we'd expect
the x\% confidence interval to contain the true value of \(\theta_1\)
about x\% of the time.

For our purposes, we'll consider the approximate 95\% confidence
interval. Because of the
\href{https://inferentialthinking.com/chapters/13/4/Using_Confidence_Intervals.html\#using-a-confidence-interval-to-test-hypotheses}{duality}
of confidence intervals and hypothesis tests, we reject the null
hypothesis at a cutoff level of 5\% if 0 is not contained in the 95\%
confidence interval for \(\hat{\theta}_1\).

To create a 95\% confidence interval, we compute the 2.5th and 97.5th
percentiles of our bootstrapped estimates of \(\theta_1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower }\OperatorTok{=}\NormalTok{ np.percentile(estimates, }\FloatTok{2.5}\NormalTok{)}
\NormalTok{upper }\OperatorTok{=}\NormalTok{ np.percentile(estimates, }\FloatTok{97.5}\NormalTok{)}

\NormalTok{conf\_interval }\OperatorTok{=}\NormalTok{ (lower, upper)}
\NormalTok{conf\_interval}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(-0.258648119568487, 1.1034243854204047)
\end{verbatim}

We find that our bootstrapped approximate 95\% confidence interval for
\(\theta_1\) is \([-0.259, 1.103]\). Immediately, we can see that 0
\emph{is} contained in this interval -- this means that we \emph{cannot}
conclude that \(\theta_1\) is non-zero! More formally, we cannot reject
the null hypothesis (that \(\theta_1\) is 0) under a 5\% cutoff.

We can repeat this process to construct 95\% confidence intervals for
the other parameters of the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\NormalTok{theta\_0\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_1\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_2\_estimates }\OperatorTok{=}\NormalTok{ []}
\NormalTok{theta\_3\_estimates }\OperatorTok{=}\NormalTok{ []}


\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
\NormalTok{    bootstrap\_resample }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[[}\StringTok{"egg\_weight"}\NormalTok{, }\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap }\OperatorTok{=}\NormalTok{ bootstrap\_resample[}\StringTok{"bird\_weight"}\NormalTok{]}
    
\NormalTok{    bootstrap\_model }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model.fit(X\_bootstrap, Y\_bootstrap)}
\NormalTok{    bootstrap\_theta\_0 }\OperatorTok{=}\NormalTok{ bootstrap\_model.intercept\_}
\NormalTok{    bootstrap\_theta\_1, bootstrap\_theta\_2, bootstrap\_theta\_3 }\OperatorTok{=}\NormalTok{ bootstrap\_model.coef\_}
    
\NormalTok{    theta\_0\_estimates.append(bootstrap\_theta\_0)}
\NormalTok{    theta\_1\_estimates.append(bootstrap\_theta\_1)}
\NormalTok{    theta\_2\_estimates.append(bootstrap\_theta\_2)}
\NormalTok{    theta\_3\_estimates.append(bootstrap\_theta\_3)}
    
\NormalTok{theta\_0\_lower, theta\_0\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_0\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_0\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_1\_lower, theta\_1\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_1\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_1\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_2\_lower, theta\_2\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_2\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_2\_estimates, }\FloatTok{97.5}\NormalTok{)}
\NormalTok{theta\_3\_lower, theta\_3\_upper }\OperatorTok{=}\NormalTok{ np.percentile(theta\_3\_estimates, }\FloatTok{2.5}\NormalTok{), np.percentile(theta\_3\_estimates, }\FloatTok{97.5}\NormalTok{)}

\CommentTok{\# Make a nice table to view results}
\NormalTok{pd.DataFrame(\{}\StringTok{"lower"}\NormalTok{:[theta\_0\_lower, theta\_1\_lower, theta\_2\_lower, theta\_3\_lower], }\StringTok{"upper"}\NormalTok{:[theta\_0\_upper, }\OperatorTok{\textbackslash{}}
\NormalTok{                theta\_1\_upper, theta\_2\_upper, theta\_3\_upper]\}, index}\OperatorTok{=}\NormalTok{[}\StringTok{"theta\_0"}\NormalTok{, }\StringTok{"theta\_1"}\NormalTok{, }\StringTok{"theta\_2"}\NormalTok{, }\StringTok{"theta\_3"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lrr}
\toprule
{} &      lower &     upper \\
\midrule
theta\_0 & -15.278542 &  5.161473 \\
theta\_1 &  -0.258648 &  1.103424 \\
theta\_2 &  -0.099138 &  0.208557 \\
theta\_3 &  -0.257141 &  0.758155 \\
\bottomrule
\end{tabular}

Something's off here. Notice that 0 is included in the 95\% confidence
interval for \emph{every} parameter of the model. Using the
interpretation we outlined above, this would suggest that we can't say
for certain that \emph{any} of the input variables impact the response
variable! This makes it seem like our model can't make any predictions
-- and yet, each model we fit in our bootstrap experiment above could
very much make predictions of \(Y\).

How can we explain this result? Think back to how we first interpreted
the parameters of a linear model. We treated each \(\theta_i\) as a
slope, where a unit increase in \(x_i\) leads to a \(\theta_i\) increase
in \(Y\), \textbf{if all other variables are held constant}. It turns
out that this last assumption is very important. If variables in our
model are somehow related to one another, then it might not be possible
to have a change in one of them while holding the others constant. This
means that our interpretation framework is no longer valid! In the
models we fit above, we incorporated \texttt{egg\_length},
\texttt{egg\_breadth}, and \texttt{egg\_weight} as input variables.
These variables are very likely related to one another -- an egg with
large \texttt{egg\_length} and \texttt{egg\_breadth} will likely be
heavy in \texttt{egg\_weight}. This means that the model parameters
cannot be meaningfully interpreted as slopes.

To support this conclusion, we can visualize the relationships between
our feature variables. Notice the strong positive association between
the features.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sns.pairplot(eggs[[}\StringTok{"egg\_length"}\NormalTok{, }\StringTok{"egg\_breadth"}\NormalTok{, }\StringTok{"egg\_weight"}\NormalTok{]])}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{inference_causality/inference_causality_files/figure-pdf/cell-8-output-1.png}

}

\end{figure}

This issue is known as \textbf{colinearity}, sometimes also called
\textbf{multicolinearity}. Colinearity occurs when one feature can be
predicted fairly accurately by a linear combination of the other
features, which happens when one feature is highly correlated with the
others.

Why is colinearity a problem? Its consequences span several aspects of
the modeling process:

\begin{itemize}
\tightlist
\item
  Slopes can't be interpreted for an inference task
\item
  If features strongly influence one another, even small changes in the
  sampled data can lead to large changes in the estimated slopes
\item
  If one feature actually is a linear combination of the other features,
  the design matrix will not be full rank and \(\mathbb{X}^T\mathbb{X}\)
  is not invertible. This means that least squares does not have a
  single solution
\end{itemize}

The take-home point is that we need to be careful with what features we
select for modeling. If two features likely encode similar information,
it is often a good idea to choose only one of them as an input variable.

Let us now consider a more interpretable model: we instead assume a true
relationship using only egg weight:

\[f_\theta(x) = \theta_0 + \theta_1 \text{egg_weight} + \epsilon\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_int }\OperatorTok{=}\NormalTok{ eggs[[}\StringTok{"egg\_weight"}\NormalTok{]]}
\NormalTok{Y\_int }\OperatorTok{=}\NormalTok{ eggs[}\StringTok{"bird\_weight"}\NormalTok{]}

\NormalTok{model\_int }\OperatorTok{=}\NormalTok{ LinearRegression()}

\NormalTok{model\_int.fit(X\_int, Y\_int)}

\CommentTok{\# This gives an array containing the fitted model parameter estimates}
\NormalTok{thetas\_int }\OperatorTok{=}\NormalTok{ model\_int.coef\_}

\CommentTok{\# Put the parameter estimates in a nice table for viewing}
\NormalTok{pd.DataFrame(\{}\StringTok{"theta\_hat"}\NormalTok{:[model\_int.intercept\_, thetas\_int[}\DecValTok{0}\NormalTok{]]\}, index}\OperatorTok{=}\NormalTok{[}\StringTok{"theta\_0"}\NormalTok{, }\StringTok{"theta\_1"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{lr}
\toprule
{} &  theta\_hat \\
\midrule
theta\_0 &  -0.058272 \\
theta\_1 &   0.718515 \\
\bottomrule
\end{tabular}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set a random seed so you generate the same random sample as staff}
\CommentTok{\# In the "real world", we wouldn\textquotesingle{}t do this}
\NormalTok{np.random.seed(}\DecValTok{1337}\NormalTok{)}

\CommentTok{\# Set the sample size of each bootstrap sample}
\NormalTok{n }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(eggs)}

\CommentTok{\# Create a list to store all the bootstrapped estimates}
\NormalTok{estimates\_int }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# Generate a bootstrap resample from \textasciigrave{}eggs\textasciigrave{} and find an estimate for theta\_1 using this sample. }
\CommentTok{\# Repeat 10000 times.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10000}\NormalTok{):}
\NormalTok{    bootstrap\_resample\_int }\OperatorTok{=}\NormalTok{ eggs.sample(n, replace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    X\_bootstrap\_int }\OperatorTok{=}\NormalTok{ bootstrap\_resample\_int[[}\StringTok{"egg\_weight"}\NormalTok{]]}
\NormalTok{    Y\_bootstrap\_int }\OperatorTok{=}\NormalTok{ bootstrap\_resample\_int[}\StringTok{"bird\_weight"}\NormalTok{]}
    
\NormalTok{    bootstrap\_model\_int }\OperatorTok{=}\NormalTok{ LinearRegression()}
\NormalTok{    bootstrap\_model\_int.fit(X\_bootstrap\_int, Y\_bootstrap\_int)}
\NormalTok{    bootstrap\_thetas\_int }\OperatorTok{=}\NormalTok{ bootstrap\_model\_int.coef\_}
    
\NormalTok{    estimates\_int.append(bootstrap\_thetas\_int[}\DecValTok{0}\NormalTok{])}

\NormalTok{plt.figure(dpi}\OperatorTok{=}\DecValTok{120}\NormalTok{)}
\NormalTok{sns.histplot(estimates\_int, stat}\OperatorTok{=}\StringTok{"density"}\NormalTok{)}
\NormalTok{plt.xlabel(}\VerbatimStringTok{r"$\textbackslash{}hat\{\textbackslash{}theta\}\_1$"}\NormalTok{)}
\NormalTok{plt.title(}\VerbatimStringTok{r"Bootstrapped estimates $\textbackslash{}hat\{\textbackslash{}theta\}\_1$ Under the Interpretable Model"}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{inference_causality/inference_causality_files/figure-pdf/cell-10-output-1.png}

}

\end{figure}

Notice how the interpretable model performs almost as well as our other
model:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ mean\_squared\_error}

\NormalTok{rmse }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y, model.predict(X))}
\NormalTok{rmse\_int }\OperatorTok{=}\NormalTok{ mean\_squared\_error(Y\_int, model\_int.predict(X\_int))}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}RMSE of Original Model: }\SpecialCharTok{\{}\NormalTok{rmse}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f\textquotesingle{}RMSE of Interpretable Model: }\SpecialCharTok{\{}\NormalTok{rmse\_int}\SpecialCharTok{\}}\SpecialStringTok{\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
RMSE of Original Model: 0.04547085380275768
RMSE of Interpretable Model: 0.04649394137555684
\end{verbatim}

Yet, the confidence interval for the true parameter \(\theta_{1}\) does
not contain zero.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower\_int }\OperatorTok{=}\NormalTok{ np.percentile(estimates\_int, }\FloatTok{2.5}\NormalTok{)}
\NormalTok{upper\_int }\OperatorTok{=}\NormalTok{ np.percentile(estimates\_int, }\FloatTok{97.5}\NormalTok{)}

\NormalTok{conf\_interval\_int }\OperatorTok{=}\NormalTok{ (lower\_int, upper\_int)}
\NormalTok{conf\_interval\_int}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(0.6029335250209632, 0.8208401738546206)
\end{verbatim}

In retrospect, it's no surprise that the weight of an egg best predicts
the weight of a newly-hatched chick.

A model with highly correlated variables prevents us from interpreting
how the variables are related to the prediction.

\hypertarget{reminder-assumptions-matter}{%
\subsection{Reminder: Assumptions
Matter}\label{reminder-assumptions-matter}}

Keep the following in mind: All inference assumes that the regression
model holds.

\begin{itemize}
\tightlist
\item
  If the model doesn't hold, the inference might not be valid.
\item
  If the
  \href{https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html?highlight=p\%20value\%20confidence\%20interval\#care-in-using-the-bootstrap-percentile-method}{assumptions
  of the bootstrap} don't hold\ldots{}

  \begin{itemize}
  \tightlist
  \item
    Sample size n is large
  \item
    Sample is representative of population distribution (drawn IID,
    unbiased) \ldots then the results of the bootstrap might not be
    valid.
  \end{itemize}
\end{itemize}

\hypertarget{correlation-and-causation}{%
\section{Correlation and Causation}\label{correlation-and-causation}}

Let us consider some questions in an arbitrary regression problem.

What does \(\theta_{j}\) mean in our regression?

\begin{itemize}
\tightlist
\item
  Holding other variables fixed, how much should our prediction change
  with \(X_{j}\)?
\end{itemize}

For simple linear regression, this boils down to the correlation
coefficient

\begin{itemize}
\tightlist
\item
  Does having more \(x\) predict more \(y\) (and by how much)?
\end{itemize}

\textbf{Examples}: Are homes with granite countertops worth more money?
Is college GPA higher for students who win a certain scholarship? Are
breastfed babies less likely to develop asthma? Do cancer patients given
some aggressive treatment have a higher 5-year survival rate? Are people
who smoke more likely to get cancer?

These sound like causal questions, but they are not!

\hypertarget{prediction-vs-causation}{%
\subsection{Prediction vs Causation}\label{prediction-vs-causation}}

Questions about \textbf{correlation / prediction}:

\begin{itemize}
\tightlist
\item
  Are homes with granite countertops worth more money?
\item
  Is college GPA higher for students who win a certain scholarship?
\item
  Are breastfed babies less likely to develop asthma?
\item
  Do cancer patients given some aggressive treatment have a higher
  5-year survival rate?
\item
  Are people who smoke more likely to get cancer?
\end{itemize}

Questions about \textbf{causality}:

\begin{itemize}
\tightlist
\item
  How much do granite countertops \textbf{raise} the value of a house?
\item
  Does getting the scholarship \textbf{improve} students' GPAs?
\item
  Does breastfeeding \textbf{protect} babies against asthma?
\item
  Does the treatment \textbf{improve} cancer survival?
\item
  Does smoking \textbf{cause} cancer?
\end{itemize}

Causal questions are about the \textbf{effects} of
\textbf{interventions} (not just passive observation)

\begin{itemize}
\tightlist
\item
  Regression coefficients sometimes called ``effects'' -- can be
  deceptive!
\end{itemize}

Only one of these questions can be answered using the data alone:

\begin{itemize}
\tightlist
\item
  \textbf{Predictive question:} Are breastfed babies healthier?
\item
  \textbf{Causal question:} Does breastfeeding improve babies' health?
\end{itemize}

Possible explanations for why breastfed babies are healthier on average:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Causal effect:} breastfeeding makes babies healthier
\item
  \textbf{Reverse causality:} healthier babies more likely to
  successfully breastfeed
\item
  \textbf{Common cause:} healthier / richer parents have healthier
  babies and are more likely to breastfeed
\end{enumerate}

We cannot tell which explanations are true (or to what extent) just by
observing (\(x\),\(y\)) pairs

Causal questions implicitly involve \textbf{counterfactuals}:

\begin{itemize}
\tightlist
\item
  \textbf{Would} the \textbf{same} breastfed babies have been less
  healthy \textbf{if} they hadn't been breastfed?
\item
  Explanation 1 implies they would be, explanations 2 and 3 do not
\end{itemize}

\hypertarget{confounders}{%
\subsection{Confounders}\label{confounders}}

A \textbf{confounder} is a variable that affects both T and Y,
distorting the correlation between them (e.g.~rich parents →
breastfeeding, baby's health) Can be measured covariate, or unmeasured
variable we don't know about

Confounders generally cause problems

\textbf{Common assumption:} all confounders are observed
(\textbf{ignorability})

\hypertarget{terminology}{%
\subsection{Terminology}\label{terminology}}

Let us define some terms that will help us understand causal effects.

In prediction, we had two kinds of variables

\begin{itemize}
\tightlist
\item
  \textbf{Response} (\(Y\)): what we are trying to predict
\item
  \textbf{Predictors} (\(X\)): inputs to our prediction
\end{itemize}

More kinds of variables in causal inference

\begin{itemize}
\tightlist
\item
  \textbf{Response} (\(Y\)): the outcome of interest
\item
  \textbf{Treatment} (\(T\)): the variable we might intervene on
\item
  \textbf{Covariate} (\(X\)): other variables we measured that may
  affect \(T\) and/or \(Y\)
\end{itemize}

For this lecture, \(T\) is a \textbf{binary (0/1)} variable:

\begin{itemize}
\tightlist
\item
  \(T=1\) is usually called treatment, and \(T=0\) is usually called
  control
\item
  e.g., breastfed or not, smoker or not
\end{itemize}

\hypertarget{neyman-rubin-causal-model}{%
\subsection{Neyman-Rubin Causal Model}\label{neyman-rubin-causal-model}}

Causal questions are about \textbf{counterfactuals}:

\begin{itemize}
\tightlist
\item
  What would have happened if T were different?
\item
  What will happen if we set T differently in the future?
\end{itemize}

We assume every individual has two \textbf{potential outcomes}

\begin{itemize}
\tightlist
\item
  \(Y_{i}(1)\): value of \(y_{i}\) if \(T_{i} = 1\) (\textbf{treated
  outcome})
\item
  \(Y_{i}(0)\): value of \(y_{i}\) if \(T_{i} = 0\) (\textbf{control
  outcome})
\end{itemize}

For each individual in the data set, we observe:

\begin{itemize}
\tightlist
\item
  Covariates \(x_{i}\)
\item
  Treatment \(T_{i}\)
\item
  Response \(y_{i} = Y_{i}(T_{i})\)
\end{itemize}

We will assume (\(x_{i}\), \(T_{i}\), \(y_{i} = Y_{i}(T_{i})\)) tuples
iid for \(i = 1,..., n\)

\hypertarget{average-treatment-effect}{%
\subsection{Average Treatment Effect}\label{average-treatment-effect}}

For each individual, the \textbf{treatment effect} is
\(Y_{i}(1)-Y_{i}(0)\)

Most common thing to estimate is the \textbf{Average Treatment Effect
(ATE)}

\[ATE = \mathbb{E}[Y(1)-Y(0)] = \mathbb{E}[Y(1)] - \mathbb{E}[Y(0)]\]

Can we just take the sample mean?

\[\hat{ATE} = \frac{1}{n}\sum_{i=1}^{n}Y_{i}(1) - Y_{i}(0)\]

We cannot. Why? We only observe one of \(Y_{i}(1)\), \(Y_{i}(0)\).

\textbf{Fundamental problem of causal inference:} We only ever observe
one potential outcome

To draw causal conclusions, we need some causal assumption relating the
observed to the unobserved units

Instead of \(\frac{1}{n}\sum_{i=1}^{n}Y_{i}(1) - Y_{i}(0)\), what if we
took the difference between the sample mean for each group?

\[\hat{ATE} = \frac{1}{n_{1}}\sum_{i: T_{i} = 1}{Y_{i}(1)} - \frac{1}{n_{0}}\sum_{i: T_{i} = 0}{Y_{i}(0)} = \frac{1}{n_{1}}\sum_{i: T_{i} = 1}{y_{i}} - \frac{1}{n_{0}}\sum_{i: T_{i} = 0}{y_{i}}\]

Is this estimator of \(ATE\) unbiased? Thus, this proposed \(\hat{ATE}\)
is not suitable for our purposes.

if treatment assignment comes from random coin flips, then the treated
units are an iid random sample of size \(n_{1}\) from the population of
\(Y_{i}(1)\).

This means that,

\[\mathbb{E}[\frac{1}{n_{1}}\sum_{i: T_{i} = 1}{y_{i}}] = \mathbb{E}[Y_{i}(1)]\]

Similarly,

\[\mathbb{E}[\frac{1}{n_{0}}\sum_{i: T_{i} = 0}{y_{i}}] = \mathbb{E}[Y_{i}(0)]\]

which allows us to conclude that \(\hat{ATE}\) is an unbiased estimator
of \(ATE\):

\[\mathbb{E}[\hat{ATE}] = ATE\]

\hypertarget{randomized-experiments}{%
\subsection{Randomized Experiments}\label{randomized-experiments}}

However, often, randomy assigning treatments is impractical or
unethical. For example, assigning a treatment of cigarettes would likely
be impractical and unethical.

An alternative to bypass this issue is to utilize \textbf{observational
studies}.

Experiments:

Observational Study:

\hypertarget{covariate-adjustment}{%
\section{Covariate Adjustment}\label{covariate-adjustment}}

What to do about confounders?

\begin{itemize}
\tightlist
\item
  \textbf{Ignorability assumption:} all important confounders are in the
  data set!
\end{itemize}

\textbf{One idea:} come up with a model that includes them, such as:

\[Y_{i}(t) = \theta_{0} + \theta_{1}x_{1} + ... + \theta_{p}x_{p} + \tau{t} + \epsilon\]

\textbf{Question:} what is the \(ATE\) in this model? \(\tau\)

Approach can work but is \textbf{fragile}. Breaks if:

\begin{itemize}
\tightlist
\item
  Important covariates are missing or true dependence on \(x\) is
  nonlinear
\item
  Sometimes pejoratively called \textbf{``casual inference''}
\end{itemize}

\hypertarget{covariate-adjustment-without-parametric-assumptions}{%
\subsection{Covariate adjustment without parametric
assumptions}\label{covariate-adjustment-without-parametric-assumptions}}

What to do about confounders?

\begin{itemize}
\tightlist
\item
  \textbf{Ignorability assumption:} all possible confounders are in the
  data set!
\end{itemize}

\textbf{One idea:} come up with a model that includes them, such as:

\[Y_{i}(t) = f_{\theta}(x, t) + \epsilon\]

Then:

\[ATE = \frac{1}{n}\sum_{i=1}^{n}{f_{\theta}(x_i, 1) - f_{\theta}(x_i, 0)}\]

With enough data, we may be able to learn \(f_{\theta}\) very accurately

\begin{itemize}
\tightlist
\item
  Very difficult if x is high-dimensional / functional form is highly
  nonlinear
\item
  Need additional assumption: \textbf{overlap}
\end{itemize}

\hypertarget{other-methods}{%
\subsection{Other Methods}\label{other-methods}}

Causal inference is hard, and covariate adjustment is often not the best
approach

Many other methods doing some combination of:

\begin{itemize}
\tightlist
\item
  Modeling treatment T as a function of covariates x
\item
  Modeling the outcome y as a function of x, T
\end{itemize}

What if we don't believe in ignorability? Other methods look for a

\begin{itemize}
\tightlist
\item
  Favorite example: \textbf{regression discontinuity}
\end{itemize}



\end{document}
