<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Decision Trees (from Spring 2025) – Data 100: Principles and Techniques of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-42ed7287b56c5c583e2c2463c9cf4916.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link active" data-scroll-target="#multi-class-classification">Multi-class Classification</a></li>
  <li><a href="#decision-trees-conceptually" id="toc-decision-trees-conceptually" class="nav-link" data-scroll-target="#decision-trees-conceptually">Decision Trees, conceptually</a>
  <ul>
  <li><a href="#decision-trees-in-sklearn" id="toc-decision-trees-in-sklearn" class="nav-link" data-scroll-target="#decision-trees-in-sklearn">Decision Trees in <code>sklearn</code></a></li>
  <li><a href="#evaluating-tree-accuracy" id="toc-evaluating-tree-accuracy" class="nav-link" data-scroll-target="#evaluating-tree-accuracy">Evaluating Tree Accuracy</a></li>
  <li><a href="#overfitting-tree-example" id="toc-overfitting-tree-example" class="nav-link" data-scroll-target="#overfitting-tree-example">Overfitting Tree Example</a></li>
  </ul></li>
  <li><a href="#decision-tree-generation-algorithm" id="toc-decision-tree-generation-algorithm" class="nav-link" data-scroll-target="#decision-tree-generation-algorithm">Decision Tree Generation Algorithm</a>
  <ul>
  <li><a href="#entropy" id="toc-entropy" class="nav-link" data-scroll-target="#entropy">Entropy</a></li>
  <li><a href="#generating-trees-with-entropy" id="toc-generating-trees-with-entropy" class="nav-link" data-scroll-target="#generating-trees-with-entropy">Generating Trees with Entropy</a></li>
  </ul></li>
  <li><a href="#avoiding-overfitting" id="toc-avoiding-overfitting" class="nav-link" data-scroll-target="#avoiding-overfitting">Avoiding Overfitting</a>
  <ul>
  <li><a href="#restricting-tree-complexity" id="toc-restricting-tree-complexity" class="nav-link" data-scroll-target="#restricting-tree-complexity">Restricting Tree Complexity</a>
  <ul>
  <li><a href="#preventing-growth" id="toc-preventing-growth" class="nav-link" data-scroll-target="#preventing-growth">Preventing Growth</a></li>
  <li><a href="#pruning" id="toc-pruning" class="nav-link" data-scroll-target="#pruning">Pruning</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest">Random Forest</a></li>
  </ul></li>
  <li><a href="#summary-and-context" id="toc-summary-and-context" class="nav-link" data-scroll-target="#summary-and-context">Summary and Context</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Decision Trees (from Spring 2025)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Outcomes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Understand how to generate a decision tree</li>
<li>Implement decision tree using <code>sklearn</code></li>
<li>Understand ways to improve decision trees, including random forests</li>
</ul>
</div>
</div>
<div id="6f2233d9" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<p>In our second pass of the data science lifecycle, we’ve introduced new tools for data processing and exploratory data analysis. Now let’s turn our attention to modeling. Today, we will introduce a new model called decision trees.</p>
<section id="multi-class-classification" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification">Multi-class Classification</h2>
<p>In our discussion about logistics regression, we mainly focused on the task of “binary classification” where we try to distinguish between two classes of labels (e.g., “spam” or “ham”). In real life though, we often have multi-class labels. For example, the Fashion-MNIST dataset we worked with last week have multiple classes of clothing items. How would we classify this kind of data?</p>
<p>As an example, let’s consider a dataset about the iris flowers. The dataset consists of 150 flower measurements from 3 different species: setosa, versicolor, and virginica. For each, we have measurements of the flower’s <code>“petal length”</code>, <code>“petal width”</code>, <code>“sepal length”</code>, and <code>“sepal width”</code>.</p>
<div id="181b836f" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>iris_data <span class="op">=</span> pd.read_csv(<span class="st">"data/iris.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>iris_data.sample(<span class="dv">5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">73</th>
<td>6.1</td>
<td>2.8</td>
<td>4.7</td>
<td>1.2</td>
<td>versicolor</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">8</th>
<td>4.4</td>
<td>2.9</td>
<td>1.4</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">141</th>
<td>6.9</td>
<td>3.1</td>
<td>5.1</td>
<td>2.3</td>
<td>virginica</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">76</th>
<td>6.8</td>
<td>2.8</td>
<td>4.8</td>
<td>1.4</td>
<td>versicolor</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">99</th>
<td>5.7</td>
<td>2.8</td>
<td>4.1</td>
<td>1.3</td>
<td>versicolor</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The goal today is to use these four measurements to try and predict the <code>species</code> of the iris flower.</p>
<p>Let’s see how we can use logistic regression to classify this dataset with three classes. To do this, we will use the petal measurements: <code>petal_length</code> and <code>petal_width</code>:</p>
<div id="1633fcc8" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> LogisticRegression(multi_class <span class="op">=</span> <span class="st">'ovr'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> logistic_regression_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                                                          iris_data[<span class="st">"species"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Notice when constructing the classifier, we specified <code>multi_class = 'ovr'</code>.</p>
<p><code>ovr</code> means “one-vs.-rest”, which means <code>sklearn</code> trains three logistic regression binary classifiers:</p>
<ul>
<li>Classifier 1: Setosa or not.</li>
<li>Classifier 2: Versicolor or not.</li>
<li>Classifier 3: Virginica or not.</li>
</ul>
<p>Then the <code>model.predict()</code> function will output the classifier that returns the highest probability:</p>
<div id="8af774ce" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>logistic_regression_model.predict([[<span class="fl">1.4</span>, <span class="fl">0.2</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array(['setosa'], dtype=object)</code></pre>
</div>
</div>
<p>We can also visualize the decision boundary of this classifier. As we can see, each of three classifiers still has linear decision boundaries, but there are now three lines instead of 1.</p>
<div id="8aa727eb" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> logistic_regression_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">7</span>)<span class="op">;</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">2.8</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-6-output-1.png" width="617" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It turns out there are other (arguably) better models to use when it comes to multi-class classification.</p>
<p>In Data 100 so far, we’ve been talking about linear models. In Linear regression, the output is the weighted sum of the features. In logistic regression, the output is <span class="math inline">\(\sigma\)</span>(the weighted sum of the features).</p>
<p>Today, we are going into the territory of non-linear models. There are many non-linear models, decision tree is one of them.</p>
</section>
<section id="decision-trees-conceptually" class="level2">
<h2 class="anchored" data-anchor-id="decision-trees-conceptually">Decision Trees, conceptually</h2>
<p>A decision tree is simply a tree of questions that must be answered in a sequence to yield a predicted classification. For example, below is a classifier for animals. It starts by asking how many legs the animal has. If the answer is no, we immediately say snake. If the answer is four, the model is not quite sure, so we must ask an additional question: does it purr? If the answer is yes, the animal is a cat; and if the answer is no, the animal is a dog. The other possibility is that the number of legs was two, in that case we have up to two additional questions we may need to ask before eventually deciding if it is a kangaroo, human, or parrot. Obviously this decision tree does not cover all animals. In principle, with the right sequence of questions, we could build a giant decision tree that could differentiate a vast number of animals.</p>
<p><img src="images/animal_tree.png" alt="Example of a decision tree" width="600"></p>
<p>For our iris dataset, we can come up with a decision tree just by looking at the points. It’s important to note in the following decision tree we use multiple features in a single node to make the tree simpler, but in reality decision tree will only make a split based on one feature at a time. We will see an example of an actual decision tree in the next section.</p>
<p><img src="images/manual_tree.png" alt="Example of a decision tree on the iris data" width="600"></p>
<p>Just by looking at the tree, it seems we reached 100% accuracy on this dataset. However, this should call for some concerns about potential overfitting.</p>
<section id="decision-trees-in-sklearn" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees-in-sklearn">Decision Trees in <code>sklearn</code></h3>
<p>We can implement a decision tree easily using <code>sklearn</code>. Like other models we’ve been working with in this class so far, we would create an instance of the model, fit it on our training data, and use it to make predictions.</p>
<div id="efe3d345" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], iris_data[<span class="st">"species"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>You might noticed that we specified <code>criterion = 'entropy'</code> when instantiating the model. We will talk more about “entropy” in a later section.</p>
<p>We can use the fitted decision tree model to predict four random points in our original data:</p>
<div id="f31be919" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>four_random_rows <span class="op">=</span> iris_data.sample(<span class="dv">4</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>four_random_rows</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">91</th>
<td>6.1</td>
<td>3.0</td>
<td>4.6</td>
<td>1.4</td>
<td>versicolor</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">98</th>
<td>5.1</td>
<td>2.5</td>
<td>3.0</td>
<td>1.1</td>
<td>versicolor</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">27</th>
<td>5.2</td>
<td>3.5</td>
<td>1.5</td>
<td>0.2</td>
<td>setosa</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">41</th>
<td>4.5</td>
<td>2.3</td>
<td>1.3</td>
<td>0.3</td>
<td>setosa</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="cbe4c728" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>decision_tree_model.predict(four_random_rows[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array(['versicolor', 'versicolor', 'setosa', 'setosa'], dtype=object)</code></pre>
</div>
</div>
<p>We can also visualize how the decision tree is constructed using a package called <code>GraphViz</code>.</p>
<div id="760da650" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>],  </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>graph</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-10-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In each box/node, we see:</p>
<ul>
<li>The decision rule applied to that node.</li>
<li>The entropy at that node (more later).</li>
<li>The number of samples that remain after applying all of the rules above this node.</li>
<li>The number of samples that remain in each class.</li>
<li>The most likely class. If there is a tie, this is the class that comes first.</li>
</ul>
<p>This visualization allows us to see exactly how the decision tree classifies each point. For example, if the <code>petal_length</code> of an iris flower is less than or equal to 1.75, we would classify it as <code>setosa</code>.</p>
<p>Like with the logistic regression model earlier, we can visualize the decision boundary of this tree.</p>
<div id="d08201c9" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-11-output-1.png" width="611" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Decision tree has nonlinear boundary, and appears to get 100% accuracy. But did we actually get 100% accuracy? Let’s check!</p>
</section>
<section id="evaluating-tree-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-tree-accuracy">Evaluating Tree Accuracy</h3>
<p>To get the accuracy of a decision tree, we use the <code>accuracy_score</code> method implemented in <code>sklearn</code>.</p>
<div id="eb2366bd" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> decision_tree_model.predict(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>accuracy_score(predictions, iris_data[<span class="st">"species"</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.9933333333333333</code></pre>
</div>
</div>
<p>Hmm, it looks like we didn’t get a perfect accuracy on the dataset. What happened?</p>
<div class="columns">
<div class="column" style="width:55%;">
<p>To understand why, let’s look back at the decision tree from earlier. In particular, let’s look at one particular node, zoomed in on the right.</p>
<p>It looks like this node has three samples yet to be classified, with 1 versicolor and 2 virginicas, but the decision tree stopped here without splitting any further. This results in us classifying the one versicolor wrong.</p>
<p>Why did the decision tree stop? Let’s check these three points by following the decision tree.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:40%;">
<p><img src="images/problematic_node.png" alt="A node where decision tree failed" width="300"></p>
</div>
</div>
<div id="62d5c2ba" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>iris_data[(iris_data[<span class="st">"petal_length"</span>]<span class="op">&gt;</span> <span class="fl">2.45</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_width"</span>]<span class="op">&gt;</span> <span class="fl">1.75</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_length"</span>]<span class="op">&lt;=</span><span class="fl">4.85</span>)]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">70</th>
<td>5.9</td>
<td>3.2</td>
<td>4.8</td>
<td>1.8</td>
<td>versicolor</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">126</th>
<td>6.2</td>
<td>2.8</td>
<td>4.8</td>
<td>1.8</td>
<td>virginica</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">138</th>
<td>6.0</td>
<td>3.0</td>
<td>4.8</td>
<td>1.8</td>
<td>virginica</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>It turns out, because we only used <code>petal_length</code> and <code>petal_width</code> to fit the decision tree, and these three points have the exact same measurements for <code>petal_length</code> and <code>petal_width</code>, decision tree was not able to come up with a decision rule to distinguish these three data points!</p>
<p>In general, decision trees (when unrestricted) will always have a perfect accuracy on the training data, <strong>except</strong> when there are samples from different categories with the exact same features. For example, if the versicolor above has a <code>petal_length</code> of 4.8001, we’d have 100% training accuracy.</p>
<p>Again, this tendency for perfect accuracy should give us concern about overfitting. As a result of trying to perfectly fit the training data, decision tree models are extremely sensitive to new data points and therefore have high variance.</p>
</section>
<section id="overfitting-tree-example" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-tree-example">Overfitting Tree Example</h3>
<p>Let’s see an example where a decision tree clearly overfits.</p>
<p>Instead of the <code>petal</code> measurements, let’s use the <code>sepal</code> measurements to train the decision tree. Here’s a scatterplot of the dataset with the <code>sepal</code> measurements. We can see the virginicas are much more intertwined with versicolors, compared to the plot with the <code>petal</code> measurements.</p>
<div id="45461e54" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-14-output-1.png" width="611" height="454" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The decision tree fitted with these two features is very complex, as visualized below. We can see that the tree makes some extremely specific decision rules trying to perfectly capture the details in the dataset.</p>
<div id="86df030f" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">"entropy"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>]], iris_data[<span class="st">"species"</span>])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">4</span>, <span class="dv">8</span>, <span class="fl">0.02</span>),</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="fl">1.9</span>, <span class="fl">4.5</span>, <span class="fl">0.02</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> sepal_decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">False</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-15-output-1.png" width="611" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We can see the actual tree looks very complicated.</p>
<div id="85cb4283" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(sepal_decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>],  </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>graph2 <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>graph2</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<figure class="figure">
<p><img src="decision_tree_files/figure-html/cell-16-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If we set aside a validation set and check the accuracy of this decision tree on the validation set, we will get around 0.65. This is an indication that we are overfitting.</p>
<p>How does this happened? Is there any way we can reduce overfitting? To understand this, we first need to understand how a decision tree is generated algorithmically.</p>
</section>
</section>
<section id="decision-tree-generation-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="decision-tree-generation-algorithm">Decision Tree Generation Algorithm</h2>
<p>To see how decision tree works, we first need to define some new metrics.</p>
<section id="entropy" class="level3">
<h3 class="anchored" data-anchor-id="entropy">Entropy</h3>
<p>Let <span class="math inline">\(p_C\)</span> be the proportion of data points in a node belonging to class <span class="math inline">\(C\)</span>. For example, for the node at the top of the decision tree we saw earlier, there are 38 setosas, 37 versicolors, and 35 virginicas. So</p>
<p><span class="math display">\[
\begin{align*}
&amp;p_0 = 38/110 \approx 0.35\\
&amp;p_1 = 37/110 \approx 0.34\\
&amp;p_2 = 35/110 \approx 0.32.
\end{align*}
\]</span></p>
<p>With this, the <strong>entropy</strong> <span class="math inline">\(S\)</span> of a node is defined as</p>
<p><span class="math display">\[
\Large
S = -\sum_{C}p_C \log_{2}(p_C).
\]</span></p>
<p>Notice we are using <span class="math inline">\(\log\)</span> of base 2 here. This is the only place in the class we are using this particular <span class="math inline">\(\log\)</span>; everywhere else <span class="math inline">\(\log\)</span> has base <span class="math inline">\(e\)</span>.</p>
<p>The term “entropy” comes from information theory and it measure how “unpredictable” or “chaotic” a system is. The bigger the entropy, the more unpredictable a node is. In particular,</p>
<ul>
<li>A node where all data are part of the same class has zero entropy.
<ul>
<li><span class="math inline">\(−1 \log_2⁡ 1 = 0\)</span> bits.</li>
</ul></li>
<li>A node where data are evenly split between two classes has entropy 1.
<ul>
<li><span class="math inline">\(−0.5 \log_2⁡ 0.5 − 0.5 \log_2⁡ 0.5 = 1\)</span> bit.</li>
</ul></li>
<li>A node where data are evenly split between 3 classes has entropy 1.58.
<ul>
<li><span class="math inline">\(3 × (−0.33 \log_2⁡ 0.33) = 1.58\)</span> bits.</li>
</ul></li>
<li>A node where data are evenly split into C classes has entropy <span class="math inline">\(\log_2 C\)</span>.
<ul>
<li><span class="math inline">\(C × (−1/C \log_2⁡ 1/C) = −\log_2⁡ 1/C = \log_2⁡ C\)</span> bits.</li>
</ul></li>
</ul>
</section>
<section id="generating-trees-with-entropy" class="level3">
<h3 class="anchored" data-anchor-id="generating-trees-with-entropy">Generating Trees with Entropy</h3>
<p>How do we use entropy to generate a decision tree?</p>
<p>Decision trees are comprised of many different <em>splits</em>, so to generate a tree, we need to be able to come up with optimal splits given a node. To measure how good a split is, we use the <strong>weighted entropy</strong>.</p>
<p>Suppose a given split results in two nodes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> number of samples each. The loss (or <strong>weighted entropy</strong>) of that split is given by:</p>
<p><span class="math display">\[
\Large
L = \frac{N_{1} S(X) + N_{2} S(Y)}{N_{1} + N_{2}}
\]</span></p>
<p>where <span class="math inline">\(S(X)\)</span> and <span class="math inline">\(S(Y)\)</span> are the entropies of nodes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Intuitively, the weighted entropy of a split measures how unpredictable the result of the split is. We want to make the resulting nodes as predictable as possible, therefore we need to minimize the weighted entropy. In decision trees, the way we minimize the weighted entropy of a split is to try out different combinations of features and splitting values.</p>
<p>Now we have everything we need to state the decision tree generation algorithm.</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Decision Tree Generation Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>All of the data starts in the root node</li>
<li>Repeat the following until every node is either <em>pure</em> or <em>unsplittable</em>:
<ul>
<li>Pick the <em>best feature</em> <span class="math inline">\(x\)</span> and <em>best split value</em> <span class="math inline">\(\beta\)</span> such that the <strong>weighted entropy</strong> of the resulting split is minimized, e.g.&nbsp;<span class="math inline">\(x\)</span> = <code>petal_width</code>, <span class="math inline">\(\beta = 0.8\)</span> has a loss of <span class="math inline">\(0.66\)</span></li>
<li>Split data into two nodes, one where <span class="math inline">\(x \leq \beta\)</span>, and one where <span class="math inline">\(x &gt; \beta\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
<p>What do we mean by “pure” or “unsplittable”?</p>
<ul>
<li>A node that has only samples from one class is called a “<strong>pure</strong>” node.</li>
<li>A node that has overlapping data points from different classes and thus that cannot be split is called “<strong>unsplittable</strong>”. We saw an example of this in our first decision tree, fitted with <code>petal_length</code> and <code>petal_width</code>.</li>
</ul>
</section>
</section>
<section id="avoiding-overfitting" class="level2">
<h2 class="anchored" data-anchor-id="avoiding-overfitting">Avoiding Overfitting</h2>
<p>Now that we have an idea how decision tree works, let’s turn our attention to the issue of overfitting and discuss ways we can avoid or reduce overfitting with decision trees.</p>
<section id="restricting-tree-complexity" class="level3">
<h3 class="anchored" data-anchor-id="restricting-tree-complexity">Restricting Tree Complexity</h3>
<p>A “fully grown” decision tree built with our algorithm runs the risk of overfitting. In linear models, we introduced the notion of “regularization”. However, regularization doesn’t make sense in the decision tree context as decision trees don’t have parameters that act as weights, so we can’t use L1 or L2 regularization. We have to come up with new ideas.</p>
<p>The first category of ideas is to disallow fully grown trees.</p>
<section id="preventing-growth" class="level4">
<h4 class="anchored" data-anchor-id="preventing-growth">Preventing Growth</h4>
<p>Since decision trees tend to come up with extremely specific rules to attempt to fit the training data perfectly, we can set one or more special rules to prevent the tree from fully growing.</p>
<p>Examples of this include:</p>
<ul>
<li>Don’t split nodes with &lt; 1% of the samples.
<ul>
<li>In <code>sklearn</code>, this hyperparameter is called “<code>min_samples_split</code>”.</li>
</ul></li>
<li>Don’t allow nodes to be more than 7 levels deep in the tree.
<ul>
<li>In <code>sklearn</code>, this hyperparameter is called “<code>max_depth</code>”.</li>
</ul></li>
</ul>
</section>
<section id="pruning" class="level4">
<h4 class="anchored" data-anchor-id="pruning">Pruning</h4>
<p>Another approach is to allow the tree to fully grow when fitting it, then cut off the less useful branches of the tree. In pruning the tree, we are usually looking for rules that affect only a small subset of points.</p>
<p>There are many ways to prune trees. One way is to use a validation set to see if keeping a given branch is worth it. Specifically, we run our model on the validation set twice, once keeping the branch, and the second time replacing the branch by its most common prediction. If there’s no or minimal impact on the validation error, then we delete the split. The idea here is that the split was not useful on unseen data.</p>
<p>Recall that when a node has multiple samples in it from different classes, the <code>predict</code> method will simply return the most common class, and the <code>predict_proba</code> method will return the fraction of samples that belong to each class.</p>
</section>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">Random Forest</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>As we’ve seen earlier, a fully-grown decision tree will almost always overfit the data. It has low model bias, but high model variance. In other words, small changes in the dataset will result in very a different decision tree. As an example, the two models below are trained on different subsets of the same data. We can see they come out very differently.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<p><img src="images/two_trees.png" alt="Two trees fitted on different subset of the same dataset" width="400"></p>
</div>
</div>
<p>The idea of random forest is to harness this variance: build many decision trees and take the majority vote.</p>
<p>To build multiple decision trees using our training data, we use our old friend bootstrap. In particular, this method is called “<strong>bagging</strong>”, which stands for Bootstrap Aggregating. In bagging, we</p>
<ul>
<li>Generate bootstrap resamples of training data.</li>
<li>Fit one model for each resample.</li>
<li>Build the final model using the average predictions of each small model.</li>
</ul>
<p>However, bagging is usually not enough to reduce model variance. In many cases, the different decision trees end up looking very similar to one another and therefore make similar predictions. The resulting model will still have low bias and high variance.</p>
<p>To improve on bagging, we add another layer of randomness by only using a <strong>random subset</strong> of <span class="math inline">\(m\)</span> features at each split. Usually we use <span class="math inline">\(m = \sqrt{p}\)</span> for decision tree classifiers, where <span class="math inline">\(p\)</span> is the total number of features.</p>
<p>The algorithm will create individual trees, each overfit in a different way. The hope is then that the overall forest will have low variance due to aggregation.</p>
<p>Let’s summarize the above by stating the random forest algorithm:</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Random Forest Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Bootstrap the training data <span class="math inline">\(T\)</span> times. For each resample, fit a decision tree by doing the following:
<ul>
<li>Start with data in one node. Repeat the following until all nodes are <em>pure</em> or <em>unsplittable</em>:
<ul>
<li>Pick an impure node</li>
<li>Pick a random subset of <span class="math inline">\(m\)</span> features. Pick the best feature <span class="math inline">\(x\)</span> and best split value <span class="math inline">\(\beta\)</span> such that the weighted entropy of the resulting split is minimized</li>
<li>Split data into two nodes, one where <span class="math inline">\(x \leq \beta\)</span>, and one where <span class="math inline">\(x &gt; \beta\)</span></li>
</ul></li>
</ul></li>
<li>To predict, ask the <span class="math inline">\(T\)</span> decision trees for their predictions and take the majority vote.</li>
</ul>
</div>
</div>
<p>Notice there are two hyperparameters in the algorithm: <span class="math inline">\(T\)</span> and <span class="math inline">\(m\)</span>. To pick the best value, we can use validation or cross-validation.</p>
<p>In this section, we talked about three approaches to address the overfitting issue with decision trees: preventing growth, pruning, and random forest. These ideas are generally “heuristic,” meaning they are not provably the best or mathematically optimal. Instead, they are ideas that somebody thought sounded good, implemented, then found to work in practice acceptably well.</p>
</section>
</section>
<section id="summary-and-context" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-context">Summary and Context</h2>
<p>To summarize what we talked about in this lecture, decision trees present an alternative philosophy to logistic regression: instead of a weighted sum of features, we use a series of yes/no questions.</p>
<p>Some advantages of a decision tree include: - It is able to capture nonlinear relationships (at risk of overfitting). - Its output is more interpretable: we know exactly how the tree made a decision. However, this is not true of random forests.</p>
<p>Some disadvantages of a decision tree include: - Care needs to be taken to avoid overfitting. - It is not as mathematically nice as some linear models (e.g.&nbsp;OLS and Ridge Regression have simple closed-form solutions).</p>
<p>In practice, people use random forests more often than decision trees because they preform much better on unseen data in general. A random forest also has the following advantages:</p>
<ul>
<li>Versatile: does both regression and classification.</li>
<li>Invariant to feature scaling and translation.</li>
<li>It can perform automatic feature selection.</li>
<li>It has nonlinear decision boundaries without complicated feature engineering.</li>
<li>It doesn’t overfit as often as other nonlinear models (e.g.&nbsp;polynomial features, decision trees).</li>
</ul>
<p>Random forest is an example of <strong>ensemble method</strong>, where we combine the knowledge of many simple models to make a sophisticated model. It is also an example of using bootstrap to reduce model variance.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ds100\.org\/course-notes\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>