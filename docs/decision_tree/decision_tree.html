<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 24&nbsp; Decision Trees</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../pca_2/pca_2.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../decision_tree/decision_tree.html"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Decision Trees</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes-su23" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link active">
<span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causal Inference and Confounding</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
</div>
</li>
<li class="sidebar-item">
<div class="sidebar-item-container"> 
<a href="./decision_tree/decision_tree.html" class="sidebar-item-text sidebar-link">
<span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
</div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Decision Trees</h2>
   
  <ul>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link active" data-scroll-target="#multi-class-classification"><span class="header-section-number">24.1</span> Multi-class Classification</a></li>
  <li><a href="#decision-trees-conceptually" id="toc-decision-trees-conceptually" class="nav-link" data-scroll-target="#decision-trees-conceptually"><span class="header-section-number">24.2</span> Decision Trees, conceptually</a>
  <ul>
  <li><a href="#decision-trees-in-sklearn" id="toc-decision-trees-in-sklearn" class="nav-link" data-scroll-target="#decision-trees-in-sklearn"><span class="header-section-number">24.2.1</span> Decision Trees in <code>sklearn</code></a></li>
  <li><a href="#evaluating-tree-accuracy" id="toc-evaluating-tree-accuracy" class="nav-link" data-scroll-target="#evaluating-tree-accuracy"><span class="header-section-number">24.2.2</span> Evaluating Tree Accuracy</a></li>
  <li><a href="#overfitting-tree-example" id="toc-overfitting-tree-example" class="nav-link" data-scroll-target="#overfitting-tree-example"><span class="header-section-number">24.2.3</span> Overfitting Tree Example</a></li>
  </ul></li>
  <li><a href="#decision-tree-generation-algorithm" id="toc-decision-tree-generation-algorithm" class="nav-link" data-scroll-target="#decision-tree-generation-algorithm"><span class="header-section-number">24.3</span> Decision Tree Generation Algorithm</a>
  <ul>
  <li><a href="#entropy" id="toc-entropy" class="nav-link" data-scroll-target="#entropy"><span class="header-section-number">24.3.1</span> Entropy</a></li>
  <li><a href="#generating-trees-with-entropy" id="toc-generating-trees-with-entropy" class="nav-link" data-scroll-target="#generating-trees-with-entropy"><span class="header-section-number">24.3.2</span> Generating Trees with Entropy</a></li>
  </ul></li>
  <li><a href="#avoiding-overfitting" id="toc-avoiding-overfitting" class="nav-link" data-scroll-target="#avoiding-overfitting"><span class="header-section-number">24.4</span> Avoiding Overfitting</a>
  <ul>
  <li><a href="#restricting-tree-complexity" id="toc-restricting-tree-complexity" class="nav-link" data-scroll-target="#restricting-tree-complexity"><span class="header-section-number">24.4.1</span> Restricting Tree Complexity</a>
  <ul>
  <li><a href="#preventing-growth" id="toc-preventing-growth" class="nav-link" data-scroll-target="#preventing-growth"><span class="header-section-number">24.4.1.1</span> Preventing Growth</a></li>
  <li><a href="#pruning" id="toc-pruning" class="nav-link" data-scroll-target="#pruning"><span class="header-section-number">24.4.1.2</span> Pruning</a></li>
  </ul></li>
  <li><a href="#random-forest" id="toc-random-forest" class="nav-link" data-scroll-target="#random-forest"><span class="header-section-number">24.4.2</span> Random Forest</a></li>
  </ul></li>
  <li><a href="#summary-and-context" id="toc-summary-and-context" class="nav-link" data-scroll-target="#summary-and-context"><span class="header-section-number">24.5</span> Summary and Context</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Decision Trees</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Understand how to generate a decision tree</li>
<li>Implement decision tree using <code>sklearn</code></li>
<li>Understand ways to improve decision trees, including random forests</li>
</ul>
</div>
</div>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In our second pass of the data science lifecycle, we’ve introduced new tools for data processing and exploratory data analysis. Now let’s turn our attention to modeling. Today, we will introduce a new model called decision trees.</p>
<section id="multi-class-classification" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="multi-class-classification"><span class="header-section-number">24.1</span> Multi-class Classification</h2>
<p>In our discussion about logistics regression, we mainly focused on the task of “binary classification” where we try to distinguish between two classes of labels (e.g., “spam” or “ham”). In real life though, we often have multi-class labels. For example, the Fashion-MNIST dataset we worked with last week have multiple classes of clothing items. How would we classify this kind of data?</p>
<p>As an example, let’s consider a dataset about the iris flowers. The dataset consists of 150 flower measurements from 3 different species: setosa, versicolor, and virginica. For each, we have measurements of the flower’s <code>“petal length”</code>, <code>“petal width”</code>, <code>“sepal length”</code>, and <code>“sepal width”</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>iris_data <span class="op">=</span> pd.read_csv(<span class="st">"data/iris.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>iris_data.sample(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">15</td>
<td>5.7</td>
<td>4.4</td>
<td>1.5</td>
<td>0.4</td>
<td>setosa</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">112</td>
<td>6.8</td>
<td>3.0</td>
<td>5.5</td>
<td>2.1</td>
<td>virginica</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">128</td>
<td>6.4</td>
<td>2.8</td>
<td>5.6</td>
<td>2.1</td>
<td>virginica</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">135</td>
<td>7.7</td>
<td>3.0</td>
<td>6.1</td>
<td>2.3</td>
<td>virginica</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">52</td>
<td>6.9</td>
<td>3.1</td>
<td>4.9</td>
<td>1.5</td>
<td>versicolor</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The goal today is to use these four measurements to try and predict the <code>species</code> of the iris flower.</p>
<p>Let’s see how we can use logistic regression to classify this dataset with three classes. To do this, we will use the petal measurements: <code>petal_length</code> and <code>petal_width</code>:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> LogisticRegression(multi_class <span class="op">=</span> <span class="st">'ovr'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> logistic_regression_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                                                          iris_data[<span class="st">"species"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice when constructing the classifier, we specified <code>multi_class = 'ovr'</code>.</p>
<p><code>ovr</code> means “one-vs.-rest”, which means <code>sklearn</code> trains three logistic regression binary classifiers:</p>
<ul>
<li>Classifier 1: Setosa or not.</li>
<li>Classifier 2: Versicolor or not.</li>
<li>Classifier 3: Virginica or not.</li>
</ul>
<p>Then the <code>model.predict()</code> function will output the classifier that returns the highest probability:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>logistic_regression_model.predict([[<span class="fl">1.4</span>, <span class="fl">0.2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array(['setosa'], dtype=object)</code></pre>
</div>
</div>
<p>We can also visualize the decision boundary of this classifier. As we can see, each of three classifiers still has linear decision boundaries, but there are now three lines instead of 1.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> logistic_regression_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">7</span>)<span class="op">;</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">2.8</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-6-output-1.png" width="617" height="450"></p>
</div>
</div>
<p>It turns out there are other (arguably) better models to use when it comes to multi-class classification.</p>
<p>In Data 100 so far, we’ve been talking about linear models. In Linear regression, the output is the weighted sum of the features. In logistic regression, the output is <span class="math inline">\(\sigma\)</span>(the weighted sum of the features).</p>
<p>Today, we are going into the territory of non-linear models. There are many non-linear models, decision tree is one of them.</p>
</section>
<section id="decision-trees-conceptually" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="decision-trees-conceptually"><span class="header-section-number">24.2</span> Decision Trees, conceptually</h2>
<p>A decision tree is simply a tree of questions that must be answered in a sequence to yield a predicted classification. For example, below is a classifier for animals. It starts by asking how many legs the animal has. If the answer is no, we immediately say snake. If the answer is four, the model is not quite sure, so we must ask an additional question: does it purr? If the answer is yes, the animal is a cat; and if the answer is no, the animal is a dog. The other possibility is that the number of legs was two, in that case we have up to two additional questions we may need to ask before eventually deciding if it is a kangaroo, human, or parrot. Obviously this decision tree does not cover all animals. In principle, with the right sequence of questions, we could build a giant decision tree that could differentiate a vast number of animals.</p>
<p><img src="images/animal_tree.png" alt="Example of a decision tree" width="600"></p>
<p>For our iris dataset, we can come up with a decision tree just by looking at the points. It’s important to note in the following decision tree we use multiple features in a single node to make the tree simpler, but in reality decision tree will only make a split based on one feature at a time. We will see an example of an actual decision tree in the next section.</p>
<p><img src="images/manual_tree.png" alt="Example of a decision tree on the iris data" width="600"></p>
<p>Just by looking at the tree, it seems we reached 100% accuracy on this dataset. However, this should call for some concerns about potential overfitting.</p>
<section id="decision-trees-in-sklearn" class="level3" data-number="24.2.1">
<h3 data-number="24.2.1" class="anchored" data-anchor-id="decision-trees-in-sklearn"><span class="header-section-number">24.2.1</span> Decision Trees in <code>sklearn</code></h3>
<p>We can implement a decision tree easily using <code>sklearn</code>. Like other models we’ve been working with in this class so far, we would create an instance of the model, fit it on our training data, and use it to make predictions.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], iris_data[<span class="st">"species"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You might noticed that we specified <code>criterion = 'entropy'</code> when instantiating the model. We will talk more about “entropy” in a later section.</p>
<p>We can use the fitted decision tree model to predict four random points in our original data:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>four_random_rows <span class="op">=</span> iris_data.sample(<span class="dv">4</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>four_random_rows</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">117</td>
<td>7.7</td>
<td>3.8</td>
<td>6.7</td>
<td>2.2</td>
<td>virginica</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">91</td>
<td>6.1</td>
<td>3.0</td>
<td>4.6</td>
<td>1.4</td>
<td>versicolor</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">78</td>
<td>6.0</td>
<td>2.9</td>
<td>4.5</td>
<td>1.5</td>
<td>versicolor</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">80</td>
<td>5.5</td>
<td>2.4</td>
<td>3.8</td>
<td>1.1</td>
<td>versicolor</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>decision_tree_model.predict(four_random_rows[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array(['virginica', 'versicolor', 'versicolor', 'versicolor'],
      dtype=object)</code></pre>
</div>
</div>
<p>We can also visualize how the decision tree is constructed using a package called <code>GraphViz</code>.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>],  </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>graph</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<p><img src="decision_tree_files/figure-html/cell-10-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>In each box/node, we see:</p>
<ul>
<li>The decision rule applied to that node.</li>
<li>The entropy at that node (more later).</li>
<li>The number of samples that remain after applying all of the rules above this node.</li>
<li>The number of samples that remain in each class.</li>
<li>The most likely class. If there is a tie, this is the class that comes first.</li>
</ul>
<p>This visualization allows us to see exactly how the decision tree classifies each point. For example, if the <code>petal_length</code> of an iris flower is less than or equal to 1.75, we would classify it as <code>setosa</code>.</p>
<p>Like with the logistic regression model earlier, we can visualize the decision boundary of this tree.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-11-output-1.png" width="611" height="450"></p>
</div>
</div>
<p>Decision tree has nonlinear boundary, and appears to get 100% accuracy. But did we actually get 100% accuracy? Let’s check!</p>
</section>
<section id="evaluating-tree-accuracy" class="level3" data-number="24.2.2">
<h3 data-number="24.2.2" class="anchored" data-anchor-id="evaluating-tree-accuracy"><span class="header-section-number">24.2.2</span> Evaluating Tree Accuracy</h3>
<p>To get the accuracy of a decision tree, we use the <code>accuracy_score</code> method implemented in <code>sklearn</code>.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> decision_tree_model.predict(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>accuracy_score(predictions, iris_data[<span class="st">"species"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>0.9933333333333333</code></pre>
</div>
</div>
<p>Hmm, it looks like we didn’t get a perfect accuracy on the dataset. What happened?</p>
<div class="columns">
<div class="column" style="width:55%;">
<p>To understand why, let’s look back at the decision tree from earlier. In particular, let’s look at one particular node, zoomed in on the right.</p>
<p>It looks like this node has three samples yet to be classified, with 1 versicolor and 2 virginicas, but the decision tree stopped here without splitting any further. This results in us classifying the one versicolor wrong.</p>
<p>Why did the decision tree stop? Let’s check these three points by following the decision tree.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:40%;">
<p><img src="images/problematic_node.png" alt="A node where decision tree failed" width="300"></p>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>iris_data[(iris_data[<span class="st">"petal_length"</span>]<span class="op">&gt;</span> <span class="fl">2.45</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_width"</span>]<span class="op">&gt;</span> <span class="fl">1.75</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_length"</span>]<span class="op">&lt;=</span><span class="fl">4.85</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
<th data-quarto-table-cell-role="th">species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">70</td>
<td>5.9</td>
<td>3.2</td>
<td>4.8</td>
<td>1.8</td>
<td>versicolor</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">126</td>
<td>6.2</td>
<td>2.8</td>
<td>4.8</td>
<td>1.8</td>
<td>virginica</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">138</td>
<td>6.0</td>
<td>3.0</td>
<td>4.8</td>
<td>1.8</td>
<td>virginica</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>It turns out, because we only used <code>petal_length</code> and <code>petal_width</code> to fit the decision tree, and these three points have the exact same measurements for <code>petal_length</code> and <code>petal_width</code>, decision tree was not able to come up with a decision rule to distinguish these three data points!</p>
<p>In general, decision trees (when unrestricted) will always have a perfect accuracy on the training data, <strong>except</strong> when there are samples from different categories with the exact same features. For example, if the versicolor above has a <code>petal_length</code> of 4.8001, we’d have 100% training accuracy.</p>
<p>Again, this tendency for perfect accuracy should give us concern about overfitting. As a result of trying to perfectly fit the training data, decision tree models are extremely sensitive to new data points and therefore have high variance.</p>
</section>
<section id="overfitting-tree-example" class="level3" data-number="24.2.3">
<h3 data-number="24.2.3" class="anchored" data-anchor-id="overfitting-tree-example"><span class="header-section-number">24.2.3</span> Overfitting Tree Example</h3>
<p>Let’s see an example where a decision tree clearly overfits.</p>
<p>Instead of the <code>petal</code> measurements, let’s use the <code>sepal</code> measurements to train the decision tree. Here’s a scatterplot of the dataset with the <code>sepal</code> measurements. We can see the virginicas are much more intertwined with versicolors, compared to the plot with the <code>petal</code> measurements.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-14-output-1.png" width="611" height="454"></p>
</div>
</div>
<p>The decision tree fitted with these two features is very complex, as visualized below. We can see that the tree makes some extremely specific decision rules trying to perfectly capture the details in the dataset.</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">"entropy"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>]], iris_data[<span class="st">"species"</span>])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">4</span>, <span class="dv">8</span>, <span class="fl">0.02</span>),</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="fl">1.9</span>, <span class="fl">4.5</span>, <span class="fl">0.02</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> sepal_decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">False</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="decision_tree_files/figure-html/cell-15-output-1.png" width="611" height="450"></p>
</div>
</div>
<p>We can see the actual tree looks very complicated.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(sepal_decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>],  </span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>graph2 <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>graph2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<p><img src="decision_tree_files/figure-html/cell-16-output-1.svg" class="img-fluid"></p>
</div>
</div>
<p>If we set aside a validation set and check the accuracy of this decision tree on the validation set, we will get around 0.65. This is an indication that we are overfitting.</p>
<p>How does this happened? Is there any way we can reduce overfitting? To understand this, we first need to understand how a decision tree is generated algorithmically.</p>
</section>
</section>
<section id="decision-tree-generation-algorithm" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="decision-tree-generation-algorithm"><span class="header-section-number">24.3</span> Decision Tree Generation Algorithm</h2>
<p>To see how decision tree works, we first need to define some new metrics.</p>
<section id="entropy" class="level3" data-number="24.3.1">
<h3 data-number="24.3.1" class="anchored" data-anchor-id="entropy"><span class="header-section-number">24.3.1</span> Entropy</h3>
<p>Let <span class="math inline">\(p_C\)</span> be the proportion of data points in a node belonging to class <span class="math inline">\(C\)</span>. For example, for the node at the top of the decision tree we saw earlier, there are 38 setosas, 37 versicolors, and 35 virginicas. So</p>
<p><span class="math display">\[
\begin{align*}
&amp;p_0 = 38/110 \approx 0.35\\
&amp;p_1 = 37/110 \approx 0.34\\
&amp;p_2 = 35/110 \approx 0.32.
\end{align*}
\]</span></p>
<p>With this, the <strong>entropy</strong> <span class="math inline">\(S\)</span> of a node is defined as</p>
<p><span class="math display">\[
\Large
S = -\sum_{C}p_C \log_{2}(p_C).
\]</span></p>
<p>Notice we are using <span class="math inline">\(\log\)</span> of base 2 here. This is the only place in the class we are using this particular <span class="math inline">\(\log\)</span>; everywhere else <span class="math inline">\(\log\)</span> has base <span class="math inline">\(e\)</span>.</p>
<p>The term “entropy” comes from information theory and it measure how “unpredictable” or “chaotic” a system is. The bigger the entropy, the more unpredictable a node is. In particular,</p>
<ul>
<li>A node where all data are part of the same class has zero entropy.
<ul>
<li><span class="math inline">\(−1 \log_2⁡ 1 = 0\)</span> bits.</li>
</ul></li>
<li>A node where data are evenly split between two classes has entropy 1.
<ul>
<li><span class="math inline">\(−0.5 \log_2⁡ 0.5 − 0.5 \log_2⁡ 0.5 = 1\)</span> bit.</li>
</ul></li>
<li>A node where data are evenly split between 3 classes has entropy 1.58.
<ul>
<li><span class="math inline">\(3 × (−0.33 \log_2⁡ 0.33) = 1.58\)</span> bits.</li>
</ul></li>
<li>A node where data are evenly split into C classes has entropy <span class="math inline">\(\log_2 C\)</span>.
<ul>
<li><span class="math inline">\(C × (−1/C \log_2⁡ 1/C) = −\log_2⁡ 1/C = \log_2⁡ C\)</span> bits.</li>
</ul></li>
</ul>
</section>
<section id="generating-trees-with-entropy" class="level3" data-number="24.3.2">
<h3 data-number="24.3.2" class="anchored" data-anchor-id="generating-trees-with-entropy"><span class="header-section-number">24.3.2</span> Generating Trees with Entropy</h3>
<p>How do we use entropy to generate a decision tree?</p>
<p>Decision trees are comprised of many different <em>splits</em>, so to generate a tree, we need to be able to come up with optimal splits given a node. To measure how good a split is, we use the <strong>weighted entropy</strong>.</p>
<p>Suppose a given split results in two nodes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with <span class="math inline">\(N_1\)</span> and <span class="math inline">\(N_2\)</span> number of samples each. The loss (or <strong>weighted entropy</strong>) of that split is given by:</p>
<p><span class="math display">\[
\Large
L = \frac{N_{1} S(X) + N_{2} S(Y)}{N_{1} + N_{2}}
\]</span></p>
<p>where <span class="math inline">\(S(X)\)</span> and <span class="math inline">\(S(Y)\)</span> are the entropies of nodes <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Intuitively, the weighted entropy of a split measures how unpredictable the result of the split is. We want to make the resulting nodes as predictable as possible, therefore we need to minimize the weighted entropy. In decision trees, the way we minimize the weighted entropy of a split is to try out different combinations of features and splitting values.</p>
<p>Now we have everything we need to state the decision tree generation algorithm.</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Decision Tree Generation Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>All of the data starts in the root node</li>
<li>Repeat the following until every node is either <em>pure</em> or <em>unsplittable</em>:
<ul>
<li>Pick the <em>best feature</em> <span class="math inline">\(x\)</span> and <em>best split value</em> <span class="math inline">\(\beta\)</span> such that the <strong>weighted entropy</strong> of the resulting split is minimized, e.g.&nbsp;<span class="math inline">\(x\)</span> = <code>petal_width</code>, <span class="math inline">\(\beta = 0.8\)</span> has a loss of <span class="math inline">\(0.66\)</span></li>
<li>Split data into two nodes, one where <span class="math inline">\(x \leq \beta\)</span>, and one where <span class="math inline">\(x &gt; \beta\)</span>.</li>
</ul></li>
</ul>
</div>
</div>
<p>What do we mean by “pure” or “unsplittable”?</p>
<ul>
<li>A node that has only samples from one class is called a “<strong>pure</strong>” node.</li>
<li>A node that has overlapping data points from different classes and thus that cannot be split is called “<strong>unsplittable</strong>”. We saw an example of this in our first decision tree, fitted with <code>petal_length</code> and <code>petal_width</code>.</li>
</ul>
</section>
</section>
<section id="avoiding-overfitting" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="avoiding-overfitting"><span class="header-section-number">24.4</span> Avoiding Overfitting</h2>
<p>Now that we have an idea how decision tree works, let’s turn our attention to the issue of overfitting and discuss ways we can avoid or reduce overfitting with decision trees.</p>
<section id="restricting-tree-complexity" class="level3" data-number="24.4.1">
<h3 data-number="24.4.1" class="anchored" data-anchor-id="restricting-tree-complexity"><span class="header-section-number">24.4.1</span> Restricting Tree Complexity</h3>
<p>A “fully grown” decision tree built with our algorithm runs the risk of overfitting. In linear models, we introduced the notion of “regularization”. However, regularization doesn’t make sense in the decision tree context as decision trees don’t have parameters that act as weights, so we can’t use L1 or L2 regularization. We have to come up with new ideas.</p>
<p>The first category of ideas is to disallow fully grown trees.</p>
<section id="preventing-growth" class="level4" data-number="24.4.1.1">
<h4 data-number="24.4.1.1" class="anchored" data-anchor-id="preventing-growth"><span class="header-section-number">24.4.1.1</span> Preventing Growth</h4>
<p>Since decision trees tend to come up with extremely specific rules to attempt to fit the training data perfectly, we can set one or more special rules to prevent the tree from fully growing.</p>
<p>Examples of this include:</p>
<ul>
<li>Don’t split nodes with &lt; 1% of the samples.
<ul>
<li>In <code>sklearn</code>, this hyperparameter is called “<code>min_samples_split</code>”.</li>
</ul></li>
<li>Don’t allow nodes to be more than 7 levels deep in the tree.
<ul>
<li>In <code>sklearn</code>, this hyperparameter is called “<code>max_depth</code>”.</li>
</ul></li>
</ul>
</section>
<section id="pruning" class="level4" data-number="24.4.1.2">
<h4 data-number="24.4.1.2" class="anchored" data-anchor-id="pruning"><span class="header-section-number">24.4.1.2</span> Pruning</h4>
<p>Another approach is to allow the tree to fully grow when fitting it, then cut off the less useful branches of the tree. In pruning the tree, we are usually looking for rules that affect only a small subset of points.</p>
<p>There are many ways to prune trees. One way is to use a validation set to see if keeping a given branch is worth it. Specifically, we run our model on the validation set twice, once keeping the branch, and the second time replacing the branch by its most common prediction. If there’s no or minimal impact on the validation error, then we delete the split. The idea here is that the split was not useful on unseen data.</p>
<p>Recall that when a node has multiple samples in it from different classes, the <code>predict</code> method will simply return the most common class, and the <code>predict_proba</code> method will return the fraction of samples that belong to each class.</p>
</section>
</section>
<section id="random-forest" class="level3" data-number="24.4.2">
<h3 data-number="24.4.2" class="anchored" data-anchor-id="random-forest"><span class="header-section-number">24.4.2</span> Random Forest</h3>
<div class="columns">
<div class="column" style="width:50%;">
<p>As we’ve seen earlier, a fully-grown decision tree will almost always overfit the data. It has low model bias, but high model variance. In other words, small changes in the dataset will result in very a different decision tree. As an example, the two models below are trained on different subsets of the same data. We can see they come out very differently.</p>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<p><img src="images/two_trees.png" alt="Two trees fitted on different subset of the same dataset" width="400"></p>
</div>
</div>
<p>The idea of random forest is to harness this variance: build many decision trees and take the majority vote.</p>
<p>To build multiple decision trees using our training data, we use our old friend bootstrap. In particular, this method is called “<strong>bagging</strong>”, which stands for Bootstrap Aggregating. In bagging, we</p>
<ul>
<li>Generate bootstrap resamples of training data.</li>
<li>Fit one model for each resample.</li>
<li>Build the final model using the average predictions of each small model.</li>
</ul>
<p>However, bagging is usually not enough to reduce model variance. In many cases, the different decision trees end up looking very similar to one another and therefore make similar predictions. The resulting model will still have low bias and high variance.</p>
<p>To improve on bagging, we add another layer of randomness by only using a <strong>random subset</strong> of <span class="math inline">\(m\)</span> features at each split. Usually we use <span class="math inline">\(m = \sqrt{p}\)</span> for decision tree classifiers, where <span class="math inline">\(p\)</span> is the total number of features.</p>
<p>The algorithm will create individual trees, each overfit in a different way. The hope is then that the overall forest will have low variance due to aggregation.</p>
<p>Let’s summarize the above by stating the random forest algorithm:</p>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Random Forest Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Bootstrap the training data <span class="math inline">\(T\)</span> times. For each resample, fit a decision tree by doing the following:
<ul>
<li>Start with data in one node. Repeat the following until all nodes are <em>pure</em> or <em>unsplittable</em>:
<ul>
<li>Pick an impure node</li>
<li>Pick a random subset of <span class="math inline">\(m\)</span> features. Pick the best feature <span class="math inline">\(x\)</span> and best split value <span class="math inline">\(\beta\)</span> such that the weighted entropy of the resulting split is minimized</li>
<li>Split data into two nodes, one where <span class="math inline">\(x \leq \beta\)</span>, and one where <span class="math inline">\(x &gt; \beta\)</span></li>
</ul></li>
</ul></li>
<li>To predict, ask the <span class="math inline">\(T\)</span> decision trees for their predictions and take the majority vote.</li>
</ul>
</div>
</div>
<p>Notice there are two hyperparameters in the algorithm: <span class="math inline">\(T\)</span> and <span class="math inline">\(m\)</span>. To pick the best value, we can use validation or cross-validation.</p>
<p>In this section, we talked about three approaches to address the overfitting issue with decision trees: preventing growth, pruning, and random forest. These ideas are generally “heuristic,” meaning they are not provably the best or mathematically optimal. Instead, they are ideas that somebody thought sounded good, implemented, then found to work in practice acceptably well.</p>
</section>
</section>
<section id="summary-and-context" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="summary-and-context"><span class="header-section-number">24.5</span> Summary and Context</h2>
<p>To summarize what we talked about in this lecture, decision trees present an alternative philosophy to logistic regression: instead of a weighted sum of features, we use a series of yes/no questions.</p>
<p>Some advantages of a decision tree include: - It is able to capture nonlinear relationships (at risk of overfitting). - Its output is more interpretable: we know exactly how the tree made a decision. However, this is not true of random forests.</p>
<p>Some disadvantages of a decision tree include: - Care needs to be taken to avoid overfitting. - It is not as mathematically nice as some linear models (e.g.&nbsp;OLS and Ridge Regression have simple closed-form solutions).</p>
<p>In practice, people use random forests more often than decision trees because they preform much better on unseen data in general. A random forest also has the following advantages:</p>
<ul>
<li>Versatile: does both regression and classification.</li>
<li>Invariant to feature scaling and translation.</li>
<li>It can perform automatic feature selection.</li>
<li>It has nonlinear decision boundaries without complicated feature engineering.</li>
<li>It doesn’t overfit as often as other nonlinear models (e.g.&nbsp;polynomial features, decision trees).</li>
</ul>
<p>Random forest is an example of <strong>ensemble method</strong>, where we combine the knowledge of many simple models to make a sophisticated model. It is also an example of using bootstrap to reduce model variance.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../pca_2/pca_2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">PCA II</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb19" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Decision Trees</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Decision Trees</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand how to generate a decision tree</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement decision tree using <span class="in">`sklearn`</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand ways to improve decision trees, including random forests</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(font_scale<span class="op">=</span><span class="fl">1.5</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>In our second pass of the data science lifecycle, we've introduced new tools for data processing and exploratory data analysis. Now let's turn our attention to modeling. Today, we will introduce a new model called decision trees.</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-class Classification</span></span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>In our discussion about logistics regression, we mainly focused on the task of "binary classification" where we try to distinguish between two classes of labels (e.g., "spam" or "ham"). In real life though, we often have multi-class labels. For example, the Fashion-MNIST dataset we worked with last week have multiple classes of clothing items. How would we classify this kind of data?</span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>As an example, let's consider a dataset about the iris flowers. The dataset consists of 150 flower measurements from 3 different species: setosa, versicolor, and virginica. For each, we have measurements of the flower's <span class="in">`“petal length”`</span>, <span class="in">`“petal width”`</span>, <span class="in">`“sepal length”`</span>, and <span class="in">`“sepal width”`</span>.</span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>iris_data <span class="op">=</span> pd.read_csv(<span class="st">"data/iris.csv"</span>)</span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>iris_data.sample(<span class="dv">5</span>)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>The goal today is to use these four measurements to try and predict the <span class="in">`species`</span> of the iris flower.</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>Let's see how we can use logistic regression to classify this dataset with three classes. To do this, we will use the petal measurements: <span class="in">`petal_length`</span> and <span class="in">`petal_width`</span>:</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> LogisticRegression(multi_class <span class="op">=</span> <span class="st">'ovr'</span>)</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>logistic_regression_model <span class="op">=</span> logistic_regression_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], </span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>                                                          iris_data[<span class="st">"species"</span>])</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>Notice when constructing the classifier, we specified <span class="in">`multi_class = 'ovr'`</span>. </span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a><span class="in">`ovr`</span> means "one-vs.-rest", which means <span class="in">`sklearn`</span> trains three logistic regression binary classifiers:</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Classifier 1: Setosa or not.</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Classifier 2: Versicolor or not.</span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Classifier 3: Virginica or not.</span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>Then the <span class="in">`model.predict()`</span> function will output the classifier that returns the highest probability:</span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a>logistic_regression_model.predict([[<span class="fl">1.4</span>, <span class="fl">0.2</span>]])</span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a>We can also visualize the decision boundary of this classifier. As we can see, each of three classifiers still has linear decision boundaries, but there are now three lines instead of 1.</span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> logistic_regression_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)</span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>, <span class="dv">7</span>)<span class="op">;</span></span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="fl">2.8</span>)<span class="op">;</span></span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>It turns out there are other (arguably) better models to use when it comes to multi-class classification.</span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a>In Data 100 so far, we've been talking about linear models. In Linear regression, the output is the weighted sum of the features. In logistic regression, the output is $\sigma$(the weighted sum of the features). </span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a>Today, we are going into the territory of non-linear models. There are many non-linear models, decision tree is one of them.</span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Trees, conceptually</span></span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a>A decision tree is simply a tree of questions that must be answered in a sequence to yield a predicted classification. For example, below is a classifier for animals. It starts by asking how many legs the animal has. If the answer is no, we immediately say snake. If the answer is four, the model is not quite sure, so we must ask an additional question: does it purr?  If the answer is yes, the animal is a cat; and if the answer is no, the animal is a dog. The other possibility is that the number of legs was two, in that case we have up to two additional questions we may need to ask before eventually deciding if it is a kangaroo, human, or parrot. Obviously this decision tree does not cover all animals. In principle, with the right sequence of questions, we could build a giant decision tree that could differentiate a vast number of animals.</span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/animal_tree.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'Example of a decision tree'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a>For our iris dataset, we can come up with a decision tree just by looking at the points. It's important to note in the following decision tree we use multiple features in a single node to make the tree simpler, but in reality decision tree will only make a split based on one feature at a time. We will see an example of an actual decision tree in the next section.</span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/manual_tree.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'Example of a decision tree on the iris data'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a>Just by looking at the tree, it seems we reached 100% accuracy on this dataset. However, this should call for some concerns about potential overfitting.</span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Trees in `sklearn`</span></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a>We can implement a decision tree easily using <span class="in">`sklearn`</span>. Like other models we've been working with in this class so far, we would create an instance of the model, fit it on our training data, and use it to make predictions.</span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>)</span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a>decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]], iris_data[<span class="st">"species"</span>])</span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a>You might noticed that we specified <span class="in">`criterion = 'entropy'`</span> when instantiating the model. We will talk more about "entropy" in a later section.</span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a>We can use the fitted decision tree model to predict four random points in our original data:</span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-151"><a href="#cb19-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-152"><a href="#cb19-152" aria-hidden="true" tabindex="-1"></a>four_random_rows <span class="op">=</span> iris_data.sample(<span class="dv">4</span>)</span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a>four_random_rows</span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a>decision_tree_model.predict(four_random_rows[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a>We can also visualize how the decision tree is constructed using a package called <span class="in">`GraphViz`</span>. </span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>],  </span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a>graph</span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a>In each box/node, we see:</span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The decision rule applied to that node.</span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The entropy at that node (more later).</span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The number of samples that remain after applying all of the rules above this node.</span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The number of samples that remain in each class.</span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The most likely class. If there is a tie, this is the class that comes first.</span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a>This visualization allows us to see exactly how the decision tree classifies each point. For example, if the <span class="in">`petal_length`</span> of an iris flower is less than or equal to 1.75, we would classify it as <span class="in">`setosa`</span>.</span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a>Like with the logistic regression model earlier, we can visualize the decision boundary of this tree.</span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> ListedColormap</span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">0</span>, <span class="dv">7</span>, <span class="fl">0.02</span>),</span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="dv">0</span>, <span class="fl">2.8</span>, <span class="fl">0.02</span>))</span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"petal_length"</span>, y<span class="op">=</span><span class="st">"petal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>)<span class="op">;</span></span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a>Decision tree has nonlinear boundary, and appears to get 100% accuracy. But did we actually get 100% accuracy? Let's check!</span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluating Tree Accuracy</span></span>
<span id="cb19-210"><a href="#cb19-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-211"><a href="#cb19-211" aria-hidden="true" tabindex="-1"></a>To get the accuracy of a decision tree, we use the <span class="in">`accuracy_score`</span> method implemented in <span class="in">`sklearn`</span>.</span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> decision_tree_model.predict(iris_data[[<span class="st">"petal_length"</span>, <span class="st">"petal_width"</span>]])</span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a>accuracy_score(predictions, iris_data[<span class="st">"species"</span>])</span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a>Hmm, it looks like we didn't get a perfect accuracy on the dataset. What happened?</span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a>::: {.column width="55%"}</span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a>To understand why, let's look back at the decision tree from earlier. In particular, let's look at one particular node, zoomed in on the right.</span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a>It looks like this node has three samples yet to be classified, with 1 versicolor and 2 virginicas, but the decision tree stopped here without splitting any further. This results in us classifying the one versicolor wrong.</span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a>Why did the decision tree stop? Let's check these three points by following the decision tree.</span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a>::: {.column width="5%"}</span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a>::: {.column width="40%"}</span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/problematic_node.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'A node where decision tree failed'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'300'</span><span class="kw">&gt;</span></span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-236"><a href="#cb19-236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-237"><a href="#cb19-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a>iris_data[(iris_data[<span class="st">"petal_length"</span>]<span class="op">&gt;</span> <span class="fl">2.45</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_width"</span>]<span class="op">&gt;</span> <span class="fl">1.75</span>)<span class="op">&amp;</span>(iris_data[<span class="st">"petal_length"</span>]<span class="op">&lt;=</span><span class="fl">4.85</span>)]</span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a>It turns out, because we only used <span class="in">`petal_length`</span> and <span class="in">`petal_width`</span> to fit the decision tree, and these three points have the exact same measurements for <span class="in">`petal_length`</span> and <span class="in">`petal_width`</span>, decision tree was not able to come up with a decision rule to distinguish these three data points!</span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a>In general, decision trees (when unrestricted) will always have a perfect accuracy on the training data, **except** when there are samples from different categories with the exact same features. For example, if the versicolor above has a <span class="in">`petal_length`</span> of 4.8001, we’d have 100% training accuracy.</span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a>Again, this tendency for perfect accuracy should give us concern about overfitting. As a result of trying to perfectly fit the training data, decision tree models are extremely sensitive to new data points and therefore have high variance.</span>
<span id="cb19-249"><a href="#cb19-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-250"><a href="#cb19-250" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting Tree Example</span></span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a>Let's see an example where a decision tree clearly overfits.</span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a>Instead of the <span class="in">`petal`</span> measurements, let's use the <span class="in">`sepal`</span> measurements to train the decision tree. Here's a scatterplot of the dataset with the <span class="in">`sepal`</span> measurements. We can see the virginicas are much more intertwined with versicolors, compared to the plot with the <span class="in">`petal`</span> measurements.</span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-258"><a href="#cb19-258" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-259"><a href="#cb19-259" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a>The decision tree fitted with these two features is very complex, as visualized below. We can see that the tree makes some extremely specific decision rules trying to perfectly capture the details in the dataset.</span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-266"><a href="#cb19-266" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-267"><a href="#cb19-267" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">"entropy"</span>)</span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a>sepal_decision_tree_model <span class="op">=</span> decision_tree_model.fit(iris_data[[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>]], iris_data[<span class="st">"species"</span>])</span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a>sns_cmap <span class="op">=</span> ListedColormap(np.array(sns.color_palette())[<span class="dv">0</span>:<span class="dv">3</span>, :])</span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(<span class="dv">4</span>, <span class="dv">8</span>, <span class="fl">0.02</span>),</span>
<span id="cb19-273"><a href="#cb19-273" aria-hidden="true" tabindex="-1"></a>                     np.arange(<span class="fl">1.9</span>, <span class="fl">4.5</span>, <span class="fl">0.02</span>))</span>
<span id="cb19-274"><a href="#cb19-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a>Z_string <span class="op">=</span> sepal_decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a>categories, Z_int <span class="op">=</span> np.unique(Z_string, return_inverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int </span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a>Z_int <span class="op">=</span> Z_int.reshape(xx.shape)</span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> plt.contourf(xx, yy, Z_int, cmap<span class="op">=</span>sns_cmap)</span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data <span class="op">=</span> iris_data, x <span class="op">=</span> <span class="st">"sepal_length"</span>, y<span class="op">=</span><span class="st">"sepal_width"</span>, hue<span class="op">=</span><span class="st">"species"</span>, legend<span class="op">=</span><span class="va">False</span>)<span class="op">;</span></span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-283"><a href="#cb19-283" aria-hidden="true" tabindex="-1"></a>We can see the actual tree looks very complicated.</span>
<span id="cb19-284"><a href="#cb19-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a>dot_data <span class="op">=</span> tree.export_graphviz(sepal_decision_tree_model, out_file<span class="op">=</span><span class="va">None</span>, </span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a>                      feature_names<span class="op">=</span>[<span class="st">"sepal_length"</span>, <span class="st">"sepal_width"</span>],  </span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a>                      class_names<span class="op">=</span>[<span class="st">"setosa"</span>, <span class="st">"versicolor"</span>, <span class="st">"virginica"</span>],  </span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a>                      filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a>graph2 <span class="op">=</span> graphviz.Source(dot_data)</span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a>graph2</span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a>If we set aside a validation set and check the accuracy of this decision tree on the validation set, we will get around 0.65. This is an indication that we are overfitting.</span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a>How does this happened? Is there any way we can reduce overfitting? To understand this, we first need to understand how a decision tree is generated algorithmically.</span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Tree Generation Algorithm</span></span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a>To see how decision tree works, we first need to define some new metrics.</span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-305"><a href="#cb19-305" aria-hidden="true" tabindex="-1"></a><span class="fu">### Entropy</span></span>
<span id="cb19-306"><a href="#cb19-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a>Let $p_C$ be the proportion of data points in a node belonging to class $C$. For example, for the node at the top of the decision tree we saw earlier, there are 38 setosas, 37 versicolors, and 35 virginicas. So </span>
<span id="cb19-308"><a href="#cb19-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-309"><a href="#cb19-309" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a>&amp;p_0 = 38/110 \approx 0.35<span class="sc">\\</span></span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a>&amp;p_1 = 37/110 \approx 0.34<span class="sc">\\</span></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a>&amp;p_2 = 35/110 \approx 0.32.</span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a>With this, the **entropy** $S$ of a node is defined as</span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a>\Large</span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a>S = -\sum_{C}p_C \log_{2}(p_C).</span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a>Notice we are using $\log$ of base 2 here. This is the only place in the class we are using this particular $\log$; everywhere else $\log$ has base $e$.</span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a>The term "entropy" comes from information theory and it measure how "unpredictable" or "chaotic" a system is. The bigger the entropy, the more unpredictable a node is. In particular, </span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node where all data are part of the same class has zero entropy. </span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>$−1 \log_2⁡ 1 = 0$ bits.</span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node where data are evenly split between two classes has entropy 1. </span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>$−0.5 \log_2⁡ 0.5 − 0.5 \log_2⁡ 0.5 = 1$ bit.</span>
<span id="cb19-332"><a href="#cb19-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node where data are evenly split between 3 classes has entropy 1.58. </span>
<span id="cb19-333"><a href="#cb19-333" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>$3 × (−0.33 \log_2⁡ 0.33) = 1.58$ bits.</span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node where data are evenly split into C classes has entropy $\log_2 C$.</span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>$C × (−1/C \log_2⁡ 1/C) = −\log_2⁡ 1/C = \log_2⁡ C$ bits.</span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generating Trees with Entropy</span></span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-340"><a href="#cb19-340" aria-hidden="true" tabindex="-1"></a>How do we use entropy to generate a decision tree? </span>
<span id="cb19-341"><a href="#cb19-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a>Decision trees are comprised of many different *splits*, so to generate a tree, we need to be able to come up with optimal splits given a node. To measure how good a split is, we use the **weighted entropy**.</span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a>Suppose a given split results in two nodes $X$ and $Y$ with $N_1$ and $N_2$ number of samples each. The loss (or **weighted entropy**) of that split is given by:</span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-346"><a href="#cb19-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-347"><a href="#cb19-347" aria-hidden="true" tabindex="-1"></a>\Large</span>
<span id="cb19-348"><a href="#cb19-348" aria-hidden="true" tabindex="-1"></a>L = \frac{N_{1} S(X) + N_{2} S(Y)}{N_{1} + N_{2}}</span>
<span id="cb19-349"><a href="#cb19-349" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb19-350"><a href="#cb19-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-351"><a href="#cb19-351" aria-hidden="true" tabindex="-1"></a>where $S(X)$ and $S(Y)$ are the entropies of nodes $X$ and $Y$.</span>
<span id="cb19-352"><a href="#cb19-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-353"><a href="#cb19-353" aria-hidden="true" tabindex="-1"></a>Intuitively, the weighted entropy of a split measures how unpredictable the result of the split is. We want to make the resulting nodes as predictable as possible, therefore we need to minimize the weighted entropy. In decision trees, the way we minimize the weighted entropy of a split is to try out different combinations of features and splitting values.</span>
<span id="cb19-354"><a href="#cb19-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-355"><a href="#cb19-355" aria-hidden="true" tabindex="-1"></a>Now we have everything we need to state the decision tree generation algorithm.</span>
<span id="cb19-356"><a href="#cb19-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-357"><a href="#cb19-357" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb19-358"><a href="#cb19-358" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Tree Generation Algorithm</span></span>
<span id="cb19-359"><a href="#cb19-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-360"><a href="#cb19-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>All of the data starts in the root node</span>
<span id="cb19-361"><a href="#cb19-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Repeat the following until every node is either *pure* or *unsplittable*:</span>
<span id="cb19-362"><a href="#cb19-362" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Pick the *best feature* $x$ and *best split value* $\beta$ such that the **weighted entropy** of the resulting split is minimized, e.g. $x$ = <span class="in">`petal_width`</span>, $\beta = 0.8$ has a loss of $0.66$</span>
<span id="cb19-363"><a href="#cb19-363" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Split data into two nodes, one where $x \leq \beta$, and one where $x &gt; \beta$.</span>
<span id="cb19-364"><a href="#cb19-364" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-365"><a href="#cb19-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-366"><a href="#cb19-366" aria-hidden="true" tabindex="-1"></a>What do we mean by "pure" or "unsplittable"?</span>
<span id="cb19-367"><a href="#cb19-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-368"><a href="#cb19-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node that has only samples from one class is called a “**pure**” node. </span>
<span id="cb19-369"><a href="#cb19-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A node that has overlapping data points from different classes and thus that cannot be split is called “**unsplittable**”. We saw an example of this in our first decision tree, fitted with <span class="in">`petal_length`</span> and <span class="in">`petal_width`</span>.</span>
<span id="cb19-370"><a href="#cb19-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-371"><a href="#cb19-371" aria-hidden="true" tabindex="-1"></a><span class="fu">## Avoiding Overfitting</span></span>
<span id="cb19-372"><a href="#cb19-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-373"><a href="#cb19-373" aria-hidden="true" tabindex="-1"></a>Now that we have an idea how decision tree works, let's turn our attention to the issue of overfitting and discuss ways we can avoid or reduce overfitting with decision trees.</span>
<span id="cb19-374"><a href="#cb19-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-375"><a href="#cb19-375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Restricting Tree Complexity</span></span>
<span id="cb19-376"><a href="#cb19-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-377"><a href="#cb19-377" aria-hidden="true" tabindex="-1"></a>A “fully grown” decision tree built with our algorithm runs the risk of overfitting. In linear models, we introduced the notion of “regularization”. However, regularization doesn’t make sense in the decision tree context as decision trees don't have parameters that act as weights, so we can’t use L1 or L2 regularization. We have to come up with new ideas.</span>
<span id="cb19-378"><a href="#cb19-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-379"><a href="#cb19-379" aria-hidden="true" tabindex="-1"></a>The first category of ideas is to disallow fully grown trees.</span>
<span id="cb19-380"><a href="#cb19-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-381"><a href="#cb19-381" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Preventing Growth</span></span>
<span id="cb19-382"><a href="#cb19-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-383"><a href="#cb19-383" aria-hidden="true" tabindex="-1"></a>Since decision trees tend to come up with extremely specific rules to attempt to fit the training data perfectly, we can set one or more special rules to prevent the tree from fully growing.</span>
<span id="cb19-384"><a href="#cb19-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-385"><a href="#cb19-385" aria-hidden="true" tabindex="-1"></a>Examples of this include:</span>
<span id="cb19-386"><a href="#cb19-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-387"><a href="#cb19-387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Don’t split nodes with &lt; 1% of the samples.  </span>
<span id="cb19-388"><a href="#cb19-388" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>In <span class="in">`sklearn`</span>, this hyperparameter is called “<span class="in">`min_samples_split`</span>”.</span>
<span id="cb19-389"><a href="#cb19-389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Don’t allow nodes to be more than 7 levels deep in the tree.</span>
<span id="cb19-390"><a href="#cb19-390" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>In <span class="in">`sklearn`</span>, this hyperparameter is called “<span class="in">`max_depth`</span>”.</span>
<span id="cb19-391"><a href="#cb19-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-392"><a href="#cb19-392" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pruning</span></span>
<span id="cb19-393"><a href="#cb19-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-394"><a href="#cb19-394" aria-hidden="true" tabindex="-1"></a>Another approach is to allow the tree to fully grow when fitting it, then cut off the less useful branches of the tree. In pruning the tree, we are usually looking for rules that affect only a small subset of points.</span>
<span id="cb19-395"><a href="#cb19-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-396"><a href="#cb19-396" aria-hidden="true" tabindex="-1"></a>There are many ways to prune trees. One way is to use a validation set to see if keeping a given branch is worth it. Specifically, we run our model on the validation set twice, once keeping the branch, and the second time replacing the branch by its most common prediction. If there's no or minimal impact on the validation error, then we delete the split. The idea here is that the split was not useful on unseen data.</span>
<span id="cb19-397"><a href="#cb19-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-398"><a href="#cb19-398" aria-hidden="true" tabindex="-1"></a>Recall that when a node has multiple samples in it from different classes, the <span class="in">`predict`</span> method will simply return the most common class, and the <span class="in">`predict_proba`</span> method will return the fraction of samples that belong to each class.</span>
<span id="cb19-399"><a href="#cb19-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-400"><a href="#cb19-400" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest</span></span>
<span id="cb19-401"><a href="#cb19-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-402"><a href="#cb19-402" aria-hidden="true" tabindex="-1"></a>::: {.columns}</span>
<span id="cb19-403"><a href="#cb19-403" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb19-404"><a href="#cb19-404" aria-hidden="true" tabindex="-1"></a>As we've seen earlier, a fully-grown decision tree will almost always overfit the data. It has low model bias, but high model variance. In other words, small changes in the dataset will result in very a different decision tree. As an example, the two models below are trained on different subsets of the same data. We can see they come out very differently.</span>
<span id="cb19-405"><a href="#cb19-405" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-406"><a href="#cb19-406" aria-hidden="true" tabindex="-1"></a>::: {.column width="5%"}</span>
<span id="cb19-407"><a href="#cb19-407" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-408"><a href="#cb19-408" aria-hidden="true" tabindex="-1"></a>::: {.column width="45%"}</span>
<span id="cb19-409"><a href="#cb19-409" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/two_trees.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'Two trees fitted on different subset of the same dataset'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'400'</span><span class="kw">&gt;</span></span>
<span id="cb19-410"><a href="#cb19-410" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-411"><a href="#cb19-411" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-412"><a href="#cb19-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-413"><a href="#cb19-413" aria-hidden="true" tabindex="-1"></a>The idea of random forest is to harness this variance: build many decision trees and take the majority vote.</span>
<span id="cb19-414"><a href="#cb19-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-415"><a href="#cb19-415" aria-hidden="true" tabindex="-1"></a>To build multiple decision trees using our training data, we use our old friend bootstrap. In particular, this method is called "**bagging**", which stands for Bootstrap Aggregating. In bagging, we</span>
<span id="cb19-416"><a href="#cb19-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-417"><a href="#cb19-417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generate bootstrap resamples of training data.</span>
<span id="cb19-418"><a href="#cb19-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fit one model for each resample.</span>
<span id="cb19-419"><a href="#cb19-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Build the final model using the average predictions of each small model.</span>
<span id="cb19-420"><a href="#cb19-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-421"><a href="#cb19-421" aria-hidden="true" tabindex="-1"></a>However, bagging is usually not enough to reduce model variance. In many cases, the different decision trees end up looking very similar to one another and therefore make similar predictions. The resulting model will still have low bias and high variance.</span>
<span id="cb19-422"><a href="#cb19-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-423"><a href="#cb19-423" aria-hidden="true" tabindex="-1"></a>To improve on bagging, we add another layer of randomness by only using a **random subset** of $m$ features at each split. Usually we use $m = \sqrt{p}$ for decision tree classifiers, where $p$ is the total number of features.</span>
<span id="cb19-424"><a href="#cb19-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-425"><a href="#cb19-425" aria-hidden="true" tabindex="-1"></a>The algorithm will create individual trees, each overfit in a different way. The hope is then that the overall forest will have low variance due to aggregation.</span>
<span id="cb19-426"><a href="#cb19-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-427"><a href="#cb19-427" aria-hidden="true" tabindex="-1"></a>Let's summarize the above by stating the random forest algorithm:</span>
<span id="cb19-428"><a href="#cb19-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-429"><a href="#cb19-429" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb19-430"><a href="#cb19-430" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forest Algorithm</span></span>
<span id="cb19-431"><a href="#cb19-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-432"><a href="#cb19-432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bootstrap the training data $T$ times. For each resample, fit a decision tree by doing the following:</span>
<span id="cb19-433"><a href="#cb19-433" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Start with data in one node. Repeat the following until all nodes are *pure* or *unsplittable*:</span>
<span id="cb19-434"><a href="#cb19-434" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>Pick an impure node</span>
<span id="cb19-435"><a href="#cb19-435" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>Pick a random subset of $m$ features. Pick the best feature $x$ and best split value $\beta$ such that the weighted entropy of the resulting split is minimized</span>
<span id="cb19-436"><a href="#cb19-436" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>Split data into two nodes, one where $x \leq \beta$, and one where $x &gt; \beta$</span>
<span id="cb19-437"><a href="#cb19-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To predict, ask the $T$ decision trees for their predictions and take the majority vote.</span>
<span id="cb19-438"><a href="#cb19-438" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-439"><a href="#cb19-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-440"><a href="#cb19-440" aria-hidden="true" tabindex="-1"></a>Notice there are two hyperparameters in the algorithm: $T$ and $m$. To pick the best value, we can use validation or cross-validation.</span>
<span id="cb19-441"><a href="#cb19-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-442"><a href="#cb19-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-443"><a href="#cb19-443" aria-hidden="true" tabindex="-1"></a>In this section, we talked about three approaches to address the overfitting issue with decision trees: preventing growth, pruning, and random forest. These ideas are generally “heuristic,” meaning they are not provably the best or mathematically optimal. Instead, they are ideas that somebody thought sounded good, implemented, then found to work in practice acceptably well.</span>
<span id="cb19-444"><a href="#cb19-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-445"><a href="#cb19-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-446"><a href="#cb19-446" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary and Context</span></span>
<span id="cb19-447"><a href="#cb19-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-448"><a href="#cb19-448" aria-hidden="true" tabindex="-1"></a>To summarize what we talked about in this lecture, decision trees present an alternative philosophy to logistic regression: instead of a weighted sum of features, we use a series of yes/no questions.</span>
<span id="cb19-449"><a href="#cb19-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-450"><a href="#cb19-450" aria-hidden="true" tabindex="-1"></a>Some advantages of a decision tree include:</span>
<span id="cb19-451"><a href="#cb19-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It is able to capture nonlinear relationships (at risk of overfitting).</span>
<span id="cb19-452"><a href="#cb19-452" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Its output is more interpretable: we know exactly how the tree made a decision. However, this is not true of random forests.</span>
<span id="cb19-453"><a href="#cb19-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-454"><a href="#cb19-454" aria-hidden="true" tabindex="-1"></a>Some disadvantages of a decision tree include:</span>
<span id="cb19-455"><a href="#cb19-455" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Care needs to be taken to avoid overfitting.</span>
<span id="cb19-456"><a href="#cb19-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It is not as mathematically nice as some linear models (e.g. OLS and Ridge Regression have simple closed-form solutions).</span>
<span id="cb19-457"><a href="#cb19-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-458"><a href="#cb19-458" aria-hidden="true" tabindex="-1"></a>In practice, people use random forests more often than decision trees because they preform much better on unseen data in general. A random forest also has the following advantages:</span>
<span id="cb19-459"><a href="#cb19-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-460"><a href="#cb19-460" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Versatile: does both regression and classification.</span>
<span id="cb19-461"><a href="#cb19-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Invariant to feature scaling and translation.</span>
<span id="cb19-462"><a href="#cb19-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It can perform automatic feature selection.</span>
<span id="cb19-463"><a href="#cb19-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It has nonlinear decision boundaries without complicated feature engineering.</span>
<span id="cb19-464"><a href="#cb19-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It doesn’t overfit as often as other nonlinear models (e.g. polynomial features, decision trees).</span>
<span id="cb19-465"><a href="#cb19-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-466"><a href="#cb19-466" aria-hidden="true" tabindex="-1"></a>Random forest is an example of **ensemble method**, where we combine the knowledge of many simple models to make a sophisticated model. It is also an example of using bootstrap to reduce model variance.</span>
<span id="cb19-467"><a href="#cb19-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-468"><a href="#cb19-468" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>