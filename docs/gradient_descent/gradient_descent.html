<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.21">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; sklearn and Gradient Descent (Summer 2025) – Principles and Techniques of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../feature_engineering/feature_engineering.html" rel="next">
<link href="../case_study_HCE/case_study_HCE.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ea1d7ac60288e0f1efdbc993fd8432ae.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-48faffd076f6e246ab7435acea77d2f2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.18.2.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../gradient_descent/gradient_descent.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent (Summer 2025)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../data100_logo.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../modeling_slr/modeling_slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modeling &amp; SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Parameter Inference and Bootstrapping (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II (Summer 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Clustering (Summer 2025)</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">sklearn and Gradient Descent</h2>
   
  <ul>
  <li><a href="#ols-recap" id="toc-ols-recap" class="nav-link active" data-scroll-target="#ols-recap"><span class="header-section-number">14.1</span> OLS Recap</a>
  <ul>
  <li><a href="#choose-a-model" id="toc-choose-a-model" class="nav-link" data-scroll-target="#choose-a-model"><span class="header-section-number">14.1.1</span> 1. Choose a model</a></li>
  <li><a href="#choose-a-loss-function" id="toc-choose-a-loss-function" class="nav-link" data-scroll-target="#choose-a-loss-function"><span class="header-section-number">14.1.2</span> 2. Choose a loss function</a></li>
  <li><a href="#fit-the-model" id="toc-fit-the-model" class="nav-link" data-scroll-target="#fit-the-model"><span class="header-section-number">14.1.3</span> 3. Fit the model</a>
  <ul>
  <li><a href="#uniqueness-of-a-solution" id="toc-uniqueness-of-a-solution" class="nav-link" data-scroll-target="#uniqueness-of-a-solution"><span class="header-section-number">14.1.3.1</span> Uniqueness of a Solution</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sklearn" id="toc-sklearn" class="nav-link" data-scroll-target="#sklearn"><span class="header-section-number">14.2</span> <code>sklearn</code></a>
  <ul>
  <li><a href="#implementing-derived-formulas-in-code" id="toc-implementing-derived-formulas-in-code" class="nav-link" data-scroll-target="#implementing-derived-formulas-in-code"><span class="header-section-number">14.2.1</span> Implementing Derived Formulas in Code</a></li>
  <li><a href="#the-sklearn-workflow" id="toc-the-sklearn-workflow" class="nav-link" data-scroll-target="#the-sklearn-workflow"><span class="header-section-number">14.2.2</span> The <code>sklearn</code> Workflow</a></li>
  </ul></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="header-section-number">14.3</span> Gradient Descent</a>
  <ul>
  <li><a href="#minimizing-an-arbitrary-1d-function" id="toc-minimizing-an-arbitrary-1d-function" class="nav-link" data-scroll-target="#minimizing-an-arbitrary-1d-function"><span class="header-section-number">14.3.1</span> Minimizing an Arbitrary 1D Function</a>
  <ul>
  <li><a href="#the-naive-approach-guess-and-check" id="toc-the-naive-approach-guess-and-check" class="nav-link" data-scroll-target="#the-naive-approach-guess-and-check"><span class="header-section-number">14.3.1.1</span> The Naive Approach: Guess and Check</a></li>
  <li><a href="#scipy.optimize.minimize" id="toc-scipy.optimize.minimize" class="nav-link" data-scroll-target="#scipy.optimize.minimize"><span class="header-section-number">14.3.1.2</span> <code>Scipy.optimize.minimize</code></a></li>
  <li><a href="#digging-into-gradient-descent" id="toc-digging-into-gradient-descent" class="nav-link" data-scroll-target="#digging-into-gradient-descent"><span class="header-section-number">14.3.1.3</span> Digging into Gradient Descent</a></li>
  <li><a href="#algorithm-attempt-1" id="toc-algorithm-attempt-1" class="nav-link" data-scroll-target="#algorithm-attempt-1"><span class="header-section-number">14.3.1.4</span> Algorithm Attempt 1</a></li>
  <li><a href="#algorithm-attempt-2" id="toc-algorithm-attempt-2" class="nav-link" data-scroll-target="#algorithm-attempt-2"><span class="header-section-number">14.3.1.5</span> Algorithm Attempt 2</a></li>
  </ul></li>
  <li><a href="#convexity" id="toc-convexity" class="nav-link" data-scroll-target="#convexity"><span class="header-section-number">14.3.2</span> Convexity</a></li>
  <li><a href="#gradient-descent-in-1-dimension" id="toc-gradient-descent-in-1-dimension" class="nav-link" data-scroll-target="#gradient-descent-in-1-dimension"><span class="header-section-number">14.3.3</span> Gradient Descent in 1 Dimension</a>
  <ul>
  <li><a href="#gradient-descent-on-the-tips-dataset" id="toc-gradient-descent-on-the-tips-dataset" class="nav-link" data-scroll-target="#gradient-descent-on-the-tips-dataset"><span class="header-section-number">14.3.3.1</span> Gradient Descent on the <code>tips</code> Dataset</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent (Summer 2025)</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Apply the <code>sklearn</code> library for model creation and training</li>
<li>Optimizing complex models</li>
<li>Identifying cases where straight calculus or geometric arguments won’t help solve the loss function</li>
<li>Applying gradient descent for numerical optimization</li>
</ul>
</div>
</div>
</div>
<div id="26d0794a" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>pd.options.mode.chained_assignment <span class="op">=</span> <span class="va">None</span>  <span class="co"># default='warn'</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</div>
<section id="ols-recap" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="ols-recap"><span class="header-section-number">14.1</span> OLS Recap</h2>
<section id="choose-a-model" class="level3" data-number="14.1.1">
<h3 data-number="14.1.1" class="anchored" data-anchor-id="choose-a-model"><span class="header-section-number">14.1.1</span> 1. Choose a model</h3>
<p>Recall that when using multiple linear regression, we can generate a prediction for each of our <span class="math inline">\(n\)</span> data points:</p>
<p><span class="math display">\[\hat{y} =\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{p}x_{p}\]</span></p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/ols_matrices_old.png" alt="ols_matrices_old" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>In the previous lecture, we used p+1 features to account for the intercept, <span class="math inline">\(\theta_0\)</span>. This makes slides and notation messy.<br>
Let’s redefine <strong>p as the number of columns in our covariate matrix</strong> and <strong>add a column of 1s</strong> to encode the intercept (if desired). If we choose to add a column of 1s, then <span class="math inline">\(x_1\)</span> can be a 1 for every data point.</p>
<p><span class="math display">\[\hat{y} =\theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{p}x_{p}\]</span></p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/ols_matrices_new.png" alt="ols_matrices_new" width="600">
</td>
</tr>
</tbody></table>
</div>
</section>
<section id="choose-a-loss-function" class="level3" data-number="14.1.2">
<h3 data-number="14.1.2" class="anchored" data-anchor-id="choose-a-loss-function"><span class="header-section-number">14.1.2</span> 2. Choose a loss function</h3>
<p>Recall that we then choose the mean squared error loss function shown below where the prediction vector <span class="math inline">\(\hat{\mathbb{Y}}\)</span> depends on <span class="math inline">\(\theta\)</span>. <span class="math display">\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} (||\mathbb{Y} - \hat{\mathbb{Y}}||_2)^2\]</span></p>
</section>
<section id="fit-the-model" class="level3" data-number="14.1.3">
<h3 data-number="14.1.3" class="anchored" data-anchor-id="fit-the-model"><span class="header-section-number">14.1.3</span> 3. Fit the model</h3>
<p>We can then minimize the average loss with calculus or geometry. See the previous lecture for a derivation on the Normal Equation (<span class="math inline">\(\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\)</span>) using geometry. We can see what the matrices look like with our new interpretation where <span class="math inline">\(\mathbb{X}\)</span> is now an <span class="math inline">\(n\)</span> by <span class="math inline">\(p\)</span> matrix instead of an <span class="math inline">\(n\)</span> by <span class="math inline">\(p+1\)</span> matrix.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/ols_solution_matrices.png" alt="ols_solution_matrices" width="400">
</td>
</tr>
</tbody></table>
</div>
<p>To summarize:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Model</th>
<th>Estimate</th>
<th>Unique?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Constant Model + MSE</td>
<td><span class="math inline">\(\hat{y} = \theta_0\)</span></td>
<td><span class="math inline">\(\hat{\theta}_0 = mean(y) = \bar{y}\)</span></td>
<td><strong>Yes</strong>. Any set of values has a unique mean.</td>
</tr>
<tr class="even">
<td>Constant Model + MAE</td>
<td><span class="math inline">\(\hat{y} = \theta_0\)</span></td>
<td><span class="math inline">\(\hat{\theta}_0 = median(y)\)</span></td>
<td><strong>Yes</strong>, if odd. <strong>No</strong>, if even. Return the average of the middle 2 values.</td>
</tr>
<tr class="odd">
<td>Simple Linear Regression + MSE</td>
<td><span class="math inline">\(\hat{y} = \theta_0 + \theta_1x\)</span></td>
<td><span class="math inline">\(\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x}\)</span> <span class="math inline">\(\hat{\theta}_1 = r\frac{\sigma_y}{\sigma_x}\)</span></td>
<td><strong>Yes</strong>. Any set of non-constant* values has a unique mean, SD, and correlation coefficient.</td>
</tr>
<tr class="even">
<td><strong>OLS</strong> (Linear Model + MSE)</td>
<td><span class="math inline">\(\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}\)</span></td>
<td><span class="math inline">\(\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\)</span></td>
<td><strong>Yes</strong>, if <span class="math inline">\(\mathbb{X}\)</span> is full column rank (all columns are linearly independent, # of datapoints &gt;&gt;&gt; # of features).</td>
</tr>
</tbody>
</table>
<section id="uniqueness-of-a-solution" class="level4" data-number="14.1.3.1">
<h4 data-number="14.1.3.1" class="anchored" data-anchor-id="uniqueness-of-a-solution"><span class="header-section-number">14.1.3.1</span> Uniqueness of a Solution</h4>
<p>In most settings, the number of observations (<span class="math inline">\(n\)</span>) is much greater than the number of features (<span class="math inline">\(p\)</span>). Note that at least one solution always exists because intuitively, we can always draw a line of best fit for a given set of data, but there may be multiple lines that are “equally good”. (Formal proof is beyond this course.) Let’s now revisit the interpretation for uniqueness of a solution at the end of the last lecture, but with the new notation of <span class="math inline">\(p\)</span> instead of <span class="math inline">\(p+1\)</span> features.</p>
<p>The Least Squares estimate <span class="math inline">\(\hat{\theta}\)</span> is <strong>unique</strong> if and only if <span class="math inline">\(\mathbb{X}\)</span> is <strong>full column rank</strong>.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<ul>
<li>We know the solution to the normal equation <span class="math inline">\(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{X}^T\mathbb{Y}\)</span> is the least square estimate that minimizes the squared loss.</li>
<li><span class="math inline">\(\hat{\theta}\)</span> has a <strong>unique</strong> solution <span class="math inline">\(\iff\)</span> the square matrix <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> is <strong>invertible</strong> <span class="math inline">\(\iff\)</span> <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> is full rank.
<ul>
<li>The <strong>column rank</strong> of a square matrix is the max number of linearly independent columns it contains.</li>
<li>An <span class="math inline">\(n\)</span> x <span class="math inline">\(n\)</span> square matrix is deemed full column rank when all of its columns are linearly independent. That is, its rank would be equal to <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has shape <span class="math inline">\(p \times p\)</span>, and therefore has max rank <span class="math inline">\(p\)</span>.</li>
</ul></li>
<li><span class="math inline">\(rank(\mathbb{X}^T\mathbb{X})\)</span> = <span class="math inline">\(rank(\mathbb{X})\)</span> (proof out of scope).</li>
<li>Therefore, <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has rank <span class="math inline">\(p\)</span> <span class="math inline">\(\iff\)</span> <span class="math inline">\(\mathbb{X}\)</span> has rank <span class="math inline">\(p\)</span> <span class="math inline">\(\iff \mathbb{X}\)</span> is full column rank.</li>
</ul>
</div>
</div>
</div>
<p>Therefore, if <span class="math inline">\(\mathbb{X}\)</span> is not full column rank, we will not have unique estimates. This can happen for two major reasons.</p>
<ol type="1">
<li>If our design matrix <span class="math inline">\(\mathbb{X}\)</span> is “<strong>wide</strong>”:
<ul>
<li>If n &lt; p, then we have way more features (columns) than observations (rows).</li>
<li>Then <span class="math inline">\(rank(\mathbb{X})\)</span> = min(n, p) &lt; p, so <span class="math inline">\(\hat{\theta}\)</span> is not unique.</li>
<li>Typically we have n &gt;&gt; p so this is less of an issue.</li>
</ul></li>
<li>If our design matrix <span class="math inline">\(\mathbb{X}\)</span> has features that are <strong>linear combinations</strong> of other features:
<ul>
<li>By definition, rank of <span class="math inline">\(\mathbb{X}\)</span> is number of linearly independent columns in <span class="math inline">\(\mathbb{X}\)</span>.</li>
<li>Example: If “Width”, “Height”, and “Perimeter” are all columns,
<ul>
<li>Perimeter = 2 * Width + 2 * Height <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\mathbb{X}\)</span> is not full rank.</li>
</ul></li>
<li>Important with one-hot encoding (to discuss later).</li>
</ul></li>
</ol>
<p>Let’s now explore how to use the normal equations with a real-world dataset in the next section.</p>
</section>
</section>
</section>
<section id="sklearn" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="sklearn"><span class="header-section-number">14.2</span> <code>sklearn</code></h2>
<section id="implementing-derived-formulas-in-code" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="implementing-derived-formulas-in-code"><span class="header-section-number">14.2.1</span> Implementing Derived Formulas in Code</h3>
<p>Throughout this lecture, we’ll refer to the <code>penguins</code> dataset.</p>
<div id="c41ceb70" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> penguins[penguins[<span class="st">"species"</span>] <span class="op">==</span> <span class="st">"Adelie"</span>].dropna()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>penguins.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">species</th>
<th data-quarto-table-cell-role="th">island</th>
<th data-quarto-table-cell-role="th">bill_length_mm</th>
<th data-quarto-table-cell-role="th">bill_depth_mm</th>
<th data-quarto-table-cell-role="th">flipper_length_mm</th>
<th data-quarto-table-cell-role="th">body_mass_g</th>
<th data-quarto-table-cell-role="th">sex</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.1</td>
<td>18.7</td>
<td>181.0</td>
<td>3750.0</td>
<td>Male</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.5</td>
<td>17.4</td>
<td>186.0</td>
<td>3800.0</td>
<td>Female</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>Adelie</td>
<td>Torgersen</td>
<td>40.3</td>
<td>18.0</td>
<td>195.0</td>
<td>3250.0</td>
<td>Female</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">4</th>
<td>Adelie</td>
<td>Torgersen</td>
<td>36.7</td>
<td>19.3</td>
<td>193.0</td>
<td>3450.0</td>
<td>Female</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">5</th>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.3</td>
<td>20.6</td>
<td>190.0</td>
<td>3650.0</td>
<td>Male</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Our goal will be to predict the value of the <code>"bill_depth_mm"</code> for a particular penguin given its <code>"flipper_length_mm"</code> and <code>"body_mass_g"</code>. We’ll also add a bias column of all ones to represent the intercept term of our models.</p>
<div id="0af3dcf5" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a bias column of all ones to `penguins`</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"bias"</span>] <span class="op">=</span> np.ones(<span class="bu">len</span>(penguins), dtype<span class="op">=</span><span class="bu">int</span>) </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the design matrix, X...</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we use .to_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">"bias"</span>, <span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]].to_numpy()</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ...as well as the target variable, Y</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Again, we use .to_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[[<span class="st">"bill_depth_mm"</span>]].to_numpy()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>In the lecture on ordinary least squares, we expressed multiple linear regression using matrix notation.</p>
<p><span class="math display">\[\hat{\mathbb{Y}} = \mathbb{X}\theta\]</span></p>
<p>We used a geometric approach to derive the following expression for the optimal model parameters:</p>
<p><span class="math display">\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\]</span></p>
<p>That’s a whole lot of matrix manipulation. How do we implement it in <code>python</code>?</p>
<p>There are three operations we need to perform here: multiplying matrices, taking transposes, and finding inverses.</p>
<ul>
<li>To perform matrix multiplication, use the <code>@</code> operator</li>
<li>To take a transpose, call the <code>.T</code> attribute of an <code>NumPy</code> array or <code>DataFrame</code></li>
<li>To compute an inverse, use <code>NumPy</code>’s in-built method <code>np.linalg.inv</code></li>
</ul>
<p>Putting this all together, we can compute the OLS estimate for the optimal model parameters, stored in the array <code>theta_hat</code>.</p>
<div id="0fc28786" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>theta_hat <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> Y</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>theta_hat</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[1.10029953e+01],
       [9.82848689e-03],
       [1.47749591e-03]])</code></pre>
</div>
</div>
<p>To make predictions using our optimized parameter values, we matrix-multiply the design matrix with the parameter vector:</p>
<p><span class="math display">\[\hat{\mathbb{Y}} = \mathbb{X}\theta\]</span></p>
<div id="438403ef" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Y_hat <span class="op">=</span> X <span class="op">@</span> theta_hat</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(Y_hat).head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>18.322561</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>18.445578</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>17.721412</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>17.997254</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>18.263268</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="the-sklearn-workflow" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="the-sklearn-workflow"><span class="header-section-number">14.2.2</span> The <code>sklearn</code> Workflow</h3>
<p>We’ve already saved a lot of time (and avoided tedious calculations) by translating our derived formulas into code. However, we still had to go through the process of writing out the linear algebra ourselves.</p>
<p>To make life <em>even easier</em>, we can turn to the <code>sklearn</code> <a href="https://scikit-learn.org/stable/"><code>python</code> library</a>. <code>sklearn</code> is a robust library of machine learning tools used extensively in research and industry. It is the standard for simple machine learning tasks and gives us a wide variety of in-built modeling frameworks and methods, so we’ll keep returning to <code>sklearn</code> techniques as we progress through Data 100.</p>
<p>Regardless of the specific type of model being implemented, <code>sklearn</code> follows a standard set of steps for creating a model:</p>
<ol type="1">
<li><p>Import the <code>LinearRegression</code> model from <code>sklearn</code></p>
<pre><code>from sklearn.linear_model import LinearRegression</code></pre></li>
<li><p>Create a model object. This generates a new instance of the model class. You can think of it as making a new “copy” of a standard “template” for a model. In code, this looks like:</p>
<pre><code>my_model = LinearRegression()</code></pre></li>
<li><p>Fit the model to the <code>X</code> design matrix and <code>Y</code> target vector. This calculates the optimal model parameters “behind the scenes” without us explicitly working through the calculations ourselves. The fitted parameters are then stored within the model for use in future predictions:</p>
<pre><code>my_model.fit(X, Y)</code></pre></li>
<li><p>Use the fitted model to make predictions on the <code>X</code> input data using <code>.predict</code>.</p>
<pre><code>my_model.predict(X)</code></pre></li>
</ol>
<p>To extract the fitted parameters, we can use:</p>
<pre><code>my_model.coef_

my_model.intercept_</code></pre>
<p>Let’s put this into action with our multiple regression task!</p>
<p><strong>1. Initialize an instance of the model class</strong></p>
<p><code>sklearn</code> stores “templates” of useful models for machine learning. We begin the modeling process by making a “copy” of one of these templates for our own use. Model initialization looks like <code>ModelClass()</code>, where <code>ModelClass</code> is the type of model we wish to create.</p>
<p>For now, let’s create a linear regression model using <code>LinearRegression</code>.</p>
<p><code>my_model</code> is now an instance of the <code>LinearRegression</code> class. You can think of it as the “idea” of a linear regression model. We haven’t trained it yet, so it doesn’t know any model parameters and cannot be used to make predictions. In fact, we haven’t even told it what data to use for modeling! It simply waits for further instructions.</p>
<div id="ab2e377d" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>my_model <span class="op">=</span> LinearRegression()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>2. Train the model using <code>.fit</code></strong></p>
<p>Before the model can make predictions, we will need to fit it to our training data. When we fit the model, <code>sklearn</code> will run gradient descent behind the scenes to determine the optimal model parameters. It will then save these model parameters to our model instance for future use.</p>
<p>All <code>sklearn</code> model classes include a <code>.fit</code> method, which is used to fit the model. It takes in two inputs: the design matrix, <code>X</code>, and the target variable, <code>Y</code>.</p>
<p>Let’s start by fitting a model with just one feature: the flipper length. We create a design matrix <code>X</code> by pulling out the <code>"flipper_length_mm"</code> column from the <code>DataFrame</code>.</p>
<div id="fe976502" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># .fit expects a 2D data design matrix, so we use double brackets to extract a DataFrame</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">"flipper_length_mm"</span>]]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[<span class="st">"bill_depth_mm"</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>my_model.fit(X, Y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
</div>
</div>
<p>Notice that we use <strong>double brackets</strong> to extract this column. Why double brackets instead of just single brackets? The <code>.fit</code> method, by default, expects to receive <strong>2-dimensional</strong> data – some kind of data that includes both rows and columns. Writing <code>penguins["flipper_length_mm"]</code> would return a 1D <code>Series</code>, causing <code>sklearn</code> to error. We avoid this by writing <code>penguins[["flipper_length_mm"]]</code> to produce a 2D <code>DataFrame</code>.</p>
<p>And in just three lines of code, our model has run gradient descent to determine the optimal model parameters! Our single-feature model takes the form:</p>
<p><span class="math display">\[\text{bill depth} = \theta_0 + \theta_1 \text{flipper length}\]</span></p>
<p>Note that <code>LinearRegression</code> will automatically include an intercept term.</p>
<p>The fitted model parameters are stored as attributes of the model instance. <code>my_model.intercept_</code> will return the value of <span class="math inline">\(\hat{\theta}_0\)</span> as a scalar. <code>my_model.coef_</code> will return all values <span class="math inline">\(\hat{\theta}_1,
\hat{\theta}_2, ...\)</span> in an array. Because our model only contains one feature, we see just the value of <span class="math inline">\(\hat{\theta}_1\)</span> in the cell below.</p>
<div id="7d653b44" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The intercept term, theta_0</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>my_model.intercept_</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>7.297305899612299</code></pre>
</div>
</div>
<div id="95b147f8" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># All parameters theta_1, ..., theta_p</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>my_model.coef_</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([0.05812622])</code></pre>
</div>
</div>
<p><strong>3. Use the fitted model to make predictions</strong></p>
<p>Now that the model has been trained, we can use it to make predictions! To do so, we use the <code>.predict</code> method. <code>.predict</code> takes in one argument: the design matrix that should be used to generate predictions. To understand how the model performs on the training set, we would pass in the training data. Alternatively, to make predictions on unseen data, we would pass in a new dataset that wasn’t used to train the model.</p>
<p>Below, we call <code>.predict</code> to generate model predictions on the original training data. As before, we use double brackets to ensure that we extract 2-dimensional data.</p>
<div id="8e9f0382" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>Y_hat_one_feature <span class="op">=</span> my_model.predict(penguins[[<span class="st">"flipper_length_mm"</span>]])</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>Y_hat_one_feature[:<span class="dv">10</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([17.81815239, 18.10878351, 18.63191952, 18.51566707, 18.3412884 ,
       17.81815239, 18.63191952, 17.87627861, 18.39941463, 18.80629819])</code></pre>
</div>
</div>
<p>The <code>sklearn</code> package also provides a function <code>mean_squared_error()</code> <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html">documentation</a> that computes the MSE from a list of observations and predictions. This avoids us having to manually compute MSE by first computing residuals.</p>
<div id="cca22d2e" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The MSE of the model is </span><span class="sc">{</span>mean_squared_error(Y, Y_hat_one_feature)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The MSE of the model is 1.333877879980637</code></pre>
</div>
</div>
<p>What if we wanted a model with two features?</p>
<p><span class="math display">\[\text{bill depth} = \theta_0 + \theta_1 \text{flipper length} + \theta_2 \text{body mass}\]</span></p>
<p>We repeat this three-step process by intializing a new model object, then calling <code>.fit</code> and <code>.predict</code> as before.</p>
<div id="cd108196" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: initialize LinearRegression model</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>two_feature_model <span class="op">=</span> LinearRegression()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: fit the model</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>X_two_features <span class="op">=</span> penguins[[<span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[<span class="st">"bill_depth_mm"</span>]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>two_feature_model.fit(X_two_features, Y)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: make predictions</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>Y_hat_two_features <span class="op">=</span> two_feature_model.predict(X_two_features)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The MSE of the model is </span><span class="sc">{</span>mean_squared_error(Y, Y_hat_two_features)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>The MSE of the model is 0.9764070438844</code></pre>
</div>
</div>
<p>We can also see that we obtain the same predictions using <code>sklearn</code> as we did when applying the ordinary least squares formula before!</p>
<div id="54ba083b" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Y_hat from OLS"</span>:np.squeeze(Y_hat), <span class="st">"Y_hat from sklearn"</span>:Y_hat_two_features}).head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Y_hat from OLS</th>
<th data-quarto-table-cell-role="th">Y_hat from sklearn</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>18.322561</td>
<td>18.322561</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>18.445578</td>
<td>18.445578</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>17.721412</td>
<td>17.721412</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>17.997254</td>
<td>17.997254</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>18.263268</td>
<td>18.263268</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="gradient-descent" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">14.3</span> Gradient Descent</h2>
<p>At this point, we’ve grown quite familiar with the process of choosing a model and a corresponding loss function and optimizing parameters by choosing the values of <span class="math inline">\(\theta\)</span> that minimize the loss function. So far, we’ve optimized <span class="math inline">\(\theta\)</span> by</p>
<ol type="1">
<li>Using calculus to take the derivative of the loss function with respect to <span class="math inline">\(\theta\)</span>, setting it equal to 0, and solving for <span class="math inline">\(\theta\)</span>.</li>
<li>Using the geometric argument of orthogonality to derive the OLS solution <span class="math inline">\(\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}\)</span>.</li>
</ol>
<p>One thing to note, however, is that the techniques we used above can only be applied if we make some big assumptions. For the calculus approach, we assumed that the loss function was differentiable at all points and that we could algebraically solve for the zero points of the derivative; for the geometric approach, OLS <em>only</em> applies when using a linear model with MSE loss. What happens when we have more complex models with different, more complex loss functions? The techniques we’ve learned so far will not work, so we need a new optimization technique: <strong>gradient descent</strong>.</p>
<blockquote class="blockquote">
<p><strong>BIG IDEA</strong>: use an iterative algorithm to numerically compute the minimum of the loss.</p>
</blockquote>
<section id="minimizing-an-arbitrary-1d-function" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="minimizing-an-arbitrary-1d-function"><span class="header-section-number">14.3.1</span> Minimizing an Arbitrary 1D Function</h3>
<p>Let’s consider an arbitrary function. Our goal is to find the value of <span class="math inline">\(x\)</span> that minimizes this function.</p>
<div id="8c8ef857" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> arbitrary(x):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">15</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">80</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">180</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">144</span>)<span class="op">/</span><span class="dv">10</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><img src="images/arbitrary.png" alt="arbitrary" width="600"></p>
<section id="the-naive-approach-guess-and-check" class="level4" data-number="14.3.1.1">
<h4 data-number="14.3.1.1" class="anchored" data-anchor-id="the-naive-approach-guess-and-check"><span class="header-section-number">14.3.1.1</span> The Naive Approach: Guess and Check</h4>
<p>Above, we saw that the minimum is somewhere around 5.3. Let’s see if we can figure out how to find the exact minimum algorithmically from scratch. One very slow (and terrible) way would be manual guess-and-check.</p>
<div id="0b3f68d8" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>arbitrary(<span class="dv">6</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>0.0</code></pre>
</div>
</div>
<p>A somewhat better (but still slow) approach is to use brute force to try out a bunch of x values and return the one that yields the lowest loss.</p>
<div id="1d3b1706" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_minimize(f, xs):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Takes in a function f and a set of values xs. </span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculates the value of the function f at all values x in xs</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Takes the minimum value of f(x) and returns the corresponding value x </span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [f(x) <span class="cf">for</span> x <span class="kw">in</span> xs]  </span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs[np.argmin(y)]</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> [<span class="fl">5.3</span>, <span class="fl">5.31</span>, <span class="fl">5.32</span>, <span class="fl">5.33</span>, <span class="fl">5.34</span>, <span class="fl">5.35</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>simple_minimize(arbitrary, guesses)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>5.33</code></pre>
</div>
</div>
<p>This process is essentially the same as before where we made a graphical plot, it’s just that we’re only looking at 20 selected points.</p>
<div id="660b1240" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">200</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>sparse_xs <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">5</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> arbitrary(xs)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>sparse_ys <span class="op">=</span> arbitrary(sparse_xs)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(x <span class="op">=</span> xs, y <span class="op">=</span> arbitrary(xs))</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>fig.add_scatter(x <span class="op">=</span> sparse_xs, y <span class="op">=</span> arbitrary(sparse_xs), mode <span class="op">=</span> <span class="st">"markers"</span>)</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>fig.update_layout(showlegend<span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>fig.update_layout(autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>                            <div id="81a91f00-d7f1-449d-a963-f8ff81fd3fdf" class="plotly-graph-div" style="height:600px; width:800px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("81a91f00-d7f1-449d-a963-f8ff81fd3fdf")) {                    Plotly.newPlot(                        "81a91f00-d7f1-449d-a963-f8ff81fd3fdf",                        [{"hovertemplate":"x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"","orientation":"v","showlegend":false,"x":[1.0,1.0301507537688441,1.0603015075376885,1.0904522613065326,1.120603015075377,1.150753768844221,1.1809045226130652,1.2110552763819096,1.2412060301507537,1.271356783919598,1.3015075376884422,1.3316582914572863,1.3618090452261307,1.3919597989949748,1.4221105527638191,1.4522613065326633,1.4824120603015074,1.5125628140703518,1.542713567839196,1.5728643216080402,1.6030150753768844,1.6331658291457285,1.6633165829145728,1.6934673366834172,1.7236180904522613,1.7537688442211055,1.7839195979899496,1.814070351758794,1.8442211055276383,1.8743718592964824,1.9045226130653266,1.9346733668341707,1.964824120603015,1.9949748743718594,2.0251256281407035,2.0552763819095476,2.085427135678392,2.115577889447236,2.1457286432160805,2.1758793969849246,2.2060301507537687,2.2361809045226133,2.266331658291457,2.2964824120603016,2.3266331658291457,2.35678391959799,2.3869346733668344,2.417085427135678,2.4472361809045227,2.477386934673367,2.507537688442211,2.5376884422110555,2.567839195979899,2.5979899497487438,2.628140703517588,2.658291457286432,2.6884422110552766,2.7185929648241203,2.748743718592965,2.778894472361809,2.809045226130653,2.8391959798994977,2.8693467336683414,2.899497487437186,2.92964824120603,2.959798994974874,2.9899497487437188,3.020100502512563,3.050251256281407,3.080402010050251,3.1105527638190953,3.1407035175879394,3.170854271356784,3.201005025125628,3.2311557788944723,3.2613065326633164,3.2914572864321605,3.321608040201005,3.351758793969849,3.3819095477386933,3.4120603015075375,3.4422110552763816,3.472361809045226,3.5025125628140703,3.5326633165829144,3.5628140703517586,3.5929648241206027,3.6231155778894473,3.6532663316582914,3.6834170854271355,3.7135678391959797,3.743718592964824,3.7738693467336684,3.8040201005025125,3.8341708542713566,3.8643216080402008,3.8944723618090453,3.9246231155778895,3.9547738693467336,3.9849246231155777,4.015075376884422,4.045226130653266,4.075376884422111,4.105527638190955,4.135678391959798,4.165829145728643,4.1959798994974875,4.226130653266331,4.256281407035176,4.28643216080402,4.316582914572864,4.346733668341709,4.376884422110553,4.407035175879397,4.4371859296482405,4.467336683417085,4.49748743718593,4.527638190954773,4.557788944723618,4.5879396984924625,4.618090452261306,4.648241206030151,4.678391959798995,4.708542713567839,4.738693467336683,4.768844221105527,4.798994974874372,4.829145728643216,4.85929648241206,4.889447236180905,4.919597989949748,4.949748743718593,4.9798994974874375,5.010050251256281,5.040201005025126,5.0703517587939695,5.100502512562814,5.130653266331658,5.160804020100502,5.190954773869347,5.221105527638191,5.251256281407035,5.281407035175879,5.311557788944723,5.341708542713568,5.371859296482412,5.402010050251256,5.4321608040201,5.4623115577889445,5.492462311557789,5.522613065326633,5.552763819095477,5.582914572864321,5.613065326633166,5.64321608040201,5.673366834170854,5.703517587939698,5.733668341708542,5.763819095477387,5.793969849246231,5.824120603015075,5.8542713567839195,5.884422110552763,5.914572864321608,5.944723618090452,5.974874371859296,6.005025125628141,6.035175879396984,6.065326633165829,6.0954773869346734,6.125628140703517,6.155778894472362,6.185929648241205,6.21608040201005,6.2462311557788945,6.276381909547738,6.306532663316583,6.3366834170854265,6.366834170854271,6.396984924623116,6.427135678391959,6.457286432160804,6.487437185929648,6.517587939698492,6.547738693467337,6.57788944723618,6.608040201005025,6.638190954773869,6.668341708542713,6.698492462311558,6.7286432160804015,6.758793969849246,6.788944723618091,6.819095477386934,6.849246231155779,6.879396984924623,6.909547738693467,6.939698492462312,6.969849246231155,7.0],"xaxis":"x","y":[3.0,2.8197775132646994,2.6468296407545298,2.480978457571409,2.3220480221881674,2.169864376448527,2.0242555455671196,1.8850515381294826,1.7520843460920474,1.6251879447821538,1.5041982928980473,1.3889533325088705,1.2792929890546703,1.175059171346399,1.076095771565909,0.9822486652659563,0.8933657113701969,0.809296752173205,0.7298936133404282,0.6550101039082478,0.5845020162839318,0.5182271262456482,0.45604519294247436,0.39781795889439875,0.34340914999228855,0.2926844754979413,0.24551162804404497,0.20176028363418083,0.1613021016428462,0.1240107248154402,0.08976177926825812,0.05843287448851129,0.029903603334304307,0.0040555420346265695,-0.019227749810596606,-0.04006072923056081,-0.0585558698835257,-0.07482366205689459,-0.08897261266714337,-0.10110924525984047,-0.11133810000965809,-0.119761733720361,-0.12648071982483203,-0.1315936483850237,-0.13519712609199247,-0.13738577626590426,-0.1382522388560119,-0.13788717044065493,-0.13637924422729383,-0.13381515005247593,-0.13027959438184666,-0.1258553003101497,-0.12062300756120407,-0.11466147248795551,-0.10804746807244214,-0.10085578392577758,-0.09315922628821909,-0.08502861802905386,-0.0765327986467014,-0.06773862426869641,-0.05871096765167181,-0.04951271818131318,-0.04020478187242702,-0.030846081368940757,-0.021493555943828825,-0.012202161499220664,-0.0030248705662870635,0.005987327694657552,0.014785427494228998,0.023322406413939234,0.03155322540619636,0.039434828794344415,0.046926144272549666,0.053988082905993905,0.06058353913065275,0.06667739075349459,0.0722364989522987,0.0772297082758655,0.08162784664377228,0.0854037253466231,0.0885321390457932,0.09098986577366759,0.09275566693348196,0.0938102872993909,0.09413645501645647,0.09371888160064827,0.09254426193878089,0.09060127428864462,0.08788058027890884,0.08437482490911634,0.08007863654980837,0.07498862694224044,0.06910339119879154,0.062423507802634504,0.05495153860777009,0.04669202883931121,0.037651507093005424,0.027838485335746555,0.01726345890522225,0.005938906509970821,-0.006120709770470967,-0.018898944485715673,-0.03237736881442288,-0.04653557056435602,-0.061351154172564294,-0.07679974070485969,-0.0928549678563968,-0.10948848995135449,-0.12666997794297002,-0.14436711941368685,-0.16254561857489308,-0.1811691962672853,-0.2001995899604026,-0.21959655375303555,-0.23931785837306735,-0.25931929117742814,-0.2795546561521974,-0.2999757739125698,-0.32053248170270765,-0.34117263339597914,-0.3618420994948906,-0.38248476713084756,-0.40304254006464363,-0.4234553386859261,-0.4436611000135485,-0.46359577769538873,-0.483193342008542,-0.5023857798591053,-0.5211030947822792,-0.5392733069424139,-0.5568224531329065,-0.5736745867762579,-0.589751777924107,-0.6049741132570944,-0.6192596960850778,-0.632524646347008,-0.6446831006107573,-0.6556472120734724,-0.6653271505614157,-0.6736311025297482,-0.6804652710629284,-0.6857338758744731,-0.689339153306878,-0.6911813563318902,-0.6911587545502471,-0.6891676341918014,-0.6851022981155552,-0.6788550658095346,-0.6703162733909721,-0.6593742736060904,-0.6459154358302271,-0.6298241460678924,-0.6109828069525293,-0.5892718377469237,-0.5645696743426925,-0.5367527692607041,-0.5056955916510105,-0.47127062729252883,-0.4333483785933822,-0.39179736459091147,-0.3464841209514134,-0.2972731999702091,-0.24402717057191695,-0.18660661831017933,-0.12487014536754941,-0.05867437055599112,0.01212607068350735,0.08767852628116088,0.1681323275376599,0.2536387891250115,0.34435120908576666,0.4404248688334974,0.5420170331528652,0.6492869501990981,0.7623958514986725,0.8815069519487224,1.0067854498173574,1.13839852674364,1.2765153477374043,1.421307061179641,1.572946798821863,1.7316096757870127,1.897472790568304,2.070715225030267,2.251518044408317,2.4400642973086635,2.636539015708513,2.841129214955754,3.0540238937694992,3.27541403423952,3.5054926018266315,3.7444545453624185,3.9924967970495344,4.249818272461312,4.516619870542331,4.793104473607627,5.079476947343528,5.37594414080711,5.682714886426311,6.0],"yaxis":"y","type":"scatter"},{"mode":"markers","x":[1.0,2.5,4.0,5.5,7.0],"y":[3.0,-0.13125,0.0,-0.65625,6.0],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"y"}},"legend":{"tracegroupgap":0},"showlegend":false,"autosize":false,"width":800,"height":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('81a91f00-d7f1-449d-a963-f8ff81fd3fdf');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>This basic approach suffers from three major flaws:</p>
<ol type="1">
<li>If the minimum is outside our range of guesses, the answer will be completely wrong.</li>
<li>Even if our range of guesses is correct, if the guesses are too coarse, our answer will be inaccurate.</li>
<li>It is <em>very</em> computationally inefficient, considering potentially vast numbers of guesses that are useless.</li>
</ol>
</section>
<section id="scipy.optimize.minimize" class="level4" data-number="14.3.1.2">
<h4 data-number="14.3.1.2" class="anchored" data-anchor-id="scipy.optimize.minimize"><span class="header-section-number">14.3.1.2</span> <code>Scipy.optimize.minimize</code></h4>
<p>One way to minimize this mathematical function is to use the <code>scipy.optimize.minimize</code> function. It takes a function and a starting guess and tries to find the minimum.</p>
<div id="a311feb0" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># takes a function f and a starting point x0 and returns a readout </span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># with the optimal input value of x which minimizes f</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>minimize(arbitrary, x0 <span class="op">=</span> <span class="fl">3.5</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: -0.13827491292966557
        x: [ 2.393e+00]
      nit: 3
      jac: [ 6.486e-06]
 hess_inv: [[ 7.385e-01]]
     nfev: 20
     njev: 10</code></pre>
</div>
</div>
<p><code>scipy.optimize.minimize</code> is great. It may also seem a bit magical. How could you write a function that can find the minimum of any mathematical function? There are a number of ways to do this, which we’ll explore in today’s lecture, eventually arriving at the important idea of <strong>gradient descent</strong>, which is the principle that <code>scipy.optimize.minimize</code> uses.</p>
<p>It turns out that under the hood, the <code>fit</code> method for <code>LinearRegression</code> models uses gradient descent. Gradient descent is also how much of machine learning works, including even advanced neural network models.</p>
<p>In Data 100, the gradient descent process will usually be invisible to us, hidden beneath an abstraction layer. However, to be good data scientists, it’s important that we know the underlying principles that optimization functions harness to find optimal parameters.</p>
</section>
<section id="digging-into-gradient-descent" class="level4" data-number="14.3.1.3">
<h4 data-number="14.3.1.3" class="anchored" data-anchor-id="digging-into-gradient-descent"><span class="header-section-number">14.3.1.3</span> Digging into Gradient Descent</h4>
<p>Looking at the function across this domain, it is clear that the function’s minimum value occurs around <span class="math inline">\(\theta = 5.3\)</span>. Let’s pretend for a moment that we <em>couldn’t</em> see the full view of the cost function. How would we guess the value of <span class="math inline">\(\theta\)</span> that minimizes the function?</p>
<p>It turns out that the first derivative of the function can give us a clue. In the graph below, the function and its derivative are plotted, with points where the derivative is equal to 0 plotted in light green.</p>
<div id="9cc8caad" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative_arbitrary(x):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">45</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">160</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">180</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure()</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>roots <span class="op">=</span> np.array([<span class="fl">2.3927</span>, <span class="fl">3.5309</span>, <span class="fl">5.3263</span>])</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> xs, y <span class="op">=</span> arbitrary(xs), </span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"lines"</span>, name <span class="op">=</span> <span class="st">"f"</span>))</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> xs, y <span class="op">=</span> derivative_arbitrary(xs), </span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"lines"</span>, name <span class="op">=</span> <span class="st">"df"</span>, line <span class="op">=</span> {<span class="st">"dash"</span>: <span class="st">"dash"</span>}))</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> np.array(roots), y <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>roots, </span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"markers"</span>, name <span class="op">=</span> <span class="st">"df = zero"</span>, marker_size <span class="op">=</span> <span class="dv">12</span>))</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>fig.update_layout(font_size <span class="op">=</span> <span class="dv">20</span>, yaxis_range<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>fig.update_layout(autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>                            <div id="51a262f5-5249-4eee-8c1f-59c0612c7c8c" class="plotly-graph-div" style="height:600px; width:800px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("51a262f5-5249-4eee-8c1f-59c0612c7c8c")) {                    Plotly.newPlot(                        "51a262f5-5249-4eee-8c1f-59c0612c7c8c",                        [{"mode":"lines","name":"f","x":[1.0,1.0301507537688441,1.0603015075376885,1.0904522613065326,1.120603015075377,1.150753768844221,1.1809045226130652,1.2110552763819096,1.2412060301507537,1.271356783919598,1.3015075376884422,1.3316582914572863,1.3618090452261307,1.3919597989949748,1.4221105527638191,1.4522613065326633,1.4824120603015074,1.5125628140703518,1.542713567839196,1.5728643216080402,1.6030150753768844,1.6331658291457285,1.6633165829145728,1.6934673366834172,1.7236180904522613,1.7537688442211055,1.7839195979899496,1.814070351758794,1.8442211055276383,1.8743718592964824,1.9045226130653266,1.9346733668341707,1.964824120603015,1.9949748743718594,2.0251256281407035,2.0552763819095476,2.085427135678392,2.115577889447236,2.1457286432160805,2.1758793969849246,2.2060301507537687,2.2361809045226133,2.266331658291457,2.2964824120603016,2.3266331658291457,2.35678391959799,2.3869346733668344,2.417085427135678,2.4472361809045227,2.477386934673367,2.507537688442211,2.5376884422110555,2.567839195979899,2.5979899497487438,2.628140703517588,2.658291457286432,2.6884422110552766,2.7185929648241203,2.748743718592965,2.778894472361809,2.809045226130653,2.8391959798994977,2.8693467336683414,2.899497487437186,2.92964824120603,2.959798994974874,2.9899497487437188,3.020100502512563,3.050251256281407,3.080402010050251,3.1105527638190953,3.1407035175879394,3.170854271356784,3.201005025125628,3.2311557788944723,3.2613065326633164,3.2914572864321605,3.321608040201005,3.351758793969849,3.3819095477386933,3.4120603015075375,3.4422110552763816,3.472361809045226,3.5025125628140703,3.5326633165829144,3.5628140703517586,3.5929648241206027,3.6231155778894473,3.6532663316582914,3.6834170854271355,3.7135678391959797,3.743718592964824,3.7738693467336684,3.8040201005025125,3.8341708542713566,3.8643216080402008,3.8944723618090453,3.9246231155778895,3.9547738693467336,3.9849246231155777,4.015075376884422,4.045226130653266,4.075376884422111,4.105527638190955,4.135678391959798,4.165829145728643,4.1959798994974875,4.226130653266331,4.256281407035176,4.28643216080402,4.316582914572864,4.346733668341709,4.376884422110553,4.407035175879397,4.4371859296482405,4.467336683417085,4.49748743718593,4.527638190954773,4.557788944723618,4.5879396984924625,4.618090452261306,4.648241206030151,4.678391959798995,4.708542713567839,4.738693467336683,4.768844221105527,4.798994974874372,4.829145728643216,4.85929648241206,4.889447236180905,4.919597989949748,4.949748743718593,4.9798994974874375,5.010050251256281,5.040201005025126,5.0703517587939695,5.100502512562814,5.130653266331658,5.160804020100502,5.190954773869347,5.221105527638191,5.251256281407035,5.281407035175879,5.311557788944723,5.341708542713568,5.371859296482412,5.402010050251256,5.4321608040201,5.4623115577889445,5.492462311557789,5.522613065326633,5.552763819095477,5.582914572864321,5.613065326633166,5.64321608040201,5.673366834170854,5.703517587939698,5.733668341708542,5.763819095477387,5.793969849246231,5.824120603015075,5.8542713567839195,5.884422110552763,5.914572864321608,5.944723618090452,5.974874371859296,6.005025125628141,6.035175879396984,6.065326633165829,6.0954773869346734,6.125628140703517,6.155778894472362,6.185929648241205,6.21608040201005,6.2462311557788945,6.276381909547738,6.306532663316583,6.3366834170854265,6.366834170854271,6.396984924623116,6.427135678391959,6.457286432160804,6.487437185929648,6.517587939698492,6.547738693467337,6.57788944723618,6.608040201005025,6.638190954773869,6.668341708542713,6.698492462311558,6.7286432160804015,6.758793969849246,6.788944723618091,6.819095477386934,6.849246231155779,6.879396984924623,6.909547738693467,6.939698492462312,6.969849246231155,7.0],"y":[3.0,2.8197775132646994,2.6468296407545298,2.480978457571409,2.3220480221881674,2.169864376448527,2.0242555455671196,1.8850515381294826,1.7520843460920474,1.6251879447821538,1.5041982928980473,1.3889533325088705,1.2792929890546703,1.175059171346399,1.076095771565909,0.9822486652659563,0.8933657113701969,0.809296752173205,0.7298936133404282,0.6550101039082478,0.5845020162839318,0.5182271262456482,0.45604519294247436,0.39781795889439875,0.34340914999228855,0.2926844754979413,0.24551162804404497,0.20176028363418083,0.1613021016428462,0.1240107248154402,0.08976177926825812,0.05843287448851129,0.029903603334304307,0.0040555420346265695,-0.019227749810596606,-0.04006072923056081,-0.0585558698835257,-0.07482366205689459,-0.08897261266714337,-0.10110924525984047,-0.11133810000965809,-0.119761733720361,-0.12648071982483203,-0.1315936483850237,-0.13519712609199247,-0.13738577626590426,-0.1382522388560119,-0.13788717044065493,-0.13637924422729383,-0.13381515005247593,-0.13027959438184666,-0.1258553003101497,-0.12062300756120407,-0.11466147248795551,-0.10804746807244214,-0.10085578392577758,-0.09315922628821909,-0.08502861802905386,-0.0765327986467014,-0.06773862426869641,-0.05871096765167181,-0.04951271818131318,-0.04020478187242702,-0.030846081368940757,-0.021493555943828825,-0.012202161499220664,-0.0030248705662870635,0.005987327694657552,0.014785427494228998,0.023322406413939234,0.03155322540619636,0.039434828794344415,0.046926144272549666,0.053988082905993905,0.06058353913065275,0.06667739075349459,0.0722364989522987,0.0772297082758655,0.08162784664377228,0.0854037253466231,0.0885321390457932,0.09098986577366759,0.09275566693348196,0.0938102872993909,0.09413645501645647,0.09371888160064827,0.09254426193878089,0.09060127428864462,0.08788058027890884,0.08437482490911634,0.08007863654980837,0.07498862694224044,0.06910339119879154,0.062423507802634504,0.05495153860777009,0.04669202883931121,0.037651507093005424,0.027838485335746555,0.01726345890522225,0.005938906509970821,-0.006120709770470967,-0.018898944485715673,-0.03237736881442288,-0.04653557056435602,-0.061351154172564294,-0.07679974070485969,-0.0928549678563968,-0.10948848995135449,-0.12666997794297002,-0.14436711941368685,-0.16254561857489308,-0.1811691962672853,-0.2001995899604026,-0.21959655375303555,-0.23931785837306735,-0.25931929117742814,-0.2795546561521974,-0.2999757739125698,-0.32053248170270765,-0.34117263339597914,-0.3618420994948906,-0.38248476713084756,-0.40304254006464363,-0.4234553386859261,-0.4436611000135485,-0.46359577769538873,-0.483193342008542,-0.5023857798591053,-0.5211030947822792,-0.5392733069424139,-0.5568224531329065,-0.5736745867762579,-0.589751777924107,-0.6049741132570944,-0.6192596960850778,-0.632524646347008,-0.6446831006107573,-0.6556472120734724,-0.6653271505614157,-0.6736311025297482,-0.6804652710629284,-0.6857338758744731,-0.689339153306878,-0.6911813563318902,-0.6911587545502471,-0.6891676341918014,-0.6851022981155552,-0.6788550658095346,-0.6703162733909721,-0.6593742736060904,-0.6459154358302271,-0.6298241460678924,-0.6109828069525293,-0.5892718377469237,-0.5645696743426925,-0.5367527692607041,-0.5056955916510105,-0.47127062729252883,-0.4333483785933822,-0.39179736459091147,-0.3464841209514134,-0.2972731999702091,-0.24402717057191695,-0.18660661831017933,-0.12487014536754941,-0.05867437055599112,0.01212607068350735,0.08767852628116088,0.1681323275376599,0.2536387891250115,0.34435120908576666,0.4404248688334974,0.5420170331528652,0.6492869501990981,0.7623958514986725,0.8815069519487224,1.0067854498173574,1.13839852674364,1.2765153477374043,1.421307061179641,1.572946798821863,1.7316096757870127,1.897472790568304,2.070715225030267,2.251518044408317,2.4400642973086635,2.636539015708513,2.841129214955754,3.0540238937694992,3.27541403423952,3.5054926018266315,3.7444545453624185,3.9924967970495344,4.249818272461312,4.516619870542331,4.793104473607627,5.079476947343528,5.37594414080711,5.682714886426311,6.0],"type":"scatter"},{"line":{"dash":"dash"},"mode":"lines","name":"df","x":[1.0,1.0301507537688441,1.0603015075376885,1.0904522613065326,1.120603015075377,1.150753768844221,1.1809045226130652,1.2110552763819096,1.2412060301507537,1.271356783919598,1.3015075376884422,1.3316582914572863,1.3618090452261307,1.3919597989949748,1.4221105527638191,1.4522613065326633,1.4824120603015074,1.5125628140703518,1.542713567839196,1.5728643216080402,1.6030150753768844,1.6331658291457285,1.6633165829145728,1.6934673366834172,1.7236180904522613,1.7537688442211055,1.7839195979899496,1.814070351758794,1.8442211055276383,1.8743718592964824,1.9045226130653266,1.9346733668341707,1.964824120603015,1.9949748743718594,2.0251256281407035,2.0552763819095476,2.085427135678392,2.115577889447236,2.1457286432160805,2.1758793969849246,2.2060301507537687,2.2361809045226133,2.266331658291457,2.2964824120603016,2.3266331658291457,2.35678391959799,2.3869346733668344,2.417085427135678,2.4472361809045227,2.477386934673367,2.507537688442211,2.5376884422110555,2.567839195979899,2.5979899497487438,2.628140703517588,2.658291457286432,2.6884422110552766,2.7185929648241203,2.748743718592965,2.778894472361809,2.809045226130653,2.8391959798994977,2.8693467336683414,2.899497487437186,2.92964824120603,2.959798994974874,2.9899497487437188,3.020100502512563,3.050251256281407,3.080402010050251,3.1105527638190953,3.1407035175879394,3.170854271356784,3.201005025125628,3.2311557788944723,3.2613065326633164,3.2914572864321605,3.321608040201005,3.351758793969849,3.3819095477386933,3.4120603015075375,3.4422110552763816,3.472361809045226,3.5025125628140703,3.5326633165829144,3.5628140703517586,3.5929648241206027,3.6231155778894473,3.6532663316582914,3.6834170854271355,3.7135678391959797,3.743718592964824,3.7738693467336684,3.8040201005025125,3.8341708542713566,3.8643216080402008,3.8944723618090453,3.9246231155778895,3.9547738693467336,3.9849246231155777,4.015075376884422,4.045226130653266,4.075376884422111,4.105527638190955,4.135678391959798,4.165829145728643,4.1959798994974875,4.226130653266331,4.256281407035176,4.28643216080402,4.316582914572864,4.346733668341709,4.376884422110553,4.407035175879397,4.4371859296482405,4.467336683417085,4.49748743718593,4.527638190954773,4.557788944723618,4.5879396984924625,4.618090452261306,4.648241206030151,4.678391959798995,4.708542713567839,4.738693467336683,4.768844221105527,4.798994974874372,4.829145728643216,4.85929648241206,4.889447236180905,4.919597989949748,4.949748743718593,4.9798994974874375,5.010050251256281,5.040201005025126,5.0703517587939695,5.100502512562814,5.130653266331658,5.160804020100502,5.190954773869347,5.221105527638191,5.251256281407035,5.281407035175879,5.311557788944723,5.341708542713568,5.371859296482412,5.402010050251256,5.4321608040201,5.4623115577889445,5.492462311557789,5.522613065326633,5.552763819095477,5.582914572864321,5.613065326633166,5.64321608040201,5.673366834170854,5.703517587939698,5.733668341708542,5.763819095477387,5.793969849246231,5.824120603015075,5.8542713567839195,5.884422110552763,5.914572864321608,5.944723618090452,5.974874371859296,6.005025125628141,6.035175879396984,6.065326633165829,6.0954773869346734,6.125628140703517,6.155778894472362,6.185929648241205,6.21608040201005,6.2462311557788945,6.276381909547738,6.306532663316583,6.3366834170854265,6.366834170854271,6.396984924623116,6.427135678391959,6.457286432160804,6.487437185929648,6.517587939698492,6.547738693467337,6.57788944723618,6.608040201005025,6.638190954773869,6.668341708542713,6.698492462311558,6.7286432160804015,6.758793969849246,6.788944723618091,6.819095477386934,6.849246231155779,6.879396984924623,6.909547738693467,6.939698492462312,6.969849246231155,7.0],"y":[-6.1,-5.855752779706215,-5.617439626099488,-5.384994757378214,-5.158352391740783,-4.937446747385573,-4.72221204251098,-4.512582495315394,-4.3084923239972,-4.109875746754784,-3.9166669817865367,-3.728800247290849,-3.546209761466102,-3.3688297425106897,-3.1965944086229996,-3.0294379780014196,-2.867294668844335,-2.710098699350141,-2.5577842877172143,-2.410285652143955,-2.2675370108287467,-2.129472581969978,-1.9960265837660303,-1.8671332344153029,-1.7427267521161809,-1.6227413550670406,-1.5071112614662923,-1.3957706895123068,-1.288653857403483,-1.1856949833381947,-1.0868282855148437,-0.9919879821318176,-0.9011082913874986,-0.8141234314802744,-0.7309676206085385,-0.6515750769706727,-0.5758800187650707,-0.5038166641901227,-0.4353192314442083,-0.37032193872572067,-0.30875900423305325,-0.2505646461645881,-0.1956730827187158,-0.14401853209381557,-0.09553521248829214,-0.050157342100516186,-0.007819139128892516,0.03154517822820253,0.0680013917723727,0.10161528330524447,0.1324526346284074,0.1605792275434908,0.18606084385210409,0.20896326535584536,0.22935227385634108,0.247293651155195,0.26285317905403077,0.2760966393544493,0.2870898138580628,0.295898484366478,0.30258843268132407,0.30722544060420204,0.30987528993671276,0.3106037624804969,0.3094766400371327,0.3065597044082608,0.3019187373954651,0.2956195208003862,0.28772783642461375,0.278309466069777,0.2674301915374713,0.2551557946293144,0.24155205714693012,0.2266847608919079,0.2106196876658714,0.19342261927042728,0.17515933750721616,0.15589562417781053,0.13569726108383975,0.11463003002689902,0.09275971280864041,0.07015209123063641,0.04687294709450498,0.022988062201864067,-0.0014367816456569925,-0.026335802646468665,-0.05164321899897004,-0.07729324890150338,-0.10322011055251892,-0.12935802215034756,-0.15564120189339975,-0.18200386798008594,-0.20838023860876548,-0.23470453197783173,-0.26091096628567245,-0.28693375973069807,-0.3127071305112679,-0.33816529682578106,-0.36324247687263095,-0.387872888850211,-0.4119907509568804,-0.4355302813910555,-0.4584256983510954,-0.48061122003543344,-0.5020210646424061,-0.5225894503704523,-0.5422505954179314,-0.560938717983231,-0.5785880362647674,-0.5951327684608827,-0.6105071327699989,-0.6246453473904694,-0.6374816305207218,-0.648950200359127,-0.6589852751040894,-0.6675210729539799,-0.6744918121071806,-0.6798317107620733,-0.6834749871170857,-0.6853558593705884,-0.6854085457209521,-0.6835672643665817,-0.6797662335058476,-0.6739396713371661,-0.666021796058908,-0.6559468258694551,-0.643648978967201,-0.6290624735505503,-0.6121215278178852,-0.5927603599675649,-0.5709131881979829,-0.546514230707578,-0.5194977056946982,-0.48979783135773686,-0.4573488258950988,-0.42208490750512057,-0.38394029438626376,-0.3428492047368536,-0.29874585675530624,-0.2515644686400151,-0.20123925858937355,-0.1477044448017068,-0.09089424547549925,-0.030742878809076047,0.032815436999146644,0.09984648375078678,0.17041604324746232,0.24458989729080258,0.3224338276823687,0.4040136162238241,0.48939504471677536,0.5786438949628178,0.6718259487635351,0.7690069879205907,0.8702527942355687,0.9756291495100868,1.0852018355457402,1.1990366341441927,1.3171993271069824,1.439755696235784,1.5667715233321815,1.6983125901977814,1.834444678634202,1.9752335704430606,2.1207450474259644,2.2710448913845083,2.4261988841203674,2.58627280743508,2.7513324431302637,2.9214435730075934,3.0966719788686077,3.2770834425149475,3.462743745748264,3.6537186703701194,3.8500739981821313,4.051875510985894,4.259188990583084,4.472080218775238,4.690614977364044,4.914859048151049,5.144878212937897,5.380738253526147,5.622504951717474,5.870244089313519,6.124021448115786,6.383902809925985,6.649953956545687,6.922240669776488,7.200828731420051,7.485783923277927,7.777172027151755,8.075058824843131,8.37951009815372,8.690591628885068,9.00836919883884,9.332908589816588,9.664275583619997,10.002535962050592,10.347755506910039,10.7],"type":"scatter"},{"marker":{"size":12},"mode":"markers","name":"df = zero","x":[2.3927,3.5309,5.3263],"y":[0.0,0.0,0.0],"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"font":{"size":20},"yaxis":{"range":[-1,3]},"autosize":false,"width":800,"height":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('51a262f5-5249-4eee-8c1f-59c0612c7c8c');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>In the plots below, the line indicates the value of the derivative of each value of <span class="math inline">\(\theta\)</span>. The derivative is negative where it is red and positive where it is green.</p>
<p>Say we make a guess for the minimizing value of <span class="math inline">\(\theta\)</span>. Remember that we read plots from left to right, and assume that our starting <span class="math inline">\(\theta\)</span> value is to the left of the optimal <span class="math inline">\(\hat{\theta}\)</span>. If the guess “undershoots” the true minimizing value – our guess for <span class="math inline">\(\theta\)</span> is lower than the value of the <span class="math inline">\(\hat{\theta}\)</span> that minimizes the function – the derivative will be <strong>negative</strong>. This means that if we increase <span class="math inline">\(\theta\)</span> (move further to the right), then we <strong>can decrease</strong> our loss function further. If this guess “overshoots” the true minimizing value, the derivative will be positive, implying the converse.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/step.png" alt="step" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>We can use this pattern to help formulate our next guess for the optimal <span class="math inline">\(\hat{\theta}\)</span>. Consider the case where we’ve undershot <span class="math inline">\(\theta\)</span> by guessing too low of a value. We’ll want our next guess to be greater in value than our previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope “downhill” to the function’s minimum value.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/neg_step.png" alt="neg_step" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>If we’ve overshot <span class="math inline">\(\hat{\theta}\)</span> by guessing too high of a value, we’ll want our next guess to be lower in value – we want to shift our guess for <span class="math inline">\(\hat{\theta}\)</span> to the left.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/pos_step.png" alt="pos_step" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>In other words, the derivative of the function at each point tells us the direction of our next guess.</p>
<ul>
<li>A negative slope means we want to step to the right, or move in the <em>positive</em> direction.</li>
<li>A positive slope means we want to step to the left, or move in the <em>negative</em> direction.</li>
</ul>
</section>
<section id="algorithm-attempt-1" class="level4" data-number="14.3.1.4">
<h4 data-number="14.3.1.4" class="anchored" data-anchor-id="algorithm-attempt-1"><span class="header-section-number">14.3.1.4</span> Algorithm Attempt 1</h4>
<p>Armed with this knowledge, let’s try to see if we can use the derivative to optimize the function.</p>
<p>We start by making some guess for the minimizing value of <span class="math inline">\(x\)</span>. Then, we look at the derivative of the function at this value of <span class="math inline">\(x\)</span>, and step downhill in the <em>opposite</em> direction. We can express our new rule as a recurrence relation:</p>
<p><span class="math display">\[x^{(t+1)} = x^{(t)} - \frac{d}{dx} f(x^{(t)})\]</span></p>
<p>Translating this statement into English: we obtain <strong>our next guess</strong> for the minimizing value of <span class="math inline">\(x\)</span> at timestep <span class="math inline">\(t+1\)</span> (<span class="math inline">\(x^{(t+1)}\)</span>) by taking <strong>our last guess</strong> (<span class="math inline">\(x^{(t)}\)</span>) and subtracting the <strong>derivative of the function</strong> at that point (<span class="math inline">\(\frac{d}{dx} f(x^{(t)})\)</span>).</p>
<p>A few steps are shown below, where the old step is shown as a transparent point, and the next step taken is the green-filled dot.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/grad_descent_1.png" alt="grad_descent_2" width="800">
</td>
</tr>
</tbody></table>
</div>
<p>Looking pretty good! We do have a problem though – once we arrive close to the minimum value of the function, our guesses “bounce” back and forth past the minimum without ever reaching it.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/grad_descent_2.png" alt="grad_descent_2" width="500">
</td>
</tr>
</tbody></table>
</div>
<p>In other words, each step we take when updating our guess moves us too far. We can address this by decreasing the size of each step.</p>
</section>
<section id="algorithm-attempt-2" class="level4" data-number="14.3.1.5">
<h4 data-number="14.3.1.5" class="anchored" data-anchor-id="algorithm-attempt-2"><span class="header-section-number">14.3.1.5</span> Algorithm Attempt 2</h4>
<p>Let’s update our algorithm to use a <strong>learning rate</strong> (also sometimes called the step size), which controls how far we move with each update. We represent the learning rate with <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[x^{(t+1)} = x^{(t)} - \alpha \frac{d}{dx} f(x^{(t)})\]</span></p>
<p>A small <span class="math inline">\(\alpha\)</span> means that we will take small steps; a large <span class="math inline">\(\alpha\)</span> means we will take large steps. When do we stop updating? We stop updating either after a fixed number of updates or after a subsequent update doesn’t change much.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In Data 100, the learning rate is constant. More generally, however, <span class="math inline">\(\alpha\)</span> could also decrease over time, or decay.</p>
</div>
</div>
<p>Updating our function to use <span class="math inline">\(\alpha=0.3\)</span>, our algorithm successfully <strong>converges</strong> (settles on a solution and stops updating significantly, or at all) on the minimum value.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/grad_descent_3.png" alt="grad_descent_3" width="500">
</td>
</tr>
</tbody></table>
</div>
</section>
</section>
<section id="convexity" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="convexity"><span class="header-section-number">14.3.2</span> Convexity</h3>
<p>In our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum that’s just to the left?</p>
<p>If we had chosen a different starting guess for <span class="math inline">\(\theta\)</span>, or a different value for the learning rate <span class="math inline">\(\alpha\)</span>, our algorithm may have gotten “stuck” and converged on the local minimum, rather than on the true optimum value of loss.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/local.png" alt="local" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>If the loss function is <strong>convex</strong>, gradient descent is guaranteed to converge and find the global minimum of the objective function. Formally, a function <span class="math inline">\(f\)</span> is convex if: <span class="math display">\[tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)\]</span> for all <span class="math inline">\(a, b\)</span> in the domain of <span class="math inline">\(f\)</span> and <span class="math inline">\(t \in [0, 1]\)</span>.</p>
<p>To put this into words: if you drew a line between any two points on the curve, all values on the curve must be <em>on or below</em> the line. Importantly, any local minimum of a convex function is also its global minimum so we avoid the situation where the algorithm converges on some critical point that is not the minimum of the function.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/convex.png" alt="convex" width="600">
</td>
</tr>
</tbody></table>
</div>
<p>In summary, non-convex loss functions can cause problems with optimization. This means that our choice of loss function is a key factor in our modeling process. It turns out that MSE <em>is</em> convex, which is a major reason why it is such a popular choice of loss function. Gradient descent is only guaranteed to converge (given enough iterations and an appropriate step size) for convex functions.</p>
</section>
<section id="gradient-descent-in-1-dimension" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="gradient-descent-in-1-dimension"><span class="header-section-number">14.3.3</span> Gradient Descent in 1 Dimension</h3>
<blockquote class="blockquote">
<p><strong>Terminology clarification</strong>: In past lectures, we have used “loss” to refer to the error incurred on a <em>single</em> datapoint. In applications, we usually care more about the average error across <em>all</em> datapoints. Going forward, we will take the “model’s loss” to mean the model’s average error across the dataset. This is sometimes also known as the empirical risk (R), cost function, or objective function. <span class="math display">\[L(\theta) = R(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y, \hat{y})\]</span></p>
</blockquote>
<p>In our discussion above, we worked with some arbitrary function <span class="math inline">\(f\)</span>. As data scientists, we will almost always work with gradient descent in the context of optimizing <em>models</em> – specifically, we want to apply gradient descent to find the minimum of a <em>loss function</em>. In a modeling context, our goal is to minimize a loss function by choosing the minimizing model <em>parameters</em>.</p>
<p>Recall our modeling workflow from the past few lectures:</p>
<ol type="1">
<li>Define a model with some parameters <span class="math inline">\(\theta_i\)</span></li>
<li>Choose a loss function</li>
<li>Select the values of <span class="math inline">\(\theta_i\)</span> that minimize the loss function on the data</li>
</ol>
<p>Gradient descent is a powerful technique for completing this last task. By applying the gradient descent algorithm, we can select values for our parameters <span class="math inline">\(\theta_i\)</span> that will lead to the model having minimal loss on the training data.</p>
<p>When using gradient descent in a modeling context, we:</p>
<ol type="1">
<li>Make guesses for the minimizing <span class="math inline">\(\theta_i\)</span></li>
<li>Compute the derivative of the loss function <span class="math inline">\(L\)</span></li>
</ol>
<p>We can “translate” our gradient descent rule from before by replacing <span class="math inline">\(x\)</span> with <span class="math inline">\(\theta\)</span> and <span class="math inline">\(f\)</span> with <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta} L(\theta^{(t)})\]</span></p>
<section id="gradient-descent-on-the-tips-dataset" class="level4" data-number="14.3.3.1">
<h4 data-number="14.3.3.1" class="anchored" data-anchor-id="gradient-descent-on-the-tips-dataset"><span class="header-section-number">14.3.3.1</span> Gradient Descent on the <code>tips</code> Dataset</h4>
<p>To see this in action, let’s consider a case where we have a linear model with no offset. We want to predict the tip (y) given the price of a meal (x). To do this, we</p>
<ul>
<li>Choose a model: <span class="math inline">\(\hat{y} = \theta_1 x\)</span>,</li>
<li>Choose a loss function: <span class="math inline">\(L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\)</span>.</li>
</ul>
<p>Let’s apply our <code>gradient_descent</code> function from before to optimize our model on the <code>tips</code> dataset. We will try to select the best parameter <span class="math inline">\(\theta_i\)</span> to predict the <code>tip</code> <span class="math inline">\(y\)</span> from the <code>total_bill</code> <span class="math inline">\(x\)</span>.</p>
<div id="c5026398" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">total_bill</th>
<th data-quarto-table-cell-role="th">tip</th>
<th data-quarto-table-cell-role="th">sex</th>
<th data-quarto-table-cell-role="th">smoker</th>
<th data-quarto-table-cell-role="th">day</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>16.99</td>
<td>1.01</td>
<td>Female</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>2</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>10.34</td>
<td>1.66</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>3</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>21.01</td>
<td>3.50</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>3</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>23.68</td>
<td>3.31</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>2</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>24.59</td>
<td>3.61</td>
<td>Female</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>4</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can visualize the value of the MSE on our dataset for different possible choices of <span class="math inline">\(\theta_1\)</span>. To optimize our model, we want to select the value of <span class="math inline">\(\theta_1\)</span> that leads to the lowest MSE.</p>
<p>To apply gradient descent, we need to compute the derivative of the loss function with respect to our parameter <span class="math inline">\(\theta_1\)</span>.</p>
<ul>
<li>Given our loss function, <span class="math display">\[L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2\]</span></li>
<li>We take the derivative with respect to <span class="math inline">\(\theta_1\)</span> <span class="math display">\[\frac{\partial}{\partial \theta_{1}} L(\theta_1^{(t)}) = \frac{-2}{n} \sum_{i=1}^n (y_i - \theta_1^{(t)} x_i) x_i\]</span></li>
<li>Which results in the gradient descent update rule <span class="math display">\[\theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{d}{d\theta}L(\theta_1^{(t)})\]</span></li>
</ul>
<p>for some learning rate <span class="math inline">\(\alpha\)</span>.</p>
<p>Implementing this in code, we can visualize the MSE loss on the <code>tips</code> data. <strong>MSE is convex</strong>, so there is one global minimum.</p>
<div id="0aa26e20" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(df, initial_guess, alpha, n):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs n steps of gradient descent on df using learning rate alpha starting</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">       from initial_guess. Returns a numpy array of all guesses over time."""</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    guesses <span class="op">=</span> [initial_guess]</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    current_guess <span class="op">=</span> initial_guess</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(guesses) <span class="op">&lt;</span> n:</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        current_guess <span class="op">=</span> current_guess <span class="op">-</span> alpha <span class="op">*</span> df(current_guess)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        guesses.append(current_guess)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(guesses)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_single_arg(theta_1):</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the MSE on our data for the given theta1"""</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> df[<span class="st">"total_bill"</span>]</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> df[<span class="st">"tip"</span>]</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> theta_1 <span class="op">*</span> x</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y_obs) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_derivative_single_arg(theta_1):</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the derivative of the MSE on our data for the given theta1"""</span></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> df[<span class="st">"total_bill"</span>]</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> df[<span class="st">"tip"</span>]</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> theta_1 <span class="op">*</span> x</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(<span class="dv">2</span> <span class="op">*</span> (y_hat <span class="op">-</span> y_obs) <span class="op">*</span> x)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>loss_df <span class="op">=</span> pd.DataFrame({<span class="st">"theta_1"</span>:np.linspace(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">1</span>), <span class="st">"MSE"</span>:[mse_single_arg(theta_1) <span class="cf">for</span> theta_1 <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">1</span>)]})</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> gradient_descent(mse_loss_derivative_single_arg, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.0001</span>, <span class="dv">100</span>)</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_df[<span class="st">"theta_1"</span>], loss_df[<span class="st">"MSE"</span>])</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory, [mse_single_arg(guess) <span class="cf">for</span> guess <span class="kw">in</span> trajectory], c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>)</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory[<span class="op">-</span><span class="dv">1</span>], mse_single_arg(trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>)</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\t</span><span class="vs">heta_1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">L</span><span class="kw">(</span><span class="ch">\t</span><span class="vs">heta_1</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>)<span class="op">;</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final guess for theta_1: </span><span class="sc">{</span>trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Final guess for theta_1: 0.14369554654231262</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="gradient_descent_files/figure-html/cell-22-output-2.png" width="603" height="431" class="figure-img"></p>
</figure>
</div>
</div>
</div>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../case_study_HCE/case_study_HCE.html" class="pagination-link" aria-label="Case Study in Human Contexts and Ethics (Summer 2025)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics (Summer 2025)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../feature_engineering/feature_engineering.html" class="pagination-link" aria-label="Gradient Descent Continuation, Feature Engineering (Summer 2025)">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering (Summer 2025)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb37" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> sklearn and Gradient Descent (Summer 2025)</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: sklearn and Gradient Descent</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Apply the <span class="in">`sklearn`</span> library for model creation and training</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Optimizing complex models </span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Identifying cases where straight calculus or geometric arguments won't help solve the loss function</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Applying gradient descent for numerical optimization</span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>pd.options.mode.chained_assignment <span class="op">=</span> <span class="va">None</span>  <span class="co"># default='warn'</span></span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS Recap</span></span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Choose a model</span></span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>Recall that when using multiple linear regression, we can generate a prediction for each of our $n$ data points:</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>$$\hat{y} =\theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{p}x_{p}$$</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/ols_matrices_old.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'ols_matrices_old'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a>In the previous lecture, we used p+1 features to account for the intercept, $\theta_0$.  This makes slides and notation messy.  </span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a>Let’s redefine **p as the number of columns in our covariate matrix** and **add a column of 1s** to encode the intercept (if desired). If we choose to add a column of 1s, then $x_1$ can be a 1 for every data point.</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a>$$\hat{y} =\theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{p}x_{p}$$</span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/ols_matrices_new.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'ols_matrices_new'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-69"><a href="#cb37-69" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-70"><a href="#cb37-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-71"><a href="#cb37-71" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Choose a loss function</span></span>
<span id="cb37-72"><a href="#cb37-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a>Recall that we then choose the mean squared error loss function shown below where the prediction vector $\hat{\mathbb{Y}}$ depends on $\theta$.</span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a>$$R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} (||\mathbb{Y} - \hat{\mathbb{Y}}||_2)^2$$</span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-76"><a href="#cb37-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Fit the model</span></span>
<span id="cb37-77"><a href="#cb37-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-78"><a href="#cb37-78" aria-hidden="true" tabindex="-1"></a>We can then minimize the average loss with calculus or geometry. See the previous lecture for a derivation on the Normal Equation ($\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}$) using geometry. We can see what the matrices look like with our new interpretation where $\mathbb{X}$ is now an $n$ by $p$ matrix instead of an $n$ by $p+1$ matrix.</span>
<span id="cb37-79"><a href="#cb37-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-80"><a href="#cb37-80" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-81"><a href="#cb37-81" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-82"><a href="#cb37-82" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-83"><a href="#cb37-83" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/ols_solution_matrices.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'ols_solution_matrices'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'400'</span><span class="dt">&gt;</span></span>
<span id="cb37-84"><a href="#cb37-84" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-85"><a href="#cb37-85" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-86"><a href="#cb37-86" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-87"><a href="#cb37-87" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-88"><a href="#cb37-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-89"><a href="#cb37-89" aria-hidden="true" tabindex="-1"></a>To summarize:</span>
<span id="cb37-90"><a href="#cb37-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-91"><a href="#cb37-91" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>   <span class="pp">|</span> Model <span class="pp">|</span> Estimate <span class="pp">|</span> Unique? <span class="pp">|</span></span>
<span id="cb37-92"><a href="#cb37-92" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> -- <span class="pp">|</span> -- <span class="pp">|</span> -- <span class="pp">|</span>  -- <span class="pp">|</span> </span>
<span id="cb37-93"><a href="#cb37-93" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Constant Model + MSE <span class="pp">|</span> $\hat{y} = \theta_0$<span class="pp">|</span> $\hat{\theta}_0 = mean(y) = \bar{y}$ <span class="pp">|</span> **Yes**. Any set of values has a unique mean.</span>
<span id="cb37-94"><a href="#cb37-94" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Constant Model + MAE <span class="pp">|</span> $\hat{y} = \theta_0$  <span class="pp">|</span> $\hat{\theta}_0 = median(y)$ <span class="pp">|</span> **Yes**, if odd. **No**, if even. Return the average of the middle 2 values.</span>
<span id="cb37-95"><a href="#cb37-95" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Simple Linear Regression + MSE <span class="pp">|</span> $\hat{y} = \theta_0 + \theta_1x$<span class="pp">|</span> $\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x}$ $\hat{\theta}_1 = r\frac{\sigma_y}{\sigma_x}$<span class="pp">|</span> **Yes**. Any set of non-constant* values has a unique mean, SD, and correlation coefficient.</span>
<span id="cb37-96"><a href="#cb37-96" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **OLS** (Linear Model + MSE) | $\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}$| $\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}$  | **Yes**, if $\mathbb{X}$ is full column rank (all columns are linearly independent, # of datapoints &gt;&gt;&gt; # of features).</span>
<span id="cb37-97"><a href="#cb37-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-98"><a href="#cb37-98" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Uniqueness of a Solution</span></span>
<span id="cb37-99"><a href="#cb37-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-100"><a href="#cb37-100" aria-hidden="true" tabindex="-1"></a>In most settings, the number of observations ($n$) is much greater than the number of features ($p$). Note that at least one solution always exists because intuitively, we can always draw a line of best fit for a given set of data, but there may be multiple lines that are “equally good”. (Formal proof is beyond this course.) Let's now revisit the interpretation for uniqueness of a solution at the end of the last lecture, but with the new notation of $p$ instead of $p+1$ features.</span>
<span id="cb37-101"><a href="#cb37-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-102"><a href="#cb37-102" aria-hidden="true" tabindex="-1"></a>The Least Squares estimate $\hat{\theta}$ is **unique** if and only if $\mathbb{X}$ is **full column rank**.</span>
<span id="cb37-103"><a href="#cb37-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-104"><a href="#cb37-104" aria-hidden="true" tabindex="-1"></a>::: {.callout}</span>
<span id="cb37-105"><a href="#cb37-105" aria-hidden="true" tabindex="-1"></a>Proof: </span>
<span id="cb37-106"><a href="#cb37-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-107"><a href="#cb37-107" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We know the solution to the normal equation $\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{X}^T\mathbb{Y}$ is the least square estimate that minimizes the squared loss.</span>
<span id="cb37-108"><a href="#cb37-108" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\hat{\theta}$ has a **unique** solution $\iff$ the square matrix $\mathbb{X}^T\mathbb{X}$ is **invertible** $\iff$ $\mathbb{X}^T\mathbb{X}$ is full rank.</span>
<span id="cb37-109"><a href="#cb37-109" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The **column rank** of a square matrix is the max number of linearly independent columns it contains.</span>
<span id="cb37-110"><a href="#cb37-110" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>An $n$ x $n$ square matrix is deemed full column rank when all of its columns are linearly independent. That is, its rank would be equal to $n$.</span>
<span id="cb37-111"><a href="#cb37-111" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\mathbb{X}^T\mathbb{X}$ has shape $p \times p$, and therefore has max rank $p$. </span>
<span id="cb37-112"><a href="#cb37-112" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$rank(\mathbb{X}^T\mathbb{X})$ = $rank(\mathbb{X})$ (proof out of scope).</span>
<span id="cb37-113"><a href="#cb37-113" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Therefore, $\mathbb{X}^T\mathbb{X}$ has rank $p$ $\iff$  $\mathbb{X}$ has rank $p$ $\iff \mathbb{X}$ is full column rank.</span>
<span id="cb37-114"><a href="#cb37-114" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb37-115"><a href="#cb37-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-116"><a href="#cb37-116" aria-hidden="true" tabindex="-1"></a>Therefore, if $\mathbb{X}$ is not full column rank, we will not have unique estimates. This can happen for two major reasons.</span>
<span id="cb37-117"><a href="#cb37-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-118"><a href="#cb37-118" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If our design matrix $\mathbb{X}$ is "**wide**":</span>
<span id="cb37-119"><a href="#cb37-119" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>If n &lt; p, then we have way more features (columns) than observations (rows).</span>
<span id="cb37-120"><a href="#cb37-120" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Then $rank(\mathbb{X})$ = min(n, p) &lt; p, so $\hat{\theta}$ is not unique.</span>
<span id="cb37-121"><a href="#cb37-121" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Typically we have n &gt;&gt; p so this is less of an issue.</span>
<span id="cb37-122"><a href="#cb37-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-123"><a href="#cb37-123" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If our design matrix $\mathbb{X}$ has features that are **linear combinations** of other features:</span>
<span id="cb37-124"><a href="#cb37-124" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>By definition, rank of $\mathbb{X}$ is number of linearly independent columns in $\mathbb{X}$.</span>
<span id="cb37-125"><a href="#cb37-125" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Example: If “Width”, “Height”, and “Perimeter” are all columns,</span>
<span id="cb37-126"><a href="#cb37-126" aria-hidden="true" tabindex="-1"></a><span class="ss">      * </span>Perimeter = 2 * Width + 2 * Height  $\rightarrow$  $\mathbb{X}$ is not full rank.</span>
<span id="cb37-127"><a href="#cb37-127" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>Important with one-hot encoding (to discuss later).</span>
<span id="cb37-128"><a href="#cb37-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-129"><a href="#cb37-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-130"><a href="#cb37-130" aria-hidden="true" tabindex="-1"></a>Let's now explore how to use the normal equations with a real-world dataset in the next section.</span>
<span id="cb37-131"><a href="#cb37-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-132"><a href="#cb37-132" aria-hidden="true" tabindex="-1"></a><span class="fu">## `sklearn`</span></span>
<span id="cb37-133"><a href="#cb37-133" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementing Derived Formulas in Code</span></span>
<span id="cb37-134"><a href="#cb37-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-135"><a href="#cb37-135" aria-hidden="true" tabindex="-1"></a>Throughout this lecture, we'll refer to the <span class="in">`penguins`</span> dataset. </span>
<span id="cb37-136"><a href="#cb37-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-139"><a href="#cb37-139" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-140"><a href="#cb37-140" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-141"><a href="#cb37-141" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-142"><a href="#cb37-142" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb37-143"><a href="#cb37-143" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-144"><a href="#cb37-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-145"><a href="#cb37-145" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>)</span>
<span id="cb37-146"><a href="#cb37-146" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> penguins[penguins[<span class="st">"species"</span>] <span class="op">==</span> <span class="st">"Adelie"</span>].dropna()</span>
<span id="cb37-147"><a href="#cb37-147" aria-hidden="true" tabindex="-1"></a>penguins.head()</span>
<span id="cb37-148"><a href="#cb37-148" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-149"><a href="#cb37-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-150"><a href="#cb37-150" aria-hidden="true" tabindex="-1"></a>Our goal will be to predict the value of the <span class="in">`"bill_depth_mm"`</span> for a particular penguin given its <span class="in">`"flipper_length_mm"`</span> and <span class="in">`"body_mass_g"`</span>. We'll also add a bias column of all ones to represent the intercept term of our models.</span>
<span id="cb37-151"><a href="#cb37-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-154"><a href="#cb37-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-155"><a href="#cb37-155" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a bias column of all ones to `penguins`</span></span>
<span id="cb37-156"><a href="#cb37-156" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"bias"</span>] <span class="op">=</span> np.ones(<span class="bu">len</span>(penguins), dtype<span class="op">=</span><span class="bu">int</span>) </span>
<span id="cb37-157"><a href="#cb37-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-158"><a href="#cb37-158" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the design matrix, X...</span></span>
<span id="cb37-159"><a href="#cb37-159" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we use .to_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form</span></span>
<span id="cb37-160"><a href="#cb37-160" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">"bias"</span>, <span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]].to_numpy()</span>
<span id="cb37-161"><a href="#cb37-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-162"><a href="#cb37-162" aria-hidden="true" tabindex="-1"></a><span class="co"># ...as well as the target variable, Y</span></span>
<span id="cb37-163"><a href="#cb37-163" aria-hidden="true" tabindex="-1"></a><span class="co"># Again, we use .to_numpy() to convert our DataFrame into a NumPy array so it is in Matrix form</span></span>
<span id="cb37-164"><a href="#cb37-164" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[[<span class="st">"bill_depth_mm"</span>]].to_numpy()</span>
<span id="cb37-165"><a href="#cb37-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-166"><a href="#cb37-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-167"><a href="#cb37-167" aria-hidden="true" tabindex="-1"></a>In the lecture on ordinary least squares, we expressed multiple linear regression using matrix notation.</span>
<span id="cb37-168"><a href="#cb37-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-169"><a href="#cb37-169" aria-hidden="true" tabindex="-1"></a>$$\hat{\mathbb{Y}} = \mathbb{X}\theta$$</span>
<span id="cb37-170"><a href="#cb37-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-171"><a href="#cb37-171" aria-hidden="true" tabindex="-1"></a>We used a geometric approach to derive the following expression for the optimal model parameters:</span>
<span id="cb37-172"><a href="#cb37-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-173"><a href="#cb37-173" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}$$</span>
<span id="cb37-174"><a href="#cb37-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-175"><a href="#cb37-175" aria-hidden="true" tabindex="-1"></a>That's a whole lot of matrix manipulation. How do we implement it in <span class="in">`python`</span>?</span>
<span id="cb37-176"><a href="#cb37-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-177"><a href="#cb37-177" aria-hidden="true" tabindex="-1"></a>There are three operations we need to perform here: multiplying matrices, taking transposes, and finding inverses. </span>
<span id="cb37-178"><a href="#cb37-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-179"><a href="#cb37-179" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To perform matrix multiplication, use the <span class="in">`@`</span> operator</span>
<span id="cb37-180"><a href="#cb37-180" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To take a transpose, call the <span class="in">`.T`</span> attribute of an <span class="in">`NumPy`</span> array or <span class="in">`DataFrame`</span></span>
<span id="cb37-181"><a href="#cb37-181" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To compute an inverse, use <span class="in">`NumPy`</span>'s in-built method <span class="in">`np.linalg.inv`</span></span>
<span id="cb37-182"><a href="#cb37-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-183"><a href="#cb37-183" aria-hidden="true" tabindex="-1"></a>Putting this all together, we can compute the OLS estimate for the optimal model parameters, stored in the array <span class="in">`theta_hat`</span>.</span>
<span id="cb37-184"><a href="#cb37-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-187"><a href="#cb37-187" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-188"><a href="#cb37-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb37-189"><a href="#cb37-189" aria-hidden="true" tabindex="-1"></a>theta_hat <span class="op">=</span> np.linalg.inv(X.T <span class="op">@</span> X) <span class="op">@</span> X.T <span class="op">@</span> Y</span>
<span id="cb37-190"><a href="#cb37-190" aria-hidden="true" tabindex="-1"></a>theta_hat</span>
<span id="cb37-191"><a href="#cb37-191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-192"><a href="#cb37-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-193"><a href="#cb37-193" aria-hidden="true" tabindex="-1"></a>To make predictions using our optimized parameter values, we matrix-multiply the design matrix with the parameter vector:</span>
<span id="cb37-194"><a href="#cb37-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-195"><a href="#cb37-195" aria-hidden="true" tabindex="-1"></a>$$\hat{\mathbb{Y}} = \mathbb{X}\theta$$</span>
<span id="cb37-196"><a href="#cb37-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-199"><a href="#cb37-199" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-200"><a href="#cb37-200" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb37-201"><a href="#cb37-201" aria-hidden="true" tabindex="-1"></a>Y_hat <span class="op">=</span> X <span class="op">@</span> theta_hat</span>
<span id="cb37-202"><a href="#cb37-202" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(Y_hat).head()</span>
<span id="cb37-203"><a href="#cb37-203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-204"><a href="#cb37-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-205"><a href="#cb37-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### The `sklearn` Workflow</span></span>
<span id="cb37-206"><a href="#cb37-206" aria-hidden="true" tabindex="-1"></a>We've already saved a lot of time (and avoided tedious calculations) by translating our derived formulas into code. However, we still had to go through the process of writing out the linear algebra ourselves. </span>
<span id="cb37-207"><a href="#cb37-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-208"><a href="#cb37-208" aria-hidden="true" tabindex="-1"></a>To make life *even easier*, we can turn to the <span class="in">`sklearn`</span> <span class="co">[</span><span class="ot">`python` library</span><span class="co">](https://scikit-learn.org/stable/)</span>. <span class="in">`sklearn`</span> is a robust library of machine learning tools used extensively in research and industry. It is the standard for simple machine learning tasks and gives us a wide variety of in-built modeling frameworks and methods, so we'll keep returning to <span class="in">`sklearn`</span> techniques as we progress through Data 100. </span>
<span id="cb37-209"><a href="#cb37-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-210"><a href="#cb37-210" aria-hidden="true" tabindex="-1"></a>Regardless of the specific type of model being implemented, <span class="in">`sklearn`</span> follows a standard set of steps for creating a model: </span>
<span id="cb37-211"><a href="#cb37-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-212"><a href="#cb37-212" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Import the <span class="in">`LinearRegression`</span> model from <span class="in">`sklearn`</span></span>
<span id="cb37-213"><a href="#cb37-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-214"><a href="#cb37-214" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb37-215"><a href="#cb37-215" aria-hidden="true" tabindex="-1"></a><span class="in">    from sklearn.linear_model import LinearRegression</span></span>
<span id="cb37-216"><a href="#cb37-216" aria-hidden="true" tabindex="-1"></a><span class="in">    ```</span></span>
<span id="cb37-217"><a href="#cb37-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-218"><a href="#cb37-218" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Create a model object. This generates a new instance of the model class. You can think of it as making a new "copy" of a standard "template" for a model. In code, this looks like:</span>
<span id="cb37-219"><a href="#cb37-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-220"><a href="#cb37-220" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb37-221"><a href="#cb37-221" aria-hidden="true" tabindex="-1"></a><span class="in">    my_model = LinearRegression()</span></span>
<span id="cb37-222"><a href="#cb37-222" aria-hidden="true" tabindex="-1"></a><span class="in">    ```</span></span>
<span id="cb37-223"><a href="#cb37-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-224"><a href="#cb37-224" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-225"><a href="#cb37-225" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fit the model to the <span class="in">`X`</span> design matrix and <span class="in">`Y`</span> target vector. This calculates the optimal model parameters "behind the scenes" without us explicitly working through the calculations ourselves. The fitted parameters are then stored within the model for use in future predictions:</span>
<span id="cb37-226"><a href="#cb37-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-227"><a href="#cb37-227" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb37-228"><a href="#cb37-228" aria-hidden="true" tabindex="-1"></a><span class="in">    my_model.fit(X, Y)</span></span>
<span id="cb37-229"><a href="#cb37-229" aria-hidden="true" tabindex="-1"></a><span class="in">     ```</span></span>
<span id="cb37-230"><a href="#cb37-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-231"><a href="#cb37-231" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-232"><a href="#cb37-232" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Use the fitted model to make predictions on the <span class="in">`X`</span> input data using <span class="in">`.predict`</span>. </span>
<span id="cb37-233"><a href="#cb37-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-234"><a href="#cb37-234" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb37-235"><a href="#cb37-235" aria-hidden="true" tabindex="-1"></a><span class="in">    my_model.predict(X)</span></span>
<span id="cb37-236"><a href="#cb37-236" aria-hidden="true" tabindex="-1"></a><span class="in">    ```</span></span>
<span id="cb37-237"><a href="#cb37-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-238"><a href="#cb37-238" aria-hidden="true" tabindex="-1"></a>To extract the fitted parameters, we can use:</span>
<span id="cb37-239"><a href="#cb37-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-240"><a href="#cb37-240" aria-hidden="true" tabindex="-1"></a>  <span class="in">```</span></span>
<span id="cb37-241"><a href="#cb37-241" aria-hidden="true" tabindex="-1"></a><span class="in">  my_model.coef_</span></span>
<span id="cb37-242"><a href="#cb37-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-243"><a href="#cb37-243" aria-hidden="true" tabindex="-1"></a><span class="in">  my_model.intercept_</span></span>
<span id="cb37-244"><a href="#cb37-244" aria-hidden="true" tabindex="-1"></a><span class="in">  ```</span></span>
<span id="cb37-245"><a href="#cb37-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-246"><a href="#cb37-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-247"><a href="#cb37-247" aria-hidden="true" tabindex="-1"></a>Let's put this into action with our multiple regression task!</span>
<span id="cb37-248"><a href="#cb37-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-249"><a href="#cb37-249" aria-hidden="true" tabindex="-1"></a>**1. Initialize an instance of the model class**</span>
<span id="cb37-250"><a href="#cb37-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-251"><a href="#cb37-251" aria-hidden="true" tabindex="-1"></a><span class="in">`sklearn`</span> stores "templates" of useful models for machine learning. We begin the modeling process by making a "copy" of one of these templates for our own use. Model initialization looks like <span class="in">`ModelClass()`</span>, where <span class="in">`ModelClass`</span> is the type of model we wish to create.</span>
<span id="cb37-252"><a href="#cb37-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-253"><a href="#cb37-253" aria-hidden="true" tabindex="-1"></a>For now, let's create a linear regression model using <span class="in">`LinearRegression`</span>. </span>
<span id="cb37-254"><a href="#cb37-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-255"><a href="#cb37-255" aria-hidden="true" tabindex="-1"></a><span class="in">`my_model`</span> is now an instance of the <span class="in">`LinearRegression`</span> class. You can think of it as the "idea" of a linear regression model. We haven't trained it yet, so it doesn't know any model parameters and cannot be used to make predictions. In fact, we haven't even told it what data to use for modeling! It simply waits for further instructions.</span>
<span id="cb37-256"><a href="#cb37-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-259"><a href="#cb37-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-260"><a href="#cb37-260" aria-hidden="true" tabindex="-1"></a>my_model <span class="op">=</span> LinearRegression()</span>
<span id="cb37-261"><a href="#cb37-261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-262"><a href="#cb37-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-263"><a href="#cb37-263" aria-hidden="true" tabindex="-1"></a>**2. Train the model using `.fit`**</span>
<span id="cb37-264"><a href="#cb37-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-265"><a href="#cb37-265" aria-hidden="true" tabindex="-1"></a>Before the model can make predictions, we will need to fit it to our training data. When we fit the model, <span class="in">`sklearn`</span> will run gradient descent behind the scenes to determine the optimal model parameters. It will then save these model parameters to our model instance for future use. </span>
<span id="cb37-266"><a href="#cb37-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-267"><a href="#cb37-267" aria-hidden="true" tabindex="-1"></a>All <span class="in">`sklearn`</span> model classes include a <span class="in">`.fit`</span> method, which is used to fit the model. It takes in two inputs: the design matrix, <span class="in">`X`</span>, and the target variable, <span class="in">`Y`</span>. </span>
<span id="cb37-268"><a href="#cb37-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-269"><a href="#cb37-269" aria-hidden="true" tabindex="-1"></a>Let's start by fitting a model with just one feature: the flipper length. We create a design matrix <span class="in">`X`</span> by pulling out the <span class="in">`"flipper_length_mm"`</span> column from the <span class="in">`DataFrame`</span>. </span>
<span id="cb37-270"><a href="#cb37-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-273"><a href="#cb37-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-274"><a href="#cb37-274" aria-hidden="true" tabindex="-1"></a><span class="co"># .fit expects a 2D data design matrix, so we use double brackets to extract a DataFrame</span></span>
<span id="cb37-275"><a href="#cb37-275" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[[<span class="st">"flipper_length_mm"</span>]]</span>
<span id="cb37-276"><a href="#cb37-276" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[<span class="st">"bill_depth_mm"</span>]</span>
<span id="cb37-277"><a href="#cb37-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-278"><a href="#cb37-278" aria-hidden="true" tabindex="-1"></a>my_model.fit(X, Y)</span>
<span id="cb37-279"><a href="#cb37-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-280"><a href="#cb37-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-281"><a href="#cb37-281" aria-hidden="true" tabindex="-1"></a>Notice that we use **double brackets** to extract this column. Why double brackets instead of just single brackets? The `.fit` method, by default, expects to receive **2-dimensional** data – some kind of data that includes both rows and columns. Writing <span class="in">`penguins["flipper_length_mm"]`</span> would return a 1D <span class="in">`Series`</span>, causing <span class="in">`sklearn`</span> to error. We avoid this by writing <span class="in">`penguins[["flipper_length_mm"]]`</span> to produce a 2D <span class="in">`DataFrame`</span>. </span>
<span id="cb37-282"><a href="#cb37-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-283"><a href="#cb37-283" aria-hidden="true" tabindex="-1"></a>And in just three lines of code, our model has run gradient descent to determine the optimal model parameters! Our single-feature model takes the form:</span>
<span id="cb37-284"><a href="#cb37-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-285"><a href="#cb37-285" aria-hidden="true" tabindex="-1"></a>$$\text{bill depth} = \theta_0 + \theta_1 \text{flipper length}$$</span>
<span id="cb37-286"><a href="#cb37-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-287"><a href="#cb37-287" aria-hidden="true" tabindex="-1"></a>Note that <span class="in">`LinearRegression`</span> will automatically include an intercept term. </span>
<span id="cb37-288"><a href="#cb37-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-289"><a href="#cb37-289" aria-hidden="true" tabindex="-1"></a>The fitted model parameters are stored as attributes of the model instance. <span class="in">`my_model.intercept_`</span> will return the value of $\hat{\theta}_0$ as a scalar. `my_model.coef_` will return all values $\hat{\theta}_1, </span>
<span id="cb37-290"><a href="#cb37-290" aria-hidden="true" tabindex="-1"></a>\hat{\theta}_2, ...$ in an array. Because our model only contains one feature, we see just the value of $\hat{\theta}_1$ in the cell below.</span>
<span id="cb37-291"><a href="#cb37-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-294"><a href="#cb37-294" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-295"><a href="#cb37-295" aria-hidden="true" tabindex="-1"></a><span class="co"># The intercept term, theta_0</span></span>
<span id="cb37-296"><a href="#cb37-296" aria-hidden="true" tabindex="-1"></a>my_model.intercept_</span>
<span id="cb37-297"><a href="#cb37-297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-298"><a href="#cb37-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-301"><a href="#cb37-301" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-302"><a href="#cb37-302" aria-hidden="true" tabindex="-1"></a><span class="co"># All parameters theta_1, ..., theta_p</span></span>
<span id="cb37-303"><a href="#cb37-303" aria-hidden="true" tabindex="-1"></a>my_model.coef_</span>
<span id="cb37-304"><a href="#cb37-304" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-305"><a href="#cb37-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-306"><a href="#cb37-306" aria-hidden="true" tabindex="-1"></a>**3. Use the fitted model to make predictions**</span>
<span id="cb37-307"><a href="#cb37-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-308"><a href="#cb37-308" aria-hidden="true" tabindex="-1"></a>Now that the model has been trained, we can use it to make predictions! To do so, we use the <span class="in">`.predict`</span> method. <span class="in">`.predict`</span> takes in one argument: the design matrix that should be used to generate predictions. To understand how the model performs on the training set, we would pass in the training data. Alternatively, to make predictions on unseen data, we would pass in a new dataset that wasn't used to train the model.</span>
<span id="cb37-309"><a href="#cb37-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-310"><a href="#cb37-310" aria-hidden="true" tabindex="-1"></a>Below, we call <span class="in">`.predict`</span> to generate model predictions on the original training data. As before, we use double brackets to ensure that we extract 2-dimensional data.</span>
<span id="cb37-311"><a href="#cb37-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-314"><a href="#cb37-314" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-315"><a href="#cb37-315" aria-hidden="true" tabindex="-1"></a>Y_hat_one_feature <span class="op">=</span> my_model.predict(penguins[[<span class="st">"flipper_length_mm"</span>]])</span>
<span id="cb37-316"><a href="#cb37-316" aria-hidden="true" tabindex="-1"></a>Y_hat_one_feature[:<span class="dv">10</span>]</span>
<span id="cb37-317"><a href="#cb37-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-318"><a href="#cb37-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-319"><a href="#cb37-319" aria-hidden="true" tabindex="-1"></a>The <span class="in">`sklearn`</span> package also provides a function <span class="in">`mean_squared_error()`</span> <span class="co">[</span><span class="ot">documentation</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)</span> that computes the MSE from a list of observations and predictions. This avoids us having to manually compute MSE by first computing residuals.</span>
<span id="cb37-320"><a href="#cb37-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-323"><a href="#cb37-323" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-324"><a href="#cb37-324" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb37-325"><a href="#cb37-325" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The MSE of the model is </span><span class="sc">{</span>mean_squared_error(Y, Y_hat_one_feature)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-326"><a href="#cb37-326" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-327"><a href="#cb37-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-328"><a href="#cb37-328" aria-hidden="true" tabindex="-1"></a>What if we wanted a model with two features? </span>
<span id="cb37-329"><a href="#cb37-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-330"><a href="#cb37-330" aria-hidden="true" tabindex="-1"></a>$$\text{bill depth} = \theta_0 + \theta_1 \text{flipper length} + \theta_2 \text{body mass}$$</span>
<span id="cb37-331"><a href="#cb37-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-332"><a href="#cb37-332" aria-hidden="true" tabindex="-1"></a>We repeat this three-step process by intializing a new model object, then calling <span class="in">`.fit`</span> and <span class="in">`.predict`</span> as before.</span>
<span id="cb37-333"><a href="#cb37-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-336"><a href="#cb37-336" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-337"><a href="#cb37-337" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: initialize LinearRegression model</span></span>
<span id="cb37-338"><a href="#cb37-338" aria-hidden="true" tabindex="-1"></a>two_feature_model <span class="op">=</span> LinearRegression()</span>
<span id="cb37-339"><a href="#cb37-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-340"><a href="#cb37-340" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: fit the model</span></span>
<span id="cb37-341"><a href="#cb37-341" aria-hidden="true" tabindex="-1"></a>X_two_features <span class="op">=</span> penguins[[<span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]]</span>
<span id="cb37-342"><a href="#cb37-342" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> penguins[<span class="st">"bill_depth_mm"</span>]</span>
<span id="cb37-343"><a href="#cb37-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-344"><a href="#cb37-344" aria-hidden="true" tabindex="-1"></a>two_feature_model.fit(X_two_features, Y)</span>
<span id="cb37-345"><a href="#cb37-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-346"><a href="#cb37-346" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: make predictions</span></span>
<span id="cb37-347"><a href="#cb37-347" aria-hidden="true" tabindex="-1"></a>Y_hat_two_features <span class="op">=</span> two_feature_model.predict(X_two_features)</span>
<span id="cb37-348"><a href="#cb37-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-349"><a href="#cb37-349" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The MSE of the model is </span><span class="sc">{</span>mean_squared_error(Y, Y_hat_two_features)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-350"><a href="#cb37-350" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-351"><a href="#cb37-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-352"><a href="#cb37-352" aria-hidden="true" tabindex="-1"></a>We can also see that we obtain the same predictions using <span class="in">`sklearn`</span> as we did when applying the ordinary least squares formula before! </span>
<span id="cb37-353"><a href="#cb37-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-356"><a href="#cb37-356" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-357"><a href="#cb37-357" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-358"><a href="#cb37-358" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Y_hat from OLS"</span>:np.squeeze(Y_hat), <span class="st">"Y_hat from sklearn"</span>:Y_hat_two_features}).head()</span>
<span id="cb37-359"><a href="#cb37-359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-360"><a href="#cb37-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-361"><a href="#cb37-361" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient Descent </span></span>
<span id="cb37-362"><a href="#cb37-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-363"><a href="#cb37-363" aria-hidden="true" tabindex="-1"></a>At this point, we've grown quite familiar with the process of choosing a model and a corresponding loss function and optimizing parameters by choosing the values of $\theta$ that minimize the loss function. So far, we've optimized $\theta$ by</span>
<span id="cb37-364"><a href="#cb37-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-365"><a href="#cb37-365" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Using calculus to take the derivative of the loss function with respect to $\theta$, setting it equal to 0, and solving for $\theta$.</span>
<span id="cb37-366"><a href="#cb37-366" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using the geometric argument of orthogonality to derive the OLS solution $\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}$.</span>
<span id="cb37-367"><a href="#cb37-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-368"><a href="#cb37-368" aria-hidden="true" tabindex="-1"></a>One thing to note, however, is that the techniques we used above can only be applied if we make some big assumptions. For the calculus approach, we assumed that the loss function was differentiable at all points and that we could algebraically solve for the zero points of the derivative; for the geometric approach, OLS *only* applies when using a linear model with MSE loss. What happens when we have more complex models with different, more complex loss functions? The techniques we've learned so far will not work, so we need a new optimization technique: **gradient descent**. </span>
<span id="cb37-369"><a href="#cb37-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-370"><a href="#cb37-370" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **BIG IDEA**: use an iterative algorithm to numerically compute the minimum of the loss.</span></span>
<span id="cb37-371"><a href="#cb37-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-372"><a href="#cb37-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Minimizing an Arbitrary 1D Function</span></span>
<span id="cb37-373"><a href="#cb37-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-374"><a href="#cb37-374" aria-hidden="true" tabindex="-1"></a>Let's consider an arbitrary function. Our goal is to find the value of $x$ that minimizes this function.</span>
<span id="cb37-375"><a href="#cb37-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-378"><a href="#cb37-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-379"><a href="#cb37-379" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> arbitrary(x):</span>
<span id="cb37-380"><a href="#cb37-380" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x<span class="op">**</span><span class="dv">4</span> <span class="op">-</span> <span class="dv">15</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> <span class="dv">80</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">180</span><span class="op">*</span>x <span class="op">+</span> <span class="dv">144</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb37-381"><a href="#cb37-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-382"><a href="#cb37-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-383"><a href="#cb37-383" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/arbitrary.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'arbitrary'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-384"><a href="#cb37-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-385"><a href="#cb37-385" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Naive Approach: Guess and Check</span></span>
<span id="cb37-386"><a href="#cb37-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-387"><a href="#cb37-387" aria-hidden="true" tabindex="-1"></a>Above, we saw that the minimum is somewhere around 5.3. Let's see if we can figure out how to find the exact minimum algorithmically from scratch. One very slow (and terrible) way would be manual guess-and-check.</span>
<span id="cb37-388"><a href="#cb37-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-391"><a href="#cb37-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-392"><a href="#cb37-392" aria-hidden="true" tabindex="-1"></a>arbitrary(<span class="dv">6</span>)</span>
<span id="cb37-393"><a href="#cb37-393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-394"><a href="#cb37-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-395"><a href="#cb37-395" aria-hidden="true" tabindex="-1"></a>A somewhat better (but still slow) approach is to use brute force to try out a bunch of x values and return the one that yields the lowest loss.</span>
<span id="cb37-396"><a href="#cb37-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-399"><a href="#cb37-399" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-400"><a href="#cb37-400" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_minimize(f, xs):</span>
<span id="cb37-401"><a href="#cb37-401" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Takes in a function f and a set of values xs. </span></span>
<span id="cb37-402"><a href="#cb37-402" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculates the value of the function f at all values x in xs</span></span>
<span id="cb37-403"><a href="#cb37-403" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Takes the minimum value of f(x) and returns the corresponding value x </span></span>
<span id="cb37-404"><a href="#cb37-404" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [f(x) <span class="cf">for</span> x <span class="kw">in</span> xs]  </span>
<span id="cb37-405"><a href="#cb37-405" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs[np.argmin(y)]</span>
<span id="cb37-406"><a href="#cb37-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-407"><a href="#cb37-407" aria-hidden="true" tabindex="-1"></a>guesses <span class="op">=</span> [<span class="fl">5.3</span>, <span class="fl">5.31</span>, <span class="fl">5.32</span>, <span class="fl">5.33</span>, <span class="fl">5.34</span>, <span class="fl">5.35</span>]</span>
<span id="cb37-408"><a href="#cb37-408" aria-hidden="true" tabindex="-1"></a>simple_minimize(arbitrary, guesses)</span>
<span id="cb37-409"><a href="#cb37-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-410"><a href="#cb37-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-411"><a href="#cb37-411" aria-hidden="true" tabindex="-1"></a>This process is essentially the same as before where we made a graphical plot, it's just that we're only looking at 20 selected points.</span>
<span id="cb37-412"><a href="#cb37-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-415"><a href="#cb37-415" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-416"><a href="#cb37-416" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-417"><a href="#cb37-417" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">200</span>)</span>
<span id="cb37-418"><a href="#cb37-418" aria-hidden="true" tabindex="-1"></a>sparse_xs <span class="op">=</span> np.linspace(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">5</span>)</span>
<span id="cb37-419"><a href="#cb37-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-420"><a href="#cb37-420" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> arbitrary(xs)</span>
<span id="cb37-421"><a href="#cb37-421" aria-hidden="true" tabindex="-1"></a>sparse_ys <span class="op">=</span> arbitrary(sparse_xs)</span>
<span id="cb37-422"><a href="#cb37-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-423"><a href="#cb37-423" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(x <span class="op">=</span> xs, y <span class="op">=</span> arbitrary(xs))</span>
<span id="cb37-424"><a href="#cb37-424" aria-hidden="true" tabindex="-1"></a>fig.add_scatter(x <span class="op">=</span> sparse_xs, y <span class="op">=</span> arbitrary(sparse_xs), mode <span class="op">=</span> <span class="st">"markers"</span>)</span>
<span id="cb37-425"><a href="#cb37-425" aria-hidden="true" tabindex="-1"></a>fig.update_layout(showlegend<span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb37-426"><a href="#cb37-426" aria-hidden="true" tabindex="-1"></a>fig.update_layout(autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-427"><a href="#cb37-427" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb37-428"><a href="#cb37-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-429"><a href="#cb37-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-430"><a href="#cb37-430" aria-hidden="true" tabindex="-1"></a>This basic approach suffers from three major flaws:</span>
<span id="cb37-431"><a href="#cb37-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-432"><a href="#cb37-432" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>If the minimum is outside our range of guesses, the answer will be completely wrong.</span>
<span id="cb37-433"><a href="#cb37-433" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Even if our range of guesses is correct, if the guesses are too coarse, our answer will be inaccurate.</span>
<span id="cb37-434"><a href="#cb37-434" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>It is *very* computationally inefficient, considering potentially vast numbers of guesses that are useless.</span>
<span id="cb37-435"><a href="#cb37-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-436"><a href="#cb37-436" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `Scipy.optimize.minimize`</span></span>
<span id="cb37-437"><a href="#cb37-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-438"><a href="#cb37-438" aria-hidden="true" tabindex="-1"></a>One way to minimize this mathematical function is to use the <span class="in">`scipy.optimize.minimize`</span> function. It takes a function and a starting guess and tries to find the minimum.</span>
<span id="cb37-439"><a href="#cb37-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-442"><a href="#cb37-442" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-443"><a href="#cb37-443" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb37-444"><a href="#cb37-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-445"><a href="#cb37-445" aria-hidden="true" tabindex="-1"></a><span class="co"># takes a function f and a starting point x0 and returns a readout </span></span>
<span id="cb37-446"><a href="#cb37-446" aria-hidden="true" tabindex="-1"></a><span class="co"># with the optimal input value of x which minimizes f</span></span>
<span id="cb37-447"><a href="#cb37-447" aria-hidden="true" tabindex="-1"></a>minimize(arbitrary, x0 <span class="op">=</span> <span class="fl">3.5</span>)</span>
<span id="cb37-448"><a href="#cb37-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-449"><a href="#cb37-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-450"><a href="#cb37-450" aria-hidden="true" tabindex="-1"></a><span class="in">`scipy.optimize.minimize`</span> is great. It may also seem a bit magical. How could you write a function that can find the minimum of any mathematical function? There are a number of ways to do this, which we'll explore in today's lecture, eventually arriving at the important idea of **gradient descent**, which is the principle that <span class="in">`scipy.optimize.minimize`</span> uses.</span>
<span id="cb37-451"><a href="#cb37-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-452"><a href="#cb37-452" aria-hidden="true" tabindex="-1"></a>It turns out that under the hood, the <span class="in">`fit`</span> method for <span class="in">`LinearRegression`</span> models uses gradient descent. Gradient descent is also how much of machine learning works, including even advanced neural network models. </span>
<span id="cb37-453"><a href="#cb37-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-454"><a href="#cb37-454" aria-hidden="true" tabindex="-1"></a>In Data 100, the gradient descent process will usually be invisible to us, hidden beneath an abstraction layer. However, to be good data scientists, it's important that we know the underlying principles that optimization functions harness to find optimal parameters.</span>
<span id="cb37-455"><a href="#cb37-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-456"><a href="#cb37-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-457"><a href="#cb37-457" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Digging into Gradient Descent</span></span>
<span id="cb37-458"><a href="#cb37-458" aria-hidden="true" tabindex="-1"></a>Looking at the function across this domain, it is clear that the function's minimum value occurs around $\theta = 5.3$. Let's pretend for a moment that we *couldn't* see the full view of the cost function. How would we guess the value of $\theta$ that minimizes the function? </span>
<span id="cb37-459"><a href="#cb37-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-460"><a href="#cb37-460" aria-hidden="true" tabindex="-1"></a>It turns out that the first derivative of the function can give us a clue. In the graph below, the function and its derivative are plotted, with points where the derivative is equal to 0 plotted in light green.</span>
<span id="cb37-461"><a href="#cb37-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-464"><a href="#cb37-464" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-465"><a href="#cb37-465" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-466"><a href="#cb37-466" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb37-467"><a href="#cb37-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-468"><a href="#cb37-468" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> derivative_arbitrary(x):</span>
<span id="cb37-469"><a href="#cb37-469" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">3</span> <span class="op">-</span> <span class="dv">45</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">160</span><span class="op">*</span>x <span class="op">-</span> <span class="dv">180</span>)<span class="op">/</span><span class="dv">10</span></span>
<span id="cb37-470"><a href="#cb37-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-471"><a href="#cb37-471" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure()</span>
<span id="cb37-472"><a href="#cb37-472" aria-hidden="true" tabindex="-1"></a>roots <span class="op">=</span> np.array([<span class="fl">2.3927</span>, <span class="fl">3.5309</span>, <span class="fl">5.3263</span>])</span>
<span id="cb37-473"><a href="#cb37-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-474"><a href="#cb37-474" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> xs, y <span class="op">=</span> arbitrary(xs), </span>
<span id="cb37-475"><a href="#cb37-475" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"lines"</span>, name <span class="op">=</span> <span class="st">"f"</span>))</span>
<span id="cb37-476"><a href="#cb37-476" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> xs, y <span class="op">=</span> derivative_arbitrary(xs), </span>
<span id="cb37-477"><a href="#cb37-477" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"lines"</span>, name <span class="op">=</span> <span class="st">"df"</span>, line <span class="op">=</span> {<span class="st">"dash"</span>: <span class="st">"dash"</span>}))</span>
<span id="cb37-478"><a href="#cb37-478" aria-hidden="true" tabindex="-1"></a>fig.add_trace(go.Scatter(x <span class="op">=</span> np.array(roots), y <span class="op">=</span> <span class="dv">0</span><span class="op">*</span>roots, </span>
<span id="cb37-479"><a href="#cb37-479" aria-hidden="true" tabindex="-1"></a>                         mode <span class="op">=</span> <span class="st">"markers"</span>, name <span class="op">=</span> <span class="st">"df = zero"</span>, marker_size <span class="op">=</span> <span class="dv">12</span>))</span>
<span id="cb37-480"><a href="#cb37-480" aria-hidden="true" tabindex="-1"></a>fig.update_layout(font_size <span class="op">=</span> <span class="dv">20</span>, yaxis_range<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb37-481"><a href="#cb37-481" aria-hidden="true" tabindex="-1"></a>fig.update_layout(autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-482"><a href="#cb37-482" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb37-483"><a href="#cb37-483" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-484"><a href="#cb37-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-485"><a href="#cb37-485" aria-hidden="true" tabindex="-1"></a>In the plots below, the line indicates the value of the derivative of each value of $\theta$. The derivative is negative where it is red and positive where it is green.</span>
<span id="cb37-486"><a href="#cb37-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-487"><a href="#cb37-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-488"><a href="#cb37-488" aria-hidden="true" tabindex="-1"></a>Say we make a guess for the minimizing value of $\theta$. Remember that we read plots from left to right, and assume that our starting $\theta$ value is to the left of the optimal $\hat{\theta}$. If the guess "undershoots" the true minimizing value – our guess for $\theta$ is lower than the value of the $\hat{\theta}$ that minimizes the function – the derivative will be **negative**. This means that if we increase $\theta$ (move further to the right), then we **can decrease** our loss function further. If this guess "overshoots" the true minimizing value, the derivative will be positive, implying the converse.</span>
<span id="cb37-489"><a href="#cb37-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-490"><a href="#cb37-490" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-491"><a href="#cb37-491" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-492"><a href="#cb37-492" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-493"><a href="#cb37-493" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/step.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'step'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-494"><a href="#cb37-494" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-495"><a href="#cb37-495" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-496"><a href="#cb37-496" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-497"><a href="#cb37-497" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-498"><a href="#cb37-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-499"><a href="#cb37-499" aria-hidden="true" tabindex="-1"></a>We can use this pattern to help formulate our next guess for the optimal $\hat{\theta}$. Consider the case where we've undershot $\theta$ by guessing too low of a value. We'll want our next guess to be greater in value than our previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope "downhill" to the function's minimum value.</span>
<span id="cb37-500"><a href="#cb37-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-501"><a href="#cb37-501" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-502"><a href="#cb37-502" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-503"><a href="#cb37-503" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-504"><a href="#cb37-504" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/neg_step.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'neg_step'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-505"><a href="#cb37-505" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-506"><a href="#cb37-506" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-507"><a href="#cb37-507" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-508"><a href="#cb37-508" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-509"><a href="#cb37-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-510"><a href="#cb37-510" aria-hidden="true" tabindex="-1"></a>If we've overshot $\hat{\theta}$ by guessing too high of a value, we'll want our next guess to be lower in value – we want to shift our guess for $\hat{\theta}$ to the left. </span>
<span id="cb37-511"><a href="#cb37-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-512"><a href="#cb37-512" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-513"><a href="#cb37-513" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-514"><a href="#cb37-514" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-515"><a href="#cb37-515" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/pos_step.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'pos_step'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-516"><a href="#cb37-516" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-517"><a href="#cb37-517" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-518"><a href="#cb37-518" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-519"><a href="#cb37-519" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-520"><a href="#cb37-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-521"><a href="#cb37-521" aria-hidden="true" tabindex="-1"></a>In other words, the derivative of the function at each point tells us the direction of our next guess.</span>
<span id="cb37-522"><a href="#cb37-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-523"><a href="#cb37-523" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A negative slope means we want to step to the right, or move in the *positive* direction. </span>
<span id="cb37-524"><a href="#cb37-524" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A positive slope means we want to step to the left, or move in the *negative* direction.</span>
<span id="cb37-525"><a href="#cb37-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-526"><a href="#cb37-526" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Algorithm Attempt 1</span></span>
<span id="cb37-527"><a href="#cb37-527" aria-hidden="true" tabindex="-1"></a>Armed with this knowledge, let's try to see if we can use the derivative to optimize the function.</span>
<span id="cb37-528"><a href="#cb37-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-529"><a href="#cb37-529" aria-hidden="true" tabindex="-1"></a>We start by making some guess for the minimizing value of $x$. Then, we look at the derivative of the function at this value of $x$, and step downhill in the *opposite* direction. We can express our new rule as a recurrence relation:</span>
<span id="cb37-530"><a href="#cb37-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-531"><a href="#cb37-531" aria-hidden="true" tabindex="-1"></a>$$x^{(t+1)} = x^{(t)} - \frac{d}{dx} f(x^{(t)})$$</span>
<span id="cb37-532"><a href="#cb37-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-533"><a href="#cb37-533" aria-hidden="true" tabindex="-1"></a>Translating this statement into English: we obtain **our next guess** for the minimizing value of $x$ at timestep $t+1$ ($x^{(t+1)}$) by taking **our last guess** ($x^{(t)}$) and subtracting the **derivative of the function** at that point ($\frac{d}{dx} f(x^{(t)})$).</span>
<span id="cb37-534"><a href="#cb37-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-535"><a href="#cb37-535" aria-hidden="true" tabindex="-1"></a>A few steps are shown below, where the old step is shown as a transparent point, and the next step taken is the green-filled dot.</span>
<span id="cb37-536"><a href="#cb37-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-537"><a href="#cb37-537" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-538"><a href="#cb37-538" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-539"><a href="#cb37-539" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-540"><a href="#cb37-540" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/grad_descent_1.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'grad_descent_2'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'800'</span><span class="dt">&gt;</span></span>
<span id="cb37-541"><a href="#cb37-541" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-542"><a href="#cb37-542" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-543"><a href="#cb37-543" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-544"><a href="#cb37-544" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-545"><a href="#cb37-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-546"><a href="#cb37-546" aria-hidden="true" tabindex="-1"></a>Looking pretty good! We do have a problem though – once we arrive close to the minimum value of the function, our guesses "bounce" back and forth past the minimum without ever reaching it.</span>
<span id="cb37-547"><a href="#cb37-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-548"><a href="#cb37-548" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-549"><a href="#cb37-549" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-550"><a href="#cb37-550" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-551"><a href="#cb37-551" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/grad_descent_2.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'grad_descent_2'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'500'</span><span class="dt">&gt;</span></span>
<span id="cb37-552"><a href="#cb37-552" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-553"><a href="#cb37-553" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-554"><a href="#cb37-554" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-555"><a href="#cb37-555" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-556"><a href="#cb37-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-557"><a href="#cb37-557" aria-hidden="true" tabindex="-1"></a>In other words, each step we take when updating our guess moves us too far. We can address this by decreasing the size of each step. </span>
<span id="cb37-558"><a href="#cb37-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-559"><a href="#cb37-559" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Algorithm Attempt 2</span></span>
<span id="cb37-560"><a href="#cb37-560" aria-hidden="true" tabindex="-1"></a>Let's update our algorithm to use a **learning rate** (also sometimes called the step size), which controls how far we move with each update. We represent the learning rate with $\alpha$. </span>
<span id="cb37-561"><a href="#cb37-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-562"><a href="#cb37-562" aria-hidden="true" tabindex="-1"></a>$$x^{(t+1)} = x^{(t)} - \alpha \frac{d}{dx} f(x^{(t)})$$</span>
<span id="cb37-563"><a href="#cb37-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-564"><a href="#cb37-564" aria-hidden="true" tabindex="-1"></a>A small $\alpha$ means that we will take small steps; a large $\alpha$ means we will take large steps. When do we stop updating? We stop updating either after a fixed number of updates or after a subsequent update doesn't change much.</span>
<span id="cb37-565"><a href="#cb37-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-566"><a href="#cb37-566" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb37-567"><a href="#cb37-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-568"><a href="#cb37-568" aria-hidden="true" tabindex="-1"></a>In Data 100, the learning rate is constant. More generally, however, $\alpha$ could also decrease over time, or decay.</span>
<span id="cb37-569"><a href="#cb37-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-570"><a href="#cb37-570" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb37-571"><a href="#cb37-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-572"><a href="#cb37-572" aria-hidden="true" tabindex="-1"></a>Updating our function to use $\alpha=0.3$, our algorithm successfully **converges** (settles on a solution and stops updating significantly, or at all) on the minimum value.</span>
<span id="cb37-573"><a href="#cb37-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-574"><a href="#cb37-574" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-575"><a href="#cb37-575" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-576"><a href="#cb37-576" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-577"><a href="#cb37-577" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/grad_descent_3.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'grad_descent_3'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'500'</span><span class="dt">&gt;</span></span>
<span id="cb37-578"><a href="#cb37-578" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-579"><a href="#cb37-579" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-580"><a href="#cb37-580" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-581"><a href="#cb37-581" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-582"><a href="#cb37-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-583"><a href="#cb37-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convexity</span></span>
<span id="cb37-584"><a href="#cb37-584" aria-hidden="true" tabindex="-1"></a>In our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum that's just to the left? </span>
<span id="cb37-585"><a href="#cb37-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-586"><a href="#cb37-586" aria-hidden="true" tabindex="-1"></a>If we had chosen a different starting guess for $\theta$, or a different value for the learning rate $\alpha$, our algorithm may have gotten "stuck" and converged on the local minimum, rather than on the true optimum value of loss. </span>
<span id="cb37-587"><a href="#cb37-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-588"><a href="#cb37-588" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-589"><a href="#cb37-589" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-590"><a href="#cb37-590" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-591"><a href="#cb37-591" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/local.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'local'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-592"><a href="#cb37-592" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-593"><a href="#cb37-593" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-594"><a href="#cb37-594" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-595"><a href="#cb37-595" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-596"><a href="#cb37-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-597"><a href="#cb37-597" aria-hidden="true" tabindex="-1"></a>If the loss function is **convex**, gradient descent is guaranteed to converge and find the global minimum of the objective function. Formally, a function $f$ is convex if:</span>
<span id="cb37-598"><a href="#cb37-598" aria-hidden="true" tabindex="-1"></a>$$tf(a) + (1-t)f(b) \geq f(ta + (1-t)b)$$</span>
<span id="cb37-599"><a href="#cb37-599" aria-hidden="true" tabindex="-1"></a>for all $a, b$ in the domain of $f$ and $t \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$.</span>
<span id="cb37-600"><a href="#cb37-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-601"><a href="#cb37-601" aria-hidden="true" tabindex="-1"></a>To put this into words: if you drew a line between any two points on the curve, all values on the curve must be *on or below* the line. Importantly, any local minimum of a convex function is also its global minimum so we avoid the situation where the algorithm converges on some critical point that is not the minimum of the function.</span>
<span id="cb37-602"><a href="#cb37-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-603"><a href="#cb37-603" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">div</span><span class="ot"> align</span><span class="op">=</span><span class="st">"middle"</span><span class="dt">&gt;</span></span>
<span id="cb37-604"><a href="#cb37-604" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">table</span><span class="ot"> style</span><span class="op">=</span><span class="st">"width:100%"</span><span class="dt">&gt;</span></span>
<span id="cb37-605"><a href="#cb37-605" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;</span><span class="kw">tr</span><span class="ot"> align</span><span class="op">=</span><span class="st">"center"</span><span class="dt">&gt;</span></span>
<span id="cb37-606"><a href="#cb37-606" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;</span><span class="kw">td</span><span class="dt">&gt;&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"images/convex.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">'convex'</span><span class="ot"> width</span><span class="op">=</span><span class="st">'600'</span><span class="dt">&gt;</span></span>
<span id="cb37-607"><a href="#cb37-607" aria-hidden="true" tabindex="-1"></a>      <span class="dt">&lt;/</span><span class="kw">td</span><span class="dt">&gt;</span></span>
<span id="cb37-608"><a href="#cb37-608" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&lt;/</span><span class="kw">tr</span><span class="dt">&gt;</span></span>
<span id="cb37-609"><a href="#cb37-609" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;/</span><span class="kw">table</span><span class="dt">&gt;</span></span>
<span id="cb37-610"><a href="#cb37-610" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">div</span><span class="dt">&gt;</span></span>
<span id="cb37-611"><a href="#cb37-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-612"><a href="#cb37-612" aria-hidden="true" tabindex="-1"></a>In summary, non-convex loss functions can cause problems with optimization. This means that our choice of loss function is a key factor in our modeling process. It turns out that MSE *is* convex, which is a major reason why it is such a popular choice of loss function. Gradient descent is only guaranteed to converge (given enough iterations and an appropriate step size) for convex functions.</span>
<span id="cb37-613"><a href="#cb37-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-614"><a href="#cb37-614" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent in 1 Dimension</span></span>
<span id="cb37-615"><a href="#cb37-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-616"><a href="#cb37-616" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Terminology clarification**: In past lectures, we have used “loss” to refer to the error incurred on a *single* datapoint. In applications, we usually care more about the average error across *all* datapoints. Going forward, we will take the “model’s loss” to mean the model’s average error across the dataset. This is sometimes also known as the empirical risk (R), cost function, or objective function. $$L(\theta) = R(\theta) = \frac{1}{n} \sum_{i=1}^{n} L(y, \hat{y})$$</span></span>
<span id="cb37-617"><a href="#cb37-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-618"><a href="#cb37-618" aria-hidden="true" tabindex="-1"></a>In our discussion above, we worked with some arbitrary function $f$. As data scientists, we will almost always work with gradient descent in the context of optimizing *models* – specifically, we want to apply gradient descent to find the minimum of a *loss function*. In a modeling context, our goal is to minimize a loss function by choosing the minimizing model *parameters*.</span>
<span id="cb37-619"><a href="#cb37-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-620"><a href="#cb37-620" aria-hidden="true" tabindex="-1"></a>Recall our modeling workflow from the past few lectures: </span>
<span id="cb37-621"><a href="#cb37-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-622"><a href="#cb37-622" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Define a model with some parameters $\theta_i$</span>
<span id="cb37-623"><a href="#cb37-623" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Choose a loss function </span>
<span id="cb37-624"><a href="#cb37-624" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Select the values of $\theta_i$ that minimize the loss function on the data</span>
<span id="cb37-625"><a href="#cb37-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-626"><a href="#cb37-626" aria-hidden="true" tabindex="-1"></a>Gradient descent is a powerful technique for completing this last task. By applying the gradient descent algorithm, we can select values for our parameters $\theta_i$ that will lead to the model having minimal loss on the training data.</span>
<span id="cb37-627"><a href="#cb37-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-628"><a href="#cb37-628" aria-hidden="true" tabindex="-1"></a>When using gradient descent in a modeling context, we:</span>
<span id="cb37-629"><a href="#cb37-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-630"><a href="#cb37-630" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Make guesses for the minimizing $\theta_i$</span>
<span id="cb37-631"><a href="#cb37-631" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the derivative of the loss function $L$</span>
<span id="cb37-632"><a href="#cb37-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-633"><a href="#cb37-633" aria-hidden="true" tabindex="-1"></a>We can "translate" our gradient descent rule from before by replacing $x$ with $\theta$ and $f$ with $L$:</span>
<span id="cb37-634"><a href="#cb37-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-635"><a href="#cb37-635" aria-hidden="true" tabindex="-1"></a>$$\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta} L(\theta^{(t)})$$</span>
<span id="cb37-636"><a href="#cb37-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-637"><a href="#cb37-637" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Gradient Descent on the `tips` Dataset </span></span>
<span id="cb37-638"><a href="#cb37-638" aria-hidden="true" tabindex="-1"></a>To see this in action, let's consider a case where we have a linear model with no offset. We want to predict the tip (y) given the price of a meal (x). To do this, we</span>
<span id="cb37-639"><a href="#cb37-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-640"><a href="#cb37-640" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Choose a model: $\hat{y} = \theta_1 x$,</span>
<span id="cb37-641"><a href="#cb37-641" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Choose a loss function: $L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2$.</span>
<span id="cb37-642"><a href="#cb37-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-643"><a href="#cb37-643" aria-hidden="true" tabindex="-1"></a>Let's apply our <span class="in">`gradient_descent`</span> function from before to optimize our model on the <span class="in">`tips`</span> dataset. We will try to select the best parameter $\theta_i$ to predict the <span class="in">`tip`</span> $y$ from the <span class="in">`total_bill`</span> $x$.</span>
<span id="cb37-644"><a href="#cb37-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-647"><a href="#cb37-647" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-648"><a href="#cb37-648" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb37-649"><a href="#cb37-649" aria-hidden="true" tabindex="-1"></a>df.head()</span>
<span id="cb37-650"><a href="#cb37-650" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-651"><a href="#cb37-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-652"><a href="#cb37-652" aria-hidden="true" tabindex="-1"></a>We can visualize the value of the MSE on our dataset for different possible choices of $\theta_1$. To optimize our model, we want to select the value of $\theta_1$ that leads to the lowest MSE.</span>
<span id="cb37-653"><a href="#cb37-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-654"><a href="#cb37-654" aria-hidden="true" tabindex="-1"></a>To apply gradient descent, we need to compute the derivative of the loss function with respect to our parameter $\theta_1$.</span>
<span id="cb37-655"><a href="#cb37-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-656"><a href="#cb37-656" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Given our loss function, $$L(\theta) = MSE(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \theta_1x_i)^2$$</span>
<span id="cb37-657"><a href="#cb37-657" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We take the derivative with respect to $\theta_1$ $$\frac{\partial}{\partial \theta_{1}} L(\theta_1^{(t)}) = \frac{-2}{n} \sum_{i=1}^n (y_i - \theta_1^{(t)} x_i) x_i$$</span>
<span id="cb37-658"><a href="#cb37-658" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Which results in the gradient descent update rule</span>
<span id="cb37-659"><a href="#cb37-659" aria-hidden="true" tabindex="-1"></a>$$\theta_1^{(t+1)} = \theta_1^{(t)} - \alpha \frac{d}{d\theta}L(\theta_1^{(t)})$$</span>
<span id="cb37-660"><a href="#cb37-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-661"><a href="#cb37-661" aria-hidden="true" tabindex="-1"></a>for some learning rate $\alpha$.</span>
<span id="cb37-662"><a href="#cb37-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-663"><a href="#cb37-663" aria-hidden="true" tabindex="-1"></a>Implementing this in code, we can visualize the MSE loss on the <span class="in">`tips`</span> data. **MSE is convex**, so there is one global minimum.</span>
<span id="cb37-664"><a href="#cb37-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-667"><a href="#cb37-667" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-668"><a href="#cb37-668" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb37-669"><a href="#cb37-669" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(df, initial_guess, alpha, n):</span>
<span id="cb37-670"><a href="#cb37-670" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Performs n steps of gradient descent on df using learning rate alpha starting</span></span>
<span id="cb37-671"><a href="#cb37-671" aria-hidden="true" tabindex="-1"></a><span class="co">       from initial_guess. Returns a numpy array of all guesses over time."""</span></span>
<span id="cb37-672"><a href="#cb37-672" aria-hidden="true" tabindex="-1"></a>    guesses <span class="op">=</span> [initial_guess]</span>
<span id="cb37-673"><a href="#cb37-673" aria-hidden="true" tabindex="-1"></a>    current_guess <span class="op">=</span> initial_guess</span>
<span id="cb37-674"><a href="#cb37-674" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(guesses) <span class="op">&lt;</span> n:</span>
<span id="cb37-675"><a href="#cb37-675" aria-hidden="true" tabindex="-1"></a>        current_guess <span class="op">=</span> current_guess <span class="op">-</span> alpha <span class="op">*</span> df(current_guess)</span>
<span id="cb37-676"><a href="#cb37-676" aria-hidden="true" tabindex="-1"></a>        guesses.append(current_guess)</span>
<span id="cb37-677"><a href="#cb37-677" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-678"><a href="#cb37-678" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(guesses)</span>
<span id="cb37-679"><a href="#cb37-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-680"><a href="#cb37-680" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_single_arg(theta_1):</span>
<span id="cb37-681"><a href="#cb37-681" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the MSE on our data for the given theta1"""</span></span>
<span id="cb37-682"><a href="#cb37-682" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> df[<span class="st">"total_bill"</span>]</span>
<span id="cb37-683"><a href="#cb37-683" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> df[<span class="st">"tip"</span>]</span>
<span id="cb37-684"><a href="#cb37-684" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> theta_1 <span class="op">*</span> x</span>
<span id="cb37-685"><a href="#cb37-685" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y_obs) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb37-686"><a href="#cb37-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-687"><a href="#cb37-687" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_derivative_single_arg(theta_1):</span>
<span id="cb37-688"><a href="#cb37-688" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the derivative of the MSE on our data for the given theta1"""</span></span>
<span id="cb37-689"><a href="#cb37-689" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> df[<span class="st">"total_bill"</span>]</span>
<span id="cb37-690"><a href="#cb37-690" aria-hidden="true" tabindex="-1"></a>    y_obs <span class="op">=</span> df[<span class="st">"tip"</span>]</span>
<span id="cb37-691"><a href="#cb37-691" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> theta_1 <span class="op">*</span> x</span>
<span id="cb37-692"><a href="#cb37-692" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-693"><a href="#cb37-693" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(<span class="dv">2</span> <span class="op">*</span> (y_hat <span class="op">-</span> y_obs) <span class="op">*</span> x)</span>
<span id="cb37-694"><a href="#cb37-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-695"><a href="#cb37-695" aria-hidden="true" tabindex="-1"></a>loss_df <span class="op">=</span> pd.DataFrame({<span class="st">"theta_1"</span>:np.linspace(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">1</span>), <span class="st">"MSE"</span>:[mse_single_arg(theta_1) <span class="cf">for</span> theta_1 <span class="kw">in</span> np.linspace(<span class="op">-</span><span class="fl">1.5</span>, <span class="dv">1</span>)]})</span>
<span id="cb37-696"><a href="#cb37-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-697"><a href="#cb37-697" aria-hidden="true" tabindex="-1"></a>trajectory <span class="op">=</span> gradient_descent(mse_loss_derivative_single_arg, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">0.0001</span>, <span class="dv">100</span>)</span>
<span id="cb37-698"><a href="#cb37-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-699"><a href="#cb37-699" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_df[<span class="st">"theta_1"</span>], loss_df[<span class="st">"MSE"</span>])</span>
<span id="cb37-700"><a href="#cb37-700" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory, [mse_single_arg(guess) <span class="cf">for</span> guess <span class="kw">in</span> trajectory], c<span class="op">=</span><span class="st">"white"</span>, edgecolor<span class="op">=</span><span class="st">"firebrick"</span>)</span>
<span id="cb37-701"><a href="#cb37-701" aria-hidden="true" tabindex="-1"></a>plt.scatter(trajectory[<span class="op">-</span><span class="dv">1</span>], mse_single_arg(trajectory[<span class="op">-</span><span class="dv">1</span>]), c<span class="op">=</span><span class="st">"firebrick"</span>)</span>
<span id="cb37-702"><a href="#cb37-702" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"</span><span class="dv">$</span><span class="ch">\t</span><span class="vs">heta_1</span><span class="dv">$</span><span class="vs">"</span>)</span>
<span id="cb37-703"><a href="#cb37-703" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"</span><span class="dv">$</span><span class="vs">L</span><span class="kw">(</span><span class="ch">\t</span><span class="vs">heta_1</span><span class="kw">)</span><span class="dv">$</span><span class="vs">"</span>)<span class="op">;</span></span>
<span id="cb37-704"><a href="#cb37-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-705"><a href="#cb37-705" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final guess for theta_1: </span><span class="sc">{</span>trajectory[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-706"><a href="#cb37-706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>