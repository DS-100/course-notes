<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 15&nbsp; Cross Validation and Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../probability_1/probability_1.html" rel="next">
<link href="../feature_engineering/feature_engineering.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="../Principles-and-Techniques-of-Data-Science.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_climate/case_study_climate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Case Study in Climate and Physical Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Inference &amp; Causality</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">sql_I.html</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link active" data-scroll-target="#cross-validation"><span class="toc-section-number">15.1</span>  Cross Validation</a>
  <ul>
  <li><a href="#the-holdout-method" id="toc-the-holdout-method" class="nav-link" data-scroll-target="#the-holdout-method"><span class="toc-section-number">15.1.1</span>  The Holdout Method</a>
  <ul>
  <li><a href="#hyperparameters" id="toc-hyperparameters" class="nav-link" data-scroll-target="#hyperparameters"><span class="toc-section-number">15.1.1.1</span>  Hyperparameters</a></li>
  </ul></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="toc-section-number">15.1.2</span>  K-Fold Cross Validation</a>
  <ul>
  <li><a href="#hyperparameter-selection-example" id="toc-hyperparameter-selection-example" class="nav-link" data-scroll-target="#hyperparameter-selection-example"><span class="toc-section-number">15.1.2.1</span>  Hyperparameter Selection Example</a></li>
  <li><a href="#picking-k" id="toc-picking-k" class="nav-link" data-scroll-target="#picking-k"><span class="toc-section-number">15.1.2.2</span>  Picking K</a></li>
  </ul></li>
  <li><a href="#test-sets" id="toc-test-sets" class="nav-link" data-scroll-target="#test-sets"><span class="toc-section-number">15.1.3</span>  Test Sets</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="toc-section-number">15.2</span>  Regularization</a>
  <ul>
  <li><a href="#constraining-gradient-descent" id="toc-constraining-gradient-descent" class="nav-link" data-scroll-target="#constraining-gradient-descent"><span class="toc-section-number">15.2.0.1</span>  Constraining Gradient Descent</a></li>
  <li><a href="#l2-regularization" id="toc-l2-regularization" class="nav-link" data-scroll-target="#l2-regularization"><span class="toc-section-number">15.2.1</span>  L2 Regularization</a>
  <ul>
  <li><a href="#the-constrained-form" id="toc-the-constrained-form" class="nav-link" data-scroll-target="#the-constrained-form"><span class="toc-section-number">15.2.1.1</span>  The Constrained Form</a></li>
  <li><a href="#the-functional-form" id="toc-the-functional-form" class="nav-link" data-scroll-target="#the-functional-form"><span class="toc-section-number">15.2.1.2</span>  The Functional Form</a></li>
  <li><a href="#closed-form-solution" id="toc-closed-form-solution" class="nav-link" data-scroll-target="#closed-form-solution"><span class="toc-section-number">15.2.1.3</span>  Closed Form Solution</a></li>
  <li><a href="#implementation-of-ridge-regression" id="toc-implementation-of-ridge-regression" class="nav-link" data-scroll-target="#implementation-of-ridge-regression"><span class="toc-section-number">15.2.1.4</span>  Implementation of Ridge Regression</a></li>
  </ul></li>
  <li><a href="#scaling-data-for-regularization" id="toc-scaling-data-for-regularization" class="nav-link" data-scroll-target="#scaling-data-for-regularization"><span class="toc-section-number">15.2.2</span>  Scaling Data for Regularization</a></li>
  <li><a href="#l1-regularization" id="toc-l1-regularization" class="nav-link" data-scroll-target="#l1-regularization"><span class="toc-section-number">15.2.3</span>  L1 Regularization</a>
  <ul>
  <li><a href="#the-constrained-form-1" id="toc-the-constrained-form-1" class="nav-link" data-scroll-target="#the-constrained-form-1"><span class="toc-section-number">15.2.3.1</span>  The Constrained Form</a></li>
  <li><a href="#the-functional-form-1" id="toc-the-functional-form-1" class="nav-link" data-scroll-target="#the-functional-form-1"><span class="toc-section-number">15.2.3.2</span>  The Functional Form</a></li>
  <li><a href="#implementation-of-lasso-regression" id="toc-implementation-of-lasso-regression" class="nav-link" data-scroll-target="#implementation-of-lasso-regression"><span class="toc-section-number">15.2.3.3</span>  Implementation of Lasso Regression</a></li>
  </ul></li>
  <li><a href="#summary-of-regularization-methods" id="toc-summary-of-regularization-methods" class="nav-link" data-scroll-target="#summary-of-regularization-methods"><span class="toc-section-number">15.2.4</span>  Summary of Regularization Methods</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Consider the question: how do we control the complexity of our model? To answer this, we must know precisely when our model begins to overfit. The key to this lies in evaluating the model on unseen data using a process called Cross-Validation. A second point this note will address is how to combat overfitting – namely, through a technique known as regularization.</p>
<section id="cross-validation" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">15.1</span> Cross Validation</h2>
<p>From the last lecture, we learned that <em>increasing</em> model complexity <em>decreased</em> our model’s training error but increased variance. This makes intuitive sense; adding more features causes our model to better fit the given data, but generalize worse to new data. For this reason, a low training error is not representative of our model’s underlying performance – this may be a side effect of overfitting.</p>
<p>Truly, the only way to know when our model overfits is by evaluating it on unseen data. Unfortunately, that means we need to wait for more data. This may be very expensive and time consuming.</p>
<p>How should we proceed? In this section, we will build up a viable solution to this problem.</p>
<section id="the-holdout-method" class="level3" data-number="15.1.1">
<h3 data-number="15.1.1" class="anchored" data-anchor-id="the-holdout-method"><span class="header-section-number">15.1.1</span> The Holdout Method</h3>
<p>The simplest approach to avoid overfitting is to keep some of our data secret from ourselves. This is known as the <strong>holdout method</strong>. We will train our models on <em>most</em> of the available data points – known as the <strong>training data</strong> . We’ll then evaluate the models’ performance on the unseen data points (called the <strong>validation set</strong>) to measure overfitting.</p>
<p>Imagine the following example where we wish to train a model on 35 data points. We choose the training set to be a random sample of 25 of these points, and the validation set to be the remaining 10 points. Using this data, we train 7 models, each with a higher polynomial degree than the last.</p>
<p>We get the following mean squared error (MSE) on the training data.</p>
<div class="columns">
<div class="column" style="width:40%;">
<p><img src="images/mse_graph.png" alt="mse_graph" width="400"></p>
</div><div class="column" style="width:20%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:30%;">
<p><img src="images/mse_errors.png" alt="mse_errors" width="120"></p>
</div>
</div>
<p>Using these same models, we compute the MSE on our 10 validation points and observe the following.</p>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="images/mse_validation_graph.png" alt="mse_validation_graph" width="400"></p>
</div><div class="column" style="width:20%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:30%;">
<p><img src="images/mse_validation_errors.png" alt="mse_validation_errors" width="250"></p>
</div>
</div>
<p>Notice how the training error monotonically <em>decreases</em> as polynomial degree <em>increases</em>. This is consistent with our knowledge. However, validation error first <em>decreases</em>, then <em>increases</em> (from k = 3). These higher degree models are performing worse on unseen data – this indicates they are overfitting. As such, the best choice of polynomial degree in this example is <strong>k = 2</strong>.</p>
<p>More generally, we can represent this relationship with the following diagram.</p>
<p><img src="images/training_validation_curve.png" alt="training_validation_curve" width="500"></p>
<p>Our goal is to train a model with complexity near the red line. Note that this relationship is a simplification of the real-world. But for purposes of Data 100, this is good enough.</p>
<section id="hyperparameters" class="level4" data-number="15.1.1.1">
<h4 data-number="15.1.1.1" class="anchored" data-anchor-id="hyperparameters"><span class="header-section-number">15.1.1.1</span> Hyperparameters</h4>
<p>In machine learning, a <strong>hyperparameter</strong> is a value that controls the learning process. In our example, we built seven models, each of which had a hyperparameter <code>k</code> that controlled the polynomial degree of the model.</p>
<p>To choose between hyperparameters, we use the validation set. This was evident in our example above; we found that <code>k</code> = 2 had the lowest validation error. However, this holdout method is a bit naive. Imagine our random sample of 10 validation points coincidentally favored a higher degree polynomial. This would have led us to incorrectly favor a more complex model. In other words, our small amount of validation data may be different from real-world data.</p>
<p>To minimize this possiblity, we need to evaluate our model on more data. Decreasing the size of the training set is not an option – doing so will worsen our model. How should we proceed?</p>
</section>
</section>
<section id="k-fold-cross-validation" class="level3" data-number="15.1.2">
<h3 data-number="15.1.2" class="anchored" data-anchor-id="k-fold-cross-validation"><span class="header-section-number">15.1.2</span> K-Fold Cross Validation</h3>
<p>In the holdout method, we train a model on <em>only</em> the training set, and assess the quality <em>only</em> on the validation set. On the other hand, <strong>K-Fold Cross Validation</strong> is a technique that determines the quality of a hyperparameter by evaluating a model-hyperparameter combination on <code>k</code> independent “folds” of data, which together make up the <em>entire</em> dataset. This is a more robust alternative to the holdout method. Let’s break down each step.</p>
<p><strong>Note</strong>: The <code>k</code> in k-fold cross validation is different from the <code>k</code> polynomial degree discussed in the earlier example.</p>
<p>In the k-fold cross-validation approach, we split our data into <code>k</code> equally sized groups (often called folds). In the example where <code>k</code> = 5:</p>
<p><img src="images/cvfolds.png" class="img-fluid"></p>
<p>To determine the “quality” of a particular hyperparameter:</p>
<ul>
<li>Pick a fold, which we’ll call the validation fold. Train a model on all other <code>k-1</code> folds. Compute an error on the validation fold.</li>
<li>Repeat the step above for all <code>k</code> possible choices of validation fold, each time training a new model.</li>
<li>Average the <code>k</code> validation fold errors. This will give a single error for the hyperparameter.</li>
</ul>
<p>For <code>k</code> = 5, we have the following partitions. At each iteration, we train a model on the blue data and validate on the orange data. This gives us a total of 5 errors which we average to obtain a single representative error for the hyperparameter.</p>
<p><img src="images/kfold.png" class="img-fluid"></p>
<p>Note that the value of the hyperparameter is fixed during this process. By doing so, we can be confident in the hyperparameter’s performance on the <em>entire</em> dataset. To compare multiple choices of a hyperparameter –say <code>m</code> choices of hyperparameter– we run k-fold cross validation <code>m</code> times. The smallest of the <code>m</code> resulting errors corresponds to the best hyperparameter value.</p>
<section id="hyperparameter-selection-example" class="level4" data-number="15.1.2.1">
<h4 data-number="15.1.2.1" class="anchored" data-anchor-id="hyperparameter-selection-example"><span class="header-section-number">15.1.2.1</span> Hyperparameter Selection Example</h4>
<p>K-fold cross validation can aid in choosing the best hyperparameter values in respect to our model and loss function.</p>
<p>Consider an example where we run k-fold cross validation with <code>k</code> = 3. We are implementing a model that depends on a hyperparameter <span class="math inline">\(\alpha\)</span>, and we are searching for an <span class="math inline">\(\alpha\)</span> that minimizes our loss function. We have narrowed down our hyperparameters such that <span class="math inline">\(\alpha = [0.01, 0.1, 1, 10]\)</span>.</p>
<p><img src="images/cv_hyperparam_selection.png" class="img-fluid"></p>
<p>The losses of the model are shown per <code>k</code> fold of training data (arrows are pointing to the loss value) for each value of <span class="math inline">\(\alpha\)</span>. The average loss over the k-fold cross validation is displayed to the right of all the k-fold losses.</p>
<p>To determine the best <span class="math inline">\(\alpha\)</span> value, we must compare the average loss over the k-folds of training data. <span class="math inline">\(\alpha = 0.01\)</span> yields us an average loss of <span class="math inline">\(5\)</span>, <span class="math inline">\(\alpha = 0.1\)</span> yields us an average loss of <span class="math inline">\(4.67\)</span>, <span class="math inline">\(\alpha = 1\)</span> yields us an average loss of <span class="math inline">\(7\)</span>, and <span class="math inline">\(\alpha = 10\)</span> yields us an average loss of <span class="math inline">\(10.67\)</span>.</p>
<p>Thus, we would select <span class="math inline">\(\alpha = 0.1\)</span> as our hyperparameter value as it results in the lowest average loss over the k-folds of training data out of our possible <span class="math inline">\(\alpha\)</span> values.</p>
</section>
<section id="picking-k" class="level4" data-number="15.1.2.2">
<h4 data-number="15.1.2.2" class="anchored" data-anchor-id="picking-k"><span class="header-section-number">15.1.2.2</span> Picking K</h4>
<p>Typical choices of <code>k</code> are 5, 10, and N, where N is the number of data points.</p>
<p><code>k</code> = N is known as “leave one out cross validation”, and will typically give you the best results.</p>
<ul>
<li>In this approach, each validation set is only one point.</li>
<li>Every point gets a chance to get used as the validation set.</li>
</ul>
<p>However, <code>k</code> = N is very expensive and requires you to fit a large number of models.</p>
</section>
</section>
<section id="test-sets" class="level3" data-number="15.1.3">
<h3 data-number="15.1.3" class="anchored" data-anchor-id="test-sets"><span class="header-section-number">15.1.3</span> Test Sets</h3>
<p>Suppose we’re researchers building a state of the art regression model. We choose a model with the lowest validation error and want to report this out to the world. Unfortunately, our validation error may be biased; that is, not representative of error on real-world data. This is because during our hyperparameter search, we were implicitly “learning” from our validation data by tuning our model to achieve better results. Before reporting our results, we should run our model on a special <strong>test set</strong> that we’ve never seen or used for any purpose whatsoever.</p>
<p>A test set can be something that we generate ourselves, or it can be a common dataset whose true values are unknown. In the case of the former, we can split our our available data into 3 partitions: the training set, testing set, and validation set. The exact amount of data allocated to each partition varies, but a common split is 80% train, 10% test, 10% validation.</p>
<p>Below is an implementation of extracting a training, testing and validation set using <code>sklearn.train_test_split</code> on a data matrix <code>X</code> and an array of observations <code>y</code>.</p>
<p><code>X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.1)</code></p>
<p><code>X_train, X_val, y_train, y_val = train_test_split(X_train_valid, y_train_valid, test_size=0.11)</code></p>
<p>As a recap:</p>
<ul>
<li>Training set used to pick parameters.</li>
<li>Validation set used to pick hyperparameters (or pick between different models).</li>
<li>Test set used to provide an unbiased error at the end.</li>
</ul>
<p>Here is an idealized relationship between training error, test error, and validation error.</p>
<p><img src="images/training_test_valid_curve.png" alt="training_test_valid_curve" width="500"></p>
<p>Notice how the test error behaves similarily to the validation error. Both come from data that is unseen during the model training process, so both are fairly good estimates of real-world data. Of the two, the test error is more unbiased for the reasons mentioned above.</p>
<p>As before, the optimal complexity level exists where validation error is minimized. Logically, we can’t design a model that minimizes test error because we don’t use the test set until final evaluation.</p>
</section>
</section>
<section id="regularization" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">15.2</span> Regularization</h2>
<p>Earlier, we found an optimal model complexity by choosing the hyperparameter that minimized validation error. This was the polynomial degree <code>k</code>= 2. Tweaking the “complexity” was simple; it was only a matter of adjusting the polynomial degree.</p>
<p>However, in most machine learning problems, complexity is defined differently. Today, we’ll explore two different definitions of complexity: the <em>squared</em> and <em>absolute</em> magnitude of <span class="math inline">\(\theta_i\)</span> coefficients.</p>
<section id="constraining-gradient-descent" class="level4" data-number="15.2.0.1">
<h4 data-number="15.2.0.1" class="anchored" data-anchor-id="constraining-gradient-descent"><span class="header-section-number">15.2.0.1</span> Constraining Gradient Descent</h4>
<p>Before we discuss these definitions, let’s first familiarize ourselves with the concept of <strong>constrained gradient descent</strong>. Imagine we have a two feature model with coeffiecient weights of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>. Below we’ve plotted a two dimensional contour plot of the OLS loss surface – darker areas indicate regions of lower loss. Gradient descent will find the optimal paramaters during training <span class="math inline">\((\theta_1, \theta_2) = \hat\theta_{No Reg.}\)</span>.</p>
<p><img src="images/constrained_gd.png" alt="constrained_gd" width="700"></p>
<p>Suppose we arbitrarily decide that gradient descent can never land outside of the green ball.</p>
<p><img src="images/green_constrained_gd_sol.png" alt="green_constrained_gd" width="700"></p>
<p>Gradient descent finds a new solution at <span class="math inline">\(\hat\theta_{Reg.}\)</span>. This is far from the global optimal solution <span class="math inline">\(\hat\theta_{No Reg.}\)</span> – however, it is the closest point that lives in the green ball. In other words, <span class="math inline">\(\hat\theta_{Reg.}\)</span> is the <strong>constrained optimal solution</strong>.</p>
<p>The size of this ball is completely arbitrary. Increasing its size allows for a constrained solution closer to the global optimum, and vice versa.</p>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="images/small_constrained_gd.png" alt="small_constrained_gd" width="450"></p>
</div><div class="column" style="width:20%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:30%;">
<p><img src="images/large_constrained_gd.png" alt="large_constrained_gd" width="425"></p>
</div>
</div>
<p>In fact, the size of this ball is inherently linked to model complexity. A smaller ball constrains (<span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>) more than a larger ball. This is synonymous with the behavior of a <em>less</em> complex model, which finds a solution farther from the optimum. A <em>larger</em> ball, on the other hand, is synonymous to a <em>more</em> complex model that can achieve a near optimal solution.</p>
<p>Consider the extreme case where the radius is infinitely small. The solution to every constrained modeling problem would lie on the origin, at <span class="math inline">\((\theta_1, \theta_2) = (0, 0)\)</span>. This is equivalent to the constant model we studied – the least complex of all models. In the case where the radius is infinitely large, the optimal constrained solution exists at <span class="math inline">\(\hat\theta_{No Reg.}\)</span> itself! This is the solution obtained from OLS with no limitations on complexity.</p>
<p><img src="images/train_test_validation_curve_with_complexity.png" alt="train_test_validation_curve_with_complexity" width="550"></p>
<p>The intercept coefficient is typically <em>not</em> constrained; <span class="math inline">\(\theta_0\)</span> can be any value. This way, if all <span class="math inline">\(\theta_i = 0\)</span> except <span class="math inline">\(\theta_0\)</span>, the resulting model is a constant model (and <span class="math inline">\(\theta_0\)</span> is the mean of all observations).</p>
</section>
<section id="l2-regularization" class="level3" data-number="15.2.1">
<h3 data-number="15.2.1" class="anchored" data-anchor-id="l2-regularization"><span class="header-section-number">15.2.1</span> L2 Regularization</h3>
<section id="the-constrained-form" class="level4" data-number="15.2.1.1">
<h4 data-number="15.2.1.1" class="anchored" data-anchor-id="the-constrained-form"><span class="header-section-number">15.2.1.1</span> The Constrained Form</h4>
<p><strong>Regularization</strong> is the formal term that describes the process of limiting a model’s complexity. This is done by constraining the solution of a cost function, much like how we constrained the set of permissible (<span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>) above. <strong>L2 Regularization</strong>, commonly referred to as <strong>Ridge Regression</strong>, is the technique of constraining our model’s parameters to lie within a <em>ball</em> around the origin. Formally, it is defined as</p>
<p><span class="math display">\[\min_{\theta} \frac{1}{n} || Y - X\theta ||\]</span></p>
<center>
such that <span class="math inline">\(\sum_{j=1}^{d} \theta_j^{2} \le Q\)</span>
</center>
<p><br></p>
<p>The mathematical definition of complexity in Ridge Regression is <span class="math inline">\(\sum_{j=1}^{d} \theta_j^{2} \le Q\)</span>. This formulation of complexity limits the total squared magnitude of the coefficients to some constant <span class="math inline">\(Q\)</span>. In two dimensional space, this is <span class="math inline">\(\theta_{1}^{2} + \theta_{2}^{2} \le Q\)</span>. You’ll recognize this as the equation of a circle with axes <span class="math inline">\(\theta_{1}, \theta_{2}\)</span> and radius <span class="math inline">\(Q\)</span>. In higher dimensions, this circle becomes a hypersphere and is conventionally referred to as the <strong>L2 norm ball</strong>. Decreasing <span class="math inline">\(Q\)</span> shrinks the norm ball, and limits the complexity of the model (discussed in the previous section). Likewise, expanding the norm ball increases the allowable model complexity.</p>
<p>Without the constraint <span class="math inline">\(\sum_{j=1}^{d} \theta_j^{2} \le Q\)</span>, the optimal solution is <span class="math inline">\(\hat{\theta} = \hat\theta_{No Reg.}\)</span>. With an appropriate value of <span class="math inline">\(Q\)</span> applied to the constraint, the solution <span class="math inline">\(\hat{\theta} = \hat\theta_{Reg.}\)</span> is sub-optimal on the training data but generalizes better to new data.</p>
</section>
<section id="the-functional-form" class="level4" data-number="15.2.1.2">
<h4 data-number="15.2.1.2" class="anchored" data-anchor-id="the-functional-form"><span class="header-section-number">15.2.1.2</span> The Functional Form</h4>
<p>Unfortunately, the function above requires some work. It’s not easy to mathematically optimize over a constraint. Instead, in most machine learning text, you’ll see a different formulation of Ridge Regression.</p>
<p><span class="math display">\[\min_{\theta} \frac{1}{n} || Y - X\theta || + \alpha \sum_{j=1}^{d} \theta_j^{2}\]</span></p>
<p>These two equations are equivalent by Lagrangian Duality (not in scope).</p>
<p>Notice that we’ve replaced the constraint with a second term in our cost function. We’re now minimizing a function with a regularization term that penalizes large coefficients. The <span class="math inline">\(\alpha\)</span> factor controls the degree of regularization. In fact, <span class="math inline">\(\alpha \approx \frac{1}{Q}\)</span>. <br> To understand why, let’s consider these 2 extreme examples:</p>
<ul>
<li><p>Assume <span class="math inline">\(\alpha \rightarrow \infty\)</span>. Then, <span class="math inline">\(\alpha \sum_{j=1}^{d} \theta_j^{2}\)</span> dominates the cost function. To minimize this term, we set <span class="math inline">\(\theta_j = 0\)</span> for all <span class="math inline">\(j \ge 1\)</span>. This is a very constrained model that is mathematically equivalent to the constant model. Earlier, we explained the constant model also arises when the L2 norm ball radius <span class="math inline">\(Q \rightarrow 0\)</span>.</p></li>
<li><p>Assume <span class="math inline">\(\alpha \rightarrow 0\)</span>. Then, <span class="math inline">\(\alpha \sum_{j=1}^{d} \theta_j^{2}\)</span> is infinitely small. Minimizing the cost function is equivalent to <span class="math inline">\(\min_{\theta} \frac{1}{n} || Y - X\theta ||\)</span>. This is just OLS, and the optimal solution is the global minimum <span class="math inline">\(\hat{\theta} = \hat\theta_{No Reg.}\)</span>. We showed that the global optimum is achieved when the L2 norm ball radius <span class="math inline">\(Q \rightarrow \infty\)</span>.</p></li>
</ul>
</section>
<section id="closed-form-solution" class="level4" data-number="15.2.1.3">
<h4 data-number="15.2.1.3" class="anchored" data-anchor-id="closed-form-solution"><span class="header-section-number">15.2.1.3</span> Closed Form Solution</h4>
<p>An additional benefit to Ridge Regression is that it has a closed form solution.</p>
<p><span class="math display">\[\hat\theta_{ridge} = (X^TX + n\alpha I)^{-1}X^TY\]</span></p>
<p>This solution exists even if there is linear dependence in the columns of the data matrix. We will not derive this result in Data 100, as it involves a fair bit of matrix calculus.</p>
</section>
<section id="implementation-of-ridge-regression" class="level4" data-number="15.2.1.4">
<h4 data-number="15.2.1.4" class="anchored" data-anchor-id="implementation-of-ridge-regression"><span class="header-section-number">15.2.1.4</span> Implementation of Ridge Regression</h4>
<p>Of course, <code>sklearn</code> has a built-in implementation of Ridge Regression. Simply import the <code>Ridge</code> class of the <code>sklearn.linear_model</code> library.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will various Ridge Regression models on the familiar <code>vehicles</code> DataFrame from last lecture. This will help solidfy some of the theoretical concepts discussed earlier.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>vehicles <span class="op">=</span> pd.read_csv(<span class="st">"data/vehicle_data.csv"</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>vehicles_mpg <span class="op">=</span> pd.read_csv(<span class="st">"data/vehicle_mpg.csv"</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>vehicles.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>cylinders^2</th>
      <th>displacement^2</th>
      <th>horsepower^2</th>
      <th>weight^2</th>
      <th>acceleration^2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>64</td>
      <td>94249.0</td>
      <td>16900.0</td>
      <td>12278016</td>
      <td>144.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>64</td>
      <td>122500.0</td>
      <td>27225.0</td>
      <td>13638249</td>
      <td>132.25</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>64</td>
      <td>101124.0</td>
      <td>22500.0</td>
      <td>11806096</td>
      <td>121.00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>64</td>
      <td>92416.0</td>
      <td>22500.0</td>
      <td>11785489</td>
      <td>144.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>64</td>
      <td>91204.0</td>
      <td>19600.0</td>
      <td>11895601</td>
      <td>110.25</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Here, we fit an extremeley regularized model without an intercept. Note the small coefficient values.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ridge_model_large_reg <span class="op">=</span> Ridge(alpha <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>ridge_model_large_reg.fit(vehicles, vehicles_mpg)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>ridge_model_large_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[ 8.56292915e-04, -5.92399474e-02, -9.81013894e-02,
        -9.66985253e-03, -5.08353226e-03,  1.49576895e-02,
         1.04959034e-04,  1.14786826e-04,  9.07086742e-07,
        -4.60397349e-04]])</code></pre>
</div>
</div>
<p>Note how Ridge Regression effectively <strong>spreads a small weight across many features</strong>.</p>
<p>When we apply very little regularization, our coefficients increase in size. Notice how they are identical to the coefficients retrieved from the <code>LinearRegression</code> model. This indicates the radius <span class="math inline">\(Q\)</span> of the L2 norm ball is massive and encompasses the unregularized optimal solution. Once again, we see that <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(Q\)</span> are inversely related.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>ridge_model_small_reg <span class="op">=</span> Ridge(alpha <span class="op">=</span> <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>ridge_model_small_reg.fit(vehicles, vehicles_mpg)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>ridge_model_small_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[-8.06754383e-01, -6.32025048e-02, -2.92851012e-01,
        -3.41032156e-03, -1.43877512e+00,  1.25829303e-01,
         7.72841216e-05,  6.99398090e-04,  3.11031744e-07,
         3.16084838e-02]])</code></pre>
</div>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>linear_model.fit(vehicles, vehicles_mpg)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>linear_model.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([[-8.06756280e-01, -6.32024872e-02, -2.92851021e-01,
        -3.41032211e-03, -1.43877559e+00,  1.25829450e-01,
         7.72840884e-05,  6.99398114e-04,  3.11031832e-07,
         3.16084973e-02]])</code></pre>
</div>
</div>
</section>
</section>
<section id="scaling-data-for-regularization" class="level3" data-number="15.2.2">
<h3 data-number="15.2.2" class="anchored" data-anchor-id="scaling-data-for-regularization"><span class="header-section-number">15.2.2</span> Scaling Data for Regularization</h3>
<p>One issue with our approach is that our features are on vastly different scales. For example, <code>weight^2</code> is in the millions, while the number of <code>cylinders</code> are under 10. Intuitively, the coefficient value for <code>weight^2</code> must be very small to offset the large magnitude of the feature. On the other hand, the coefficient of the <code>cylinders</code> feature is likely quite large in comparison. We see these claims are true in the <code>LinearRegression</code> model above.</p>
<p>However, a problem arises in Ridge Regression. If we constrain our coefficients to a small region around the origin, we are unfairly restricting larger coefficients – like that of the <code>cylinders</code> feature. A smaller coefficient – that of the <code>weight^2</code> feature – likely lies within this region, so the value changes very little. Compare the coefficients of the regularized and unregularized <code>Ridge</code> models above, and you’ll see this is true.</p>
<p>Therefore, it’s imperative to standardize your data. We can do so using z-scores.</p>
<p><span class="math display">\[z_k = \frac{x_k - u_k}{\sigma_k}\]</span></p>
<p>You’ll do this on lab 8 using the “StandardScaler” transformer. The resulting model coefficients will be all on the same scale.</p>
</section>
<section id="l1-regularization" class="level3" data-number="15.2.3">
<h3 data-number="15.2.3" class="anchored" data-anchor-id="l1-regularization"><span class="header-section-number">15.2.3</span> L1 Regularization</h3>
<p><strong>L1 Regularization</strong>, commonly referred to as <strong>Lasso Regression</strong>, is an alternate regularization technique that limits the sum of <em>absolute</em> <span class="math inline">\(\theta_i\)</span> coefficients.</p>
<section id="the-constrained-form-1" class="level4" data-number="15.2.3.1">
<h4 data-number="15.2.3.1" class="anchored" data-anchor-id="the-constrained-form-1"><span class="header-section-number">15.2.3.1</span> The Constrained Form</h4>
<p><span class="math display">\[\min_{\theta} \frac{1}{n} || Y - X\theta ||\]</span></p>
<center>
such that <span class="math inline">\(\sum_{j=1}^{d} |\theta_j| \le Q\)</span>
</center>
<p><br></p>
<p>In two dimensions, our constraint equation is <span class="math inline">\(|\theta_1| + |\theta_2| \le Q\)</span>. This is the graph of a diamond centered on the origin with endpoints <span class="math inline">\(Q\)</span> units away on each axis.</p>
<p><img src="images/lasso_constrained.png" alt="lasso_constrained" width="700"></p>
</section>
<section id="the-functional-form-1" class="level4" data-number="15.2.3.2">
<h4 data-number="15.2.3.2" class="anchored" data-anchor-id="the-functional-form-1"><span class="header-section-number">15.2.3.2</span> The Functional Form</h4>
<p>A more convenient way to express Lasso Regression is as follows:</p>
<p><span class="math display">\[\min_{\theta} \frac{1}{n} || Y - X\theta || + \alpha \sum_{j=1}^{d} |\theta_j|\]</span></p>
<p>As with Ridge Regression, the hyperparameter <span class="math inline">\(\alpha\)</span> has the same effect on Lasso Regression. That is, <em>increasing</em> &nbsp;<span class="math inline">\(\alpha\)</span> (equivalently, <em>decreasing</em> &nbsp;<span class="math inline">\(Q\)</span>) <em>increases</em> the amount of regularization, and vice versa.</p>
<p>Unfortunately, Lasso Regression does not have a closed form solution – the cost function is not differentiable everywhere. Specifically, the sum <span class="math inline">\(\sum_{j=1}^{d} |\theta_j|\)</span> is problematic because it is composed of absolute value functions, each of which are non-differentiable at the origin.</p>
<p>So why use Lasso Regression? As we’ll see shortly, it is great at implicit <strong>feature selection</strong>.</p>
</section>
<section id="implementation-of-lasso-regression" class="level4" data-number="15.2.3.3">
<h4 data-number="15.2.3.3" class="anchored" data-anchor-id="implementation-of-lasso-regression"><span class="header-section-number">15.2.3.3</span> Implementation of Lasso Regression</h4>
<p>Lasso Regression is great at reducing complexity by eliminating the least important features in a model. It does so by setting their respective feature weights to <span class="math inline">\(0\)</span>. See the following example.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lasso_model <span class="op">=</span> Lasso(alpha <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>standardized_vehicles<span class="op">=</span>(vehicles<span class="op">-</span>vehicles.mean())<span class="op">/</span>vehicles.std()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>lasso_model.fit(standardized_vehicles, vehicles_mpg)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>lasso_model.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([-0.14009981, -0.28452369, -1.14351999, -4.11329618,  0.        ,
       -0.        , -0.        , -0.        , -0.        ,  0.        ])</code></pre>
</div>
</div>
<p>Notice how we standardized our data first. Lasso Regression then set the coefficients of our squared features to <span class="math inline">\(0\)</span> – presumably, these are the least important predictors of <code>mpg</code>.</p>
</section>
</section>
<section id="summary-of-regularization-methods" class="level3" data-number="15.2.4">
<h3 data-number="15.2.4" class="anchored" data-anchor-id="summary-of-regularization-methods"><span class="header-section-number">15.2.4</span> Summary of Regularization Methods</h3>
<p>A summary of our regression models is shwon below:</p>
<p><img src="images/reg_models_sum.png" class="img-fluid"></p>
<p>Understanding the distinction between Ridge Regression and Lasso Regression is important. We’ve provided a helpful visual that summarizes the key differences.</p>
<p><img src="https://people.eecs.berkeley.edu/~jrs/189/ridgelassoItayEvron.gif" class="img-fluid"></p>
<p>This diagram displays the L1 and L2 constrained solution for various orientations of the OLS loss surface. Notice how the L1 (Lasso) solution almost always lies on some axis, or edge of the diamond. Graphically, this makes sense; the edges of the diamond are the farthest from the origin, and usually closest to the global optimum. When this happens, only one feature has a non-zero coefficient; this “feature selection” argument extends quite nicely to multiple features in higher dimensional space.</p>
<p>The L2 (Ridge) solution, however, typically has an optimal solution in some quadrant of the graph. Every point on the circumference of the L2 norm ball is equidistant from the origin, and thus similar in distance to the global optimum. As such, this technique of regularization is great at distributing a small weight across both features.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../feature_engineering/feature_engineering.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../probability_1/probability_1.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>