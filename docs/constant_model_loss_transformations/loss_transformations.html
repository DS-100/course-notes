<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 11&nbsp; Constant Model, Loss, and Transformations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../ols/ols.html" rel="next">
<link href="../intro_to_modeling/intro_to_modeling.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../constant_model_loss_transformations/loss_transformations.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Constant Model, Loss, and Transformations</h2>
   
  <ul>
  <li><a href="#prediction-vs.-estimation" id="toc-prediction-vs.-estimation" class="nav-link active" data-scroll-target="#prediction-vs.-estimation"><span class="header-section-number">11.0.1</span> Prediction vs.&nbsp;Estimation</a></li>
  <li><a href="#constant-model-mse" id="toc-constant-model-mse" class="nav-link" data-scroll-target="#constant-model-mse"><span class="header-section-number">11.1</span> Constant Model + MSE</a>
  <ul>
  <li><a href="#deriving-the-optimal-theta_0" id="toc-deriving-the-optimal-theta_0" class="nav-link" data-scroll-target="#deriving-the-optimal-theta_0"><span class="header-section-number">11.1.1</span> Deriving the optimal <span class="math inline">\(\theta_0\)</span></a></li>
  <li><a href="#comparing-two-different-models-both-fit-with-mse" id="toc-comparing-two-different-models-both-fit-with-mse" class="nav-link" data-scroll-target="#comparing-two-different-models-both-fit-with-mse"><span class="header-section-number">11.1.2</span> Comparing Two Different Models, Both Fit with MSE</a></li>
  </ul></li>
  <li><a href="#constant-model-mae" id="toc-constant-model-mae" class="nav-link" data-scroll-target="#constant-model-mae"><span class="header-section-number">11.2</span> Constant Model + MAE</a>
  <ul>
  <li><a href="#deriving-the-optimal-theta_0-1" id="toc-deriving-the-optimal-theta_0-1" class="nav-link" data-scroll-target="#deriving-the-optimal-theta_0-1"><span class="header-section-number">11.2.1</span> Deriving the optimal <span class="math inline">\(\theta_0\)</span></a></li>
  </ul></li>
  <li><a href="#summary-loss-optimization-calculus-and-critical-points" id="toc-summary-loss-optimization-calculus-and-critical-points" class="nav-link" data-scroll-target="#summary-loss-optimization-calculus-and-critical-points"><span class="header-section-number">11.3</span> Summary: Loss Optimization, Calculus, and Critical Points</a></li>
  <li><a href="#comparing-loss-functions" id="toc-comparing-loss-functions" class="nav-link" data-scroll-target="#comparing-loss-functions"><span class="header-section-number">11.4</span> Comparing Loss Functions</a></li>
  <li><a href="#transformations-to-fit-linear-models" id="toc-transformations-to-fit-linear-models" class="nav-link" data-scroll-target="#transformations-to-fit-linear-models"><span class="header-section-number">11.5</span> Transformations to Fit Linear Models</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">11.6</span> Multiple Linear Regression</a></li>
  <li><a href="#bonus-calculating-constant-model-mse-using-an-algebraic-trick" id="toc-bonus-calculating-constant-model-mse-using-an-algebraic-trick" class="nav-link" data-scroll-target="#bonus-calculating-constant-model-mse-using-an-algebraic-trick"><span class="header-section-number">11.7</span> Bonus: Calculating Constant Model MSE Using an Algebraic Trick</a>
  <ul>
  <li><a href="#note" id="toc-note" class="nav-link" data-scroll-target="#note"><span class="header-section-number">11.7.0.0.1</span> Note</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Derive the optimal model parameters for the constant model under MSE and MAE cost functions.</li>
<li>Evaluate the differences between MSE and MAE risk.</li>
<li>Understand the need for linearization of variables and apply the Tukey-Mosteller bulge diagram for transformations.</li>
</ul>
</div>
</div>
</div>
<p>Last time, we introduced the modeling process. We set up a framework to predict target variables as functions of our features, following a set workflow:</p>
<ol type="1">
<li>Choose a model - how should we represent the world?</li>
<li>Choose a loss function - how do we quantify prediction error?</li>
<li>Fit the model - how do we choose the best parameter of our model given our data?</li>
<li>Evaluate model performance - how do we evaluate whether this process gave rise to a good model?</li>
</ol>
<p>To illustrate this process, we derived the optimal model parameters under simple linear regression (SLR) with mean squared error (MSE) as the cost function. A summary of the SLR modeling process is shown below:</p>
<div data-align="middle">
<p><img src="images/slr_modeling.png" alt="modeling" width="600"></p>
</div>
<p>At the end of last lecture, we dived deeper into step 4 - evaluating model performance - using SLR as an example. In this lecture, we’ll also explore the modeling process with new models, continue familiarizing ourselves with the modeling process by finding the best model parameters under a new model, the constant model, and test out two different loss functions to understand how our choice of loss influences model design. Later on, we’ll consider what happens when a linear model isn’t the best choice to capture trends in our data and what solutions there are to create better models.</p>
<p>Before we get into the modeling process, let’s quickly review some important terminology.</p>
<section id="prediction-vs.-estimation" class="level3" data-number="11.0.1">
<h3 data-number="11.0.1" class="anchored" data-anchor-id="prediction-vs.-estimation"><span class="header-section-number">11.0.1</span> Prediction vs.&nbsp;Estimation</h3>
<p>The terms prediction and estimation are often used somewhat interchangeably, but there is a subtle difference between them. <strong>Estimation</strong> is the task of using data to calculate model parameters. <strong>Prediction</strong> is the task of using a model to predict outputs for unseen data. In our simple linear regression model,</p>
<p><span class="math display">\[\hat{y} = \hat{\theta_0} + \hat{\theta_1}x\]</span></p>
<p>we <strong>estimate</strong> the parameters by minimizing average loss; then, we <strong>predict</strong> using these estimations. <strong>Least Squares Estimation</strong> is when we choose the parameters that minimize MSE.</p>
</section>
<section id="constant-model-mse" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="constant-model-mse"><span class="header-section-number">11.1</span> Constant Model + MSE</h2>
<p>Now, we’ll shift from the SLR model to the <strong>constant model</strong>, also known as a summary statistic. The constant model is slightly different from the simple linear regression model we’ve explored previously. Rather than generating predictions from an inputted feature variable, the constant model always <em>predicts the same constant number</em>. This ignores any relationships between variables. For example, let’s say we want to predict the number of drinks a boba shop sells in a day. Boba tea sales likely depend on the time of year, the weather, how the customers feel, whether school is in session, etc., but the constant model ignores these factors in favor of a simpler model. In other words, the constant model employs a <strong>simplifying assumption</strong>.</p>
<p>It is also a parametric, statistical model:</p>
<p><span class="math display">\[\hat{y} = \theta_0\]</span></p>
<p><span class="math inline">\(\theta_0\)</span> is the parameter of the constant model, just as <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> were the parameters in SLR. Since our parameter <span class="math inline">\(\theta_0\)</span> is 1-dimensional (<span class="math inline">\(\theta_0 \in \mathbb{R}\)</span>), we now have no input to our model and will always predict <span class="math inline">\(\hat{y} = \theta_0\)</span>.</p>
<section id="deriving-the-optimal-theta_0" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="deriving-the-optimal-theta_0"><span class="header-section-number">11.1.1</span> Deriving the optimal <span class="math inline">\(\theta_0\)</span></h3>
<p>Our task now is to determine what value of <span class="math inline">\(\theta_0\)</span> best represents the optimal model – in other words, what number should we guess each time to have the lowest possible <strong>average loss</strong> on our data?</p>
<p>Like before, we’ll use Mean Squared Error (MSE). Recall that the MSE is average squared loss (L2 loss) over the data <span class="math inline">\(D = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p><span class="math display">\[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \hat{y_i})^2 \]</span></p>
<p>Our modeling process now looks like this:</p>
<ol type="1">
<li>Choose a model: constant model</li>
<li>Choose a loss function: L2 loss</li>
<li>Fit the model</li>
<li>Evaluate model performance</li>
</ol>
<p>Given the <strong>constant model</strong> <span class="math inline">\(\hat{y} = \theta_0\)</span>, we can rewrite the MSE equation as</p>
<p><span class="math display">\[\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2 \]</span></p>
<p>We can <strong>fit the model</strong> by finding the optimal <span class="math inline">\(\hat{\theta_0}\)</span> that minimizes the MSE using a calculus approach.</p>
<ol type="1">
<li>Differentiate with respect to <span class="math inline">\(\theta_0\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{align}
\frac{d}{d\theta_0}\text{R}(\theta) &amp; = \frac{d}{d\theta_0}(\frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2)
\\ &amp;= \frac{1}{n}\sum^{n}_{i=1} \frac{d}{d\theta_0}  (y_i - \theta_0)^2 \quad \quad \text{a derivative of sum is a sum of derivatives}
\\ &amp;= \frac{1}{n}\sum^{n}_{i=1} 2 (y_i - \theta_0) (-1) \quad \quad \text{chain rule}
\\ &amp;= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \quad \quad \text{simplify constants}
\end{align}
\]</span></p>
<ol start="2" type="1">
<li><p>Set the derivative equation equal to 0:</p>
<p><span class="math display">\[
0 = {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})
\]</span></p></li>
<li><p>Solve for <span class="math inline">\(\hat{\theta_0}\)</span></p></li>
</ol>
<p><span class="math display">\[
\begin{align}
0 &amp;= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})
\\ &amp;= \sum^{n}_{i=1} (y_i - \hat{\theta_0}) \quad \quad \text{divide both sides by} \frac{-2}{n}
\\ &amp;= \left(\sum^{n}_{i=1} y_i\right) - \left(\sum^{n}_{i=1} \hat{\theta_0}\right) \quad \quad \text{separate sums}
\\ &amp;= \left(\sum^{n}_{i=1} y_i\right) - (n \cdot \hat{\theta_0}) \quad \quad  \text{c + c + … + c = nc}
\\ n \cdot \hat{\theta_0} &amp;= \sum^{n}_{i=1} y_i
\\ \hat{\theta_0} &amp;= \frac{1}{n} \sum^{n}_{i=1} y_i
\\ \hat{\theta_0} &amp;= \bar{y}
\end{align}
\]</span></p>
<p>Let’s take a moment to interpret this result. <span class="math inline">\(\hat{\theta_0} = \bar{y}\)</span> is the optimal parameter for constant model + MSE. It holds true regardless of what data sample you have, and it provides some formal reasoning as to why the mean is such a common summary statistic.</p>
<p>Our optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:</p>
<p><span class="math display">\[R(\hat{\theta_0}) = \min_{\theta_0} R(\theta_0)\]</span></p>
<p>To restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, <span class="math inline">\(\hat{\theta_0}\)</span>, is the value of <span class="math inline">\(\theta_0\)</span> that minimizes the cost <span class="math inline">\(R\)</span>.</p>
<p>For modeling purposes, we care less about the minimum value of cost, <span class="math inline">\(R(\hat{\theta_0})\)</span>, and more about the <em>value of <span class="math inline">\(\theta\)</span></em> that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:</p>
<p><span class="math display">\[\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)\]</span></p>
<p>That is, we want to find the <strong>arg</strong>ument <span class="math inline">\(\theta\)</span> that <strong>min</strong>imizes the cost function.</p>
</section>
<section id="comparing-two-different-models-both-fit-with-mse" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="comparing-two-different-models-both-fit-with-mse"><span class="header-section-number">11.1.2</span> Comparing Two Different Models, Both Fit with MSE</h3>
<p>Now that we’ve explored the constant model with an L2 loss, we can compare it to the SLR model that we learned last lecture. Consider the dataset below, which contains information about the ages and lengths of dugongs. Supposed we wanted to predict dugong ages:</p>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 38%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Constant Model</th>
<th>Simple Linear Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>model</td>
<td><span class="math inline">\(\hat{y} = \theta_0\)</span></td>
<td><span class="math inline">\(\hat{y} = \theta_0 + \theta_1 x\)</span></td>
</tr>
<tr class="even">
<td>data</td>
<td>sample of ages <span class="math inline">\(D = \{y_1, y_2, ..., y_n\}\)</span></td>
<td>sample of ages <span class="math inline">\(D = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\)</span></td>
</tr>
<tr class="odd">
<td>dimensions</td>
<td><span class="math inline">\(\hat{\theta_0}\)</span> is 1-D</td>
<td><span class="math inline">\(\hat{\theta} = [\hat{\theta_0}, \hat{\theta_1}]\)</span> is 2-D</td>
</tr>
<tr class="even">
<td>loss surface</td>
<td>2-D <img src="images/constant_loss_surface.png" class="img-fluid"></td>
<td>3-D <img src="images/slr_loss_surface.png" class="img-fluid"></td>
</tr>
<tr class="odd">
<td>loss model</td>
<td><span class="math inline">\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)</span></td>
<td><span class="math inline">\(\hat{R}(\theta_0, \theta_1) = \frac{1}{n}\sum^{n}_{i=1} (y_i - (\theta_0 + \theta_1 x))^2\)</span></td>
</tr>
<tr class="even">
<td>RMSE</td>
<td>7.72</td>
<td>4.31</td>
</tr>
<tr class="odd">
<td>predictions visualized</td>
<td>rug plot <img src="images/dugong_rug.png" class="img-fluid"></td>
<td>scatter plot <img src="images/dugong_scatter.png" class="img-fluid"></td>
</tr>
</tbody>
</table>
<p>(Notice how the points for our SLR scatter plot are visually not a great linear fit. We’ll come back to this).</p>
<p>The code for generating the graphs and models is included below, but we won’t go over it in too much depth.</p>
<div id="e8cb6992" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>dugongs <span class="op">=</span> pd.read_csv(<span class="st">"data/dugongs.csv"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>data_constant <span class="op">=</span> dugongs[<span class="st">"Age"</span>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>data_linear <span class="op">=</span> dugongs[[<span class="st">"Length"</span>, <span class="st">"Age"</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="81adf8ec" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Big font helper</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_fontsize(size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    SMALL_SIZE <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    MEDIUM_SIZE <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    BIGGER_SIZE <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> size <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        SMALL_SIZE <span class="op">=</span> MEDIUM_SIZE <span class="op">=</span> BIGGER_SIZE <span class="op">=</span> size</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"font"</span>, size<span class="op">=</span>SMALL_SIZE)  <span class="co"># controls default text sizes</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, titlesize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the axes title</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, labelsize<span class="op">=</span>MEDIUM_SIZE)  <span class="co"># fontsize of the x and y labels</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"xtick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"ytick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"legend"</span>, fontsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># legend fontsize</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"figure"</span>, titlesize<span class="op">=</span>BIGGER_SIZE)  <span class="co"># fontsize of the figure title</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="6109acc7" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constant Model + MSE</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>) <span class="co"># Revert style to default mpl</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_constant(theta, data):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.array([(y_obs <span class="op">-</span> theta) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> y_obs <span class="kw">in</span> data]), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">42</span>, <span class="dv">1000</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>l2_loss_thetas <span class="op">=</span> mse_constant(thetas, data_constant)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the loss surface</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, l2_loss_thetas)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$\theta_0$'</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'MSE'</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal point</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>thetahat <span class="op">=</span> np.mean(data_constant)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>plt.scatter([thetahat], [mse_constant(thetahat, data_constant)], s<span class="op">=</span><span class="dv">50</span>, label <span class="op">=</span> <span class="vs">r"$\hat{\theta}_0$"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="aa9e3f12" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SLR + MSE</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_linear(theta_0, theta_1, data_linear):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    data_x, data_y <span class="op">=</span> data_linear.iloc[:, <span class="dv">0</span>], data_linear.iloc[:, <span class="dv">1</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        np.array([(y <span class="op">-</span> (theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x)) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(data_x, data_y)]),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the loss surface</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>theta_0_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">80</span>, <span class="dv">20</span>, <span class="dv">80</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>theta_1_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">80</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>mse_values <span class="op">=</span> np.array(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    [[mse_linear(x, y, data_linear) <span class="cf">for</span> x <span class="kw">in</span> theta_0_values] <span class="cf">for</span> y <span class="kw">in</span> theta_1_values]</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal point</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>data_x, data_y <span class="op">=</span> data_linear.iloc[:, <span class="dv">0</span>], data_linear.iloc[:, <span class="dv">1</span>]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>theta_1_hat <span class="op">=</span> np.corrcoef(data_x, data_y)[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">*</span> np.std(data_y) <span class="op">/</span> np.std(data_x)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>theta_0_hat <span class="op">=</span> np.mean(data_y) <span class="op">-</span> theta_1_hat <span class="op">*</span> np.mean(data_x)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the 3D plot</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">"3d"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(theta_0_values, theta_1_values)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax.plot_surface(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    X, Y, mse_values, cmap<span class="op">=</span><span class="st">"viridis"</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Use alpha to make it slightly transparent</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter point using matplotlib</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>sc <span class="op">=</span> ax.scatter(</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    [theta_0_hat],</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    [theta_1_hat],</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    [mse_linear(theta_0_hat, theta_1_hat, data_linear)],</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"theta hat"</span>,</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a colorbar</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(surf, ax<span class="op">=</span>ax, shrink<span class="op">=</span><span class="fl">0.5</span>, aspect<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>cbar.set_label(<span class="st">"Cost Value"</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"MSE for different $</span><span class="ch">\\</span><span class="st">theta_0, </span><span class="ch">\\</span><span class="st">theta_1$"</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta_0$"</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta_1$"</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">"MSE"</span>)<span class="op">;</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="95f95ca1" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>yobs <span class="op">=</span> data_linear[<span class="st">"Age"</span>]  <span class="co"># The true observations y</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> data_linear[<span class="st">"Length"</span>]  <span class="co"># Needed for linear predictions</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(yobs)  <span class="co"># Predictions</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>yhats_constant <span class="op">=</span> [thetahat <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]  <span class="co"># Not used, but food for thought</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>yhats_linear <span class="op">=</span> [theta_0_hat <span class="op">+</span> theta_1_hat <span class="op">*</span> x <span class="cf">for</span> x <span class="kw">in</span> xs]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="a0adbb1e" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Constant Model Rug Plot</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In case we're in a weird style state</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>sns.set_theme()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">1.5</span>))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>sns.rugplot(yobs, height<span class="op">=</span><span class="fl">0.25</span>, lw<span class="op">=</span><span class="dv">2</span>) <span class="op">;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.axvline(thetahat, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">4</span>, label<span class="op">=</span><span class="vs">r"$\hat{\theta}_0$"</span>)<span class="op">;</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>plt.yticks([])<span class="op">;</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/nikhilreddy/course-notes/ds100env/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning:

use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-7-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ef920d42" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SLR model scatter plot </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># In case we're in a weird style state</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>sns.set_theme()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>xs, y<span class="op">=</span>yobs)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, yhats_linear, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig('dugong_line.png', bbox_inches = 'tight');</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Interpreting the RMSE (Root Mean Squared Error):</p>
<ul>
<li>Because the constant error is <strong>HIGHER</strong> than the linear error,</li>
<li>The constant model is <strong>WORSE</strong> than the linear model (at least for this metric).</li>
</ul>
</section>
</section>
<section id="constant-model-mae" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="constant-model-mae"><span class="header-section-number">11.2</span> Constant Model + MAE</h2>
<p>We see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?</p>
<p>This time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the <strong>Mean Absolute Error (MAE)</strong>.</p>
<ol type="1">
<li>Choose a model: constant model</li>
<li>Choose a loss function: L1 loss</li>
<li>Fit the model</li>
<li>Evaluate model performance</li>
</ol>
<section id="deriving-the-optimal-theta_0-1" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="deriving-the-optimal-theta_0-1"><span class="header-section-number">11.2.1</span> Deriving the optimal <span class="math inline">\(\theta_0\)</span></h3>
<p>Recall that the MAE is average <strong>absolute</strong> loss (L1 loss) over the data <span class="math inline">\(D = \{y_1, y_2, ..., y_n\}\)</span>.</p>
<p><span class="math display">\[\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \hat{y_i}| \]</span></p>
<p>Given the constant model <span class="math inline">\(\hat{y} = \theta_0\)</span>, we can write the MAE as:</p>
<p><span class="math display">\[\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \]</span></p>
<p>To fit the model, we find the optimal parameter value <span class="math inline">\(\hat{\theta_0}\)</span> that minimizes the MAE by differentiating using a calculus approach:</p>
<ol type="1">
<li>Differentiate with respect to <span class="math inline">\(\hat{\theta_0}\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{align}
\hat{R}(\theta_0) &amp;= \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| \\
\frac{d}{d\theta_0} R(\theta_0) &amp;= \frac{d}{d\theta_0} \left(\frac{1}{n} \sum^{n}_{i=1} |y_i - \theta_0| \right) \\
&amp;= \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0|
\end{align}
\]</span></p>
<ul>
<li>Here, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e.&nbsp;when <span class="math inline">\(y_i = \theta_0\)</span>). For now, we’ll ignore this issue. It turns out that disregarding this case doesn’t influence our final result.</li>
<li>To perform the derivative, consider two cases. When <span class="math inline">\(\theta_0\)</span> is <em>less than or equal to</em> <span class="math inline">\(y_i\)</span>, the term <span class="math inline">\(y_i - \theta_0\)</span> will be positive and the absolute value has no impact. When <span class="math inline">\(\theta_0\)</span> is <em>greater than</em> <span class="math inline">\(y_i\)</span>, the term <span class="math inline">\(y_i - \theta_0\)</span> will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying <span class="math inline">\(-(y_i - \theta_0) = \theta_0 - y_i\)</span>.</li>
</ul>
<p><span class="math display">\[|y_i - \theta_0| = \begin{cases} y_i - \theta_0 \quad \text{ if } \theta_0 \le y_i \\ \theta_0 - y_i \quad \text{if }\theta_0 &gt; y_i \end{cases}\]</span></p>
<ul>
<li>Taking derivatives:</li>
</ul>
<p><span class="math display">\[\frac{d}{d\theta_0} |y_i - \theta_0| = \begin{cases} \frac{d}{d\theta_0} (y_i - \theta_0) = -1 \quad \text{if }\theta_0 &lt; y_i \\ \frac{d}{d\theta_0} (\theta_0 - y_i) = 1 \quad \text{if }\theta_0 &gt; y_i \end{cases}\]</span></p>
<ul>
<li>This means that we obtain a different value for the derivative for data points where <span class="math inline">\(\theta_0 &lt; y_i\)</span> and where <span class="math inline">\(\theta_0 &gt; y_i\)</span>. We can summarize this by saying:</li>
</ul>
<p><span class="math display">\[
\frac{d}{d\theta_0} R(\theta_0) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0| \\
= \frac{1}{n} \left[\sum_{\theta_0 &lt; y_i} (-1) + \sum_{\theta_0 &gt; y_i} (+1) \right]
\]</span></p>
<ul>
<li>In other words, we take the sum of values for <span class="math inline">\(i = 1, 2, ..., n\)</span>:
<ul>
<li><span class="math inline">\(-1\)</span> if our observation <span class="math inline">\(y_i\)</span> is <em>greater than</em> our prediction <span class="math inline">\(\hat{\theta_0}\)</span></li>
<li><span class="math inline">\(+1\)</span> if our observation <span class="math inline">\(y_i\)</span> is <em>smaller than</em> our prediction <span class="math inline">\(\hat{\theta_0}\)</span></li>
</ul></li>
</ul>
<ol start="2" type="1">
<li><p>Set the derivative equation equal to 0: <span class="math display">\[ 0 = \frac{1}{n}\sum_{\hat{\theta_0} &lt; y_i} (-1) + \frac{1}{n}\sum_{\hat{\theta_0} &gt; y_i} (+1) \]</span></p></li>
<li><p>Solve for <span class="math inline">\(\hat{\theta_0}\)</span>: <span class="math display">\[ 0 = -\frac{1}{n}\sum_{\hat{\theta_0} &lt; y_i} (1) + \frac{1}{n}\sum_{\hat{\theta_0} &gt; y_i} (1)\]</span></p></li>
</ol>
<p><span class="math display">\[\sum_{\hat{\theta_0} &lt; y_i} (1) = \sum_{\hat{\theta_0} &gt; y_i} (1) \]</span></p>
<p>Thus, the constant model parameter <span class="math inline">\(\theta = \hat{\theta_0}\)</span> that minimizes MAE must satisfy:</p>
<p><span class="math display">\[ \sum_{\hat{\theta_0} &lt; y_i} (1) = \sum_{\hat{\theta_0} &gt; y_i} (1) \]</span></p>
<p>In other words, the number of observations greater than <span class="math inline">\(\theta_0\)</span> must be equal to the number of observations less than <span class="math inline">\(\theta_0\)</span>; there must be an equal number of points on the left and right sides of the equation. This is the definition of median, so our optimal value is <span class="math display">\[ \hat{\theta}_0 = median(y) \]</span></p>
</section>
</section>
<section id="summary-loss-optimization-calculus-and-critical-points" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="summary-loss-optimization-calculus-and-critical-points"><span class="header-section-number">11.3</span> Summary: Loss Optimization, Calculus, and Critical Points</h2>
<p>First, define the <strong>objective function</strong> as average loss.</p>
<ul>
<li>Plug in L1 or L2 loss.</li>
<li>Plug in the model so that the resulting expression is a function of <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>Then, find the minimum of the objective function:</p>
<ol type="1">
<li>Differentiate with respect to <span class="math inline">\(\theta\)</span>.</li>
<li>Set equal to 0.</li>
<li>Solve for <span class="math inline">\(\hat{\theta}\)</span>.</li>
<li>(If we have multiple parameters) repeat steps 1-3 with partial derivatives.</li>
</ol>
<p>Recall critical points from calculus: <span class="math inline">\(R(\hat{\theta})\)</span> could be a minimum, maximum, or saddle point!</p>
<ul>
<li>We should technically also perform the second derivative test, i.e., show <span class="math inline">\(R''(\hat{\theta}) &gt; 0\)</span>.</li>
<li>MSE has a property—<strong>convexity</strong>—that guarantees that <span class="math inline">\(R(\hat{\theta})\)</span> is a global minimum.</li>
<li>The proof of convexity for MAE is beyond this course.</li>
</ul>
</section>
<section id="comparing-loss-functions" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="comparing-loss-functions"><span class="header-section-number">11.4</span> Comparing Loss Functions</h2>
<p>We’ve now tried our hand at fitting a model under both MSE and MAE cost functions. How do the two results compare?</p>
<p>Let’s consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We’ll fit a constant model to predict the number of drinks that will be sold tomorrow.</p>
<div id="0f97ab5d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>drinks <span class="op">=</span> np.array([<span class="dv">20</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">29</span>, <span class="dv">33</span>])</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>drinks</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([20, 21, 22, 29, 33])</code></pre>
</div>
</div>
<p>From our derivations above, we know that the optimal model parameter under MSE cost is the mean of the dataset. Under MAE cost, the optimal parameter is the median of the dataset.</p>
<div id="c3b19e2c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.mean(drinks), np.median(drinks)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>(np.float64(25.0), np.float64(22.0))</code></pre>
</div>
</div>
<p>If we plot each empirical risk function across several possible values of <span class="math inline">\(\theta\)</span>, we find that each <span class="math inline">\(\hat{\theta}\)</span> does indeed correspond to the lowest value of error:</p>
<p><img src="images/error.png" alt="error" width="600"></p>
<p>Notice that the MSE above is a <strong>smooth</strong> function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE, in contrast, is not differentiable at each of its “kinks.” We’ll explore how the smoothness of the cost function can impact our ability to apply numerical optimization in a few weeks.</p>
<p>How do outliers affect each cost function? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected.</p>
<div id="8f777a75" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>drinks_with_outlier <span class="op">=</span> np.append(drinks, <span class="dv">1033</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>display(drinks_with_outlier)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>np.mean(drinks_with_outlier), np.median(drinks_with_outlier)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>array([  20,   21,   22,   29,   33, 1033])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(np.float64(193.0), np.float64(25.5))</code></pre>
</div>
</div>
<p><img src="images/outliers.png" alt="outliers" width="700"></p>
<p>This means that under the MSE, the optimal model parameter <span class="math inline">\(\hat{\theta}\)</span> is strongly affected by the presence of outliers. Under the MAE, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is <strong>sensitive</strong> to outliers, while the MAE is <strong>robust</strong> to outliers.</p>
<p>Let’s try another experiment. This time, we’ll add an additional, non-outlying datapoint to the data.</p>
<div id="fa3ad234" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>drinks_with_additional_observation <span class="op">=</span> np.append(drinks, <span class="dv">35</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>drinks_with_additional_observation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([20, 21, 22, 29, 33, 35])</code></pre>
</div>
</div>
<p>When we again visualize the cost functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are <em>infinitely</em> many optimal values for the model parameter: any value <span class="math inline">\(\hat{\theta} \in [22, 29]\)</span> will minimize the MAE. In contrast, the MSE still has a single best value for <span class="math inline">\(\hat{\theta}\)</span>. In other words, the MSE has a <strong>unique</strong> solution for <span class="math inline">\(\hat{\theta}\)</span>; the MAE is not guaranteed to have a single unique solution.</p>
<p><img src="images/mse_loss_26.png" width="350"> <img src="images/mae_loss_infinite.png" width="350"></p>
<p>To summarize our example,</p>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 38%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>MSE (Mean Squared Loss)</th>
<th>MAE (Mean Absolute Loss)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Loss Function</td>
<td><span class="math inline">\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2\)</span></td>
<td><span class="math inline">\(\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0|\)</span></td>
</tr>
<tr class="even">
<td>Optimal <span class="math inline">\(\hat{\theta_0}\)</span></td>
<td><span class="math inline">\(\hat{\theta_0} = mean(y) = \bar{y}\)</span></td>
<td><span class="math inline">\(\hat{\theta_0} = median(y)\)</span></td>
</tr>
<tr class="odd">
<td>Loss Surface</td>
<td><img src="images/mse_loss_26.png" width="250"></td>
<td><img src="images/mae_loss_infinite.png" width="250"></td>
</tr>
<tr class="even">
<td>Shape</td>
<td><strong>Smooth</strong> - easy to minimize using numerical methods (in a few weeks)</td>
<td><strong>Piecewise</strong> - at each of the “kinks,” it’s not differentiable. Harder to minimize.</td>
</tr>
<tr class="odd">
<td>Outliers</td>
<td><strong>Sensitive</strong> to outliers (since they change mean substantially). Sensitivity also depends on the dataset size.</td>
<td><strong>More robust</strong> to outliers.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\hat{\theta_0}\)</span> Uniqueness</td>
<td><strong>Unique</strong> <span class="math inline">\(\hat{\theta_0}\)</span></td>
<td><strong>Infinitely many</strong> <span class="math inline">\(\hat{\theta_0}\)</span>s</td>
</tr>
</tbody>
</table>
</section>
<section id="transformations-to-fit-linear-models" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="transformations-to-fit-linear-models"><span class="header-section-number">11.5</span> Transformations to Fit Linear Models</h2>
<p>At this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.</p>
<p>A key word above is <em>linear</em>. When we computed parameter estimates earlier, we assumed that <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> shared a roughly linear relationship. Data in the real world isn’t always so straightforward, but we can transform the data to try and obtain linearity.</p>
<p>The <strong>Tukey-Mosteller Bulge Diagram</strong> is a useful tool for summarizing what transformations can linearize the relationship between two variables. To determine what transformations might be appropriate, trace the shape of the “bulge” made by your data. Find the quadrant of the diagram that matches this bulge. The transformations shown on the vertical and horizontal axes of this quadrant can help improve the fit between the variables.</p>
<p><img src="images/bulge.png" alt="bulge" width="600"></p>
<p>Note that:</p>
<ul>
<li>There are multiple solutions. Some will fit better than others.</li>
<li>sqrt and log make a value “smaller.”</li>
<li>Raising to a power makes a value “bigger.”</li>
<li>Each of these transformations equates to increasing or decreasing the scale of an axis.</li>
</ul>
<p>Other goals in addition to linearity are possible, for example, making data appear more symmetric. Linearity allows us to fit lines to the transformed data.</p>
<p>Let’s revisit our dugongs example. The lengths and ages are plotted below:</p>
<div id="7b3b9217" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># `corrcoef` computes the correlation coefficient between two variables</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># `std` finds the standard deviation</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> dugongs[<span class="st">"Length"</span>]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dugongs[<span class="st">"Age"</span>]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> r <span class="op">*</span> np.std(y) <span class="op">/</span> np.std(x)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> np.mean(y) <span class="op">-</span> theta_1 <span class="op">*</span> np.mean(x)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, dpi<span class="op">=</span><span class="dv">200</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(x, y)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Age"</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x, y)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x, <span class="st">"tab:red"</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Age"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at the plot on the left, we see that there is a slight curvature to the data points. Plotting the SLR curve on the right results in a poor fit.</p>
<p>For SLR to perform well, we’d like there to be a rough linear trend relating <code>"Age"</code> and <code>"Length"</code>. What is making the raw data deviate from a linear relationship? Notice that the data points with <code>"Length"</code> greater than 2.6 have disproportionately high values of <code>"Age"</code> relative to the rest of the data. If we could manipulate these data points to have lower <code>"Age"</code> values, we’d “shift” these points downwards and reduce the curvature in the data. Applying a logarithmic transformation to <span class="math inline">\(y_i\)</span> (that is, taking <span class="math inline">\(\log(\)</span> <code>"Age"</code> <span class="math inline">\()\)</span> ) would achieve just that.</p>
<p>An important word on <span class="math inline">\(\log\)</span>: in Data 100 (and most upper-division STEM courses), <span class="math inline">\(\log\)</span> denotes the natural logarithm with base <span class="math inline">\(e\)</span>. The base-10 logarithm, where relevant, is indicated by <span class="math inline">\(\log_{10}\)</span>.</p>
<div id="610647c3" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.log(y)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.corrcoef(x, z)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> r <span class="op">*</span> np.std(z) <span class="op">/</span> np.std(x)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> np.mean(z) <span class="op">-</span> theta_1 <span class="op">*</span> np.mean(x)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, dpi<span class="op">=</span><span class="dv">200</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(x, z)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r"$\log{(Age)}$"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x, z)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x, <span class="st">"tab:red"</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r"$\log{(Age)}$"</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Our SLR fit looks a lot better! We now have a new target variable: the SLR model is now trying to predict the <em>log</em> of <code>"Age"</code>, rather than the untransformed <code>"Age"</code>. In other words, we are applying the transformation <span class="math inline">\(z_i = \log{(y_i)}\)</span>. Notice that the resulting model is still <strong>linear in the parameters</strong> <span class="math inline">\(\theta = [\theta_0, \theta_1]\)</span>. The SLR model becomes:</p>
<p><span class="math display">\[\hat{\log{y}} = \theta_0 + \theta_1 x\]</span> <span class="math display">\[\hat{z} = \theta_0 + \theta_1 x\]</span></p>
<p>It turns out that this linearized relationship can help us understand the underlying relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. If we rearrange the relationship above, we find:</p>
<p><span class="math display">\[\log{(y)} = \theta_0 + \theta_1 x\]</span> <span class="math display">\[y = e^{\theta_0 + \theta_1 x}\]</span> <span class="math display">\[y = (e^{\theta_0})e^{\theta_1 x}\]</span> <span class="math display">\[y_i = C e^{k x}\]</span></p>
<p>For some constants <span class="math inline">\(C\)</span> and <span class="math inline">\(k\)</span>.</p>
<p><span class="math inline">\(y\)</span> is an <em>exponential</em> function of <span class="math inline">\(x\)</span>. Applying an exponential fit to the untransformed variables corroborates this finding.</p>
<div id="a5bc3bd2" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>plt.figure(dpi<span class="op">=</span><span class="dv">120</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.exp(theta_0) <span class="op">*</span> np.exp(theta_1 <span class="op">*</span> x), <span class="st">"tab:red"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Length"</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Age"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_transformations_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>You may wonder: why did we choose to apply a log transformation specifically? Why not some other function to linearize the data?</p>
<p>Practically, many other mathematical operations that modify the relative scales of <code>"Age"</code> and <code>"Length"</code> could have worked here.</p>
</section>
<section id="multiple-linear-regression" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">11.6</span> Multiple Linear Regression</h2>
<p>Multiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:</p>
<p><span class="math display">\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]</span></p>
<p>Our predicted value of <span class="math inline">\(y\)</span>, <span class="math inline">\(\hat{y}\)</span>, is a linear combination of the single <strong>observations</strong> (features), <span class="math inline">\(x_i\)</span>, and the parameters, <span class="math inline">\(\theta_i\)</span>.</p>
<p>We’ll dive deeper into Multiple Linear Regression in the next lecture.</p>
</section>
<section id="bonus-calculating-constant-model-mse-using-an-algebraic-trick" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="bonus-calculating-constant-model-mse-using-an-algebraic-trick"><span class="header-section-number">11.7</span> Bonus: Calculating Constant Model MSE Using an Algebraic Trick</h2>
<p>Earlier, we calculated the constant model MSE using calculus. It turns out that there is a much more elegant way of performing this same minimization algebraically, without using calculus at all.</p>
<p>In this calculation, we use the fact that the <strong>sum of deviations from the mean is <span class="math inline">\(0\)</span></strong> or that <span class="math inline">\(\sum_{i=1}^{n} (y_i - \bar{y}) = 0\)</span>.</p>
<p>Let’s quickly walk through the proof for this: <span class="math display">\[
\begin{align}
\sum_{i=1}^{n} (y_i - \bar{y}) &amp;= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \bar{y} \\
&amp;= \sum_{i=1}^{n} y_i - n\bar{y} \\
&amp;= \sum_{i=1}^{n} y_i - n\frac{1}{n}\sum_{i=1}^{n}y_i \\
&amp;= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n}y_i \\
&amp; = 0
\end{align}
\]</span></p>
<p>In our calculations, we’ll also be using the definition of the variance as a sample. As a refresher:</p>
<p><span class="math display">\[\sigma_y^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})^2\]</span></p>
<p>Getting into our calculation for MSE minimization:</p>
<p><span class="math display">\[
\begin{align}
R(\theta) &amp;= {\frac{1}{n}}\sum^{n}_{i=1} (y_i - \theta)^2
\\ &amp;= \frac{1}{n}\sum^{n}_{i=1} [(y_i - \bar{y}) + (\bar{y} - \theta)]^2\quad \quad \text{using trick that a-b can be written as (a-c) + (c-b) } \\
&amp;\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \space \space \text{where a, b, and c are any numbers}
\\ &amp;= \frac{1}{n}\sum^{n}_{i=1} [(y_i - \bar{y})^2 + 2(y_i - \bar{y})(\bar{y} - \theta) + (\bar{y} - \theta)^2]
\\ &amp;= \frac{1}{n}[\sum^{n}_{i=1}(y_i - \bar{y})^2 + 2(\bar{y} - \theta)\sum^{n}_{i=1}(y_i - \bar{y}) + n(\bar{y} - \theta)^2] \quad \quad  \text{distribute sum to individual terms}
\\ &amp;= \frac{1}{n}\sum^{n}_{i=1}(y_i - \bar{y})^2 + \frac{2}{n}(\bar{y} - \theta)\cdot0 + (\bar{y} - \theta)^2 \quad \quad  \text{sum of deviations from mean is 0}
\\ &amp;= \sigma_y^2 + (\bar{y} - \theta)^2
\end{align}
\]</span></p>
<p>Since variance can’t be negative, we know that our first term, <span class="math inline">\(\sigma_y^2\)</span> is greater than or equal to <span class="math inline">\(0\)</span>. Also note, that <strong>the first term doesn’t involve <span class="math inline">\(\theta\)</span> at all</strong>, meaning changing our model won’t change this value. For the purposes of determining $#, we can then essentially ignore this term.</p>
<p>Looking at the second term, <span class="math inline">\((\bar{y} - \theta)^2\)</span>, since it is squared, we know it must be greater than or equal to <span class="math inline">\(0\)</span>. As this term does involve <span class="math inline">\(\theta\)</span>, picking the value of <span class="math inline">\(\theta\)</span> that minimizes this term will allow us to minimize our average loss. For the second term to equal <span class="math inline">\(0\)</span>, <span class="math inline">\(\theta = \bar{y}\)</span>, or in other words, <span class="math inline">\(\hat{\theta} = \bar{y} = mean(y)\)</span>.</p>
<section id="note" class="level5" data-number="11.7.0.0.1">
<h5 data-number="11.7.0.0.1" class="anchored" data-anchor-id="note"><span class="header-section-number">11.7.0.0.1</span> Note</h5>
<p>In the derivation above, we decompose the expected loss, <span class="math inline">\(R(\theta)\)</span>, into two key components: the variance of the data, <span class="math inline">\(\sigma_y^2\)</span>, and the square of the bias, <span class="math inline">\((\bar{y} - \theta)^2\)</span>. This decomposition is insightful for understanding the behavior of estimators in statistical models.</p>
<ul>
<li><p><strong>Variance, <span class="math inline">\(\sigma_y^2\)</span></strong>: This term represents the spread of the data points around their mean, <span class="math inline">\(\bar{y}\)</span>, and is a measure of the data’s inherent variability. Importantly, it does not depend on the choice of <span class="math inline">\(\theta\)</span>, meaning it’s a fixed property of the data. Variance serves as an indicator of the data’s dispersion and is crucial in understanding the dataset’s structure, but it remains constant regardless of how we adjust our model parameter <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><strong>Bias Squared, <span class="math inline">\((\bar{y} - \theta)^2\)</span></strong>: This term captures the bias of the estimator, defined as the square of the difference between the mean of the data points, <span class="math inline">\(\bar{y}\)</span>, and the parameter <span class="math inline">\(\theta\)</span>. The bias quantifies the systematic error introduced when estimating <span class="math inline">\(\theta\)</span>. Minimizing this term is essential for improving the accuracy of the estimator. When <span class="math inline">\(\theta = \bar{y}\)</span>, the bias is <span class="math inline">\(0\)</span>, indicating that the estimator is unbiased for the parameter it estimates. This highlights a critical principle in statistical estimation: choosing <span class="math inline">\(\theta\)</span> to be the sample mean, <span class="math inline">\(\bar{y}\)</span>, minimizes the average loss, rendering the estimator both efficient and unbiased for the population mean.</p></li>
</ul>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../intro_to_modeling/intro_to_modeling.html" class="pagination-link  aria-label=" &lt;span="" to="" modeling&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../ols/ols.html" class="pagination-link" aria-label="<span class='chapter-number'>12</span>&nbsp; <span class='chapter-title'>Ordinary Least Squares</span>">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb21" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> 'Constant Model, Loss, and Transformations'</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: 'Constant Model, Loss, and Transformations'</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.16.1</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: ds100env</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derive the optimal model parameters for the constant model under MSE and MAE cost functions.</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluate the differences between MSE and MAE risk.</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand the need for linearization of variables and apply the Tukey-Mosteller bulge diagram for transformations.</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>Last time, we introduced the modeling process. We set up a framework to predict target variables as functions of our features, following a set workflow:</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Choose a model - how should we represent the world?</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Choose a loss function - how do we quantify prediction error?</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fit the model - how do we choose the best parameter of our model given our data?</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Evaluate model performance - how do we evaluate whether this process gave rise to a good model?</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>To illustrate this process, we derived the optimal model parameters under simple linear regression (SLR) with mean squared error (MSE) as the cost function. A summary of the SLR modeling process is shown below:</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>&lt;div align="middle"&gt;</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>&lt;img src="images/slr_modeling.png" alt='modeling' width='600'&gt;</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>&lt;/div&gt;</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>At the end of last lecture, we dived deeper into step 4 - evaluating model performance - using SLR as an example. In this lecture, we'll also explore the modeling process with new models, continue familiarizing ourselves with the modeling process by finding the best model parameters under a new model, the constant model, and test out two different loss functions to understand how our choice of loss influences model design. Later on, we'll consider what happens when a linear model isn't the best choice to capture trends in our data and what solutions there are to create better models.</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>Before we get into the modeling process, let's quickly review some important terminology.</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prediction vs. Estimation</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>The terms prediction and estimation are often used somewhat interchangeably, but there is a subtle difference between them. **Estimation** is the task of using data to calculate model parameters. **Prediction** is the task of using a model to predict outputs for unseen data. In our simple linear regression model,</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \hat{\theta_0} + \hat{\theta_1}x$$</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>we **estimate** the parameters by minimizing average loss; then, we **predict** using these estimations. **Least Squares Estimation** is when we choose the parameters that minimize MSE.</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a><span class="fu">## Constant Model + MSE</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>Now, we'll shift from the SLR model to the **constant model**, also known as a summary statistic. The constant model is slightly different from the simple linear regression model we've explored previously. Rather than generating predictions from an inputted feature variable, the constant model always *predicts the same constant number*. This ignores any relationships between variables. For example, let's say we want to predict the number of drinks a boba shop sells in a day. Boba tea sales likely depend on the time of year, the weather, how the customers feel, whether school is in session, etc., but the constant model ignores these factors in favor of a simpler model. In other words, the constant model employs a **simplifying assumption**.</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>It is also a parametric, statistical model:</span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0$$</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>$\theta_0$ is the parameter of the constant model, just as $\theta_0$ and $\theta_1$ were the parameters in SLR.</span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>Since our parameter $\theta_0$ is 1-dimensional ($\theta_0 \in \mathbb{R}$), we now have no input to our model and will always predict $\hat{y} = \theta_0$.</span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deriving the optimal $\theta_0$</span></span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>Our task now is to determine what value of $\theta_0$ best represents the optimal model – in other words, what number should we guess each time to have the lowest possible **average loss** on our data?</span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a>Like before, we'll use Mean Squared Error (MSE). Recall that the MSE is average squared loss (L2 loss) over the data $D = <span class="sc">\{</span>y_1, y_2, ..., y_n<span class="sc">\}</span>$.</span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \hat{y_i})^2 $$</span>
<span id="cb21-82"><a href="#cb21-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-83"><a href="#cb21-83" aria-hidden="true" tabindex="-1"></a>Our modeling process now looks like this:</span>
<span id="cb21-84"><a href="#cb21-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-85"><a href="#cb21-85" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Choose a model: constant model</span>
<span id="cb21-86"><a href="#cb21-86" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Choose a loss function: L2 loss</span>
<span id="cb21-87"><a href="#cb21-87" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fit the model</span>
<span id="cb21-88"><a href="#cb21-88" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Evaluate model performance</span>
<span id="cb21-89"><a href="#cb21-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-90"><a href="#cb21-90" aria-hidden="true" tabindex="-1"></a>Given the **constant model** $\hat{y} = \theta_0$, we can rewrite the MSE equation as</span>
<span id="cb21-91"><a href="#cb21-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-92"><a href="#cb21-92" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2 $$</span>
<span id="cb21-93"><a href="#cb21-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-94"><a href="#cb21-94" aria-hidden="true" tabindex="-1"></a>We can **fit the model** by finding the optimal $\hat{\theta_0}$ that minimizes the MSE using a calculus approach.</span>
<span id="cb21-95"><a href="#cb21-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-96"><a href="#cb21-96" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Differentiate with respect to $\theta_0$:</span>
<span id="cb21-97"><a href="#cb21-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-98"><a href="#cb21-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-99"><a href="#cb21-99" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb21-100"><a href="#cb21-100" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\theta_0}\text{R}(\theta) &amp; = \frac{d}{d\theta_0}(\frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2)</span>
<span id="cb21-101"><a href="#cb21-101" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}\sum^{n}_{i=1} \frac{d}{d\theta_0}  (y_i - \theta_0)^2 \quad \quad \text{a derivative of sum is a sum of derivatives}</span>
<span id="cb21-102"><a href="#cb21-102" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}\sum^{n}_{i=1} 2 (y_i - \theta_0) (-1) \quad \quad \text{chain rule}</span>
<span id="cb21-103"><a href="#cb21-103" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \theta_0) \quad \quad \text{simplify constants}</span>
<span id="cb21-104"><a href="#cb21-104" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb21-105"><a href="#cb21-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-106"><a href="#cb21-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-107"><a href="#cb21-107" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set the derivative equation equal to 0:</span>
<span id="cb21-108"><a href="#cb21-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-109"><a href="#cb21-109" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb21-110"><a href="#cb21-110" aria-hidden="true" tabindex="-1"></a>   0 = {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})</span>
<span id="cb21-111"><a href="#cb21-111" aria-hidden="true" tabindex="-1"></a>   $$</span>
<span id="cb21-112"><a href="#cb21-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-113"><a href="#cb21-113" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Solve for $\hat{\theta_0}$</span>
<span id="cb21-114"><a href="#cb21-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-115"><a href="#cb21-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-116"><a href="#cb21-116" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb21-117"><a href="#cb21-117" aria-hidden="true" tabindex="-1"></a>0 &amp;= {\frac{-2}{n}}\sum^{n}_{i=1} (y_i - \hat{\theta_0})</span>
<span id="cb21-118"><a href="#cb21-118" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \sum^{n}_{i=1} (y_i - \hat{\theta_0}) \quad \quad \text{divide both sides by} \frac{-2}{n}</span>
<span id="cb21-119"><a href="#cb21-119" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \left(\sum^{n}_{i=1} y_i\right) - \left(\sum^{n}_{i=1} \hat{\theta_0}\right) \quad \quad \text{separate sums}</span>
<span id="cb21-120"><a href="#cb21-120" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \left(\sum^{n}_{i=1} y_i\right) - (n \cdot \hat{\theta_0}) \quad \quad  \text{c + c + … + c = nc}</span>
<span id="cb21-121"><a href="#cb21-121" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> n \cdot \hat{\theta_0} &amp;= \sum^{n}_{i=1} y_i</span>
<span id="cb21-122"><a href="#cb21-122" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \hat{\theta_0} &amp;= \frac{1}{n} \sum^{n}_{i=1} y_i</span>
<span id="cb21-123"><a href="#cb21-123" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> \hat{\theta_0} &amp;= \bar{y}</span>
<span id="cb21-124"><a href="#cb21-124" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb21-125"><a href="#cb21-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-126"><a href="#cb21-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-127"><a href="#cb21-127" aria-hidden="true" tabindex="-1"></a>Let's take a moment to interpret this result. $\hat{\theta_0} = \bar{y}$ is the optimal parameter for constant model + MSE.</span>
<span id="cb21-128"><a href="#cb21-128" aria-hidden="true" tabindex="-1"></a>It holds true regardless of what data sample you have, and it provides some formal reasoning as to why the mean is such a common summary statistic.</span>
<span id="cb21-129"><a href="#cb21-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-130"><a href="#cb21-130" aria-hidden="true" tabindex="-1"></a>Our optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:</span>
<span id="cb21-131"><a href="#cb21-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-132"><a href="#cb21-132" aria-hidden="true" tabindex="-1"></a>$$R(\hat{\theta_0}) = \min_{\theta_0} R(\theta_0)$$</span>
<span id="cb21-133"><a href="#cb21-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-134"><a href="#cb21-134" aria-hidden="true" tabindex="-1"></a>To restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, $\hat{\theta_0}$, is the value of $\theta_0$ that minimizes the cost $R$.</span>
<span id="cb21-135"><a href="#cb21-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-136"><a href="#cb21-136" aria-hidden="true" tabindex="-1"></a>For modeling purposes, we care less about the minimum value of cost, $R(\hat{\theta_0})$, and more about the *value of $\theta$* that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:</span>
<span id="cb21-137"><a href="#cb21-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-138"><a href="#cb21-138" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta} = \underset{\theta}{\operatorname{\arg\min}}\:R(\theta)$$</span>
<span id="cb21-139"><a href="#cb21-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-140"><a href="#cb21-140" aria-hidden="true" tabindex="-1"></a>That is, we want to find the **arg**ument $\theta$ that **min**imizes the cost function.</span>
<span id="cb21-141"><a href="#cb21-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-142"><a href="#cb21-142" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing Two Different Models, Both Fit with MSE</span></span>
<span id="cb21-143"><a href="#cb21-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-144"><a href="#cb21-144" aria-hidden="true" tabindex="-1"></a>Now that we've explored the constant model with an L2 loss, we can compare it to the SLR model that we learned last lecture. Consider the dataset below, which contains information about the ages and lengths of dugongs. Supposed we wanted to predict dugong ages:</span>
<span id="cb21-145"><a href="#cb21-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-146"><a href="#cb21-146" aria-hidden="true" tabindex="-1"></a>|                        | Constant Model                                                   | Simple Linear Regression                                                        |</span>
<span id="cb21-147"><a href="#cb21-147" aria-hidden="true" tabindex="-1"></a>| ---------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------- |</span>
<span id="cb21-148"><a href="#cb21-148" aria-hidden="true" tabindex="-1"></a>| model                  | $\hat{y} = \theta_0$                                             | $\hat{y} = \theta_0 + \theta_1 x$                                                |</span>
<span id="cb21-149"><a href="#cb21-149" aria-hidden="true" tabindex="-1"></a>| data                   | sample of ages $D = <span class="sc">\{</span>y_1, y_2, ..., y_n<span class="sc">\}</span>$                      | sample of ages $D = <span class="sc">\{</span>(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)<span class="sc">\}</span>$                |</span>
<span id="cb21-150"><a href="#cb21-150" aria-hidden="true" tabindex="-1"></a>| dimensions             | $\hat{\theta_0}$ is 1-D                                          | $\hat{\theta} = <span class="co">[</span><span class="ot">\hat{\theta_0}, \hat{\theta_1}</span><span class="co">]</span>$ is 2-D                        |</span>
<span id="cb21-151"><a href="#cb21-151" aria-hidden="true" tabindex="-1"></a>| loss surface           | 2-D <span class="al">![](images/constant_loss_surface.png)</span>                        | 3-D <span class="al">![](images/slr_loss_surface.png)</span>                                            |</span>
<span id="cb21-152"><a href="#cb21-152" aria-hidden="true" tabindex="-1"></a>| loss model             | $\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2$ | $\hat{R}(\theta_0, \theta_1) = \frac{1}{n}\sum^{n}_{i=1} (y_i - (\theta_0 + \theta_1 x))^2$ |</span>
<span id="cb21-153"><a href="#cb21-153" aria-hidden="true" tabindex="-1"></a>| RMSE                   | 7.72                                                             | 4.31                                                                            |</span>
<span id="cb21-154"><a href="#cb21-154" aria-hidden="true" tabindex="-1"></a>| predictions visualized | rug plot <span class="al">![](images/dugong_rug.png)</span>                              | scatter plot <span class="al">![](images/dugong_scatter.png)</span>                                     |</span>
<span id="cb21-155"><a href="#cb21-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-156"><a href="#cb21-156" aria-hidden="true" tabindex="-1"></a>(Notice how the points for our SLR scatter plot are visually not a great linear fit. We'll come back to this).</span>
<span id="cb21-157"><a href="#cb21-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-158"><a href="#cb21-158" aria-hidden="true" tabindex="-1"></a>The code for generating the graphs and models is included below, but we won't go over it in too much depth.</span>
<span id="cb21-159"><a href="#cb21-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-162"><a href="#cb21-162" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-163"><a href="#cb21-163" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-164"><a href="#cb21-164" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb21-165"><a href="#cb21-165" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-166"><a href="#cb21-166" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-167"><a href="#cb21-167" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb21-168"><a href="#cb21-168" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb21-169"><a href="#cb21-169" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb21-170"><a href="#cb21-170" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb21-171"><a href="#cb21-171" aria-hidden="true" tabindex="-1"></a>dugongs <span class="op">=</span> pd.read_csv(<span class="st">"data/dugongs.csv"</span>)</span>
<span id="cb21-172"><a href="#cb21-172" aria-hidden="true" tabindex="-1"></a>data_constant <span class="op">=</span> dugongs[<span class="st">"Age"</span>]</span>
<span id="cb21-173"><a href="#cb21-173" aria-hidden="true" tabindex="-1"></a>data_linear <span class="op">=</span> dugongs[[<span class="st">"Length"</span>, <span class="st">"Age"</span>]]</span>
<span id="cb21-174"><a href="#cb21-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-175"><a href="#cb21-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-178"><a href="#cb21-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-179"><a href="#cb21-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-180"><a href="#cb21-180" aria-hidden="true" tabindex="-1"></a><span class="co"># Big font helper</span></span>
<span id="cb21-181"><a href="#cb21-181" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_fontsize(size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-182"><a href="#cb21-182" aria-hidden="true" tabindex="-1"></a>    SMALL_SIZE <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb21-183"><a href="#cb21-183" aria-hidden="true" tabindex="-1"></a>    MEDIUM_SIZE <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-184"><a href="#cb21-184" aria-hidden="true" tabindex="-1"></a>    BIGGER_SIZE <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb21-185"><a href="#cb21-185" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> size <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb21-186"><a href="#cb21-186" aria-hidden="true" tabindex="-1"></a>        SMALL_SIZE <span class="op">=</span> MEDIUM_SIZE <span class="op">=</span> BIGGER_SIZE <span class="op">=</span> size</span>
<span id="cb21-187"><a href="#cb21-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-188"><a href="#cb21-188" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"font"</span>, size<span class="op">=</span>SMALL_SIZE)  <span class="co"># controls default text sizes</span></span>
<span id="cb21-189"><a href="#cb21-189" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, titlesize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the axes title</span></span>
<span id="cb21-190"><a href="#cb21-190" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, labelsize<span class="op">=</span>MEDIUM_SIZE)  <span class="co"># fontsize of the x and y labels</span></span>
<span id="cb21-191"><a href="#cb21-191" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"xtick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb21-192"><a href="#cb21-192" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"ytick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb21-193"><a href="#cb21-193" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"legend"</span>, fontsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># legend fontsize</span></span>
<span id="cb21-194"><a href="#cb21-194" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"figure"</span>, titlesize<span class="op">=</span>BIGGER_SIZE)  <span class="co"># fontsize of the figure title</span></span>
<span id="cb21-195"><a href="#cb21-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-196"><a href="#cb21-196" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span>
<span id="cb21-197"><a href="#cb21-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-198"><a href="#cb21-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-201"><a href="#cb21-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-202"><a href="#cb21-202" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-203"><a href="#cb21-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Constant Model + MSE</span></span>
<span id="cb21-204"><a href="#cb21-204" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>) <span class="co"># Revert style to default mpl</span></span>
<span id="cb21-205"><a href="#cb21-205" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb21-206"><a href="#cb21-206" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb21-207"><a href="#cb21-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-208"><a href="#cb21-208" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_constant(theta, data):</span>
<span id="cb21-209"><a href="#cb21-209" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.array([(y_obs <span class="op">-</span> theta) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> y_obs <span class="kw">in</span> data]), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-210"><a href="#cb21-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-211"><a href="#cb21-211" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">20</span>, <span class="dv">42</span>, <span class="dv">1000</span>)</span>
<span id="cb21-212"><a href="#cb21-212" aria-hidden="true" tabindex="-1"></a>l2_loss_thetas <span class="op">=</span> mse_constant(thetas, data_constant)</span>
<span id="cb21-213"><a href="#cb21-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-214"><a href="#cb21-214" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the loss surface</span></span>
<span id="cb21-215"><a href="#cb21-215" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, l2_loss_thetas)</span>
<span id="cb21-216"><a href="#cb21-216" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$\theta_0$'</span>)</span>
<span id="cb21-217"><a href="#cb21-217" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'MSE'</span>)</span>
<span id="cb21-218"><a href="#cb21-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-219"><a href="#cb21-219" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal point</span></span>
<span id="cb21-220"><a href="#cb21-220" aria-hidden="true" tabindex="-1"></a>thetahat <span class="op">=</span> np.mean(data_constant)</span>
<span id="cb21-221"><a href="#cb21-221" aria-hidden="true" tabindex="-1"></a>plt.scatter([thetahat], [mse_constant(thetahat, data_constant)], s<span class="op">=</span><span class="dv">50</span>, label <span class="op">=</span> <span class="vs">r"$\hat{\theta}_0$"</span>)</span>
<span id="cb21-222"><a href="#cb21-222" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span>
<span id="cb21-223"><a href="#cb21-223" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span>
<span id="cb21-224"><a href="#cb21-224" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-225"><a href="#cb21-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-228"><a href="#cb21-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-229"><a href="#cb21-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-230"><a href="#cb21-230" aria-hidden="true" tabindex="-1"></a><span class="co"># SLR + MSE</span></span>
<span id="cb21-231"><a href="#cb21-231" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_linear(theta_0, theta_1, data_linear):</span>
<span id="cb21-232"><a href="#cb21-232" aria-hidden="true" tabindex="-1"></a>    data_x, data_y <span class="op">=</span> data_linear.iloc[:, <span class="dv">0</span>], data_linear.iloc[:, <span class="dv">1</span>]</span>
<span id="cb21-233"><a href="#cb21-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(</span>
<span id="cb21-234"><a href="#cb21-234" aria-hidden="true" tabindex="-1"></a>        np.array([(y <span class="op">-</span> (theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x)) <span class="op">**</span> <span class="dv">2</span> <span class="cf">for</span> x, y <span class="kw">in</span> <span class="bu">zip</span>(data_x, data_y)]),</span>
<span id="cb21-235"><a href="#cb21-235" aria-hidden="true" tabindex="-1"></a>        axis<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb21-236"><a href="#cb21-236" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb21-237"><a href="#cb21-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-238"><a href="#cb21-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-239"><a href="#cb21-239" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting the loss surface</span></span>
<span id="cb21-240"><a href="#cb21-240" aria-hidden="true" tabindex="-1"></a>theta_0_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">80</span>, <span class="dv">20</span>, <span class="dv">80</span>)</span>
<span id="cb21-241"><a href="#cb21-241" aria-hidden="true" tabindex="-1"></a>theta_1_values <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">80</span>)</span>
<span id="cb21-242"><a href="#cb21-242" aria-hidden="true" tabindex="-1"></a>mse_values <span class="op">=</span> np.array(</span>
<span id="cb21-243"><a href="#cb21-243" aria-hidden="true" tabindex="-1"></a>    [[mse_linear(x, y, data_linear) <span class="cf">for</span> x <span class="kw">in</span> theta_0_values] <span class="cf">for</span> y <span class="kw">in</span> theta_1_values]</span>
<span id="cb21-244"><a href="#cb21-244" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-245"><a href="#cb21-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-246"><a href="#cb21-246" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal point</span></span>
<span id="cb21-247"><a href="#cb21-247" aria-hidden="true" tabindex="-1"></a>data_x, data_y <span class="op">=</span> data_linear.iloc[:, <span class="dv">0</span>], data_linear.iloc[:, <span class="dv">1</span>]</span>
<span id="cb21-248"><a href="#cb21-248" aria-hidden="true" tabindex="-1"></a>theta_1_hat <span class="op">=</span> np.corrcoef(data_x, data_y)[<span class="dv">0</span>, <span class="dv">1</span>] <span class="op">*</span> np.std(data_y) <span class="op">/</span> np.std(data_x)</span>
<span id="cb21-249"><a href="#cb21-249" aria-hidden="true" tabindex="-1"></a>theta_0_hat <span class="op">=</span> np.mean(data_y) <span class="op">-</span> theta_1_hat <span class="op">*</span> np.mean(data_x)</span>
<span id="cb21-250"><a href="#cb21-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-251"><a href="#cb21-251" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the 3D plot</span></span>
<span id="cb21-252"><a href="#cb21-252" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">5</span>))</span>
<span id="cb21-253"><a href="#cb21-253" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">"3d"</span>)</span>
<span id="cb21-254"><a href="#cb21-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-255"><a href="#cb21-255" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(theta_0_values, theta_1_values)</span>
<span id="cb21-256"><a href="#cb21-256" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax.plot_surface(</span>
<span id="cb21-257"><a href="#cb21-257" aria-hidden="true" tabindex="-1"></a>    X, Y, mse_values, cmap<span class="op">=</span><span class="st">"viridis"</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb21-258"><a href="#cb21-258" aria-hidden="true" tabindex="-1"></a>)  <span class="co"># Use alpha to make it slightly transparent</span></span>
<span id="cb21-259"><a href="#cb21-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-260"><a href="#cb21-260" aria-hidden="true" tabindex="-1"></a><span class="co"># Scatter point using matplotlib</span></span>
<span id="cb21-261"><a href="#cb21-261" aria-hidden="true" tabindex="-1"></a>sc <span class="op">=</span> ax.scatter(</span>
<span id="cb21-262"><a href="#cb21-262" aria-hidden="true" tabindex="-1"></a>    [theta_0_hat],</span>
<span id="cb21-263"><a href="#cb21-263" aria-hidden="true" tabindex="-1"></a>    [theta_1_hat],</span>
<span id="cb21-264"><a href="#cb21-264" aria-hidden="true" tabindex="-1"></a>    [mse_linear(theta_0_hat, theta_1_hat, data_linear)],</span>
<span id="cb21-265"><a href="#cb21-265" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb21-266"><a href="#cb21-266" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb21-267"><a href="#cb21-267" aria-hidden="true" tabindex="-1"></a>    s<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb21-268"><a href="#cb21-268" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"theta hat"</span>,</span>
<span id="cb21-269"><a href="#cb21-269" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-270"><a href="#cb21-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-271"><a href="#cb21-271" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a colorbar</span></span>
<span id="cb21-272"><a href="#cb21-272" aria-hidden="true" tabindex="-1"></a>cbar <span class="op">=</span> fig.colorbar(surf, ax<span class="op">=</span>ax, shrink<span class="op">=</span><span class="fl">0.5</span>, aspect<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb21-273"><a href="#cb21-273" aria-hidden="true" tabindex="-1"></a>cbar.set_label(<span class="st">"Cost Value"</span>)</span>
<span id="cb21-274"><a href="#cb21-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-275"><a href="#cb21-275" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"MSE for different $</span><span class="ch">\\</span><span class="st">theta_0, </span><span class="ch">\\</span><span class="st">theta_1$"</span>)</span>
<span id="cb21-276"><a href="#cb21-276" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta_0$"</span>)</span>
<span id="cb21-277"><a href="#cb21-277" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$</span><span class="ch">\\</span><span class="st">theta_1$"</span>)</span>
<span id="cb21-278"><a href="#cb21-278" aria-hidden="true" tabindex="-1"></a>ax.set_zlabel(<span class="st">"MSE"</span>)<span class="op">;</span></span>
<span id="cb21-279"><a href="#cb21-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-280"><a href="#cb21-280" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span>
<span id="cb21-281"><a href="#cb21-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-282"><a href="#cb21-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-285"><a href="#cb21-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-286"><a href="#cb21-286" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-287"><a href="#cb21-287" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb21-288"><a href="#cb21-288" aria-hidden="true" tabindex="-1"></a>yobs <span class="op">=</span> data_linear[<span class="st">"Age"</span>]  <span class="co"># The true observations y</span></span>
<span id="cb21-289"><a href="#cb21-289" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> data_linear[<span class="st">"Length"</span>]  <span class="co"># Needed for linear predictions</span></span>
<span id="cb21-290"><a href="#cb21-290" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(yobs)  <span class="co"># Predictions</span></span>
<span id="cb21-291"><a href="#cb21-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-292"><a href="#cb21-292" aria-hidden="true" tabindex="-1"></a>yhats_constant <span class="op">=</span> [thetahat <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]  <span class="co"># Not used, but food for thought</span></span>
<span id="cb21-293"><a href="#cb21-293" aria-hidden="true" tabindex="-1"></a>yhats_linear <span class="op">=</span> [theta_0_hat <span class="op">+</span> theta_1_hat <span class="op">*</span> x <span class="cf">for</span> x <span class="kw">in</span> xs]</span>
<span id="cb21-294"><a href="#cb21-294" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-295"><a href="#cb21-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-298"><a href="#cb21-298" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-299"><a href="#cb21-299" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-300"><a href="#cb21-300" aria-hidden="true" tabindex="-1"></a><span class="co"># Constant Model Rug Plot</span></span>
<span id="cb21-301"><a href="#cb21-301" aria-hidden="true" tabindex="-1"></a><span class="co"># In case we're in a weird style state</span></span>
<span id="cb21-302"><a href="#cb21-302" aria-hidden="true" tabindex="-1"></a>sns.set_theme()</span>
<span id="cb21-303"><a href="#cb21-303" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb21-304"><a href="#cb21-304" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb21-305"><a href="#cb21-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-306"><a href="#cb21-306" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">1.5</span>))</span>
<span id="cb21-307"><a href="#cb21-307" aria-hidden="true" tabindex="-1"></a>sns.rugplot(yobs, height<span class="op">=</span><span class="fl">0.25</span>, lw<span class="op">=</span><span class="dv">2</span>) <span class="op">;</span></span>
<span id="cb21-308"><a href="#cb21-308" aria-hidden="true" tabindex="-1"></a>plt.axvline(thetahat, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">4</span>, label<span class="op">=</span><span class="vs">r"$\hat{\theta}_0$"</span>)<span class="op">;</span></span>
<span id="cb21-309"><a href="#cb21-309" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb21-310"><a href="#cb21-310" aria-hidden="true" tabindex="-1"></a>plt.yticks([])<span class="op">;</span></span>
<span id="cb21-311"><a href="#cb21-311" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span>
<span id="cb21-312"><a href="#cb21-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-313"><a href="#cb21-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-316"><a href="#cb21-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-317"><a href="#cb21-317" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-318"><a href="#cb21-318" aria-hidden="true" tabindex="-1"></a><span class="co"># SLR model scatter plot </span></span>
<span id="cb21-319"><a href="#cb21-319" aria-hidden="true" tabindex="-1"></a><span class="co"># In case we're in a weird style state</span></span>
<span id="cb21-320"><a href="#cb21-320" aria-hidden="true" tabindex="-1"></a>sns.set_theme()</span>
<span id="cb21-321"><a href="#cb21-321" aria-hidden="true" tabindex="-1"></a>adjust_fontsize(size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb21-322"><a href="#cb21-322" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb21-323"><a href="#cb21-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-324"><a href="#cb21-324" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span>xs, y<span class="op">=</span>yobs)</span>
<span id="cb21-325"><a href="#cb21-325" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, yhats_linear, color<span class="op">=</span><span class="st">'red'</span>, lw<span class="op">=</span><span class="dv">4</span>)<span class="op">;</span></span>
<span id="cb21-326"><a href="#cb21-326" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.savefig('dugong_line.png', bbox_inches = 'tight');</span></span>
<span id="cb21-327"><a href="#cb21-327" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span>
<span id="cb21-328"><a href="#cb21-328" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-329"><a href="#cb21-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-330"><a href="#cb21-330" aria-hidden="true" tabindex="-1"></a>Interpreting the RMSE (Root Mean Squared Error):</span>
<span id="cb21-331"><a href="#cb21-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-332"><a href="#cb21-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Because the constant error is **HIGHER** than the linear error,</span>
<span id="cb21-333"><a href="#cb21-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The constant model is **WORSE** than the linear model (at least for this metric).</span>
<span id="cb21-334"><a href="#cb21-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-335"><a href="#cb21-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Constant Model + MAE</span></span>
<span id="cb21-336"><a href="#cb21-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-337"><a href="#cb21-337" aria-hidden="true" tabindex="-1"></a>We see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?</span>
<span id="cb21-338"><a href="#cb21-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-339"><a href="#cb21-339" aria-hidden="true" tabindex="-1"></a>This time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the **Mean Absolute Error (MAE)**.</span>
<span id="cb21-340"><a href="#cb21-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-341"><a href="#cb21-341" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Choose a model: constant model</span>
<span id="cb21-342"><a href="#cb21-342" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Choose a loss function: L1 loss</span>
<span id="cb21-343"><a href="#cb21-343" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fit the model</span>
<span id="cb21-344"><a href="#cb21-344" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Evaluate model performance</span>
<span id="cb21-345"><a href="#cb21-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-346"><a href="#cb21-346" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deriving the optimal $\theta_0$</span></span>
<span id="cb21-347"><a href="#cb21-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-348"><a href="#cb21-348" aria-hidden="true" tabindex="-1"></a>Recall that the MAE is average **absolute** loss (L1 loss) over the data $D = <span class="sc">\{</span>y_1, y_2, ..., y_n<span class="sc">\}</span>$.</span>
<span id="cb21-349"><a href="#cb21-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-350"><a href="#cb21-350" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \hat{y_i}| $$</span>
<span id="cb21-351"><a href="#cb21-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-352"><a href="#cb21-352" aria-hidden="true" tabindex="-1"></a>Given the constant model $\hat{y} = \theta_0$, we can write the MAE as:</span>
<span id="cb21-353"><a href="#cb21-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-354"><a href="#cb21-354" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(\theta_0) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| $$</span>
<span id="cb21-355"><a href="#cb21-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-356"><a href="#cb21-356" aria-hidden="true" tabindex="-1"></a>To fit the model, we find the optimal parameter value $\hat{\theta_0}$ that minimizes the MAE by differentiating using a calculus approach:</span>
<span id="cb21-357"><a href="#cb21-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-358"><a href="#cb21-358" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Differentiate with respect to $\hat{\theta_0}$:</span>
<span id="cb21-359"><a href="#cb21-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-360"><a href="#cb21-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-361"><a href="#cb21-361" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb21-362"><a href="#cb21-362" aria-hidden="true" tabindex="-1"></a>\hat{R}(\theta_0) &amp;= \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0| <span class="sc">\\</span></span>
<span id="cb21-363"><a href="#cb21-363" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\theta_0} R(\theta_0) &amp;= \frac{d}{d\theta_0} \left(\frac{1}{n} \sum^{n}_{i=1} |y_i - \theta_0| \right) <span class="sc">\\</span></span>
<span id="cb21-364"><a href="#cb21-364" aria-hidden="true" tabindex="-1"></a>&amp;= \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0|</span>
<span id="cb21-365"><a href="#cb21-365" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb21-366"><a href="#cb21-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-367"><a href="#cb21-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-368"><a href="#cb21-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Here, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when $y_i = \theta_0$). For now, we'll ignore this issue. It turns out that disregarding this case doesn't influence our final result.</span>
<span id="cb21-369"><a href="#cb21-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>To perform the derivative, consider two cases. When $\theta_0$ is *less than or equal to* $y_i$, the term $y_i - \theta_0$ will be positive and the absolute value has no impact. When $\theta_0$ is *greater than* $y_i$, the term $y_i - \theta_0$ will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying $-(y_i - \theta_0) = \theta_0 - y_i$.</span>
<span id="cb21-370"><a href="#cb21-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-371"><a href="#cb21-371" aria-hidden="true" tabindex="-1"></a>$$|y_i - \theta_0| = \begin{cases} y_i - \theta_0 \quad \text{ if } \theta_0 \le y_i <span class="sc">\\</span> \theta_0 - y_i \quad \text{if }\theta_0 &gt; y_i \end{cases}$$</span>
<span id="cb21-372"><a href="#cb21-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-373"><a href="#cb21-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Taking derivatives:</span>
<span id="cb21-374"><a href="#cb21-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-375"><a href="#cb21-375" aria-hidden="true" tabindex="-1"></a>$$\frac{d}{d\theta_0} |y_i - \theta_0| = \begin{cases} \frac{d}{d\theta_0} (y_i - \theta_0) = -1 \quad \text{if }\theta_0 &lt; y_i <span class="sc">\\</span> \frac{d}{d\theta_0} (\theta_0 - y_i) = 1 \quad \text{if }\theta_0 &gt; y_i \end{cases}$$</span>
<span id="cb21-376"><a href="#cb21-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-377"><a href="#cb21-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This means that we obtain a different value for the derivative for data points where $\theta_0 &lt; y_i$ and where $\theta_0 &gt; y_i$. We can summarize this by saying:</span>
<span id="cb21-378"><a href="#cb21-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-379"><a href="#cb21-379" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-380"><a href="#cb21-380" aria-hidden="true" tabindex="-1"></a>\frac{d}{d\theta_0} R(\theta_0) = \frac{1}{n} \sum^{n}_{i=1} \frac{d}{d\theta_0} |y_i - \theta_0| <span class="sc">\\</span></span>
<span id="cb21-381"><a href="#cb21-381" aria-hidden="true" tabindex="-1"></a>= \frac{1}{n} \left<span class="co">[</span><span class="ot">\sum_{\theta_0 &lt; y_i} (-1) + \sum_{\theta_0 &gt; y_i} (+1) \right</span><span class="co">]</span></span>
<span id="cb21-382"><a href="#cb21-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-383"><a href="#cb21-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-384"><a href="#cb21-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In other words, we take the sum of values for $i = 1, 2, ..., n$:</span>
<span id="cb21-385"><a href="#cb21-385" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$-1$ if our observation $y_i$ is *greater than* our prediction $\hat{\theta_0}$</span>
<span id="cb21-386"><a href="#cb21-386" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>$+1$ if our observation $y_i$ is *smaller than* our prediction $\hat{\theta_0}$</span>
<span id="cb21-387"><a href="#cb21-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-388"><a href="#cb21-388" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set the derivative equation equal to 0:</span>
<span id="cb21-389"><a href="#cb21-389" aria-hidden="true" tabindex="-1"></a>   $$ 0 = \frac{1}{n}\sum_{\hat{\theta_0} &lt; y_i} (-1) + \frac{1}{n}\sum_{\hat{\theta_0} &gt; y_i} (+1) $$</span>
<span id="cb21-390"><a href="#cb21-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-391"><a href="#cb21-391" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Solve for $\hat{\theta_0}$:</span>
<span id="cb21-392"><a href="#cb21-392" aria-hidden="true" tabindex="-1"></a>   $$ 0 = -\frac{1}{n}\sum_{\hat{\theta_0} &lt; y_i} (1) + \frac{1}{n}\sum_{\hat{\theta_0} &gt; y_i} (1)$$</span>
<span id="cb21-393"><a href="#cb21-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-394"><a href="#cb21-394" aria-hidden="true" tabindex="-1"></a>$$\sum_{\hat{\theta_0} &lt; y_i} (1) = \sum_{\hat{\theta_0} &gt; y_i} (1) $$</span>
<span id="cb21-395"><a href="#cb21-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-396"><a href="#cb21-396" aria-hidden="true" tabindex="-1"></a>Thus, the constant model parameter $\theta = \hat{\theta_0}$ that minimizes MAE must satisfy:</span>
<span id="cb21-397"><a href="#cb21-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-398"><a href="#cb21-398" aria-hidden="true" tabindex="-1"></a>$$ \sum_{\hat{\theta_0} &lt; y_i} (1) = \sum_{\hat{\theta_0} &gt; y_i} (1) $$</span>
<span id="cb21-399"><a href="#cb21-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-400"><a href="#cb21-400" aria-hidden="true" tabindex="-1"></a>In other words, the number of observations greater than $\theta_0$ must be equal to the number of observations less than $\theta_0$; there must be an equal number of points on the left and right sides of the equation. This is the definition of median, so our optimal value is</span>
<span id="cb21-401"><a href="#cb21-401" aria-hidden="true" tabindex="-1"></a>$$ \hat{\theta}_0 = median(y) $$</span>
<span id="cb21-402"><a href="#cb21-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-403"><a href="#cb21-403" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary: Loss Optimization, Calculus, and Critical Points</span></span>
<span id="cb21-404"><a href="#cb21-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-405"><a href="#cb21-405" aria-hidden="true" tabindex="-1"></a>First, define the **objective function** as average loss.</span>
<span id="cb21-406"><a href="#cb21-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-407"><a href="#cb21-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Plug in L1 or L2 loss.</span>
<span id="cb21-408"><a href="#cb21-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Plug in the model so that the resulting expression is a function of $\theta$.</span>
<span id="cb21-409"><a href="#cb21-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-410"><a href="#cb21-410" aria-hidden="true" tabindex="-1"></a>Then, find the minimum of the objective function:</span>
<span id="cb21-411"><a href="#cb21-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-412"><a href="#cb21-412" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Differentiate with respect to $\theta$.</span>
<span id="cb21-413"><a href="#cb21-413" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set equal to 0.</span>
<span id="cb21-414"><a href="#cb21-414" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Solve for $\hat{\theta}$.</span>
<span id="cb21-415"><a href="#cb21-415" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>(If we have multiple parameters) repeat steps 1-3 with partial derivatives.</span>
<span id="cb21-416"><a href="#cb21-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-417"><a href="#cb21-417" aria-hidden="true" tabindex="-1"></a>Recall critical points from calculus: $R(\hat{\theta})$ could be a minimum, maximum, or saddle point!</span>
<span id="cb21-418"><a href="#cb21-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-419"><a href="#cb21-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We should technically also perform the second derivative test, i.e., show $R''(\hat{\theta}) &gt; 0$.</span>
<span id="cb21-420"><a href="#cb21-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MSE has a property—**convexity**—that guarantees that $R(\hat{\theta})$ is a global minimum.</span>
<span id="cb21-421"><a href="#cb21-421" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The proof of convexity for MAE is beyond this course.</span>
<span id="cb21-422"><a href="#cb21-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-423"><a href="#cb21-423" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing Loss Functions</span></span>
<span id="cb21-424"><a href="#cb21-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-425"><a href="#cb21-425" aria-hidden="true" tabindex="-1"></a>We've now tried our hand at fitting a model under both MSE and MAE cost functions. How do the two results compare?</span>
<span id="cb21-426"><a href="#cb21-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-427"><a href="#cb21-427" aria-hidden="true" tabindex="-1"></a>Let's consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We'll fit a constant model to predict the number of drinks that will be sold tomorrow.</span>
<span id="cb21-428"><a href="#cb21-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-431"><a href="#cb21-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-432"><a href="#cb21-432" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb21-433"><a href="#cb21-433" aria-hidden="true" tabindex="-1"></a>drinks <span class="op">=</span> np.array([<span class="dv">20</span>, <span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">29</span>, <span class="dv">33</span>])</span>
<span id="cb21-434"><a href="#cb21-434" aria-hidden="true" tabindex="-1"></a>drinks</span>
<span id="cb21-435"><a href="#cb21-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-436"><a href="#cb21-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-437"><a href="#cb21-437" aria-hidden="true" tabindex="-1"></a>From our derivations above, we know that the optimal model parameter under MSE cost is the mean of the dataset. Under MAE cost, the optimal parameter is the median of the dataset.</span>
<span id="cb21-438"><a href="#cb21-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-441"><a href="#cb21-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-442"><a href="#cb21-442" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb21-443"><a href="#cb21-443" aria-hidden="true" tabindex="-1"></a>np.mean(drinks), np.median(drinks)</span>
<span id="cb21-444"><a href="#cb21-444" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-445"><a href="#cb21-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-446"><a href="#cb21-446" aria-hidden="true" tabindex="-1"></a>If we plot each empirical risk function across several possible values of $\theta$, we find that each $\hat{\theta}$ does indeed correspond to the lowest value of error:</span>
<span id="cb21-447"><a href="#cb21-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-448"><a href="#cb21-448" aria-hidden="true" tabindex="-1"></a>&lt;img src="images/error.png" alt='error' width='600'&gt;</span>
<span id="cb21-449"><a href="#cb21-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-450"><a href="#cb21-450" aria-hidden="true" tabindex="-1"></a>Notice that the MSE above is a **smooth** function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE, in contrast, is not differentiable at each of its "kinks." We'll explore how the smoothness of the cost function can impact our ability to apply numerical optimization in a few weeks.</span>
<span id="cb21-451"><a href="#cb21-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-452"><a href="#cb21-452" aria-hidden="true" tabindex="-1"></a>How do outliers affect each cost function? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected.</span>
<span id="cb21-453"><a href="#cb21-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-456"><a href="#cb21-456" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-457"><a href="#cb21-457" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb21-458"><a href="#cb21-458" aria-hidden="true" tabindex="-1"></a>drinks_with_outlier <span class="op">=</span> np.append(drinks, <span class="dv">1033</span>)</span>
<span id="cb21-459"><a href="#cb21-459" aria-hidden="true" tabindex="-1"></a>display(drinks_with_outlier)</span>
<span id="cb21-460"><a href="#cb21-460" aria-hidden="true" tabindex="-1"></a>np.mean(drinks_with_outlier), np.median(drinks_with_outlier)</span>
<span id="cb21-461"><a href="#cb21-461" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-462"><a href="#cb21-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-463"><a href="#cb21-463" aria-hidden="true" tabindex="-1"></a>&lt;img src="images/outliers.png" alt='outliers' width='700'&gt;</span>
<span id="cb21-464"><a href="#cb21-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-465"><a href="#cb21-465" aria-hidden="true" tabindex="-1"></a>This means that under the MSE, the optimal model parameter $\hat{\theta}$ is strongly affected by the presence of outliers. Under the MAE, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is **sensitive** to outliers, while the MAE is **robust** to outliers.</span>
<span id="cb21-466"><a href="#cb21-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-467"><a href="#cb21-467" aria-hidden="true" tabindex="-1"></a>Let's try another experiment. This time, we'll add an additional, non-outlying datapoint to the data.</span>
<span id="cb21-468"><a href="#cb21-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-471"><a href="#cb21-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-472"><a href="#cb21-472" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb21-473"><a href="#cb21-473" aria-hidden="true" tabindex="-1"></a>drinks_with_additional_observation <span class="op">=</span> np.append(drinks, <span class="dv">35</span>)</span>
<span id="cb21-474"><a href="#cb21-474" aria-hidden="true" tabindex="-1"></a>drinks_with_additional_observation</span>
<span id="cb21-475"><a href="#cb21-475" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-476"><a href="#cb21-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-477"><a href="#cb21-477" aria-hidden="true" tabindex="-1"></a>When we again visualize the cost functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are _infinitely_ many optimal values for the model parameter: any value $\hat{\theta} \in <span class="co">[</span><span class="ot">22, 29</span><span class="co">]</span>$ will minimize the MAE. In contrast, the MSE still has a single best value for $\hat{\theta}$. In other words, the MSE has a **unique** solution for $\hat{\theta}$; the MAE is not guaranteed to have a single unique solution.</span>
<span id="cb21-478"><a href="#cb21-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-479"><a href="#cb21-479" aria-hidden="true" tabindex="-1"></a>&lt;img src="images/mse_loss_26.png" width='350'&gt; &lt;img src="images/mae_loss_infinite.png" width='350'&gt;</span>
<span id="cb21-480"><a href="#cb21-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-481"><a href="#cb21-481" aria-hidden="true" tabindex="-1"></a>To summarize our example,</span>
<span id="cb21-482"><a href="#cb21-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-483"><a href="#cb21-483" aria-hidden="true" tabindex="-1"></a>|                        | MSE (Mean Squared Loss)                                                   | MAE (Mean Absolute Loss)                                                        |</span>
<span id="cb21-484"><a href="#cb21-484" aria-hidden="true" tabindex="-1"></a>| ---------------------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------- |</span>
<span id="cb21-485"><a href="#cb21-485" aria-hidden="true" tabindex="-1"></a>| Loss Function                  | $\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} (y_i - \theta_0)^2$                                            | $\hat{R}(\theta) = \frac{1}{n}\sum^{n}_{i=1} |y_i - \theta_0|$                                               |</span>
<span id="cb21-486"><a href="#cb21-486" aria-hidden="true" tabindex="-1"></a>| Optimal $\hat{\theta_0}$                   | $\hat{\theta_0} = mean(y) = \bar{y}$                      | $\hat{\theta_0} = median(y)$                 |</span>
<span id="cb21-487"><a href="#cb21-487" aria-hidden="true" tabindex="-1"></a>| Loss Surface           | &lt;img src="images/mse_loss_26.png" width='250'&gt;                        | &lt;img src="images/mae_loss_infinite.png" width='250'&gt;                                          |</span>
<span id="cb21-488"><a href="#cb21-488" aria-hidden="true" tabindex="-1"></a>| Shape            | **Smooth** - easy to minimize using numerical methods (in a few weeks)  | **Piecewise** - at each of the “kinks,” it’s not differentiable. Harder to minimize. |</span>
<span id="cb21-489"><a href="#cb21-489" aria-hidden="true" tabindex="-1"></a>| Outliers                   | **Sensitive** to outliers (since they change mean substantially). Sensitivity also depends on the dataset size.                                                            | **More robust** to outliers.                                                                           |</span>
<span id="cb21-490"><a href="#cb21-490" aria-hidden="true" tabindex="-1"></a>| $\hat{\theta_0}$ Uniqueness | **Unique** $\hat{\theta_0}$                              | **Infinitely many** $\hat{\theta_0}$s                                    |</span>
<span id="cb21-491"><a href="#cb21-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-492"><a href="#cb21-492" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transformations to Fit Linear Models</span></span>
<span id="cb21-493"><a href="#cb21-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-494"><a href="#cb21-494" aria-hidden="true" tabindex="-1"></a>At this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.</span>
<span id="cb21-495"><a href="#cb21-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-496"><a href="#cb21-496" aria-hidden="true" tabindex="-1"></a>A key word above is _linear_. When we computed parameter estimates earlier, we assumed that $x_i$ and $y_i$ shared a roughly linear relationship. Data in the real world isn't always so straightforward, but we can transform the data to try and obtain linearity.</span>
<span id="cb21-497"><a href="#cb21-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-498"><a href="#cb21-498" aria-hidden="true" tabindex="-1"></a>The **Tukey-Mosteller Bulge Diagram** is a useful tool for summarizing what transformations can linearize the relationship between two variables. To determine what transformations might be appropriate, trace the shape of the "bulge" made by your data. Find the quadrant of the diagram that matches this bulge. The transformations shown on the vertical and horizontal axes of this quadrant can help improve the fit between the variables.</span>
<span id="cb21-499"><a href="#cb21-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-500"><a href="#cb21-500" aria-hidden="true" tabindex="-1"></a>&lt;img src="images/bulge.png" alt='bulge' width='600'&gt;</span>
<span id="cb21-501"><a href="#cb21-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-502"><a href="#cb21-502" aria-hidden="true" tabindex="-1"></a>Note that:</span>
<span id="cb21-503"><a href="#cb21-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-504"><a href="#cb21-504" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>There are multiple solutions. Some will fit better than others.</span>
<span id="cb21-505"><a href="#cb21-505" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sqrt and log make a value “smaller.”</span>
<span id="cb21-506"><a href="#cb21-506" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Raising to a power makes a value “bigger.”</span>
<span id="cb21-507"><a href="#cb21-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each of these transformations equates to increasing or decreasing the scale of an axis.</span>
<span id="cb21-508"><a href="#cb21-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-509"><a href="#cb21-509" aria-hidden="true" tabindex="-1"></a>Other goals in addition to linearity are possible, for example, making data appear more symmetric.</span>
<span id="cb21-510"><a href="#cb21-510" aria-hidden="true" tabindex="-1"></a>Linearity allows us to fit lines to the transformed data.</span>
<span id="cb21-511"><a href="#cb21-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-512"><a href="#cb21-512" aria-hidden="true" tabindex="-1"></a>Let's revisit our dugongs example. The lengths and ages are plotted below:</span>
<span id="cb21-513"><a href="#cb21-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-516"><a href="#cb21-516" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-517"><a href="#cb21-517" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-518"><a href="#cb21-518" aria-hidden="true" tabindex="-1"></a><span class="co"># `corrcoef` computes the correlation coefficient between two variables</span></span>
<span id="cb21-519"><a href="#cb21-519" aria-hidden="true" tabindex="-1"></a><span class="co"># `std` finds the standard deviation</span></span>
<span id="cb21-520"><a href="#cb21-520" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> dugongs[<span class="st">"Length"</span>]</span>
<span id="cb21-521"><a href="#cb21-521" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> dugongs[<span class="st">"Age"</span>]</span>
<span id="cb21-522"><a href="#cb21-522" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb21-523"><a href="#cb21-523" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> r <span class="op">*</span> np.std(y) <span class="op">/</span> np.std(x)</span>
<span id="cb21-524"><a href="#cb21-524" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> np.mean(y) <span class="op">-</span> theta_1 <span class="op">*</span> np.mean(x)</span>
<span id="cb21-525"><a href="#cb21-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-526"><a href="#cb21-526" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, dpi<span class="op">=</span><span class="dv">200</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb21-527"><a href="#cb21-527" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(x, y)</span>
<span id="cb21-528"><a href="#cb21-528" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb21-529"><a href="#cb21-529" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="st">"Age"</span>)</span>
<span id="cb21-530"><a href="#cb21-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-531"><a href="#cb21-531" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x, y)</span>
<span id="cb21-532"><a href="#cb21-532" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x, <span class="st">"tab:red"</span>)</span>
<span id="cb21-533"><a href="#cb21-533" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb21-534"><a href="#cb21-534" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="st">"Age"</span>)<span class="op">;</span></span>
<span id="cb21-535"><a href="#cb21-535" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-536"><a href="#cb21-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-537"><a href="#cb21-537" aria-hidden="true" tabindex="-1"></a>Looking at the plot on the left, we see that there is a slight curvature to the data points. Plotting the SLR curve on the right results in a poor fit.</span>
<span id="cb21-538"><a href="#cb21-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-539"><a href="#cb21-539" aria-hidden="true" tabindex="-1"></a>For SLR to perform well, we'd like there to be a rough linear trend relating <span class="in">`"Age"`</span> and <span class="in">`"Length"`</span>. What is making the raw data deviate from a linear relationship? Notice that the data points with <span class="in">`"Length"`</span> greater than 2.6 have disproportionately high values of <span class="in">`"Age"`</span> relative to the rest of the data. If we could manipulate these data points to have lower <span class="in">`"Age"`</span> values, we'd "shift" these points downwards and reduce the curvature in the data. Applying a logarithmic transformation to $y_i$ (that is, taking $\log($ <span class="in">`"Age"`</span> $)$ ) would achieve just that.</span>
<span id="cb21-540"><a href="#cb21-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-541"><a href="#cb21-541" aria-hidden="true" tabindex="-1"></a>An important word on $\log$: in Data 100 (and most upper-division STEM courses), $\log$ denotes the natural logarithm with base $e$. The base-10 logarithm, where relevant, is indicated by $\log_{10}$.</span>
<span id="cb21-542"><a href="#cb21-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-545"><a href="#cb21-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-546"><a href="#cb21-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-547"><a href="#cb21-547" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.log(y)</span>
<span id="cb21-548"><a href="#cb21-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-549"><a href="#cb21-549" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> np.corrcoef(x, z)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb21-550"><a href="#cb21-550" aria-hidden="true" tabindex="-1"></a>theta_1 <span class="op">=</span> r <span class="op">*</span> np.std(z) <span class="op">/</span> np.std(x)</span>
<span id="cb21-551"><a href="#cb21-551" aria-hidden="true" tabindex="-1"></a>theta_0 <span class="op">=</span> np.mean(z) <span class="op">-</span> theta_1 <span class="op">*</span> np.mean(x)</span>
<span id="cb21-552"><a href="#cb21-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-553"><a href="#cb21-553" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, dpi<span class="op">=</span><span class="dv">200</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb21-554"><a href="#cb21-554" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].scatter(x, z)</span>
<span id="cb21-555"><a href="#cb21-555" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb21-556"><a href="#cb21-556" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_ylabel(<span class="vs">r"$\log{(Age)}$"</span>)</span>
<span id="cb21-557"><a href="#cb21-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-558"><a href="#cb21-558" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].scatter(x, z)</span>
<span id="cb21-559"><a href="#cb21-559" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].plot(x, theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x, <span class="st">"tab:red"</span>)</span>
<span id="cb21-560"><a href="#cb21-560" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_xlabel(<span class="st">"Length"</span>)</span>
<span id="cb21-561"><a href="#cb21-561" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_ylabel(<span class="vs">r"$\log{(Age)}$"</span>)</span>
<span id="cb21-562"><a href="#cb21-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-563"><a href="#cb21-563" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-564"><a href="#cb21-564" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-565"><a href="#cb21-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-566"><a href="#cb21-566" aria-hidden="true" tabindex="-1"></a>Our SLR fit looks a lot better! We now have a new target variable: the SLR model is now trying to predict the _log_ of <span class="in">`"Age"`</span>, rather than the untransformed <span class="in">`"Age"`</span>. In other words, we are applying the transformation $z_i = \log{(y_i)}$. Notice that the resulting model is still **linear in the parameters** $\theta = <span class="co">[</span><span class="ot">\theta_0, \theta_1</span><span class="co">]</span>$. The SLR model becomes:</span>
<span id="cb21-567"><a href="#cb21-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-568"><a href="#cb21-568" aria-hidden="true" tabindex="-1"></a>$$\hat{\log{y}} = \theta_0 + \theta_1 x$$</span>
<span id="cb21-569"><a href="#cb21-569" aria-hidden="true" tabindex="-1"></a>$$\hat{z} = \theta_0 + \theta_1 x$$</span>
<span id="cb21-570"><a href="#cb21-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-571"><a href="#cb21-571" aria-hidden="true" tabindex="-1"></a>It turns out that this linearized relationship can help us understand the underlying relationship between $x$ and $y$. If we rearrange the relationship above, we find:</span>
<span id="cb21-572"><a href="#cb21-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-573"><a href="#cb21-573" aria-hidden="true" tabindex="-1"></a>$$\log{(y)} = \theta_0 + \theta_1 x$$</span>
<span id="cb21-574"><a href="#cb21-574" aria-hidden="true" tabindex="-1"></a>$$y = e^{\theta_0 + \theta_1 x}$$</span>
<span id="cb21-575"><a href="#cb21-575" aria-hidden="true" tabindex="-1"></a>$$y = (e^{\theta_0})e^{\theta_1 x}$$</span>
<span id="cb21-576"><a href="#cb21-576" aria-hidden="true" tabindex="-1"></a>$$y_i = C e^{k x}$$</span>
<span id="cb21-577"><a href="#cb21-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-578"><a href="#cb21-578" aria-hidden="true" tabindex="-1"></a>For some constants $C$ and $k$.</span>
<span id="cb21-579"><a href="#cb21-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-580"><a href="#cb21-580" aria-hidden="true" tabindex="-1"></a>$y$ is an *exponential* function of $x$. Applying an exponential fit to the untransformed variables corroborates this finding.</span>
<span id="cb21-581"><a href="#cb21-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-584"><a href="#cb21-584" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb21-585"><a href="#cb21-585" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb21-586"><a href="#cb21-586" aria-hidden="true" tabindex="-1"></a>plt.figure(dpi<span class="op">=</span><span class="dv">120</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb21-587"><a href="#cb21-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-588"><a href="#cb21-588" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y)</span>
<span id="cb21-589"><a href="#cb21-589" aria-hidden="true" tabindex="-1"></a>plt.plot(x, np.exp(theta_0) <span class="op">*</span> np.exp(theta_1 <span class="op">*</span> x), <span class="st">"tab:red"</span>)</span>
<span id="cb21-590"><a href="#cb21-590" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Length"</span>)</span>
<span id="cb21-591"><a href="#cb21-591" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Age"</span>)<span class="op">;</span></span>
<span id="cb21-592"><a href="#cb21-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb21-593"><a href="#cb21-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-594"><a href="#cb21-594" aria-hidden="true" tabindex="-1"></a>You may wonder: why did we choose to apply a log transformation specifically? Why not some other function to linearize the data?</span>
<span id="cb21-595"><a href="#cb21-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-596"><a href="#cb21-596" aria-hidden="true" tabindex="-1"></a>Practically, many other mathematical operations that modify the relative scales of <span class="in">`"Age"`</span> and <span class="in">`"Length"`</span> could have worked here.</span>
<span id="cb21-597"><a href="#cb21-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-598"><a href="#cb21-598" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Linear Regression</span></span>
<span id="cb21-599"><a href="#cb21-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-600"><a href="#cb21-600" aria-hidden="true" tabindex="-1"></a>Multiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:</span>
<span id="cb21-601"><a href="#cb21-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-602"><a href="#cb21-602" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}$$</span>
<span id="cb21-603"><a href="#cb21-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-604"><a href="#cb21-604" aria-hidden="true" tabindex="-1"></a>Our predicted value of $y$, $\hat{y}$, is a linear combination of the single **observations** (features), $x_i$, and the parameters, $\theta_i$.</span>
<span id="cb21-605"><a href="#cb21-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-606"><a href="#cb21-606" aria-hidden="true" tabindex="-1"></a>We'll dive deeper into Multiple Linear Regression in the next lecture.</span>
<span id="cb21-607"><a href="#cb21-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-608"><a href="#cb21-608" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bonus: Calculating Constant Model MSE Using an Algebraic Trick</span></span>
<span id="cb21-609"><a href="#cb21-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-610"><a href="#cb21-610" aria-hidden="true" tabindex="-1"></a>Earlier, we calculated the constant model MSE using calculus. It turns out that there is a much more elegant way of performing this same minimization algebraically, without using calculus at all.</span>
<span id="cb21-611"><a href="#cb21-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-612"><a href="#cb21-612" aria-hidden="true" tabindex="-1"></a>In this calculation, we use the fact that the **sum of deviations from the mean is $0$** or that $\sum_{i=1}^{n} (y_i - \bar{y}) = 0$.</span>
<span id="cb21-613"><a href="#cb21-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-614"><a href="#cb21-614" aria-hidden="true" tabindex="-1"></a>Let's quickly walk through the proof for this:</span>
<span id="cb21-615"><a href="#cb21-615" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-616"><a href="#cb21-616" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb21-617"><a href="#cb21-617" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^{n} (y_i - \bar{y}) &amp;= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n} \bar{y} <span class="sc">\\</span></span>
<span id="cb21-618"><a href="#cb21-618" aria-hidden="true" tabindex="-1"></a> &amp;= \sum_{i=1}^{n} y_i - n\bar{y} <span class="sc">\\</span></span>
<span id="cb21-619"><a href="#cb21-619" aria-hidden="true" tabindex="-1"></a> &amp;= \sum_{i=1}^{n} y_i - n\frac{1}{n}\sum_{i=1}^{n}y_i <span class="sc">\\</span></span>
<span id="cb21-620"><a href="#cb21-620" aria-hidden="true" tabindex="-1"></a> &amp;= \sum_{i=1}^{n} y_i - \sum_{i=1}^{n}y_i <span class="sc">\\</span></span>
<span id="cb21-621"><a href="#cb21-621" aria-hidden="true" tabindex="-1"></a> &amp; = 0</span>
<span id="cb21-622"><a href="#cb21-622" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb21-623"><a href="#cb21-623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-624"><a href="#cb21-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-625"><a href="#cb21-625" aria-hidden="true" tabindex="-1"></a>In our calculations, we'll also be using the definition of the variance as a sample. As a refresher:</span>
<span id="cb21-626"><a href="#cb21-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-627"><a href="#cb21-627" aria-hidden="true" tabindex="-1"></a>$$\sigma_y^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \bar{y})^2$$</span>
<span id="cb21-628"><a href="#cb21-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-629"><a href="#cb21-629" aria-hidden="true" tabindex="-1"></a>Getting into our calculation for MSE minimization:</span>
<span id="cb21-630"><a href="#cb21-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-631"><a href="#cb21-631" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-632"><a href="#cb21-632" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb21-633"><a href="#cb21-633" aria-hidden="true" tabindex="-1"></a>R(\theta) &amp;= {\frac{1}{n}}\sum^{n}_{i=1} (y_i - \theta)^2</span>
<span id="cb21-634"><a href="#cb21-634" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}\sum^{n}_{i=1} <span class="co">[</span><span class="ot">(y_i - \bar{y}) + (\bar{y} - \theta)</span><span class="co">]</span>^2\quad \quad \text{using trick that a-b can be written as (a-c) + (c-b) } <span class="sc">\\</span></span>
<span id="cb21-635"><a href="#cb21-635" aria-hidden="true" tabindex="-1"></a>&amp;\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \space \space \text{where a, b, and c are any numbers}</span>
<span id="cb21-636"><a href="#cb21-636" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}\sum^{n}_{i=1} <span class="co">[</span><span class="ot">(y_i - \bar{y})^2 + 2(y_i - \bar{y})(\bar{y} - \theta) + (\bar{y} - \theta)^2</span><span class="co">]</span></span>
<span id="cb21-637"><a href="#cb21-637" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}<span class="co">[</span><span class="ot">\sum^{n}_{i=1}(y_i - \bar{y})^2 + 2(\bar{y} - \theta)\sum^{n}_{i=1}(y_i - \bar{y}) + n(\bar{y} - \theta)^2</span><span class="co">]</span> \quad \quad  \text{distribute sum to individual terms}</span>
<span id="cb21-638"><a href="#cb21-638" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \frac{1}{n}\sum^{n}_{i=1}(y_i - \bar{y})^2 + \frac{2}{n}(\bar{y} - \theta)\cdot0 + (\bar{y} - \theta)^2 \quad \quad  \text{sum of deviations from mean is 0}</span>
<span id="cb21-639"><a href="#cb21-639" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span> &amp;= \sigma_y^2 + (\bar{y} - \theta)^2</span>
<span id="cb21-640"><a href="#cb21-640" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb21-641"><a href="#cb21-641" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb21-642"><a href="#cb21-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-643"><a href="#cb21-643" aria-hidden="true" tabindex="-1"></a>Since variance can't be negative, we know that our first term, $\sigma_y^2$ is greater than or equal to $0$. Also note, that **the first term doesn't involve $\theta$ at all**, meaning changing our model won't change this value. For the purposes of determining $\hat{\theta}#, we can then essentially ignore this term.</span>
<span id="cb21-644"><a href="#cb21-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-645"><a href="#cb21-645" aria-hidden="true" tabindex="-1"></a>Looking at the second term, $(\bar{y} - \theta)^2$, since it is squared, we know it must be greater than or equal to $0$. As this term does involve $\theta$, picking the value of $\theta$ that minimizes this term will allow us to minimize our average loss. For the second term to equal $0$, $\theta = \bar{y}$, or in other words, $\hat{\theta} = \bar{y} = mean(y)$.</span>
<span id="cb21-646"><a href="#cb21-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-647"><a href="#cb21-647" aria-hidden="true" tabindex="-1"></a><span class="fu">##### Note</span></span>
<span id="cb21-648"><a href="#cb21-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-649"><a href="#cb21-649" aria-hidden="true" tabindex="-1"></a>In the derivation above, we decompose the expected loss, $R(\theta)$, into two key components: the variance of the data, $\sigma_y^2$, and the square of the bias, $(\bar{y} - \theta)^2$. This decomposition is insightful for understanding the behavior of estimators in statistical models.</span>
<span id="cb21-650"><a href="#cb21-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-651"><a href="#cb21-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Variance, $\sigma_y^2$**: This term represents the spread of the data points around their mean, $\bar{y}$, and is a measure of the data's inherent variability. Importantly, it does not depend on the choice of $\theta$, meaning it's a fixed property of the data. Variance serves as an indicator of the data's dispersion and is crucial in understanding the dataset's structure, but it remains constant regardless of how we adjust our model parameter $\theta$.</span>
<span id="cb21-652"><a href="#cb21-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-653"><a href="#cb21-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bias Squared, $(\bar{y} - \theta)^2$**: This term captures the bias of the estimator, defined as the square of the difference between the mean of the data points, $\bar{y}$, and the parameter $\theta$. The bias quantifies the systematic error introduced when estimating $\theta$. Minimizing this term is essential for improving the accuracy of the estimator. When $\theta = \bar{y}$, the bias is $0$, indicating that the estimator is unbiased for the parameter it estimates. This highlights a critical principle in statistical estimation: choosing $\theta$ to be the sample mean, $\bar{y}$, minimizes the average loss, rendering the estimator both efficient and unbiased for the population mean.</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>