<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; Gradient Descent Continuation, Feature Engineering – Principles and Techniques of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../case_study_HCE/case_study_HCE.html" rel="next">
<link href="../gradient_descent/gradient_descent.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-ca5a086e270bb62b76934925835b48c3.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
if (typeof require !== 'undefined') {
require.undef("plotly");
requirejs.config({
    paths: {
        'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']
    }
});
require(['plotly'], function(Plotly) {
    window._Plotly = Plotly;
});
}
</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../feature_engineering/feature_engineering.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../modeling_slr/modeling_slr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modeling &amp; SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Parameter Inference and Bootstrapping</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II (Old Notes from Fall 2024)</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Gradient Descent Continuation, Feature Engineering</h2>
   
  <ul>
  <li><a href="#gradient-descent-part-2" id="toc-gradient-descent-part-2" class="nav-link active" data-scroll-target="#gradient-descent-part-2"><span class="header-section-number">14.1</span> Gradient Descent Part 2</a>
  <ul>
  <li><a href="#gradient-descent-on-multi-dimensional-models" id="toc-gradient-descent-on-multi-dimensional-models" class="nav-link" data-scroll-target="#gradient-descent-on-multi-dimensional-models"><span class="header-section-number">14.1.1</span> Gradient Descent on Multi-Dimensional Models</a>
  <ul>
  <li><a href="#the-gradient-vector" id="toc-the-gradient-vector" class="nav-link" data-scroll-target="#the-gradient-vector"><span class="header-section-number">14.1.1.1</span> The Gradient Vector</a></li>
  </ul></li>
  <li><a href="#stochastic-mini-batch-gradient-descent" id="toc-stochastic-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#stochastic-mini-batch-gradient-descent"><span class="header-section-number">14.1.2</span> Stochastic (Mini-batch) Gradient Descent</a></li>
  <li><a href="#batch-mini-batch-stochastic-gradient-descent" id="toc-batch-mini-batch-stochastic-gradient-descent" class="nav-link" data-scroll-target="#batch-mini-batch-stochastic-gradient-descent"><span class="header-section-number">14.1.3</span> Batch, Mini-batch, Stochastic Gradient Descent</a></li>
  </ul></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering"><span class="header-section-number">14.2</span> Feature Engineering</a>
  <ul>
  <li><a href="#feature-functions" id="toc-feature-functions" class="nav-link" data-scroll-target="#feature-functions"><span class="header-section-number">14.2.1</span> Feature Functions</a></li>
  <li><a href="#one-hot-encoding" id="toc-one-hot-encoding" class="nav-link" data-scroll-target="#one-hot-encoding"><span class="header-section-number">14.2.2</span> One-Hot Encoding</a></li>
  <li><a href="#polynomial-features" id="toc-polynomial-features" class="nav-link" data-scroll-target="#polynomial-features"><span class="header-section-number">14.2.3</span> Polynomial Features</a></li>
  <li><a href="#complexity-and-overfitting" id="toc-complexity-and-overfitting" class="nav-link" data-scroll-target="#complexity-and-overfitting"><span class="header-section-number">14.2.4</span> Complexity and Overfitting</a></li>
  </ul></li>
  <li><a href="#bonus-time-complexity-of-the-normal-equation" id="toc-bonus-time-complexity-of-the-normal-equation" class="nav-link" data-scroll-target="#bonus-time-complexity-of-the-normal-equation"><span class="header-section-number">14.3</span> [Bonus] Time Complexity of the Normal Equation</a></li>
  <li><a href="#bonus-stochastic-gradient-descent-in-pytorch" id="toc-bonus-stochastic-gradient-descent-in-pytorch" class="nav-link" data-scroll-target="#bonus-stochastic-gradient-descent-in-pytorch"><span class="header-section-number">14.4</span> [Bonus] Stochastic Gradient Descent in <code>PyTorch</code></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Recognize the value of feature engineering as a tool to improve model performance</li>
<li>Implement polynomial feature generation and one hot encoding</li>
<li>Understand the interactions between model complexity, model variance, and training error</li>
</ul>
</div>
</div>
</div>
<p>At this point, we’ve grown quite familiar with the modeling process. We’ve introduced the concept of loss, used it to fit several types of models, and, most recently, extended our analysis to multiple regression. Along the way, we’ve forged our way through the mathematics of deriving the optimal model parameters in all its gory detail. It’s time to make our lives a little easier – let’s implement the modeling process in code!</p>
<p>In this lecture, we’ll explore two techniques for model fitting:</p>
<ol type="1">
<li>Translating our derived formulas for regression to <code>python</code></li>
<li>Using <code>python</code>’s <code>sklearn</code> package</li>
</ol>
<p>With our new programming frameworks in hand, we will also add sophistication to our models by introducing more complex features to enhance model performance.</p>
<section id="gradient-descent-part-2" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="gradient-descent-part-2"><span class="header-section-number">14.1</span> Gradient Descent Part 2</h2>
<p>Before we dive into feature engineering, let’s expand on gradient descent, which we began covering in the last lecture. Recall that gradient descent is a powerful technique for choosing the model parameters that minimize the loss function.</p>
<section id="gradient-descent-on-multi-dimensional-models" class="level3" data-number="14.1.1">
<h3 data-number="14.1.1" class="anchored" data-anchor-id="gradient-descent-on-multi-dimensional-models"><span class="header-section-number">14.1.1</span> Gradient Descent on Multi-Dimensional Models</h3>
<p>The function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, <span class="math inline">\(\theta\)</span>. However, models usually have a cost function with multiple parameters that need to be optimized. For example, simple linear regression has 2 parameters: <span class="math display">\[\hat{y} + \theta_0 + \theta_1x\]</span> and multiple linear regression has <span class="math inline">\(p+1\)</span> parameters: <span class="math display">\[\mathbb{Y} = \theta_0 + \theta_1 \Bbb{X}_{:,1} + \theta_2 \Bbb{X}_{:,2} + \cdots + \theta_p \Bbb{X}_{:,p}\]</span></p>
<p>We’ll need to expand gradient descent so we can update our guesses for all model parameters all in one go.</p>
<p>With multiple parameters to optimize, we consider a <strong>loss surface</strong>, or the model’s loss for a particular <em>combination</em> of possible parameter values.</p>
<div id="71ad9a9a" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="1bbc9e9d" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(theta, X, y_obs):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X <span class="op">@</span> theta</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y_obs) <span class="op">**</span> <span class="dv">2</span>)    </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>tips_with_bias <span class="op">=</span> df.copy()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>tips_with_bias[<span class="st">"bias"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>tips_with_bias <span class="op">=</span> tips_with_bias[[<span class="st">"bias"</span>, <span class="st">"total_bill"</span>]]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>uvalues <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>vvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.35</span>, <span class="dv">10</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>(u,v) <span class="op">=</span> np.meshgrid(uvalues, vvalues)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.vstack((u.flatten(),v.flatten()))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_single_arg(theta):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_loss(theta, tips_with_bias, df[<span class="st">"tip"</span>])</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> np.array([mse_loss_single_arg(t) <span class="cf">for</span> t <span class="kw">in</span> thetas.T])</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>loss_surface <span class="op">=</span> go.Surface(x<span class="op">=</span>u, y<span class="op">=</span>v, z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>ind <span class="op">=</span> np.argmin(MSE)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> go.Scatter3d(name <span class="op">=</span> <span class="st">"Optimal Point"</span>,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [thetas.T[ind,<span class="dv">0</span>]], y <span class="op">=</span> [thetas.T[ind,<span class="dv">1</span>]], </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> [MSE[ind]],</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, optimal_point])</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>), autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>                            <div id="57892b1b-d7f1-4a00-8eaa-f11eeec839ad" class="plotly-graph-div" style="height:600px; width:800px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("57892b1b-d7f1-4a00-8eaa-f11eeec839ad")) {                    Plotly.newPlot(                        "57892b1b-d7f1-4a00-8eaa-f11eeec839ad",                        [{"x":[[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0]],"y":[[-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1,-0.1],[-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001,-0.05000000000000001],[-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17,-1.3877787807814457e-17],[0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999,0.04999999999999999],[0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998,0.09999999999999998],[0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997,0.14999999999999997],[0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998,0.19999999999999998],[0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997,0.24999999999999997],[0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993,0.29999999999999993],[0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35,0.35]],"z":[[29.123031979508195,26.96047116185995,24.896675776310463,22.931645822859743,21.065381301507795,19.297882212254603,17.629148555100183,16.059180330044526,14.587977537087637,13.21554017622951],[18.833628650614756,17.11075544680986,15.486647675103727,13.961305335496359,12.534728427987758,11.20691695257792,9.97787090926685,8.847590298054545,7.816075118941006,6.883325371926231],[10.896283606557377,9.613098016595831,8.428677858733053,7.343023132969036,6.356133839303786,5.468009977737301,4.678651548269582,3.9880585509006288,3.3962309856304405,2.903168852459017],[5.310996847336067,4.467498871217872,3.722766327198442,3.0767992152777786,2.5295975354558795,2.0811612877327472,1.7314904721083795,1.4805850885827772,1.3284451371559403,1.275070617827869],[2.0777683729508207,1.6739580106759773,1.3689130804998992,1.1626335824225869,1.05511951644404,1.0463708825642581,1.1363876807832418,1.325169911100991,1.6127175735175057,1.9990306680327863],[1.196598183401639,1.232475434970147,1.3671181186374208,1.6005262344034599,1.9326997822682639,2.3636387622318336,2.8933431742941687,3.521813018455269,4.249048294715135,5.075049003073768],[2.667486278688523,3.1430511441003834,3.717381441611009,4.390477171220399,5.162338332928555,6.032964926735477,7.002356952641165,8.070514410645615,9.237437300748834,10.503125622950817],[6.490432658811471,7.405685138066682,8.419703049420658,9.532486392873402,10.74403516842491,12.054349376075182,13.463429015824218,14.971274087672022,16.57788459161859,18.283260527663924],[12.665437323770481,14.020377416869042,15.474082942066373,17.02655389936246,18.677790288757322,20.427792110250945,22.27655936384334,24.224092049534487,26.27039016732441,28.4154537172131],[21.19250027356557,22.98712798050748,24.880521119548167,26.87267969068761,28.96360369392582,31.153293129262796,33.44174799669854,35.828968296233036,38.314954027866314,40.89970519159835]],"type":"surface"},{"marker":{"color":"red","size":10},"name":"Optimal Point","x":[1.1111111111111112],"y":[0.09999999999999998],"z":[1.0463708825642581],"type":"scatter3d"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"scene":{"xaxis":{"title":{"text":"theta0"}},"yaxis":{"title":{"text":"theta1"}},"zaxis":{"title":{"text":"MSE"}}},"autosize":false,"width":800,"height":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('57892b1b-d7f1-4a00-8eaa-f11eeec839ad');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>We can also visualize a bird’s-eye view of the loss surface from above using a contour plot:</p>
<div id="99cdbd3d" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> go.Contour(x<span class="op">=</span>u[<span class="dv">0</span>], y<span class="op">=</span>v[:, <span class="dv">0</span>], z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(contour)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>, autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>                            <div id="c7aecfb1-c2ec-4f4e-ba64-88d78c3c4ccc" class="plotly-graph-div" style="height:600px; width:800px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c7aecfb1-c2ec-4f4e-ba64-88d78c3c4ccc")) {                    Plotly.newPlot(                        "c7aecfb1-c2ec-4f4e-ba64-88d78c3c4ccc",                        [{"x":[0.0,0.2222222222222222,0.4444444444444444,0.6666666666666666,0.8888888888888888,1.1111111111111112,1.3333333333333333,1.5555555555555554,1.7777777777777777,2.0],"y":[-0.1,-0.05000000000000001,-1.3877787807814457e-17,0.04999999999999999,0.09999999999999998,0.14999999999999997,0.19999999999999998,0.24999999999999997,0.29999999999999993,0.35],"z":[[29.123031979508195,26.96047116185995,24.896675776310463,22.931645822859743,21.065381301507795,19.297882212254603,17.629148555100183,16.059180330044526,14.587977537087637,13.21554017622951],[18.833628650614756,17.11075544680986,15.486647675103727,13.961305335496359,12.534728427987758,11.20691695257792,9.97787090926685,8.847590298054545,7.816075118941006,6.883325371926231],[10.896283606557377,9.613098016595831,8.428677858733053,7.343023132969036,6.356133839303786,5.468009977737301,4.678651548269582,3.9880585509006288,3.3962309856304405,2.903168852459017],[5.310996847336067,4.467498871217872,3.722766327198442,3.0767992152777786,2.5295975354558795,2.0811612877327472,1.7314904721083795,1.4805850885827772,1.3284451371559403,1.275070617827869],[2.0777683729508207,1.6739580106759773,1.3689130804998992,1.1626335824225869,1.05511951644404,1.0463708825642581,1.1363876807832418,1.325169911100991,1.6127175735175057,1.9990306680327863],[1.196598183401639,1.232475434970147,1.3671181186374208,1.6005262344034599,1.9326997822682639,2.3636387622318336,2.8933431742941687,3.521813018455269,4.249048294715135,5.075049003073768],[2.667486278688523,3.1430511441003834,3.717381441611009,4.390477171220399,5.162338332928555,6.032964926735477,7.002356952641165,8.070514410645615,9.237437300748834,10.503125622950817],[6.490432658811471,7.405685138066682,8.419703049420658,9.532486392873402,10.74403516842491,12.054349376075182,13.463429015824218,14.971274087672022,16.57788459161859,18.283260527663924],[12.665437323770481,14.020377416869042,15.474082942066373,17.02655389936246,18.677790288757322,20.427792110250945,22.27655936384334,24.224092049534487,26.27039016732441,28.4154537172131],[21.19250027356557,22.98712798050748,24.880521119548167,26.87267969068761,28.96360369392582,31.153293129262796,33.44174799669854,35.828968296233036,38.314954027866314,40.89970519159835]],"type":"contour"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"title":{"text":"theta0"}},"yaxis":{"title":{"text":"theta1"}},"autosize":false,"width":800,"height":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('c7aecfb1-c2ec-4f4e-ba64-88d78c3c4ccc');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<section id="the-gradient-vector" class="level4" data-number="14.1.1.1">
<h4 data-number="14.1.1.1" class="anchored" data-anchor-id="the-gradient-vector"><span class="header-section-number">14.1.1.1</span> The Gradient Vector</h4>
<p>As before, the derivative of the loss function tells us the best way towards the minimum value.</p>
<p>On a 2D (or higher) surface, the best way to go down (gradient) is described by a <em>vector</em>.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/loss_surface.png" alt="loss_surface" width="600">
</td>
</tr>
</tbody></table>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Math Aside: Partial Derivatives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>For an equation with multiple variables, we take a <strong>partial derivative</strong> by differentiating with respect to just one variable at a time. The partial derivative is denoted with a <span class="math inline">\(\partial\)</span>. Intuitively, we want to see how the function changes if we only vary one variable while holding other variables constant.</li>
<li>Using <span class="math inline">\(f(x, y) = 3x^2 + y\)</span> as an example,
<ul>
<li>taking the partial derivative with respect to x and treating y as a constant gives us <span class="math inline">\(\frac{\partial f}{\partial x} = 6x\)</span></li>
<li>taking the partial derivative with respect to y and treating x as a constant gives us <span class="math inline">\(\frac{\partial f}{\partial y} = 1\)</span></li>
</ul></li>
</ul>
</div>
</div>
<p>For the <em>vector</em> of parameter values <span class="math inline">\(\vec{\theta} = \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
         \end{bmatrix}\)</span>, we take the <em>partial derivative</em> of loss with respect to each parameter: <span class="math inline">\(\frac{\partial L}{\partial \theta_0}\)</span> and <span class="math inline">\(\frac{\partial L}{\partial \theta_1}\)</span>.</p>
<blockquote class="blockquote">
<p>For example, consider the 2D function: <span class="math display">\[f(\theta_0, \theta_1) = 8 \theta_0^2 + 3\theta_0\theta_1\]</span> For a function of 2 variables <span class="math inline">\(f(\theta_0, \theta_1)\)</span>, we define the gradient <span class="math display">\[
\begin{align}
\frac{\partial f}{\partial \theta_{0}} &amp;= 16\theta_0 + 3\theta_1 \\
\frac{\partial f}{\partial \theta_{1}} &amp;= 3\theta_0 \\
\nabla_{\vec{\theta}} f(\vec{\theta}) &amp;=  \begin{bmatrix} 16\theta_0 + 3\theta_1 \\ 3\theta_0 \\ \end{bmatrix}
\end{align}
\]</span></p>
</blockquote>
<p>The <strong>gradient vector</strong> of a generic function of <span class="math inline">\(p+1\)</span> variables is therefore <span class="math display">\[\nabla_{\vec{\theta}} L =  \begin{bmatrix} \frac{\partial L}{\partial \theta_0} \\ \frac{\partial L}{\partial \theta_1} \\ \vdots \\ \frac{\partial L}{\partial \theta_p} \end{bmatrix}\]</span> where <span class="math inline">\(- \nabla_\vec{\theta} L\)</span> always points in the downhill direction of the surface. We can interpret each gradient as: “If I nudge the <span class="math inline">\(i\)</span>th model weight, what happens to loss?”</p>
<p>We can use this to update our 1D gradient rule for models with multiple parameters.</p>
<ul>
<li><p>Recall our 1D update rule: <span class="math display">\[\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})\]</span></p></li>
<li><p>For models with multiple parameters, we work in terms of vectors: <span class="math display">\[\begin{bmatrix}
         \theta_{0}^{(t+1)} \\
         \theta_{1}^{(t+1)} \\
         \vdots
       \end{bmatrix} = \begin{bmatrix}
         \theta_{0}^{(t)} \\
         \theta_{1}^{(t)} \\
         \vdots
       \end{bmatrix} - \alpha \begin{bmatrix}
         \frac{\partial L}{\partial \theta_{0}} \\
         \frac{\partial L}{\partial \theta_{1}} \\
         \vdots \\
       \end{bmatrix}\]</span></p></li>
<li><p>Written in a more compact form, <span class="math display">\[\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)}) \]</span></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is a vector with our model weights</li>
<li><span class="math inline">\(L\)</span> is the loss function</li>
<li><span class="math inline">\(\alpha\)</span> is the learning rate (ours is constant, but other techniques use an <span class="math inline">\(\alpha\)</span> that decreases over time)</li>
<li><span class="math inline">\(\vec{\theta}^{(t)}\)</span> is the current value of <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(\vec{\theta}^{(t+1)}\)</span> is the next value of <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(\nabla_{\vec{\theta}} L(\vec{\theta}^{(t)})\)</span> is the gradient of the loss function evaluated at the current <span class="math inline">\(\theta\)</span>: <span class="math display">\[\frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))\]</span></li>
</ul></li>
</ul>
<p>Note that <span class="math inline">\(-\nabla_{\vec{\theta}} L\)</span> always points in the <strong>downhill direction</strong> of the surface.</p>
<p>Let’s now walk through an example of calculating and updating the gradient vector. Say our model and loss are: <span class="math display">\[\begin{align}
f_{\vec{\theta}}(\vec{x}) &amp;= \vec{x}^T\vec{\theta} = \theta_0x_0 + \theta_1x_1
\\l(y, \hat{y}) &amp;= (y - \hat{y})^2
\end{align}
\]</span></p>
<p>Plugging in <span class="math inline">\(f_{\vec{\theta}}(\vec{x})\)</span> for <span class="math inline">\(\hat{y}\)</span>, our loss function becomes <span class="math inline">\(l(\vec{\theta}, \vec{x}, y_i) = (y_i - \theta_0x_0 - \theta_1x_1)^2\)</span>.</p>
<p>To calculate our gradient vector, we can start by computing the partial derivative of the loss function with respect to <span class="math inline">\(\theta_0\)</span>: <span class="math display">\[\frac{\partial}{\partial \theta_{0}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_0)\]</span></p>
<p>Let’s now do the same but with respect to <span class="math inline">\(\theta_1\)</span>: <span class="math display">\[\frac{\partial}{\partial \theta_{1}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_1)\]</span></p>
<p>Putting this together, our gradient vector is: <span class="math display">\[\nabla_{\theta} l(\vec{\theta}, \vec{x}, y_i) =  \begin{bmatrix} -2(y_i - \theta_0x_0 - \theta_1x_1)(x_0) \\ -2(y_i - \theta_0x_0 - \theta_1x_1)(x_1) \end{bmatrix}\]</span></p>
<p>Remember that we need to keep updating <span class="math inline">\(\theta\)</span> until the algorithm <strong>converges</strong> to a solution and stops updating significantly (or at all). When updating <span class="math inline">\(\theta\)</span>, we’ll have a fixed number of updates and subsequent updates will be quite small (we won’t change <span class="math inline">\(\theta\)</span> by much).</p>
</section>
</section>
<section id="stochastic-mini-batch-gradient-descent" class="level3" data-number="14.1.2">
<h3 data-number="14.1.2" class="anchored" data-anchor-id="stochastic-mini-batch-gradient-descent"><span class="header-section-number">14.1.2</span> Stochastic (Mini-batch) Gradient Descent</h3>
<p>Formally, the algorithm we derived above is called <strong>batch gradient descent.</strong> For each iteration of the algorithm, the derivative of loss is computed across the <em>entire</em> batch of all <span class="math inline">\(n\)</span> datapoints. While this update rule works well in theory, it is not practical in most circumstances. For large datasets (with perhaps billions of datapoints), finding the gradient across all the data is incredibly computationally taxing; gradient descent will converge slowly because each individual update is slow.</p>
<p><strong>Stochastic (mini-batch) gradient descent</strong> tries to address this issue. In stochastic descent, only a <em>sample</em> of the full dataset is used at each update. We estimate the true gradient of the loss surface using just that sample of data. The <strong>batch size</strong> is the number of data points used in each sample. The sampling strategy is generally without replacement (data is shuffled and batch size examples are selected one at a time.)</p>
<p>Each complete “pass” through the data is known as a <strong>training epoch</strong>. After shuffling the data, in a single <strong>training epoch</strong> of stochastic gradient descent, we</p>
<ul>
<li>Compute the gradient on the first x% of the data. Update the parameter guesses.</li>
<li>Compute the gradient on the next x% of the data. Update the parameter guesses.</li>
<li><span class="math inline">\(\dots\)</span></li>
<li>Compute the gradient on the last x% of the data. Update the parameter guesses.</li>
</ul>
<p>Every data point appears once in a single training epoch. We then perform several training epochs until we’re satisfied.</p>
<p>Batch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for <span class="math inline">\(\vec{\theta}\)</span> at each iteration, there’s a chance the algorithm will not progress towards the true minimum of loss with each update. Over the longer term, these stochastic techniques should still converge towards the optimal solution.</p>
<p>The diagrams below represent a “bird’s eye view” of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal <span class="math inline">\(\hat{\theta}\)</span>. Stochastic gradient descent, in contrast, “hops around” on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step.</p>
<div data-align="middle">
<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/stochastic.png" alt="stochastic" width="600">
</td>
</tr>
</tbody></table>
</div>
</section>
<section id="batch-mini-batch-stochastic-gradient-descent" class="level3" data-number="14.1.3">
<h3 data-number="14.1.3" class="anchored" data-anchor-id="batch-mini-batch-stochastic-gradient-descent"><span class="header-section-number">14.1.3</span> Batch, Mini-batch, Stochastic Gradient Descent</h3>
<p>The time complexity of a single gradient descent step takes only <span class="math inline">\(O(nd)\)</span> time where <span class="math inline">\(n\)</span> is the number of samples (rows) and <span class="math inline">\(d\)</span> is the number of features (columns).</p>
<center>
<img src="images/complexity_grad_descent.png" alt="complexity_grad_descent" width="600">
</center>
<p>Suppose we run <span class="math inline">\(T\)</span> iterations. The final complexity would then be <span class="math inline">\(O(Tnd)\)</span>. Typically, <span class="math inline">\(n\)</span> is much larger than <span class="math inline">\(T\)</span> and <span class="math inline">\(d\)</span>. How can we reduce the cost of this algorithm using a technique from Data 100? Do we really need to use <span class="math inline">\(n\)</span> data points? We don’t! Instead, we can use stochastic gradient descent.</p>
<p>We know that our true gradient of <span class="math inline">\(\nabla_{\vec{\theta}} L (\vec{\theta^{(t)}}) = \frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))\)</span> has a time complexity of <span class="math inline">\(O(nd)\)</span>. Instead of using all <span class="math inline">\(n\)</span> samples to calculate the true gradient of the loss surface, let’s use a <strong>probability sample</strong> of our data to <strong>approximate the gradient</strong>!</p>
<p>Say we sample <span class="math inline">\(b\)</span> records (<span class="math inline">\(s_1, \cdots, s_b\)</span>) from our <span class="math inline">\(n\)</span> datapoints. Our new (stochastic) gradient descent function will be: <span class="math display">\[\nabla_{\vec{\theta}} L (\vec{\theta}^{(t)}) \approx \frac{1}{\textcolor{red}{b}}\sum_{i=1}^{\textcolor{red}{b}}\nabla_{\vec{\theta}} l(y_{\textcolor{red}{s_i}}, f_{\vec{\theta}^{(t)}}(X_{\textcolor{red}{s_i}}))\]</span> and will now have a time complexity of <span class="math inline">\(O(bd)\)</span>, which is much faster! For more on computational complexity, see the bonus section at the end.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>How do we decide our mini-batch size (<span class="math inline">\(b\)</span>), or the number of datapoints in our sample? Mini-batch size is typically <strong>small</strong>.</p>
<p>The original Stochastic Gradient Descent algorithm uses <span class="math inline">\(b=1\)</span> so that <em>only one sample is used</em> to approximate the gradient at a time. When choosing <span class="math inline">\(b\)</span>, there are several factors to consider: - larger <span class="math inline">\(b\)</span>: better <em>gradient estimates</em>, <em>parallelism</em>, and other <em>systems factors</em>, but also rapidly diminishing returns with more complexity - smaller <span class="math inline">\(b\)</span>: faster and has <em>more frequent updates</em></p>
<p>It is up to data scientists to balance the tradeoff between batch size and time complexity.</p>
<p>Stochastic gradient descent helps us <em>estimate</em> the true gradient of the loss surface using just that sample of data, and this allows us to compute gradients quicker. To sample data, there are two sampling strategies we can use:</p>
<ol type="1">
<li><strong>Shuffle the data</strong> and select batch size records at a time.</li>
<li>Take a <strong>simple random sample</strong> for each gradient computation.</li>
</ol>
</div>
</div>
<p>Summarizing our two gradient descent techniques:</p>
<ul>
<li><strong>(Batch) Gradient Descent</strong>: GD computes the <strong>true</strong> descent and always descends towards the true minimum of the loss based on a batch (a full dataset). While accurate, it can often be computationally expensive.</li>
</ul>
<center>
<img src="images/gd.png" alt="batch_grad_descent" width="300">
</center>
<ul>
<li><strong>(Mini-batch) Stochastic Gradient Descent</strong>: SGD <strong>approximates</strong> the true gradient descent based off of mini-batch (a subset of a dataset). It may not descend towards the true minimum with each update, but it’s often less computationally expensive than batch gradient descent. Note that it is mini-batch, because we create a smaller, randomized sample of the batch to run SGD on it.</li>
</ul>
<center>
<img src="images/sgd.png" alt="stochastic_grad_descent" width="300">
</center>
<p>To summarize the tradeoffs of batch size:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Smaller Batch Size</th>
<th>Larger Batch Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Pros</strong></td>
<td>More frequent gradient updates</td>
<td>Leverage hardware acceleration to improve overall system performance and higher quality gradient updates</td>
</tr>
<tr class="even">
<td><strong>Cons</strong></td>
<td>More variability in the gradient estimates</td>
<td>Less frequent gradient updates</td>
</tr>
</tbody>
</table>
<p>The typical solution is to set batch size to ensure sufficient hardware utilization.</p>
</section>
</section>
<section id="feature-engineering" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="feature-engineering"><span class="header-section-number">14.2</span> Feature Engineering</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Motivation
</div>
</div>
<div class="callout-body-container callout-body">
<p>At this point in the course, we’ve equipped ourselves with some powerful techniques to build and optimize models. We’ve explored how to develop models of multiple variables, as well as how to transform variables to help <strong>linearize</strong> a dataset and fit these models to maximize their performance.</p>
<p>All of this was done with one major caveat: the regression models we’ve worked with so far are all <strong>linear in the input variables</strong>. We’ve assumed that our predictions should be some combination of linear variables. While this works well in some cases, the real world isn’t always so straightforward. We’ll learn an important method to address this issue – feature engineering – and consider some new problems that can arise when we do so.</p>
</div>
</div>
<p>Feature engineering is the process of <em>transforming</em> raw features into <em>more informative features</em> that can be used in modeling or EDA tasks and improve model performance.</p>
<p>Feature engineering allows you to:</p>
<ul>
<li>Capture domain knowledge</li>
<li>Express non-linear relationships using linear models</li>
<li>Use non-numeric (qualitative) features in models</li>
</ul>
<section id="feature-functions" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="feature-functions"><span class="header-section-number">14.2.1</span> Feature Functions</h3>
<p>A <strong>feature function</strong> describes the transformations we apply to raw features in a dataset to create a design matrix of transformed features (typically denoted as <span class="math inline">\(\Phi\)</span>). When we apply the feature function to our original dataset <span class="math inline">\(\mathbb{X}\)</span>, the result, <span class="math inline">\(\Phi(\mathbb{X})\)</span>, is a transformed design matrix ready to be used in modeling.</p>
<p>For example, we might design a feature function that computes the square of an existing feature and adds it to the design matrix. In this case, our existing matrix <span class="math inline">\([x]\)</span> is transformed to <span class="math inline">\([x, x^2]\)</span>. Its <em>dimension</em> increases from 1 to 2. Often, the dimension of the <em>featurized</em> dataset increases as seen here.</p>
<center>
<img src="images/phi.png" alt="phi" width="700">
</center>
<p>The new features introduced by the feature function can then be used in modeling. Often, we use the symbol <span class="math inline">\(\phi_i\)</span> to represent transformed features after feature engineering.</p>
<p><span class="math display">\[
\begin{align}
\hat{y} &amp;= \theta_0 + \theta_1 x + \theta_2 x^2 \\
\hat{y} &amp;= \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2
\end{align}
\]</span></p>
<p>In matrix notation, the symbol <span class="math inline">\(\Phi\)</span> is sometimes used to denote the design matrix after feature engineering has been performed. Note that in the usage below, <span class="math inline">\(\Phi\)</span> is now a feature-engineered matrix, rather than a function.</p>
<p><span class="math display">\[\hat{\mathbb{Y}} = \Phi \theta\]</span></p>
<p>More formally, we describe a feature function as transforming the original <span class="math inline">\(\mathbb{R}^{n \times p}\)</span> dataset <span class="math inline">\(\mathbb{X}\)</span> to a featurized <span class="math inline">\(\mathbb{R}^{n \times p'}\)</span> dataset <span class="math inline">\(\mathbb{\Phi}\)</span>, where <span class="math inline">\(p'\)</span> is typically greater than <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[\mathbb{X} \in \mathbb{R}^{n \times p} \longrightarrow \Phi \in \mathbb{R}^{n \times p'}\]</span></p>
</section>
<section id="one-hot-encoding" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="one-hot-encoding"><span class="header-section-number">14.2.2</span> One-Hot Encoding</h3>
<p>A particularly powerful use of feature engineering is to allow us to perform regression on non-numeric features. <strong>One-hot encoding</strong> is a feature engineering technique that generates numeric features from categorical data, allowing us to use our usual methods to fit a regression model on the data.</p>
<p>To illustrate how this works, we’ll refer back to the <code>tips</code> dataset from previous lectures. Consider the <code>"day"</code> column of the dataset:</p>
<div id="b1b17866" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>tips <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tips.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">total_bill</th>
<th data-quarto-table-cell-role="th">tip</th>
<th data-quarto-table-cell-role="th">sex</th>
<th data-quarto-table-cell-role="th">smoker</th>
<th data-quarto-table-cell-role="th">day</th>
<th data-quarto-table-cell-role="th">time</th>
<th data-quarto-table-cell-role="th">size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>16.99</td>
<td>1.01</td>
<td>Female</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>10.34</td>
<td>1.66</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>21.01</td>
<td>3.50</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>23.68</td>
<td>3.31</td>
<td>Male</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>24.59</td>
<td>3.61</td>
<td>Female</td>
<td>No</td>
<td>Sun</td>
<td>Dinner</td>
<td>4</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>At first glance, it doesn’t seem possible to fit a regression model to this non-numeric data – we can’t directly perform any mathematical operations on the entry “Sun”.</p>
<p>To resolve this, we instead create a new table with a feature for each unique value in the original <code>"day"</code> column. We then iterate through the <code>"day"</code> column. For each entry in <code>"day"</code> we fill the corresponding feature in the new table with 1. All other features are set to 0.</p>
<center>
<img src="images/ohe.png" alt="ohe" width="600">
</center>
<p><br></p>
In short, each category of a categorical variable gets its own feature
<ul>
<li>
Value = 1 if a row belongs to the category
</li>
<li>
Value = 0 otherwise
</li>
</ul>
<p>The <code>OneHotEncoder</code> class of <code>sklearn</code> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out">documentation</a>) offers a quick way to perform this one-hot encoding. You will explore its use in detail in the lab. For now, recognize that we follow a very similar workflow to when we were working with the <code>LinearRegression</code> class: we initialize a <code>OneHotEncoder</code> object, fit it to our data, and finally use <code>.transform()</code> to apply the fitted encoder.</p>
<div id="a75c8d53" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a OneHotEncoder object</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>ohe <span class="op">=</span> OneHotEncoder()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the encoder</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>ohe.fit(tips[[<span class="st">"day"</span>]])</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the encoder to transform the raw "day" feature</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>encoded_day <span class="op">=</span> ohe.transform(tips[[<span class="st">"day"</span>]]).toarray()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>encoded_day_df <span class="op">=</span> pd.DataFrame(encoded_day, columns<span class="op">=</span>ohe.get_feature_names_out())</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>encoded_day_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">day_Fri</th>
<th data-quarto-table-cell-role="th">day_Sat</th>
<th data-quarto-table-cell-role="th">day_Sun</th>
<th data-quarto-table-cell-role="th">day_Thur</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The one-hot encoded features can then be used in the design matrix to train a model:</p>
<center>
<img src="images/ohemodel.png" alt="ohemodel" width="600">
</center>
<p><span class="math display">\[\hat{y} = \theta_1 (\text{total}\_\text{bill}) + \theta_2 (\text{size}) + \theta_3 (\text{day}\_\text{Fri}) + \theta_4 (\text{day}\_\text{Sat}) + \theta_5 (\text{day}\_\text{Sun}) + \theta_6 (\text{day}\_\text{Thur})\]</span></p>
<p>Or in shorthand:</p>
<p><span class="math display">\[\hat{y} = \theta_{1}\phi_{1} + \theta_{2}\phi_{2} + \theta_{3}\phi_{3} + \theta_{4}\phi_{4} + \theta_{5}\phi_{5} + \theta_{6}\phi_{6}\]</span></p>
<p>Now, the <code>day</code> feature (or rather, the four new boolean features that represent day) can be used to fit a model.</p>
<p>Using <code>sklearn</code> to fit the new model, we can determine the model coefficients, allowing us to understand how each feature impacts the predicted tip.</p>
<div id="d3ff3dc3" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>data_w_ohe <span class="op">=</span> tips[[<span class="st">"total_bill"</span>, <span class="st">"size"</span>, <span class="st">"day"</span>]].join(encoded_day_df).drop(columns <span class="op">=</span> <span class="st">"day"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>ohe_model <span class="op">=</span> lm.LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>) <span class="co">#Tell sklearn to not add an additional bias column. Why?</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>ohe_model.fit(data_w_ohe, tips[<span class="st">"tip"</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Feature"</span>:data_w_ohe.columns, <span class="st">"Model Coefficient"</span>:ohe_model.coef_})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Feature</th>
<th data-quarto-table-cell-role="th">Model Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>total_bill</td>
<td>0.092994</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>size</td>
<td>0.187132</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>day_Fri</td>
<td>0.745787</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>day_Sat</td>
<td>0.621129</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>day_Sun</td>
<td>0.732289</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>day_Thur</td>
<td>0.668294</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>For example, when looking at the coefficient for <code>day_Fri</code>, we can now understand the impact of it being Friday on the predicted tip — if it is a Friday, the predicted tip increases by approximately $0.75.</p>
<p>When one-hot encoding, keep in mind that any set of one-hot encoded columns will always sum to a column of all ones, representing the bias column. More formally, the bias column is a linear combination of the OHE columns.</p>
<center>
<img src="images/bias.png" alt="bias" width="600">
</center>
<p>We must be careful not to include this bias column in our design matrix. Otherwise, there will be linear dependence in the model, meaning <span class="math inline">\(\mathbb{X}^{\top}\mathbb{X}\)</span> would no longer be invertible, and our OLS estimate <span class="math inline">\(\hat{\theta} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}\)</span> fails.</p>
<p>To resolve this issue, we simply omit one of the one-hot encoded columns <em>or</em> do not include an intercept term. The adjusted design matrices are shown below.</p>
<center>
<img src="images/remove.png" alt="remove" width="600">
</center>
<p>Either approach works — we still retain the same information as the omitted column being a linear combination of the remaining columns.</p>
</section>
<section id="polynomial-features" class="level3" data-number="14.2.3">
<h3 data-number="14.2.3" class="anchored" data-anchor-id="polynomial-features"><span class="header-section-number">14.2.3</span> Polynomial Features</h3>
<p>We have encountered a few cases now where models with linear features have performed poorly on datasets that show clear <em>non-linear</em> curvature.</p>
<p>As an example, consider the <code>vehicles</code> dataset, which contains information about cars. Suppose we want to use the <code>hp</code> (horsepower) of a car to predict its <code>"mpg"</code> (gas mileage in miles per gallon). If we visualize the relationship between these two variables, we see a non-linear curvature. Fitting a linear model to these variables results in a high (poor) value of RMSE.</p>
<p><span class="math display">\[\hat{y} = \theta_0 + \theta_1 (\text{hp})\]</span></p>
<div id="eaf4e587" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>pd.options.mode.chained_assignment <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>vehicles <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna().rename(columns <span class="op">=</span> {<span class="st">"horsepower"</span>: <span class="st">"hp"</span>}).sort_values(<span class="st">"hp"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vehicles[[<span class="st">"hp"</span>]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> vehicles[<span class="st">"mpg"</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>hp_model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>hp_model.fit(X, Y)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>hp_model_predictions <span class="op">=</span> hp_model.predict(X)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>vehicles, x<span class="op">=</span><span class="st">"hp"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>plt.plot(vehicles[<span class="st">"hp"</span>], hp_model_predictions, c<span class="op">=</span><span class="st">"tab:red"</span>)<span class="op">;</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE of model with (hp) feature: </span><span class="sc">{</span>np<span class="sc">.</span>mean((Y<span class="op">-</span>hp_model_predictions)<span class="op">**</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MSE of model with (hp) feature: 23.943662938603108</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="feature_engineering_files/figure-html/cell-8-output-2.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As we can see from the plot, the data follows a curved line rather than a straight one. To capture this non-linearity, we can incorporate <strong>non-linear</strong> features. Let’s introduce a <strong>polynomial</strong> term, <span class="math inline">\(\text{hp}^2\)</span>, into our regression model. The model now takes the form:</p>
<p><span class="math display">\[\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)\]</span> <span class="math display">\[\hat{y} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2\]</span></p>
<p>How can we fit a model with non-linear features? We can use the exact same techniques as before: ordinary least squares, gradient descent, or <code>sklearn</code>. This is because our new model is still a <strong>linear model</strong>. Although it contains non-linear <em>features</em>, it is linear with respect to the model <em>parameters</em>. All of our previous work on fitting models was done under the assumption that we were working with linear models. Because our new model is still linear, we can apply our existing methods to determine the optimal parameters.</p>
<div id="79092436" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a hp^2 feature to the design matrix</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vehicles[[<span class="st">"hp"</span>]]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"hp^2"</span>] <span class="op">=</span> vehicles[<span class="st">"hp"</span>]<span class="op">**</span><span class="dv">2</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Use sklearn to fit the model</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>hp2_model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>hp2_model.fit(X, Y)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>hp2_model_predictions <span class="op">=</span> hp2_model.predict(X)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>vehicles, x<span class="op">=</span><span class="st">"hp"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.plot(vehicles[<span class="st">"hp"</span>], hp2_model_predictions, c<span class="op">=</span><span class="st">"tab:red"</span>)<span class="op">;</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE of model with (hp^2) feature: </span><span class="sc">{</span>np<span class="sc">.</span>mean((Y<span class="op">-</span>hp2_model_predictions)<span class="op">**</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE of model with (hp^2) feature: 18.98476890761722</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="feature_engineering_files/figure-html/cell-9-output-2.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking a lot better! By incorporating a squared feature, we are able to capture the curvature of the dataset. Our model is now a parabola centered on our data. Notice that our new model’s error has decreased relative to the original model with linear features.</p>
</section>
<section id="complexity-and-overfitting" class="level3" data-number="14.2.4">
<h3 data-number="14.2.4" class="anchored" data-anchor-id="complexity-and-overfitting"><span class="header-section-number">14.2.4</span> Complexity and Overfitting</h3>
<p>We’ve seen now that feature engineering allows us to build all sorts of features to improve the performance of the model. In particular, we saw that designing a more complex feature (squaring <code>hp</code> in the <code>vehicles</code> data previously) substantially improved the model’s ability to capture non-linear relationships. To take full advantage of this, we might be inclined to design increasingly complex features. Consider the following three models, each of different order (the maximum exponent power of each model):</p>
<ul>
<li>Model with order 2: <span class="math inline">\(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)\)</span></li>
<li>Model with order 3: <span class="math inline">\(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3)\)</span></li>
<li>Model with order 4: <span class="math inline">\(\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3) + \theta_4 (\text{hp}^4)\)</span></li>
</ul>
<center>
<img src="images/degree_comparison.png" alt="degree_comparison" width="900">
</center>
<p>As we can see in the plots above, MSE continues to decrease with each additional polynomial term. To visualize it further, let’s plot models as the complexity increases from 0 to 7:</p>
<center>
<img src="images/degree_comparison2.png" alt="degree_comparison" width="850">
</center>
<p>When we use our model to make predictions on the same data that was used to fit the model, we find that the MSE decreases with each additional polynomial term (as our model gets more complex). The <strong>training error</strong> is the model’s error when generating predictions from the same data that was used for training purposes. We can conclude that the training error goes down as the complexity of the model increases.</p>
<center>
<img src="images/train_error.png" alt="train_error" width="400">
</center>
<p>This seems like good news – when working on the <strong>training data</strong>, we can improve model performance by designing increasingly complex models.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Math Fact: Polynomial Degrees
</div>
</div>
<div class="callout-body-container callout-body">
<p>Given <span class="math inline">\(N\)</span> overlapping data points, we can always find a polynomial of degree <span class="math inline">\(N-1\)</span> that goes through all those points.</p>
For example, there always exists a degree-4 polynomial curve that can perfectly model a dataset of 5 datapoints:
<center>
<img src="images/perfect_poly_fits.png" alt="train_error" width="600">
</center>
</div>
</div>
<p>However, high model complexity comes with its own set of issues. When building the <code>vehicles</code> models above, we trained the models on the <em>entire</em> dataset and then evaluated their performance on this same dataset. In reality, we are likely to instead train the model on a <em>sample</em> from the population, then use it to make predictions on data it didn’t encounter during training.</p>
<p>Let’s walk through a more realistic example. Say we are given a training dataset of just 6 datapoints and want to train a model to then make predictions on a <em>different</em> set of points. We may be tempted to make a highly complex model (e.g., degree 5), especially given it makes perfect predictions on the training data as clear on the left. However, as shown in the graph on the right, this model would perform <em>horribly</em> on the rest of the population!</p>
<center>
<img src="images/complex.png" alt="complex" width="600">
</center>
<p>This phenomenon called <strong>overfitting</strong>. The model effectively just memorized the training data it encountered when it was fitted, leaving it unable to <strong>generalize</strong> well to data it didn’t encounter during training. This is a problem: we want models that are generalizable to “unseen” data.</p>
<p>Additionally, since complex models are sensitive to the specific dataset used to train them, they have high <strong>variance</strong>. A model with high variance tends to <em>vary</em> more dramatically when trained on different datasets. Going back to our example above, we can see our degree-5 model varies erratically when we fit it to different samples of 6 points from <code>vehicles</code>.</p>
<center>
<img src="images/resamples.png" alt="resamples" width="800">
</center>
<p>We now face a dilemma: we know that we can <strong>decrease training error</strong> by increasing model complexity, but models that are <em>too</em> complex start to overfit and can’t be reapplied to new datasets due to <strong>high variance</strong>.</p>
<center>
<img src="images/bvt.png" alt="bvt" width="400">
</center>
<p>We can see that there is a clear trade-off that comes from the complexity of our model. As model complexity increases, the model’s error on the training data decreases. At the same time, the model’s variance tends to increase.</p>
<p>The takeaway here: we need to strike a balance in the complexity of our models; we want models that are generalizable to “unseen” data. A model that is too simple won’t be able to capture the key relationships between our variables of interest; a model that is too complex runs the risk of overfitting.</p>
<p>This begs the question: how do we control the complexity of a model? Stay tuned for Lecture 16 on Cross-Validation and Regularization!</p>
</section>
</section>
<section id="bonus-time-complexity-of-the-normal-equation" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="bonus-time-complexity-of-the-normal-equation"><span class="header-section-number">14.3</span> [Bonus] Time Complexity of the Normal Equation</h2>
<p>Let’s now dive deeper into gradient and stochastic gradient descent. We discussed earlier how finding the gradient across all the data is extremeley computationally taxing and takes a lot of resources to calculate. Here we’ll explore why this is true.</p>
<p>We know that the solution to the normal equation is <span class="math inline">\(\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\)</span>. Let’s break this down and determine the computational complexity for this solution.</p>
<center>
<img src="images/complexity_normal_solution.png" alt="complexity_normal_solution" width="600">
</center>
<p>Let <span class="math inline">\(n\)</span> be the number of samples (rows) and <span class="math inline">\(d\)</span> be the number of features (columns).</p>
<ul>
<li>Computing <span class="math inline">\((\mathbb{X}^{\top}\mathbb{X})\)</span> takes <span class="math inline">\(O(nd^2)\)</span> time, and it’s inverse takes another <span class="math inline">\(O(d^3)\)</span> time to calculate; overall, <span class="math inline">\((\mathbb{X}^{\top}\mathbb{X})^{-1}\)</span> takes <span class="math inline">\(O(nd^2) + O(d^3)\)</span> time.</li>
<li><span class="math inline">\(\mathbb{X}^{\top}\mathbb{Y}\)</span> takes <span class="math inline">\(O(nd)\)</span> time.</li>
<li>Multiplying <span class="math inline">\((\mathbb{X}^{\top}\mathbb{X})^{-1}\)</span> and <span class="math inline">\(\mathbb{X}^{\top}\mathbb{Y}\)</span> takes <span class="math inline">\(O(d^2)\)</span> time.</li>
</ul>
<p>In total, calculating the solution to the normal equation takes <span class="math inline">\(O(nd^2) + O(d^3) + O(nd) + O(d^2)\)</span> time. We can see that <span class="math inline">\(O(nd^2) + O(d^3)\)</span> dominates the complexity — this can be problematic for high-dimensional models and very large datasets.</p>
</section>
<section id="bonus-stochastic-gradient-descent-in-pytorch" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="bonus-stochastic-gradient-descent-in-pytorch"><span class="header-section-number">14.4</span> [Bonus] Stochastic Gradient Descent in <code>PyTorch</code></h2>
<p>While this material is out of scope for Data 100, it is useful if you plan to enter a career in data science!</p>
<p>In practice, you will use software packages such as <code>PyTorch</code> when computing gradients and implementing gradient descent. You’ll often follow three main steps:</p>
<ol type="1">
<li>Sample a batch of the data.</li>
<li>Compute the loss and the gradient.</li>
<li>Update your gradient until you reach an appropriate estimate of the true gradient.</li>
</ol>
<center>
<img src="images/pytorchsgd.png" alt="pytorch_sgd" width="500">
</center>
<p>If you want to learn more, this <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">Intro to PyTorch tutorial</a> is a great resource to get started!</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../gradient_descent/gradient_descent.html" class="pagination-link" aria-label="sklearn and Gradient Descent">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../case_study_HCE/case_study_HCE.html" class="pagination-link" aria-label="Case Study in Human Contexts and Ethics">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb11" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Gradient Descent Continuation, Feature Engineering</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Gradient Descent Continuation, Feature Engineering</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.16.1</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Recognize the value of feature engineering as a tool to improve model performance</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Implement polynomial feature generation and one hot encoding</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Understand the interactions between model complexity, model variance, and training error</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>At this point, we've grown quite familiar with the modeling process. We've introduced the concept of loss, used it to fit several types of models, and, most recently, extended our analysis to multiple regression. Along the way, we've forged our way through the mathematics of deriving the optimal model parameters in all its gory detail. It's time to make our lives a little easier – let's implement the modeling process in code!</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>In this lecture, we'll explore two techniques for model fitting:</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Translating our derived formulas for regression to <span class="in">`python`</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using <span class="in">`python`</span>'s <span class="in">`sklearn`</span> package</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>With our new programming frameworks in hand, we will also add sophistication to our models by introducing more complex features to enhance model performance. </span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient Descent Part 2</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>Before we dive into feature engineering, let's expand on gradient descent, which we began covering in the last lecture. Recall that gradient descent is a powerful technique for choosing the model parameters that minimize the loss function.  </span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent on Multi-Dimensional Models</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>The function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, $\theta$. However, models usually have a cost function with multiple parameters that need to be optimized. For example, simple linear regression has 2 parameters: </span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>$$\hat{y} + \theta_0 + \theta_1x$$ </span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>and multiple linear regression has $p+1$ parameters: </span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>$$\mathbb{Y} = \theta_0 + \theta_1 \Bbb{X}_{:,1} + \theta_2 \Bbb{X}_{:,2} + \cdots + \theta_p \Bbb{X}_{:,p}$$</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>We'll need to expand gradient descent so we can update our guesses for all model parameters all in one go.</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>With multiple parameters to optimize, we consider a **loss surface**, or the model's loss for a particular *combination* of possible parameter values.</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss(theta, X, y_obs):</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> X <span class="op">@</span> theta</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y_hat <span class="op">-</span> y_obs) <span class="op">**</span> <span class="dv">2</span>)    </span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>tips_with_bias <span class="op">=</span> df.copy()</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>tips_with_bias[<span class="st">"bias"</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>tips_with_bias <span class="op">=</span> tips_with_bias[[<span class="st">"bias"</span>, <span class="st">"total_bill"</span>]]</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>uvalues <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">10</span>)</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>vvalues <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.35</span>, <span class="dv">10</span>)</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>(u,v) <span class="op">=</span> np.meshgrid(uvalues, vvalues)</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.vstack((u.flatten(),v.flatten()))</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_loss_single_arg(theta):</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_loss(theta, tips_with_bias, df[<span class="st">"tip"</span>])</span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>MSE <span class="op">=</span> np.array([mse_loss_single_arg(t) <span class="cf">for</span> t <span class="kw">in</span> thetas.T])</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a>loss_surface <span class="op">=</span> go.Surface(x<span class="op">=</span>u, y<span class="op">=</span>v, z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>ind <span class="op">=</span> np.argmin(MSE)</span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a>optimal_point <span class="op">=</span> go.Scatter3d(name <span class="op">=</span> <span class="st">"Optimal Point"</span>,</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [thetas.T[ind,<span class="dv">0</span>]], y <span class="op">=</span> [thetas.T[ind,<span class="dv">1</span>]], </span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> [MSE[ind]],</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>    marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">10</span>, color<span class="op">=</span><span class="st">"red"</span>))</span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(data<span class="op">=</span>[loss_surface, optimal_point])</span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>fig.update_layout(scene <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>,</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a>    zaxis_title <span class="op">=</span> <span class="st">"MSE"</span>), autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>We can also visualize a bird's-eye view of the loss surface from above using a contour plot:</span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a>contour <span class="op">=</span> go.Contour(x<span class="op">=</span>u[<span class="dv">0</span>], y<span class="op">=</span>v[:, <span class="dv">0</span>], z<span class="op">=</span>np.reshape(MSE, u.shape))</span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> go.Figure(contour)</span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a>    xaxis_title <span class="op">=</span> <span class="st">"theta0"</span>,</span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a>    yaxis_title <span class="op">=</span> <span class="st">"theta1"</span>, autosize<span class="op">=</span><span class="va">False</span>, width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Gradient Vector</span></span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a>As before, the derivative of the loss function tells us the best way towards the minimum value.</span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a>On a 2D (or higher) surface, the best way to go down (gradient) is described by a *vector*.</span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a>&lt;div align="middle"&gt;</span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a>  &lt;table style="width:100%"&gt;</span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a>    &lt;tr align="center"&gt;</span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a>      &lt;td&gt;&lt;img src="images/loss_surface.png" alt='loss_surface' width='600'&gt;</span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a>      &lt;/td&gt;</span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a>    &lt;/tr&gt;</span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a>  &lt;/table&gt;</span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a>&lt;/div&gt;</span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a><span class="fu">### Math Aside: Partial Derivatives </span></span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For an equation with multiple variables, we take a **partial derivative** by differentiating with respect to just one variable at a time. The partial derivative is denoted with a $\partial$. Intuitively, we want to see how the function changes if we only vary one variable while holding other variables constant. </span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using $f(x, y) = 3x^2 + y$ as an example,</span>
<span id="cb11-144"><a href="#cb11-144" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>taking the partial derivative with respect to x and treating y as a constant gives us $\frac{\partial f}{\partial x} = 6x$</span>
<span id="cb11-145"><a href="#cb11-145" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>taking the partial derivative with respect to y and treating x as a constant gives us $\frac{\partial f}{\partial y} = 1$</span>
<span id="cb11-146"><a href="#cb11-146" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb11-147"><a href="#cb11-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a>For the *vector* of parameter values $\vec{\theta} = \begin{bmatrix}</span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a>           \theta_{0} <span class="sc">\\</span></span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a>           \theta_{1} <span class="sc">\\</span></span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}$, we take the *partial derivative* of loss with respect to each parameter: $\frac{\partial L}{\partial \theta_0}$ and $\frac{\partial L}{\partial \theta_1}$.</span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For example, consider the 2D function: $$f(\theta_0, \theta_1) = 8 \theta_0^2 + 3\theta_0\theta_1$$</span></span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For a function of 2 variables $f(\theta_0, \theta_1)$, we define the gradient </span></span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a><span class="at">$$</span></span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a><span class="at">\begin{align}</span></span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a><span class="at">\frac{\partial f}{\partial \theta_{0}} &amp;= 16\theta_0 + 3\theta_1 </span><span class="sc">\\</span></span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a><span class="at">\frac{\partial f}{\partial \theta_{1}} &amp;= 3\theta_0 </span><span class="sc">\\</span></span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a><span class="at">\nabla_{\vec{\theta}} f(\vec{\theta}) &amp;=  \begin{bmatrix} 16\theta_0 + 3\theta_1 </span><span class="sc">\\</span><span class="at"> 3\theta_0 </span><span class="sc">\\</span><span class="at"> \end{bmatrix}</span></span>
<span id="cb11-160"><a href="#cb11-160" aria-hidden="true" tabindex="-1"></a><span class="at">\end{align}</span></span>
<span id="cb11-161"><a href="#cb11-161" aria-hidden="true" tabindex="-1"></a><span class="at">$$</span></span>
<span id="cb11-162"><a href="#cb11-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-163"><a href="#cb11-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-164"><a href="#cb11-164" aria-hidden="true" tabindex="-1"></a>The **gradient vector** of a generic function of $p+1$ variables is therefore </span>
<span id="cb11-165"><a href="#cb11-165" aria-hidden="true" tabindex="-1"></a>$$\nabla_{\vec{\theta}} L =  \begin{bmatrix} \frac{\partial L}{\partial \theta_0} <span class="sc">\\</span> \frac{\partial L}{\partial \theta_1} <span class="sc">\\</span> \vdots <span class="sc">\\</span> \frac{\partial L}{\partial \theta_p} \end{bmatrix}$$</span>
<span id="cb11-166"><a href="#cb11-166" aria-hidden="true" tabindex="-1"></a>where $- \nabla_\vec{\theta} L$ always points in the downhill direction of the surface. We can interpret each gradient as: "If I nudge the $i$th model weight, what happens to loss?"</span>
<span id="cb11-167"><a href="#cb11-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-168"><a href="#cb11-168" aria-hidden="true" tabindex="-1"></a>We can use this to update our 1D gradient rule for models with multiple parameters. </span>
<span id="cb11-169"><a href="#cb11-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-170"><a href="#cb11-170" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Recall our 1D update rule: $$\theta^{(t+1)} = \theta^{(t)} - \alpha \frac{d}{d\theta}L(\theta^{(t)})$$ </span>
<span id="cb11-171"><a href="#cb11-171" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For models with multiple parameters, we work in terms of vectors:</span>
<span id="cb11-172"><a href="#cb11-172" aria-hidden="true" tabindex="-1"></a>$$\begin{bmatrix}</span>
<span id="cb11-173"><a href="#cb11-173" aria-hidden="true" tabindex="-1"></a>           \theta_{0}^{(t+1)} <span class="sc">\\</span></span>
<span id="cb11-174"><a href="#cb11-174" aria-hidden="true" tabindex="-1"></a>           \theta_{1}^{(t+1)} <span class="sc">\\</span></span>
<span id="cb11-175"><a href="#cb11-175" aria-hidden="true" tabindex="-1"></a>           \vdots</span>
<span id="cb11-176"><a href="#cb11-176" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix} = \begin{bmatrix}</span>
<span id="cb11-177"><a href="#cb11-177" aria-hidden="true" tabindex="-1"></a>           \theta_{0}^{(t)} <span class="sc">\\</span></span>
<span id="cb11-178"><a href="#cb11-178" aria-hidden="true" tabindex="-1"></a>           \theta_{1}^{(t)} <span class="sc">\\</span></span>
<span id="cb11-179"><a href="#cb11-179" aria-hidden="true" tabindex="-1"></a>           \vdots</span>
<span id="cb11-180"><a href="#cb11-180" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix} - \alpha \begin{bmatrix}</span>
<span id="cb11-181"><a href="#cb11-181" aria-hidden="true" tabindex="-1"></a>           \frac{\partial L}{\partial \theta_{0}} <span class="sc">\\</span></span>
<span id="cb11-182"><a href="#cb11-182" aria-hidden="true" tabindex="-1"></a>           \frac{\partial L}{\partial \theta_{1}} <span class="sc">\\</span></span>
<span id="cb11-183"><a href="#cb11-183" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb11-184"><a href="#cb11-184" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}$$</span>
<span id="cb11-185"><a href="#cb11-185" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-186"><a href="#cb11-186" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Written in a more compact form, $$\vec{\theta}^{(t+1)} = \vec{\theta}^{(t)} - \alpha \nabla_{\vec{\theta}} L(\theta^{(t)}) $$</span>
<span id="cb11-187"><a href="#cb11-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-188"><a href="#cb11-188" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\theta$ is a vector with our model weights</span>
<span id="cb11-189"><a href="#cb11-189" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$L$ is the loss function</span>
<span id="cb11-190"><a href="#cb11-190" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\alpha$ is the learning rate (ours is constant, but other techniques use an $\alpha$ that decreases over time)</span>
<span id="cb11-191"><a href="#cb11-191" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\vec{\theta}^{(t)}$ is the current value of $\theta$</span>
<span id="cb11-192"><a href="#cb11-192" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\vec{\theta}^{(t+1)}$ is the next value of $\theta$</span>
<span id="cb11-193"><a href="#cb11-193" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\nabla_{\vec{\theta}} L(\vec{\theta}^{(t)})$ is the gradient of the loss function evaluated at the current $\theta$: $$\frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))$$</span>
<span id="cb11-194"><a href="#cb11-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-195"><a href="#cb11-195" aria-hidden="true" tabindex="-1"></a>Note that $-\nabla_{\vec{\theta}} L$ always points in the **downhill direction** of the surface. </span>
<span id="cb11-196"><a href="#cb11-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-197"><a href="#cb11-197" aria-hidden="true" tabindex="-1"></a>Let's now walk through an example of calculating and updating the gradient vector. Say our model and loss are:</span>
<span id="cb11-198"><a href="#cb11-198" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb11-199"><a href="#cb11-199" aria-hidden="true" tabindex="-1"></a>f_{\vec{\theta}}(\vec{x}) &amp;= \vec{x}^T\vec{\theta} = \theta_0x_0 + \theta_1x_1</span>
<span id="cb11-200"><a href="#cb11-200" aria-hidden="true" tabindex="-1"></a><span class="sc">\\</span>l(y, \hat{y}) &amp;= (y - \hat{y})^2</span>
<span id="cb11-201"><a href="#cb11-201" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb11-202"><a href="#cb11-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-203"><a href="#cb11-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-204"><a href="#cb11-204" aria-hidden="true" tabindex="-1"></a>Plugging in $f_{\vec{\theta}}(\vec{x})$ for $\hat{y}$, our loss function becomes $l(\vec{\theta}, \vec{x}, y_i) = (y_i - \theta_0x_0 - \theta_1x_1)^2$.</span>
<span id="cb11-205"><a href="#cb11-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-206"><a href="#cb11-206" aria-hidden="true" tabindex="-1"></a>To calculate our gradient vector, we can start by computing the partial derivative of the loss function with respect to $\theta_0$: $$\frac{\partial}{\partial \theta_{0}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_0)$$</span>
<span id="cb11-207"><a href="#cb11-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-208"><a href="#cb11-208" aria-hidden="true" tabindex="-1"></a>Let's now do the same but with respect to $\theta_1$: $$\frac{\partial}{\partial \theta_{1}} l(\vec{\theta}, \vec{x}, y_i) = 2(y_i - \theta_0x_0 - \theta_1x_1)(-x_1)$$</span>
<span id="cb11-209"><a href="#cb11-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-210"><a href="#cb11-210" aria-hidden="true" tabindex="-1"></a>Putting this together, our gradient vector is: </span>
<span id="cb11-211"><a href="#cb11-211" aria-hidden="true" tabindex="-1"></a>$$\nabla_{\theta} l(\vec{\theta}, \vec{x}, y_i) =  \begin{bmatrix} -2(y_i - \theta_0x_0 - \theta_1x_1)(x_0) <span class="sc">\\</span> -2(y_i - \theta_0x_0 - \theta_1x_1)(x_1) \end{bmatrix}$$</span>
<span id="cb11-212"><a href="#cb11-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-213"><a href="#cb11-213" aria-hidden="true" tabindex="-1"></a>Remember that we need to keep updating $\theta$ until the algorithm **converges** to a solution and stops updating significantly (or at all). When updating $\theta$, we'll have a fixed number of updates and subsequent updates will be quite small (we won't change $\theta$ by much).</span>
<span id="cb11-214"><a href="#cb11-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-215"><a href="#cb11-215" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stochastic (Mini-batch) Gradient Descent</span></span>
<span id="cb11-216"><a href="#cb11-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-217"><a href="#cb11-217" aria-hidden="true" tabindex="-1"></a>Formally, the algorithm we derived above is called **batch gradient descent.** For each iteration of the algorithm, the derivative of loss is computed across the *entire* batch of all $n$ datapoints. While this update rule works well in theory, it is not practical in most circumstances. For large datasets (with perhaps billions of datapoints), finding the gradient across all the data is incredibly computationally taxing; gradient descent will converge slowly because each individual update is slow.</span>
<span id="cb11-218"><a href="#cb11-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-219"><a href="#cb11-219" aria-hidden="true" tabindex="-1"></a>**Stochastic (mini-batch) gradient descent** tries to address this issue. In stochastic descent, only a *sample* of the full dataset is used at each update. We estimate the true gradient of the loss surface using just that sample of data. The **batch size** is the number of data points used in each sample. The sampling strategy is generally without replacement (data is shuffled and batch size examples are selected one at a time.)</span>
<span id="cb11-220"><a href="#cb11-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-221"><a href="#cb11-221" aria-hidden="true" tabindex="-1"></a>Each complete "pass" through the data is known as a **training epoch**. After shuffling the data, in a single **training epoch** of stochastic gradient descent, we</span>
<span id="cb11-222"><a href="#cb11-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-223"><a href="#cb11-223" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute the gradient on the first x% of the data. Update the parameter guesses.</span>
<span id="cb11-224"><a href="#cb11-224" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute the gradient on the next x% of the data. Update the parameter guesses.</span>
<span id="cb11-225"><a href="#cb11-225" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\dots$</span>
<span id="cb11-226"><a href="#cb11-226" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute the gradient on the last x% of the data. Update the parameter guesses.</span>
<span id="cb11-227"><a href="#cb11-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-228"><a href="#cb11-228" aria-hidden="true" tabindex="-1"></a>Every data point appears once in a single training epoch. We then perform several training epochs until we're satisfied.</span>
<span id="cb11-229"><a href="#cb11-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-230"><a href="#cb11-230" aria-hidden="true" tabindex="-1"></a>Batch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for $\vec{\theta}$ at each iteration, there's a chance the algorithm will not progress towards the true minimum of loss with each update. Over the longer term, these stochastic techniques should still converge towards the optimal solution. </span>
<span id="cb11-231"><a href="#cb11-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-232"><a href="#cb11-232" aria-hidden="true" tabindex="-1"></a>The diagrams below represent a "bird's eye view" of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal $\hat{\theta}$. Stochastic gradient descent, in contrast, "hops around" on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step.</span>
<span id="cb11-233"><a href="#cb11-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-234"><a href="#cb11-234" aria-hidden="true" tabindex="-1"></a>&lt;div align="middle"&gt;</span>
<span id="cb11-235"><a href="#cb11-235" aria-hidden="true" tabindex="-1"></a>  &lt;table style="width:100%"&gt;</span>
<span id="cb11-236"><a href="#cb11-236" aria-hidden="true" tabindex="-1"></a>    &lt;tr align="center"&gt;</span>
<span id="cb11-237"><a href="#cb11-237" aria-hidden="true" tabindex="-1"></a>      &lt;td&gt;&lt;img src="images/stochastic.png" alt='stochastic' width='600'&gt;</span>
<span id="cb11-238"><a href="#cb11-238" aria-hidden="true" tabindex="-1"></a>      &lt;/td&gt;</span>
<span id="cb11-239"><a href="#cb11-239" aria-hidden="true" tabindex="-1"></a>    &lt;/tr&gt;</span>
<span id="cb11-240"><a href="#cb11-240" aria-hidden="true" tabindex="-1"></a>  &lt;/table&gt;</span>
<span id="cb11-241"><a href="#cb11-241" aria-hidden="true" tabindex="-1"></a>&lt;/div&gt;</span>
<span id="cb11-242"><a href="#cb11-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-243"><a href="#cb11-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-244"><a href="#cb11-244" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch, Mini-batch, Stochastic Gradient Descent</span></span>
<span id="cb11-245"><a href="#cb11-245" aria-hidden="true" tabindex="-1"></a>The time complexity of a single gradient descent step takes only $O(nd)$ time where $n$ is the number of samples (rows) and $d$ is the number of features (columns).</span>
<span id="cb11-246"><a href="#cb11-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-247"><a href="#cb11-247" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/complexity_grad_descent.png" alt='complexity_grad_descent' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-248"><a href="#cb11-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-249"><a href="#cb11-249" aria-hidden="true" tabindex="-1"></a>Suppose we run $T$ iterations. The final complexity would then be $O(Tnd)$. Typically, $n$ is much larger than $T$ and $d$. How can we reduce the cost of this algorithm using a technique from Data 100? Do we really need to use $n$ data points? We don't! Instead, we can use stochastic gradient descent.</span>
<span id="cb11-250"><a href="#cb11-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-251"><a href="#cb11-251" aria-hidden="true" tabindex="-1"></a>We know that our true gradient of $\nabla_{\vec{\theta}} L (\vec{\theta^{(t)}}) = \frac{1}{n}\sum_{i=1}^{n}\nabla_{\vec{\theta}} l(y_i, f_{\vec{\theta}^{(t)}}(X_i))$ has a time complexity of $O(nd)$. Instead of using all $n$ samples to calculate the true gradient of the loss surface, let's use a **probability sample** of our data to **approximate the gradient**! </span>
<span id="cb11-252"><a href="#cb11-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-253"><a href="#cb11-253" aria-hidden="true" tabindex="-1"></a>Say we sample $b$ records ($s_1, \cdots, s_b$) from our $n$ datapoints. Our new (stochastic) gradient descent function will be: $$\nabla_{\vec{\theta}} L (\vec{\theta}^{(t)}) \approx \frac{1}{\textcolor{red}{b}}\sum_{i=1}^{\textcolor{red}{b}}\nabla_{\vec{\theta}} l(y_{\textcolor{red}{s_i}}, f_{\vec{\theta}^{(t)}}(X_{\textcolor{red}{s_i}}))$$ and will now have a time complexity of $O(bd)$, which is much faster! For more on computational complexity, see the bonus section at the end.</span>
<span id="cb11-254"><a href="#cb11-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-255"><a href="#cb11-255" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb11-256"><a href="#cb11-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-257"><a href="#cb11-257" aria-hidden="true" tabindex="-1"></a>How do we decide our mini-batch size ($b$), or the number of datapoints in our sample? Mini-batch size is typically **small**. </span>
<span id="cb11-258"><a href="#cb11-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-259"><a href="#cb11-259" aria-hidden="true" tabindex="-1"></a>The original Stochastic Gradient Descent algorithm uses $b=1$ so that *only one sample is used* to approximate the gradient at a time. </span>
<span id="cb11-260"><a href="#cb11-260" aria-hidden="true" tabindex="-1"></a>When choosing $b$, there are several factors to consider: </span>
<span id="cb11-261"><a href="#cb11-261" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>larger $b$: better *gradient estimates*, *parallelism*, and other *systems factors*, but also rapidly diminishing returns with more complexity</span>
<span id="cb11-262"><a href="#cb11-262" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>smaller $b$: faster and has *more frequent updates*</span>
<span id="cb11-263"><a href="#cb11-263" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-264"><a href="#cb11-264" aria-hidden="true" tabindex="-1"></a>It is up to data scientists to balance the tradeoff between batch size and time complexity.</span>
<span id="cb11-265"><a href="#cb11-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-266"><a href="#cb11-266" aria-hidden="true" tabindex="-1"></a>Stochastic gradient descent helps us *estimate* the true gradient of the loss surface using just that sample of data, and this allows us to compute gradients quicker. To sample data, there are two sampling strategies we can use:</span>
<span id="cb11-267"><a href="#cb11-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-268"><a href="#cb11-268" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Shuffle the data** and select batch size records at a time.</span>
<span id="cb11-269"><a href="#cb11-269" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Take a **simple random sample** for each gradient computation.</span>
<span id="cb11-270"><a href="#cb11-270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb11-271"><a href="#cb11-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-272"><a href="#cb11-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-273"><a href="#cb11-273" aria-hidden="true" tabindex="-1"></a>Summarizing our two gradient descent techniques:</span>
<span id="cb11-274"><a href="#cb11-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-275"><a href="#cb11-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**(Batch) Gradient Descent**: GD computes the **true** descent and always descends towards the true minimum of the loss based on a batch (a full dataset). While accurate, it can often be computationally expensive.</span>
<span id="cb11-276"><a href="#cb11-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-277"><a href="#cb11-277" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/gd.png" alt='batch_grad_descent' width='300'&gt;&lt;/center&gt;</span>
<span id="cb11-278"><a href="#cb11-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-279"><a href="#cb11-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**(Mini-batch) Stochastic Gradient Descent**: SGD **approximates** the true gradient descent based off of mini-batch (a subset of a dataset). It may not descend towards the true minimum with each update, but it's often less computationally expensive than batch gradient descent. Note that it is mini-batch, because we create a smaller, randomized sample of the batch to run SGD on it.</span>
<span id="cb11-280"><a href="#cb11-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-281"><a href="#cb11-281" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/sgd.png" alt='stochastic_grad_descent' width='300'&gt;&lt;/center&gt;</span>
<span id="cb11-282"><a href="#cb11-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-283"><a href="#cb11-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-284"><a href="#cb11-284" aria-hidden="true" tabindex="-1"></a>To summarize the tradeoffs of batch size: </span>
<span id="cb11-285"><a href="#cb11-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-286"><a href="#cb11-286" aria-hidden="true" tabindex="-1"></a>|  | Smaller Batch Size | Larger Batch Size | </span>
<span id="cb11-287"><a href="#cb11-287" aria-hidden="true" tabindex="-1"></a>| -- | -- | -- | </span>
<span id="cb11-288"><a href="#cb11-288" aria-hidden="true" tabindex="-1"></a>| **Pros** | More frequent gradient updates | Leverage hardware acceleration to improve overall system performance and higher quality gradient updates | </span>
<span id="cb11-289"><a href="#cb11-289" aria-hidden="true" tabindex="-1"></a>| **Cons** | More variability in the gradient estimates | Less frequent gradient updates |</span>
<span id="cb11-290"><a href="#cb11-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-291"><a href="#cb11-291" aria-hidden="true" tabindex="-1"></a>The typical solution is to set batch size to ensure sufficient hardware utilization.</span>
<span id="cb11-292"><a href="#cb11-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-293"><a href="#cb11-293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Engineering</span></span>
<span id="cb11-294"><a href="#cb11-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-295"><a href="#cb11-295" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb11-296"><a href="#cb11-296" aria-hidden="true" tabindex="-1"></a><span class="fu">### Motivation</span></span>
<span id="cb11-297"><a href="#cb11-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-298"><a href="#cb11-298" aria-hidden="true" tabindex="-1"></a>At this point in the course, we've equipped ourselves with some powerful techniques to build and optimize models. We've explored how to develop models of multiple variables, as well as how to transform variables to help **linearize** a dataset and fit these models to maximize their performance.</span>
<span id="cb11-299"><a href="#cb11-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-300"><a href="#cb11-300" aria-hidden="true" tabindex="-1"></a>All of this was done with one major caveat: the regression models we've worked with so far are all **linear in the input variables**. We've assumed that our predictions should be some combination of linear variables. While this works well in some cases, the real world isn't always so straightforward. We'll learn an important method to address this issue – feature engineering – and consider some new problems that can arise when we do so.</span>
<span id="cb11-301"><a href="#cb11-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-302"><a href="#cb11-302" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb11-303"><a href="#cb11-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-304"><a href="#cb11-304" aria-hidden="true" tabindex="-1"></a>Feature engineering is the process of *transforming* raw features into *more informative features* that can be used in modeling or EDA tasks and improve model performance.</span>
<span id="cb11-305"><a href="#cb11-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-306"><a href="#cb11-306" aria-hidden="true" tabindex="-1"></a>Feature engineering allows you to:</span>
<span id="cb11-307"><a href="#cb11-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-308"><a href="#cb11-308" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Capture domain knowledge </span>
<span id="cb11-309"><a href="#cb11-309" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Express non-linear relationships using linear models</span>
<span id="cb11-310"><a href="#cb11-310" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Use non-numeric (qualitative) features in models</span>
<span id="cb11-311"><a href="#cb11-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-312"><a href="#cb11-312" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Functions</span></span>
<span id="cb11-313"><a href="#cb11-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-314"><a href="#cb11-314" aria-hidden="true" tabindex="-1"></a>A **feature function** describes the transformations we apply to raw features in a dataset to create a design matrix of transformed features (typically denoted as $\Phi$). When we apply the feature function to our original dataset $\mathbb{X}$, the result, $\Phi(\mathbb{X})$, is a transformed design matrix ready to be used in modeling. </span>
<span id="cb11-315"><a href="#cb11-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-316"><a href="#cb11-316" aria-hidden="true" tabindex="-1"></a>For example, we might design a feature function that computes the square of an existing feature and adds it to the design matrix. In this case, our existing matrix $<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ is transformed to $<span class="co">[</span><span class="ot">x, x^2</span><span class="co">]</span>$. Its *dimension* increases from 1 to 2. Often, the dimension of the *featurized* dataset increases as seen here.</span>
<span id="cb11-317"><a href="#cb11-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-318"><a href="#cb11-318" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/phi.png" alt='phi' width='700'&gt;&lt;/center&gt;</span>
<span id="cb11-319"><a href="#cb11-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-320"><a href="#cb11-320" aria-hidden="true" tabindex="-1"></a>The new features introduced by the feature function can then be used in modeling. Often, we use the symbol $\phi_i$ to represent transformed features after feature engineering. </span>
<span id="cb11-321"><a href="#cb11-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-322"><a href="#cb11-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-323"><a href="#cb11-323" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb11-324"><a href="#cb11-324" aria-hidden="true" tabindex="-1"></a>\hat{y} &amp;= \theta_0 + \theta_1 x + \theta_2 x^2 <span class="sc">\\</span></span>
<span id="cb11-325"><a href="#cb11-325" aria-hidden="true" tabindex="-1"></a>\hat{y} &amp;= \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2</span>
<span id="cb11-326"><a href="#cb11-326" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb11-327"><a href="#cb11-327" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb11-328"><a href="#cb11-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-329"><a href="#cb11-329" aria-hidden="true" tabindex="-1"></a>In matrix notation, the symbol $\Phi$ is sometimes used to denote the design matrix after feature engineering has been performed. Note that in the usage below, $\Phi$ is now a feature-engineered matrix, rather than a function.</span>
<span id="cb11-330"><a href="#cb11-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-331"><a href="#cb11-331" aria-hidden="true" tabindex="-1"></a>$$\hat{\mathbb{Y}} = \Phi \theta$$</span>
<span id="cb11-332"><a href="#cb11-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-333"><a href="#cb11-333" aria-hidden="true" tabindex="-1"></a>More formally, we describe a feature function as transforming the original $\mathbb{R}^{n \times p}$ dataset $\mathbb{X}$ to a featurized $\mathbb{R}^{n \times p'}$ dataset $\mathbb{\Phi}$, where $p'$ is typically greater than $p$. </span>
<span id="cb11-334"><a href="#cb11-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-335"><a href="#cb11-335" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X} \in \mathbb{R}^{n \times p} \longrightarrow \Phi \in \mathbb{R}^{n \times p'}$$</span>
<span id="cb11-336"><a href="#cb11-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-337"><a href="#cb11-337" aria-hidden="true" tabindex="-1"></a><span class="fu">### One-Hot Encoding</span></span>
<span id="cb11-338"><a href="#cb11-338" aria-hidden="true" tabindex="-1"></a>A particularly powerful use of feature engineering is to allow us to perform regression on non-numeric features. **One-hot encoding** is a feature engineering technique that generates numeric features from categorical data, allowing us to use our usual methods to fit a regression model on the data.</span>
<span id="cb11-339"><a href="#cb11-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-340"><a href="#cb11-340" aria-hidden="true" tabindex="-1"></a>To illustrate how this works, we'll refer back to the <span class="in">`tips`</span> dataset from previous lectures. Consider the <span class="in">`"day"`</span> column of the dataset:</span>
<span id="cb11-341"><a href="#cb11-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-344"><a href="#cb11-344" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-345"><a href="#cb11-345" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb11-346"><a href="#cb11-346" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-347"><a href="#cb11-347" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb11-348"><a href="#cb11-348" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-349"><a href="#cb11-349" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb11-350"><a href="#cb11-350" aria-hidden="true" tabindex="-1"></a>tips <span class="op">=</span> sns.load_dataset(<span class="st">"tips"</span>)</span>
<span id="cb11-351"><a href="#cb11-351" aria-hidden="true" tabindex="-1"></a>tips.head()</span>
<span id="cb11-352"><a href="#cb11-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-353"><a href="#cb11-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-354"><a href="#cb11-354" aria-hidden="true" tabindex="-1"></a>   At first glance, it doesn't seem possible to fit a regression model to this non-numeric data – we can't directly perform any mathematical operations on the entry "Sun". </span>
<span id="cb11-355"><a href="#cb11-355" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-356"><a href="#cb11-356" aria-hidden="true" tabindex="-1"></a>To resolve this, we instead create a new table with a feature for each unique value in the original <span class="in">`"day"`</span> column. We then iterate through the <span class="in">`"day"`</span> column. For each entry in <span class="in">`"day"`</span> we fill the corresponding feature in the new table with 1. All other features are set to 0.</span>
<span id="cb11-357"><a href="#cb11-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-358"><a href="#cb11-358" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/ohe.png" alt='ohe' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-359"><a href="#cb11-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-360"><a href="#cb11-360" aria-hidden="true" tabindex="-1"></a>&lt;br&gt; </span>
<span id="cb11-361"><a href="#cb11-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-362"><a href="#cb11-362" aria-hidden="true" tabindex="-1"></a>In short, each category of a categorical variable gets its own feature</span>
<span id="cb11-363"><a href="#cb11-363" aria-hidden="true" tabindex="-1"></a>&lt;ul&gt;</span>
<span id="cb11-364"><a href="#cb11-364" aria-hidden="true" tabindex="-1"></a>   &lt;li&gt;</span>
<span id="cb11-365"><a href="#cb11-365" aria-hidden="true" tabindex="-1"></a>      Value = 1 if a row belongs to the category</span>
<span id="cb11-366"><a href="#cb11-366" aria-hidden="true" tabindex="-1"></a>   &lt;/li&gt;</span>
<span id="cb11-367"><a href="#cb11-367" aria-hidden="true" tabindex="-1"></a>   &lt;li&gt;</span>
<span id="cb11-368"><a href="#cb11-368" aria-hidden="true" tabindex="-1"></a>      Value = 0 otherwise</span>
<span id="cb11-369"><a href="#cb11-369" aria-hidden="true" tabindex="-1"></a>   &lt;/li&gt;</span>
<span id="cb11-370"><a href="#cb11-370" aria-hidden="true" tabindex="-1"></a>&lt;/ul&gt;</span>
<span id="cb11-371"><a href="#cb11-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-372"><a href="#cb11-372" aria-hidden="true" tabindex="-1"></a>The <span class="in">`OneHotEncoder`</span> class of <span class="in">`sklearn`</span> (<span class="co">[</span><span class="ot">documentation</span><span class="co">](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out)</span>) offers a quick way to perform this one-hot encoding. You will explore its use in detail in the lab. For now, recognize that we follow a very similar workflow to when we were working with the <span class="in">`LinearRegression`</span> class: we initialize a <span class="in">`OneHotEncoder`</span> object, fit it to our data, and finally use <span class="in">`.transform()`</span> to apply the fitted encoder.</span>
<span id="cb11-373"><a href="#cb11-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-376"><a href="#cb11-376" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-377"><a href="#cb11-377" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb11-378"><a href="#cb11-378" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb11-379"><a href="#cb11-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-380"><a href="#cb11-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a OneHotEncoder object</span></span>
<span id="cb11-381"><a href="#cb11-381" aria-hidden="true" tabindex="-1"></a>ohe <span class="op">=</span> OneHotEncoder()</span>
<span id="cb11-382"><a href="#cb11-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-383"><a href="#cb11-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the encoder</span></span>
<span id="cb11-384"><a href="#cb11-384" aria-hidden="true" tabindex="-1"></a>ohe.fit(tips[[<span class="st">"day"</span>]])</span>
<span id="cb11-385"><a href="#cb11-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-386"><a href="#cb11-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the encoder to transform the raw "day" feature</span></span>
<span id="cb11-387"><a href="#cb11-387" aria-hidden="true" tabindex="-1"></a>encoded_day <span class="op">=</span> ohe.transform(tips[[<span class="st">"day"</span>]]).toarray()</span>
<span id="cb11-388"><a href="#cb11-388" aria-hidden="true" tabindex="-1"></a>encoded_day_df <span class="op">=</span> pd.DataFrame(encoded_day, columns<span class="op">=</span>ohe.get_feature_names_out())</span>
<span id="cb11-389"><a href="#cb11-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-390"><a href="#cb11-390" aria-hidden="true" tabindex="-1"></a>encoded_day_df.head()</span>
<span id="cb11-391"><a href="#cb11-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-392"><a href="#cb11-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-393"><a href="#cb11-393" aria-hidden="true" tabindex="-1"></a>The one-hot encoded features can then be used in the design matrix to train a model:</span>
<span id="cb11-394"><a href="#cb11-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-395"><a href="#cb11-395" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/ohemodel.png" alt='ohemodel' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-396"><a href="#cb11-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-397"><a href="#cb11-397" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_1 (\text{total}<span class="sc">\_</span>\text{bill}) + \theta_2 (\text{size}) + \theta_3 (\text{day}<span class="sc">\_</span>\text{Fri}) + \theta_4 (\text{day}<span class="sc">\_</span>\text{Sat}) + \theta_5 (\text{day}<span class="sc">\_</span>\text{Sun}) + \theta_6 (\text{day}<span class="sc">\_</span>\text{Thur})$$</span>
<span id="cb11-398"><a href="#cb11-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-399"><a href="#cb11-399" aria-hidden="true" tabindex="-1"></a>Or in shorthand:</span>
<span id="cb11-400"><a href="#cb11-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-401"><a href="#cb11-401" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_{1}\phi_{1} + \theta_{2}\phi_{2} + \theta_{3}\phi_{3} + \theta_{4}\phi_{4} + \theta_{5}\phi_{5} + \theta_{6}\phi_{6}$$</span>
<span id="cb11-402"><a href="#cb11-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-403"><a href="#cb11-403" aria-hidden="true" tabindex="-1"></a>Now, the <span class="in">`day`</span> feature (or rather, the four new boolean features that represent day) can be used to fit a model.</span>
<span id="cb11-404"><a href="#cb11-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-405"><a href="#cb11-405" aria-hidden="true" tabindex="-1"></a>Using <span class="in">`sklearn`</span> to fit the new model, we can determine the model coefficients, allowing us to understand how each feature impacts the predicted tip.</span>
<span id="cb11-406"><a href="#cb11-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-409"><a href="#cb11-409" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-410"><a href="#cb11-410" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb11-411"><a href="#cb11-411" aria-hidden="true" tabindex="-1"></a>data_w_ohe <span class="op">=</span> tips[[<span class="st">"total_bill"</span>, <span class="st">"size"</span>, <span class="st">"day"</span>]].join(encoded_day_df).drop(columns <span class="op">=</span> <span class="st">"day"</span>)</span>
<span id="cb11-412"><a href="#cb11-412" aria-hidden="true" tabindex="-1"></a>ohe_model <span class="op">=</span> lm.LinearRegression(fit_intercept<span class="op">=</span><span class="va">False</span>) <span class="co">#Tell sklearn to not add an additional bias column. Why?</span></span>
<span id="cb11-413"><a href="#cb11-413" aria-hidden="true" tabindex="-1"></a>ohe_model.fit(data_w_ohe, tips[<span class="st">"tip"</span>])</span>
<span id="cb11-414"><a href="#cb11-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-415"><a href="#cb11-415" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({<span class="st">"Feature"</span>:data_w_ohe.columns, <span class="st">"Model Coefficient"</span>:ohe_model.coef_})</span>
<span id="cb11-416"><a href="#cb11-416" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-417"><a href="#cb11-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-418"><a href="#cb11-418" aria-hidden="true" tabindex="-1"></a>For example, when looking at the coefficient for <span class="in">`day_Fri`</span>, we can now understand the impact of it being Friday on the predicted tip — if it is a Friday, the predicted tip increases by approximately $0.75.</span>
<span id="cb11-419"><a href="#cb11-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-420"><a href="#cb11-420" aria-hidden="true" tabindex="-1"></a>When one-hot encoding, keep in mind that any set of one-hot encoded columns will always sum to a column of all ones, representing the bias column. More formally, the bias column is a linear combination of the OHE columns.</span>
<span id="cb11-421"><a href="#cb11-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-422"><a href="#cb11-422" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/bias.png" alt='bias' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-423"><a href="#cb11-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-424"><a href="#cb11-424" aria-hidden="true" tabindex="-1"></a>We must be careful not to include this bias column in our design matrix. Otherwise, there will be linear dependence in the model, meaning $\mathbb{X}^{\top}\mathbb{X}$ would no longer be invertible, and our OLS estimate $\hat{\theta} = (\mathbb{X}^{\top}\mathbb{X})^{-1}\mathbb{X}^{\top}\mathbb{Y}$ fails.</span>
<span id="cb11-425"><a href="#cb11-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-426"><a href="#cb11-426" aria-hidden="true" tabindex="-1"></a>To resolve this issue, we simply omit one of the one-hot encoded columns *or* do not include an intercept term. The adjusted design matrices are shown below.</span>
<span id="cb11-427"><a href="#cb11-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-428"><a href="#cb11-428" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/remove.png" alt='remove' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-429"><a href="#cb11-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-430"><a href="#cb11-430" aria-hidden="true" tabindex="-1"></a>Either approach works — we still retain the same information as the omitted column being a linear combination of the remaining columns.</span>
<span id="cb11-431"><a href="#cb11-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-432"><a href="#cb11-432" aria-hidden="true" tabindex="-1"></a><span class="fu">### Polynomial Features</span></span>
<span id="cb11-433"><a href="#cb11-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-434"><a href="#cb11-434" aria-hidden="true" tabindex="-1"></a>We have encountered a few cases now where models with linear features have performed poorly on datasets that show clear *non-linear* curvature. </span>
<span id="cb11-435"><a href="#cb11-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-436"><a href="#cb11-436" aria-hidden="true" tabindex="-1"></a>As an example, consider the <span class="in">`vehicles`</span> dataset, which contains information about cars. Suppose we want to use the <span class="in">`hp`</span> (horsepower) of a car to predict its <span class="in">`"mpg"`</span> (gas mileage in miles per gallon). If we visualize the relationship between these two variables, we see a non-linear curvature. Fitting a linear model to these variables results in a high (poor) value of RMSE. </span>
<span id="cb11-437"><a href="#cb11-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-438"><a href="#cb11-438" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0 + \theta_1 (\text{hp})$$</span>
<span id="cb11-439"><a href="#cb11-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-442"><a href="#cb11-442" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-443"><a href="#cb11-443" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb11-444"><a href="#cb11-444" aria-hidden="true" tabindex="-1"></a>pd.options.mode.chained_assignment <span class="op">=</span> <span class="va">None</span> </span>
<span id="cb11-445"><a href="#cb11-445" aria-hidden="true" tabindex="-1"></a>vehicles <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna().rename(columns <span class="op">=</span> {<span class="st">"horsepower"</span>: <span class="st">"hp"</span>}).sort_values(<span class="st">"hp"</span>)</span>
<span id="cb11-446"><a href="#cb11-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-447"><a href="#cb11-447" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vehicles[[<span class="st">"hp"</span>]]</span>
<span id="cb11-448"><a href="#cb11-448" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> vehicles[<span class="st">"mpg"</span>]</span>
<span id="cb11-449"><a href="#cb11-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-450"><a href="#cb11-450" aria-hidden="true" tabindex="-1"></a>hp_model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb11-451"><a href="#cb11-451" aria-hidden="true" tabindex="-1"></a>hp_model.fit(X, Y)</span>
<span id="cb11-452"><a href="#cb11-452" aria-hidden="true" tabindex="-1"></a>hp_model_predictions <span class="op">=</span> hp_model.predict(X)</span>
<span id="cb11-453"><a href="#cb11-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-454"><a href="#cb11-454" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-455"><a href="#cb11-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-456"><a href="#cb11-456" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>vehicles, x<span class="op">=</span><span class="st">"hp"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)</span>
<span id="cb11-457"><a href="#cb11-457" aria-hidden="true" tabindex="-1"></a>plt.plot(vehicles[<span class="st">"hp"</span>], hp_model_predictions, c<span class="op">=</span><span class="st">"tab:red"</span>)<span class="op">;</span></span>
<span id="cb11-458"><a href="#cb11-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-459"><a href="#cb11-459" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE of model with (hp) feature: </span><span class="sc">{</span>np<span class="sc">.</span>mean((Y<span class="op">-</span>hp_model_predictions)<span class="op">**</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-460"><a href="#cb11-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-461"><a href="#cb11-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-462"><a href="#cb11-462" aria-hidden="true" tabindex="-1"></a>As we can see from the plot, the data follows a curved line rather than a straight one. To capture this non-linearity, we can incorporate **non-linear** features. Let's introduce a **polynomial** term, $\text{hp}^2$, into our regression model. The model now takes the form:</span>
<span id="cb11-463"><a href="#cb11-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-464"><a href="#cb11-464" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)$$</span>
<span id="cb11-465"><a href="#cb11-465" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0 + \theta_1 \phi_1 + \theta_2 \phi_2$$</span>
<span id="cb11-466"><a href="#cb11-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-467"><a href="#cb11-467" aria-hidden="true" tabindex="-1"></a>How can we fit a model with non-linear features? We can use the exact same techniques as before: ordinary least squares, gradient descent, or <span class="in">`sklearn`</span>. This is because our new model is still a **linear model**. Although it contains non-linear *features*, it is linear with respect to the model *parameters*. All of our previous work on fitting models was done under the assumption that we were working with linear models. Because our new model is still linear, we can apply our existing methods to determine the optimal parameters. </span>
<span id="cb11-468"><a href="#cb11-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-471"><a href="#cb11-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-472"><a href="#cb11-472" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a hp^2 feature to the design matrix</span></span>
<span id="cb11-473"><a href="#cb11-473" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> vehicles[[<span class="st">"hp"</span>]]</span>
<span id="cb11-474"><a href="#cb11-474" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"hp^2"</span>] <span class="op">=</span> vehicles[<span class="st">"hp"</span>]<span class="op">**</span><span class="dv">2</span></span>
<span id="cb11-475"><a href="#cb11-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-476"><a href="#cb11-476" aria-hidden="true" tabindex="-1"></a><span class="co"># Use sklearn to fit the model</span></span>
<span id="cb11-477"><a href="#cb11-477" aria-hidden="true" tabindex="-1"></a>hp2_model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb11-478"><a href="#cb11-478" aria-hidden="true" tabindex="-1"></a>hp2_model.fit(X, Y)</span>
<span id="cb11-479"><a href="#cb11-479" aria-hidden="true" tabindex="-1"></a>hp2_model_predictions <span class="op">=</span> hp2_model.predict(X)</span>
<span id="cb11-480"><a href="#cb11-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-481"><a href="#cb11-481" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>vehicles, x<span class="op">=</span><span class="st">"hp"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)</span>
<span id="cb11-482"><a href="#cb11-482" aria-hidden="true" tabindex="-1"></a>plt.plot(vehicles[<span class="st">"hp"</span>], hp2_model_predictions, c<span class="op">=</span><span class="st">"tab:red"</span>)<span class="op">;</span></span>
<span id="cb11-483"><a href="#cb11-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-484"><a href="#cb11-484" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE of model with (hp^2) feature: </span><span class="sc">{</span>np<span class="sc">.</span>mean((Y<span class="op">-</span>hp2_model_predictions)<span class="op">**</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-485"><a href="#cb11-485" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-486"><a href="#cb11-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-487"><a href="#cb11-487" aria-hidden="true" tabindex="-1"></a>Looking a lot better! By incorporating a squared feature, we are able to capture the curvature of the dataset. Our model is now a parabola centered on our data. Notice that our new model's error has decreased relative to the original model with linear features.</span>
<span id="cb11-488"><a href="#cb11-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-489"><a href="#cb11-489" aria-hidden="true" tabindex="-1"></a><span class="fu">### Complexity and Overfitting</span></span>
<span id="cb11-490"><a href="#cb11-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-491"><a href="#cb11-491" aria-hidden="true" tabindex="-1"></a>We've seen now that feature engineering allows us to build all sorts of features to improve the performance of the model. In particular, we saw that designing a more complex feature (squaring <span class="in">`hp`</span> in the <span class="in">`vehicles`</span> data previously) substantially improved the model's ability to capture non-linear relationships. To take full advantage of this, we might be inclined to design increasingly complex features. Consider the following three models, each of different order (the maximum exponent power of each model):</span>
<span id="cb11-492"><a href="#cb11-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-493"><a href="#cb11-493" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Model with order 2: $\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2)$</span>
<span id="cb11-494"><a href="#cb11-494" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Model with order 3: $\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3)$</span>
<span id="cb11-495"><a href="#cb11-495" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Model with order 4: $\hat{y} = \theta_0 + \theta_1 (\text{hp}) + \theta_2 (\text{hp}^2) + \theta_3 (\text{hp}^3) + \theta_4 (\text{hp}^4)$</span>
<span id="cb11-496"><a href="#cb11-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-497"><a href="#cb11-497" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/degree_comparison.png" alt='degree_comparison' width='900'&gt;&lt;/center&gt;</span>
<span id="cb11-498"><a href="#cb11-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-499"><a href="#cb11-499" aria-hidden="true" tabindex="-1"></a>As we can see in the plots above, MSE continues to decrease with each additional polynomial term. To visualize it further, let's plot models as the complexity increases from 0 to 7: </span>
<span id="cb11-500"><a href="#cb11-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-501"><a href="#cb11-501" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/degree_comparison2.png" alt='degree_comparison' width='850'&gt;&lt;/center&gt;</span>
<span id="cb11-502"><a href="#cb11-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-503"><a href="#cb11-503" aria-hidden="true" tabindex="-1"></a>When we use our model to make predictions on the same data that was used to fit the model, we find that the MSE decreases with each additional polynomial term (as our model gets more complex). The **training error** is the model's error when generating predictions from the same data that was used for training purposes. We can conclude that the training error goes down as the complexity of the model increases. </span>
<span id="cb11-504"><a href="#cb11-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-505"><a href="#cb11-505" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/train_error.png" alt='train_error' width='400'&gt;&lt;/center&gt;</span>
<span id="cb11-506"><a href="#cb11-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-507"><a href="#cb11-507" aria-hidden="true" tabindex="-1"></a>This seems like good news – when working on the **training data**, we can improve model performance by designing increasingly complex models. </span>
<span id="cb11-508"><a href="#cb11-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-509"><a href="#cb11-509" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb11-510"><a href="#cb11-510" aria-hidden="true" tabindex="-1"></a><span class="fu">## Math Fact: Polynomial Degrees</span></span>
<span id="cb11-511"><a href="#cb11-511" aria-hidden="true" tabindex="-1"></a>Given $N$ overlapping data points, we can always find a polynomial of degree $N-1$ that goes through all those points.</span>
<span id="cb11-512"><a href="#cb11-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-513"><a href="#cb11-513" aria-hidden="true" tabindex="-1"></a>For example, there always exists a degree-4 polynomial curve that can perfectly model a dataset of 5 datapoints:</span>
<span id="cb11-514"><a href="#cb11-514" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/perfect_poly_fits.png" alt='train_error' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-515"><a href="#cb11-515" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb11-516"><a href="#cb11-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-517"><a href="#cb11-517" aria-hidden="true" tabindex="-1"></a>However, high model complexity comes with its own set of issues. When building the <span class="in">`vehicles`</span> models above, we trained the models on the *entire* dataset and then evaluated their performance on this same dataset. In reality, we are likely to instead train the model on a *sample* from the population, then use it to make predictions on data it didn't encounter during training. </span>
<span id="cb11-518"><a href="#cb11-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-519"><a href="#cb11-519" aria-hidden="true" tabindex="-1"></a>Let's walk through a more realistic example. Say we are given a training dataset of just 6 datapoints and want to train a model to then make predictions on a *different* set of points. We may be tempted to make a highly complex model (e.g., degree 5), especially given it makes perfect predictions on the training data as clear on the left. However, as shown in the graph on the right, this model would perform *horribly* on the rest of the population! </span>
<span id="cb11-520"><a href="#cb11-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-521"><a href="#cb11-521" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/complex.png" alt='complex' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-522"><a href="#cb11-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-523"><a href="#cb11-523" aria-hidden="true" tabindex="-1"></a>This phenomenon called **overfitting**. The model effectively just memorized the training data it encountered when it was fitted, leaving it unable to **generalize** well to data it didn't encounter during training. This is a problem: we want models that are generalizable to “unseen” data.</span>
<span id="cb11-524"><a href="#cb11-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-525"><a href="#cb11-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-526"><a href="#cb11-526" aria-hidden="true" tabindex="-1"></a>Additionally, since complex models are sensitive to the specific dataset used to train them, they have high **variance**. A model with high variance tends to *vary* more dramatically when trained on different datasets. Going back to our example above, we can see our degree-5 model varies erratically when we fit it to different samples of 6 points from <span class="in">`vehicles`</span>. </span>
<span id="cb11-527"><a href="#cb11-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-528"><a href="#cb11-528" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/resamples.png" alt='resamples' width='800'&gt;&lt;/center&gt;</span>
<span id="cb11-529"><a href="#cb11-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-530"><a href="#cb11-530" aria-hidden="true" tabindex="-1"></a>We now face a dilemma: we know that we can **decrease training error** by increasing model complexity, but models that are *too* complex start to overfit and can't be reapplied to new datasets due to **high variance**.</span>
<span id="cb11-531"><a href="#cb11-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-532"><a href="#cb11-532" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/bvt.png" alt='bvt' width='400'&gt;&lt;/center&gt;</span>
<span id="cb11-533"><a href="#cb11-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-534"><a href="#cb11-534" aria-hidden="true" tabindex="-1"></a>We can see that there is a clear trade-off that comes from the complexity of our model. As model complexity increases, the model's error on the training data decreases. At the same time, the model's variance tends to increase.</span>
<span id="cb11-535"><a href="#cb11-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-536"><a href="#cb11-536" aria-hidden="true" tabindex="-1"></a>The takeaway here: we need to strike a balance in the complexity of our models; we want models that are generalizable to "unseen" data. A model that is too simple won't be able to capture the key relationships between our variables of interest; a model that is too complex runs the risk of overfitting. </span>
<span id="cb11-537"><a href="#cb11-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-538"><a href="#cb11-538" aria-hidden="true" tabindex="-1"></a>This begs the question: how do we control the complexity of a model? Stay tuned for Lecture 16 on Cross-Validation and Regularization!</span>
<span id="cb11-539"><a href="#cb11-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-540"><a href="#cb11-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-541"><a href="#cb11-541" aria-hidden="true" tabindex="-1"></a><span class="fu">## [Bonus] Time Complexity of the Normal Equation</span></span>
<span id="cb11-542"><a href="#cb11-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-543"><a href="#cb11-543" aria-hidden="true" tabindex="-1"></a>Let's now dive deeper into gradient and stochastic gradient descent. We discussed earlier how finding the gradient across all the data is extremeley computationally taxing and takes a lot of resources to calculate. Here we'll explore why this is true.</span>
<span id="cb11-544"><a href="#cb11-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-545"><a href="#cb11-545" aria-hidden="true" tabindex="-1"></a>We know that the solution to the normal equation is $\hat{\theta} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}$. Let's break this down and determine the computational complexity for this solution.</span>
<span id="cb11-546"><a href="#cb11-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-547"><a href="#cb11-547" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/complexity_normal_solution.png" alt='complexity_normal_solution' width='600'&gt;&lt;/center&gt;</span>
<span id="cb11-548"><a href="#cb11-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-549"><a href="#cb11-549" aria-hidden="true" tabindex="-1"></a> Let $n$ be the number of samples (rows) and $d$ be the number of features (columns). </span>
<span id="cb11-550"><a href="#cb11-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-551"><a href="#cb11-551" aria-hidden="true" tabindex="-1"></a><span class="ss"> * </span>Computing $(\mathbb{X}^{\top}\mathbb{X})$ takes $O(nd^2)$ time, and it's inverse takes another $O(d^3)$ time to calculate; overall, $(\mathbb{X}^{\top}\mathbb{X})^{-1}$ takes $O(nd^2) + O(d^3)$ time. </span>
<span id="cb11-552"><a href="#cb11-552" aria-hidden="true" tabindex="-1"></a><span class="ss"> * </span>$\mathbb{X}^{\top}\mathbb{Y}$ takes $O(nd)$ time.</span>
<span id="cb11-553"><a href="#cb11-553" aria-hidden="true" tabindex="-1"></a><span class="ss"> * </span>Multiplying $(\mathbb{X}^{\top}\mathbb{X})^{-1}$ and $\mathbb{X}^{\top}\mathbb{Y}$ takes $O(d^2)$ time. </span>
<span id="cb11-554"><a href="#cb11-554" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb11-555"><a href="#cb11-555" aria-hidden="true" tabindex="-1"></a> In total, calculating the solution to the normal equation takes $O(nd^2) + O(d^3) + O(nd) + O(d^2)$ time. We can see that $O(nd^2) + O(d^3)$ dominates the complexity — this can be problematic for high-dimensional models and very large datasets.</span>
<span id="cb11-556"><a href="#cb11-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-557"><a href="#cb11-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-558"><a href="#cb11-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-559"><a href="#cb11-559" aria-hidden="true" tabindex="-1"></a><span class="fu">## [Bonus] Stochastic Gradient Descent in `PyTorch`</span></span>
<span id="cb11-560"><a href="#cb11-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-561"><a href="#cb11-561" aria-hidden="true" tabindex="-1"></a>While this material is out of scope for Data 100, it is useful if you plan to enter a career in data science!</span>
<span id="cb11-562"><a href="#cb11-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-563"><a href="#cb11-563" aria-hidden="true" tabindex="-1"></a>In practice, you will use software packages such as <span class="in">`PyTorch`</span> when computing gradients and implementing gradient descent. You'll often follow three main steps:</span>
<span id="cb11-564"><a href="#cb11-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-565"><a href="#cb11-565" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample a batch of the data.</span>
<span id="cb11-566"><a href="#cb11-566" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Compute the loss and the gradient.</span>
<span id="cb11-567"><a href="#cb11-567" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Update your gradient until you reach an appropriate estimate of the true gradient.</span>
<span id="cb11-568"><a href="#cb11-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-569"><a href="#cb11-569" aria-hidden="true" tabindex="-1"></a>&lt;center&gt;&lt;img src="images/pytorchsgd.png" alt='pytorch_sgd' width='500'&gt;&lt;/center&gt;</span>
<span id="cb11-570"><a href="#cb11-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-571"><a href="#cb11-571" aria-hidden="true" tabindex="-1"></a>If you want to learn more, this <span class="co">[</span><span class="ot">Intro to PyTorch tutorial</span><span class="co">](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)</span> is a great resource to get started! </span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>