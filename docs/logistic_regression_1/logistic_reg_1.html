<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 23&nbsp; Logistic Regression I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../logistic_regression_2/logistic_reg_2.html" rel="next">
<link href="../sql_II/sql_II.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="../Principles-and-Techniques-of-Data-Science.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Random Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_climate/case_study_climate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Case Study in Climate and Physical Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#regression-vs.-classification" id="toc-regression-vs.-classification" class="nav-link active" data-scroll-target="#regression-vs.-classification"><span class="toc-section-number">23.1</span>  Regression vs.&nbsp;Classification</a></li>
  <li><a href="#intuition-the-coin-flip" id="toc-intuition-the-coin-flip" class="nav-link" data-scroll-target="#intuition-the-coin-flip"><span class="toc-section-number">23.2</span>  Intuition: The Coin Flip</a>
  <ul>
  <li><a href="#likelihood-of-data" id="toc-likelihood-of-data" class="nav-link" data-scroll-target="#likelihood-of-data"><span class="toc-section-number">23.2.1</span>  Likelihood of Data</a></li>
  </ul></li>
  <li><a href="#deriving-the-logistic-regression-model" id="toc-deriving-the-logistic-regression-model" class="nav-link" data-scroll-target="#deriving-the-logistic-regression-model"><span class="toc-section-number">23.3</span>  Deriving the Logistic Regression Model</a>
  <ul>
  <li><a href="#probabalistic-model" id="toc-probabalistic-model" class="nav-link" data-scroll-target="#probabalistic-model"><span class="toc-section-number">23.3.1</span>  Probabalistic Model</a></li>
  <li><a href="#odds-ratio" id="toc-odds-ratio" class="nav-link" data-scroll-target="#odds-ratio"><span class="toc-section-number">23.3.2</span>  Odds Ratio</a></li>
  <li><a href="#logit-function" id="toc-logit-function" class="nav-link" data-scroll-target="#logit-function"><span class="toc-section-number">23.3.3</span>  Logit Function</a>
  <ul>
  <li><a href="#logistic-function-inverse-of-the-logit-function" id="toc-logistic-function-inverse-of-the-logit-function" class="nav-link" data-scroll-target="#logistic-function-inverse-of-the-logit-function"><span class="toc-section-number">23.3.3.1</span>  Logistic Function: Inverse of the Logit Function</a></li>
  <li><a href="#logistic-regression-model-summary" id="toc-logistic-regression-model-summary" class="nav-link" data-scroll-target="#logistic-regression-model-summary"><span class="toc-section-number">23.3.3.2</span>  Logistic Regression Model Summary</a></li>
  </ul></li>
  <li><a href="#the-logistic-function" id="toc-the-logistic-function" class="nav-link" data-scroll-target="#the-logistic-function"><span class="toc-section-number">23.3.4</span>  The Logistic Function</a></li>
  </ul></li>
  <li><a href="#the-logistic-regression-model" id="toc-the-logistic-regression-model" class="nav-link" data-scroll-target="#the-logistic-regression-model"><span class="toc-section-number">23.4</span>  The Logistic Regression Model</a>
  <ul>
  <li><a href="#example-calcualtion" id="toc-example-calcualtion" class="nav-link" data-scroll-target="#example-calcualtion"><span class="toc-section-number">23.4.1</span>  Example Calcualtion</a></li>
  <li><a href="#parameter-interpretation" id="toc-parameter-interpretation" class="nav-link" data-scroll-target="#parameter-interpretation"><span class="toc-section-number">23.4.2</span>  Parameter Interpretation</a></li>
  <li><a href="#comparison-to-linear-regression" id="toc-comparison-to-linear-regression" class="nav-link" data-scroll-target="#comparison-to-linear-regression"><span class="toc-section-number">23.4.3</span>  Comparison to Linear Regression</a>
  <ul>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression"><span class="toc-section-number">23.4.3.1</span>  Linear Regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="toc-section-number">23.4.3.2</span>  Logistic Regression</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="toc-section-number">23.5</span>  Parameter Estimation</a>
  <ul>
  <li><a href="#pitfalls-of-squared-error-loss" id="toc-pitfalls-of-squared-error-loss" class="nav-link" data-scroll-target="#pitfalls-of-squared-error-loss"><span class="toc-section-number">23.5.1</span>  Pitfalls of Squared Error Loss</a>
  <ul>
  <li><a href="#non-convex" id="toc-non-convex" class="nav-link" data-scroll-target="#non-convex"><span class="toc-section-number">23.5.1.1</span>  1. Non-convex</a></li>
  <li><a href="#bounded" id="toc-bounded" class="nav-link" data-scroll-target="#bounded"><span class="toc-section-number">23.5.1.2</span>  2. Bounded</a></li>
  <li><a href="#conceptually-questionable" id="toc-conceptually-questionable" class="nav-link" data-scroll-target="#conceptually-questionable"><span class="toc-section-number">23.5.1.3</span>  3. Conceptually Questionable</a></li>
  </ul></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss"><span class="toc-section-number">23.5.2</span>  Cross-Entropy Loss</a>
  <ul>
  <li><a href="#convex" id="toc-convex" class="nav-link" data-scroll-target="#convex"><span class="toc-section-number">23.5.2.1</span>  1. Convex</a></li>
  <li><a href="#strong-error-penalization" id="toc-strong-error-penalization" class="nav-link" data-scroll-target="#strong-error-penalization"><span class="toc-section-number">23.5.2.2</span>  2. Strong Error Penalization</a></li>
  <li><a href="#conceptually-sound" id="toc-conceptually-sound" class="nav-link" data-scroll-target="#conceptually-sound"><span class="toc-section-number">23.5.2.3</span>  3. Conceptually Sound</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#deriving-the-logistic-regression-model-using-the-graph-of-averages" id="toc-deriving-the-logistic-regression-model-using-the-graph-of-averages" class="nav-link" data-scroll-target="#deriving-the-logistic-regression-model-using-the-graph-of-averages"><span class="toc-section-number">23.6</span>  Deriving the Logistic Regression Model Using the Graph of Averages</a>
  <ul>
  <li><a href="#the-graph-of-averages" id="toc-the-graph-of-averages" class="nav-link" data-scroll-target="#the-graph-of-averages"><span class="toc-section-number">23.6.1</span>  The Graph of Averages</a></li>
  <li><a href="#transforming-the-graph-of-averages" id="toc-transforming-the-graph-of-averages" class="nav-link" data-scroll-target="#transforming-the-graph-of-averages"><span class="toc-section-number">23.6.2</span>  Transforming the Graph of Averages</a>
  <ul>
  <li><a href="#transform-p-until-it-looks-linear" id="toc-transform-p-until-it-looks-linear" class="nav-link" data-scroll-target="#transform-p-until-it-looks-linear"><span class="toc-section-number">23.6.2.1</span>  1. Transform <span class="math inline">\(p\)</span> until it looks linear</a></li>
  <li><a href="#use-algebra-to-invert-all-transformations" id="toc-use-algebra-to-invert-all-transformations" class="nav-link" data-scroll-target="#use-algebra-to-invert-all-transformations"><span class="toc-section-number">23.6.2.2</span>  2. Use algebra to invert all transformations</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Up until now, we’ve focused on modeling regression problems - that is, predicting a <em>numerical</em> quantity from a given dataset. We explored the following workflow in the context of linear regression:</p>
<ol type="1">
<li>Choose a model</li>
<li>Choose a loss function</li>
<li>Fit the model</li>
<li>Evaluate model performance</li>
</ol>
<p>In this lecture, we will look at an entirely new modeling problem in <strong>classification</strong>, which involves predicting a <em>categorical</em> variable. We will discuss how to modify the steps in our existing modeling framework to meet the needs of this new machine learning problem.</p>
<section id="regression-vs.-classification" class="level2" data-number="23.1">
<h2 data-number="23.1" class="anchored" data-anchor-id="regression-vs.-classification"><span class="header-section-number">23.1</span> Regression vs.&nbsp;Classification</h2>
<p>You may have seen this diagram which outlines the taxonomy of machine learning.</p>
<p><img src="images/ml_taxonomy.png" class="img-fluid"></p>
<p><br> Regression and classification are both <strong>supervised learning</strong> problems, meaning they consist of models that learn from data where the <span class="math inline">\(y\)</span> observations are known. You’ve likely seen examples of classification models before - for example, the k-nearest neighbors algorithm from Data 8. This lecture will focus on another such model known as <strong>Logistic Regression</strong>.</p>
<p>More generally, a classification problem aims to <em>categorize</em> data. Today, we will focus on <strong>binary classification</strong>, a classification problem where data may only belong to one of two groups. One such example is the result of a coin toss. Many of the principles we’ll learn today can extended to three or more groups - called <strong>multiclass classification</strong>. In the next few lectures, we will learn about models designed specifically for these problems.</p>
</section>
<section id="intuition-the-coin-flip" class="level2" data-number="23.2">
<h2 data-number="23.2" class="anchored" data-anchor-id="intuition-the-coin-flip"><span class="header-section-number">23.2</span> Intuition: The Coin Flip</h2>
<p>To build some intuition for logistic regression, let’s look at an introductory example to classification: the coin flip. Suppose we observe some outcomes of a coin flip (1 = Heads, 0 = Tails).</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>flips</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>[0, 0, 1, 1, 1, 1, 0, 0, 0, 0]</code></pre>
</div>
</div>
<p>For the next flip, would you predict heads or tails?</p>
<p>A reasonable model is to assume all flips are IID (independent and identitically distributed). In other words, each flip has the same probability of returning a 1 (or heads). Let’s define a parameter <span class="math inline">\(\theta\)</span>, the probability that the next flip is a heads. We will use this parameter to inform our decision for <span class="math inline">\(\hat y\)</span>, or our 0 – 1 prediction of the next flip. If <span class="math inline">\(\theta \ge 0.5, \hat y = 1, \text{else } \hat y = 0\)</span>.</p>
<p>You may be inclined to say <span class="math inline">\(0.5\)</span> is the best choice for <span class="math inline">\(\theta\)</span>. However, notice that we made no assumption about the coin itself. The coin may be biased, so we should make our decision based only on the data. We know that exactly <span class="math inline">\(\frac{4}{10}\)</span> of the flips were heads, so we should think <span class="math inline">\(\hat \theta = 0.4\)</span>. In the next section, we will mathematically prove why this is the best possible estimate.</p>
<section id="likelihood-of-data" class="level3" data-number="23.2.1">
<h3 data-number="23.2.1" class="anchored" data-anchor-id="likelihood-of-data"><span class="header-section-number">23.2.1</span> Likelihood of Data</h3>
<p>Let’s call the result of the coin flip a random variable <span class="math inline">\(Y\)</span>. This is a Bernoulli random variable with two outcomes. <span class="math inline">\(Y\)</span> has the following distribution:</p>
<p><span class="math display">\[P(Y = y) = \begin{cases}
        1, \text{with probability }  \theta\\
        0, \text{with probability }  1 - \theta
    \end{cases} \]</span></p>
<p><span class="math inline">\(\theta\)</span> is unknown to us. But we can find the <span class="math inline">\(\theta\)</span> that makes the data we observed the most <em>likely</em>, by looking towards the <strong>likelihood</strong> of the data. The likelihood is proportional to the probability of observing the data.</p>
<p>The probability of observing 4 Heads and 6 Tails follows the Binomial Distribution.</p>
<p><span class="math display">\[\binom{10}{4} (\theta)^4 (1-\theta)^6\]</span></p>
<p>The likelihood is proportional to the probability above. To find it, simply multiply the probabilities of obtaining each coin flip.</p>
<p><span class="math display">\[(\theta)^{4} (1-\theta)^6\]</span></p>
<p>The technique known as <strong>maximum likelihood estimation</strong> finds the <span class="math inline">\(\theta\)</span> that maximizes the above likelihood. You can find this maximum by taking the derivative of the likelihood, but we’ll provide a more intuitive graphical solution.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.plot(theta, theta<span class="op">**</span><span class="dv">4</span> <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>theta)<span class="op">**</span><span class="dv">6</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="logistic_reg_1_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>You can see this function is maximized at the value <span class="math inline">\(0.4\)</span>. Thus, <span class="math inline">\(\hat\theta = 0.4\)</span>. We will revisit these methods in our derivation of the logistic regression model.</p>
</section>
</section>
<section id="deriving-the-logistic-regression-model" class="level2" data-number="23.3">
<h2 data-number="23.3" class="anchored" data-anchor-id="deriving-the-logistic-regression-model"><span class="header-section-number">23.3</span> Deriving the Logistic Regression Model</h2>
<section id="probabalistic-model" class="level3" data-number="23.3.1">
<h3 data-number="23.3.1" class="anchored" data-anchor-id="probabalistic-model"><span class="header-section-number">23.3.1</span> Probabalistic Model</h3>
<p>What if instead of a 0-1 prediction response, the model returns the probability of data belonging to binary classes? - If the model’s predicted probability is <strong>greater</strong> than 0.5, classify as <strong>class 1</strong>. - If the model’s predicted probability is <strong>less</strong> than 0.5, classify as <strong>class 0</strong>.</p>
<p>The model still uses the concept of classification activation as a way to aggregate multiple features using a weighted combination of features.</p>
<p>The probabilistic classifier learns to generate the following probabilities and with the decision threshold of 0.5, classifies datapoints into two categories:</p>
<p><img src="images/classifier_rule.png" alt="classifier_rule" width="500"></p>
</section>
<section id="odds-ratio" class="level3" data-number="23.3.2">
<h3 data-number="23.3.2" class="anchored" data-anchor-id="odds-ratio"><span class="header-section-number">23.3.2</span> Odds Ratio</h3>
<p><strong>Odds</strong> is defined as the ration of the probability of an event happening (<span class="math inline">\(p\)</span>) vs.&nbsp;it not happening (<span class="math inline">\(1-p\)</span>). - “9 to 1 against” means <span class="math inline">\(p = 0.1\)</span> - “Even odds” means <span class="math inline">\(p = 0.5\)</span> - “3 to 1 on” means <span class="math inline">\(p = 0.75\)</span></p>
<p><span class="math display">\[\text{odds}(p) = \frac{p}{1 - p}\]</span></p>
<p><strong>Domain</strong> of odds function: <span class="math inline">\([0, 1)\)</span></p>
<p><strong>Range</strong> of odds function: <span class="math inline">\([0, +\infty)\)</span></p>
<p>Thus, the odds function maps values on <span class="math inline">\([0, 1)\)</span> to <span class="math inline">\([1, \infty)\)</span>.</p>
<p><img src="images/odds_ratio.png" alt="odds_ratio" width="500"></p>
</section>
<section id="logit-function" class="level3" data-number="23.3.3">
<h3 data-number="23.3.3" class="anchored" data-anchor-id="logit-function"><span class="header-section-number">23.3.3</span> Logit Function</h3>
<p>Our end goal is to use a classification activation to output a probability value based on some unbounded real number. Currently, the odds ratio takes a probability value and outputs a positive real number. If we further transform the odds function to obtain a function that maps a probability to an unbouded real number, the inverse of that function will be our desired result.</p>
<p>Thus, we first need a transformation takes the ouput of the odds function and maps it to an unbounded real number (i.e.&nbsp;maps <span class="math inline">\([0, \infty)\)</span> to <span class="math inline">\((-\infty, \infty)\)</span>).</p>
<p>Taking the <strong>log</strong> of a positive real number creates an unbounded real number!</p>
<p>Applying this transformation to the odds ratio gives us the <strong>Logit Function</strong>:</p>
<p><span class="math display">\[\text{log-odds}(p) = \text{log}(\frac{p}{1-p})\]</span></p>
<p><img src="images/logit_graph.png" alt="logit_graph" width="500"></p>
<section id="logistic-function-inverse-of-the-logit-function" class="level4" data-number="23.3.3.1">
<h4 data-number="23.3.3.1" class="anchored" data-anchor-id="logistic-function-inverse-of-the-logit-function"><span class="header-section-number">23.3.3.1</span> Logistic Function: Inverse of the Logit Function</h4>
<p><span class="math display">\[\text{log}(\frac{p}{1-p}) = z\]</span> <span class="math display">\[\frac{p}{1-p} = e^{z}\]</span> <span class="math display">\[p = e^{z} - pe^{z}\]</span> <span class="math display">\[p + pe^{z} = e^{z}\]</span> <span class="math display">\[(1 + e^{z})p = e^{z}\]</span> <span class="math display">\[p = \frac{e^{z}}{1 + e^{z}}\]</span> <span class="math display">\[p = \frac{1}{1 + e^{-z}}\]</span></p>
</section>
<section id="logistic-regression-model-summary" class="level4" data-number="23.3.3.2">
<h4 data-number="23.3.3.2" class="anchored" data-anchor-id="logistic-regression-model-summary"><span class="header-section-number">23.3.3.2</span> Logistic Regression Model Summary</h4>
<ol type="1">
<li>Optimize the model to find parameters <span class="math inline">\(\vec{\hat{\theta}}\)</span></li>
<li>Use parameters to calculate classification activation: <span class="math inline">\(z = \vec{\hat{\theta}}^{T}\vec{x}\)</span></li>
<li>Apply the logistic function to get the probabability of a given datapoint belonging to class 1.</li>
</ol>
</section>
</section>
<section id="the-logistic-function" class="level3" data-number="23.3.4">
<h3 data-number="23.3.4" class="anchored" data-anchor-id="the-logistic-function"><span class="header-section-number">23.3.4</span> The Logistic Function</h3>
<p>While it may look daunting, the logistic regression model is just a composition of two simpler functions. One of these we know well from OLS – the linear model <span class="math inline">\(x^T\theta\)</span>. The second is the <strong>logistic function</strong> – commonly referred to as the <strong>sigmoid function</strong>.</p>
<p><span class="math display">\[\sigma(t) = \frac{1}{1 + e^{-t}}\]</span></p>
<p><img src="images/logistic_func.png" alt="logistic_func" width="500"></p>
<p>This logistic function has various properties, although we won’t prove any of them.</p>
<ol type="1">
<li>Definition: <span class="math inline">\(\sigma(t) = \frac{1}{1 + e^{-t}} = \frac{e^t}{1 + e^{t}}\)</span></li>
<li>Reflection/Symmetry: <span class="math inline">\(1 - \sigma(t) = \frac{e^{-t}}{1 + e^{-t}} = \sigma(-t)\)</span></li>
<li>Inverse: <span class="math inline">\(t = \sigma^{-1}(p) = \log(\frac{p}{1-p})\)</span></li>
<li>Domain: <span class="math inline">\(-\infty &lt; t &lt; \infty\)</span></li>
<li>Range: <span class="math inline">\(0 &lt; \sigma(t) &lt; 1\)</span></li>
</ol>
<p>You’ll notice the logistic regression model is just the logistic function evaluated on the linear model <span class="math inline">\(x^T\theta\)</span>. The logistic function squashes the result of <span class="math inline">\(x^T\theta\)</span> to a probability between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Larger values of <span class="math inline">\(x^T\theta\)</span> will be mapped to probabilities closer to <span class="math inline">\(1\)</span>, and vice versa. Putting it all together, we have:</p>
<p><span class="math display">\[P(Y = 1|x) = \sigma(x^T\theta) = \frac{1}{1 + e^{-x^T\theta}}\]</span></p>
</section>
</section>
<section id="the-logistic-regression-model" class="level2" data-number="23.4">
<h2 data-number="23.4" class="anchored" data-anchor-id="the-logistic-regression-model"><span class="header-section-number">23.4</span> The Logistic Regression Model</h2>
<p>The figure below summarizes the logistic regression process:</p>
<p><img src="images/logistic_regression_process.png" alt="logistic_regression_process" width="500"></p>
<section id="example-calcualtion" class="level3" data-number="23.4.1">
<h3 data-number="23.4.1" class="anchored" data-anchor-id="example-calcualtion"><span class="header-section-number">23.4.1</span> Example Calcualtion</h3>
<p>Suppose we want to predict the probability that a tumor is malignant, given it’s mean radius and mean smoothness. Assume that we fit a logistic regression model (with no intercept), and somehow estimate the optimal parameters: <span class="math inline">\(\hat \theta = [0.1, -0.5]\)</span>.</p>
<p>Say we encounter a new breast tumor with data: <span class="math inline">\(x = [15, 1]\)</span>. We want to answer two questions:</p>
<ol type="1">
<li>What is the probability that this tumor is malignant?</li>
<li>Should we predict the tumor is malignant or benign?</li>
</ol>
<p><span class="math display">\[P(Y = 1|x) = \sigma(x^T\hat\theta)\]</span> <span class="math display">\[= \sigma(0.1 * 15 + (-0.5) * 1) = \sigma(1)\]</span> <span class="math display">\[= \frac{1}{1 + e^{-1}}\]</span> <span class="math display">\[\approx 0.7311\]</span></p>
<p>Because the tumor has a <span class="math inline">\(73\%\)</span> chance of being malignant, we should predict <span class="math inline">\(\hat y = 1\)</span>.</p>
</section>
<section id="parameter-interpretation" class="level3" data-number="23.4.2">
<h3 data-number="23.4.2" class="anchored" data-anchor-id="parameter-interpretation"><span class="header-section-number">23.4.2</span> Parameter Interpretation</h3>
<p>In our derivation of logistic regression, we showed that <span class="math inline">\(\frac{p}{1-p} = e^{x^T\theta}\)</span>. Equivalently, <span class="math display">\[\frac{P(Y=1|x)}{P(Y=0|x)} = e^{x^T\theta}\]</span></p>
<p>Imagine our linear component has just a single feature, along with an intercept term.</p>
<p><span class="math display">\[\frac{P(Y=1|x)}{P(Y=0|x)} = e^{\theta_0 + \theta_1 x}\]</span></p>
<p>What happens when you increase <span class="math inline">\(x\)</span> by one unit?</p>
<ul>
<li>Odds is multiplied by <span class="math inline">\(e^{\theta_1}\)</span></li>
<li>If <span class="math inline">\(\theta_1 &gt; 0\)</span>, then odds increase.</li>
<li>If <span class="math inline">\(\theta_1 &lt; 0\)</span>, then odds decrease.</li>
</ul>
<p>Rememer, the odds ratio can be interpreted as the “number of successes for each failure.” When odds increase, the proportion of successes likewise increases.</p>
</section>
<section id="comparison-to-linear-regression" class="level3" data-number="23.4.3">
<h3 data-number="23.4.3" class="anchored" data-anchor-id="comparison-to-linear-regression"><span class="header-section-number">23.4.3</span> Comparison to Linear Regression</h3>
<section id="linear-regression" class="level4" data-number="23.4.3.1">
<h4 data-number="23.4.3.1" class="anchored" data-anchor-id="linear-regression"><span class="header-section-number">23.4.3.1</span> Linear Regression</h4>
<p><img src="images/linear_regression.png" alt="linear_regression" width="800"></p>
</section>
<section id="logistic-regression" class="level4" data-number="23.4.3.2">
<h4 data-number="23.4.3.2" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">23.4.3.2</span> Logistic Regression</h4>
<p><img src="images/logistic_regression.png" alt="logistic_regression" width="800"></p>
</section>
</section>
</section>
<section id="parameter-estimation" class="level2" data-number="23.5">
<h2 data-number="23.5" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">23.5</span> Parameter Estimation</h2>
<p>Having derived the logistic regression model, the next step is to estimate the optimal parameters <span class="math inline">\(\hat\theta\)</span>.</p>
<section id="pitfalls-of-squared-error-loss" class="level3" data-number="23.5.1">
<h3 data-number="23.5.1" class="anchored" data-anchor-id="pitfalls-of-squared-error-loss"><span class="header-section-number">23.5.1</span> Pitfalls of Squared Error Loss</h3>
<p>In linear regression, we found these optimal parameters by minimizing a cost function. The same applies to logistic regression. Let’s begin by evaluating the MSE loss function on the logistic regression model.</p>
<p><span class="math display">\[L(\theta) = \frac{1}{n}\sum_{i=1}^{n} (y_i - \sigma(x_i^T\theta))^2\]</span></p>
<p>However, there are 3 flaws with this approach.</p>
<section id="non-convex" class="level4" data-number="23.5.1.1">
<h4 data-number="23.5.1.1" class="anchored" data-anchor-id="non-convex"><span class="header-section-number">23.5.1.1</span> 1. Non-convex</h4>
<p>The MSE loss surface for logistic regression is non-convex. In the following example, you can see the function rises above the secant line, a clear violation of convexity.</p>
<p><img src="images/mse_secant_line.png" alt="mse_secant_line" width="500"></p>
<p>Depending on the initialization points, gradient descent may find multiple non-optimal solutions.</p>
</section>
<section id="bounded" class="level4" data-number="23.5.1.2">
<h4 data-number="23.5.1.2" class="anchored" data-anchor-id="bounded"><span class="header-section-number">23.5.1.2</span> 2. Bounded</h4>
<p>A good loss function should penalize incorrect predictions; the MSE doesn’t do this. Consider the following scenario.</p>
<ul>
<li>The predicted probability that some tumor is malignant is <span class="math inline">\(0.99\)</span> (<span class="math inline">\(\hat y = 0.99\)</span>). However, this tumor is truly benign (<span class="math inline">\(y = 0\)</span>). The loss incurred by this misprediction is only <span class="math inline">\((0 - 0.99)^2 = 0.98\)</span>.</li>
</ul>
<p>In fact, loss is always bounded <span class="math inline">\(&lt; 1\)</span>.</p>
</section>
<section id="conceptually-questionable" class="level4" data-number="23.5.1.3">
<h4 data-number="23.5.1.3" class="anchored" data-anchor-id="conceptually-questionable"><span class="header-section-number">23.5.1.3</span> 3. Conceptually Questionable</h4>
<p>The MSE loss function itself is conceptually questionable.</p>
<ul>
<li><span class="math inline">\(y\)</span> is a class label</li>
<li><span class="math inline">\(\hat y\)</span> is a probability</li>
</ul>
<p>Why are we squaring the difference of a class label and probability?</p>
</section>
</section>
<section id="cross-entropy-loss" class="level3" data-number="23.5.2">
<h3 data-number="23.5.2" class="anchored" data-anchor-id="cross-entropy-loss"><span class="header-section-number">23.5.2</span> Cross-Entropy Loss</h3>
<p>This section will introduce a more intuitive loss function - cross-entropy loss. The <strong>cross-entropy loss</strong> is defined as</p>
<p><span class="math display">\[L(\theta) = -(y\text{ log}(p) + (1-y)\text{log}(1-p))\]</span></p>
<p><span class="math display">\[\text{where } p = \sigma(x^T\theta)\]</span></p>
<p>The average risk over an entire dataset is</p>
<p><span class="math display">\[R(\theta) = -\frac{1}{n} \sum_{i=1}^{n}(y_i\text{ log}(p) + (1-y_i)\text{log}(1-p))\]</span></p>
<p>Cross-entropy loss addresses the 3 pitfalls of squared loss.</p>
<ol type="1">
<li>Convex. No local minima for logistic regression</li>
<li>A good measure of model error. Strongly penalizes bad predictions.</li>
<li>Conceptually sound.</li>
</ol>
<section id="convex" class="level4" data-number="23.5.2.1">
<h4 data-number="23.5.2.1" class="anchored" data-anchor-id="convex"><span class="header-section-number">23.5.2.1</span> 1. Convex</h4>
<p>To prove that cross-entropy loss is <em>always</em> convex, we’ll need to take its derivative, which becomes difficult and out of scope. Instead, in Data 100, we will show a proof by picture.</p>
<p>Plotted below is the cross-entropy loss function applied to the same toy dataset as before. As you’ll notice, this is a convex function - any line connecting two points lies entirely <em>above</em> the function.</p>
<p><img src="images/cle-convex.png" alt="cle-convex" width="500"></p>
</section>
<section id="strong-error-penalization" class="level4" data-number="23.5.2.2">
<h4 data-number="23.5.2.2" class="anchored" data-anchor-id="strong-error-penalization"><span class="header-section-number">23.5.2.2</span> 2. Strong Error Penalization</h4>
<p>To understand why cross-entropy loss has strong penalization, we need to take a step back. Let’s decompose the loss function into two components.</p>
<p><span class="math display">\[L(\theta) = -(y\text{ log}(p) + (1-y)\text{log}(1-p))\]</span></p>
<ol type="1">
<li><span class="math inline">\(-(1-y)\text{ log}(1 - p)\)</span>
<ul>
<li>For <span class="math inline">\(y = 0\)</span>, only this term stays in cross-entropy loss</li>
<li>See the plot on the bottom left.
<ul>
<li><span class="math inline">\(p \rightarrow 0\)</span>: zero loss</li>
<li><span class="math inline">\(p \rightarrow 1\)</span>: infinite loss</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(- y\text{ log}(p)\)</span>
<ul>
<li>For <span class="math inline">\(y = 1\)</span>, only this term stays in cross-entropy loss</li>
<li>See the plot on the bottom right.
<ul>
<li><span class="math inline">\(p \rightarrow 0\)</span>: infinite loss</li>
<li><span class="math inline">\(p \rightarrow 1\)</span>: zero loss</li>
</ul></li>
</ul></li>
</ol>
<div class="columns">
<div class="column" style="width:30%;">
<p><img src="images/cle-loss-y_is_0.png" alt="cle-loss-y_is_0" width="400"></p>
</div><div class="column" style="width:20%;">

</div><div class="column" style="width:30%;">
<p><img src="images/cle-loss-y_is_1.png" alt="cle-loss-y_is_1" width="400"></p>
</div>
</div>
</section>
<section id="conceptually-sound" class="level4" data-number="23.5.2.3">
<h4 data-number="23.5.2.3" class="anchored" data-anchor-id="conceptually-sound"><span class="header-section-number">23.5.2.3</span> 3. Conceptually Sound</h4>
<p>To understand why cross-entropy loss makes a great intuitive loss function, we will look towards maximum likelihood estimation in the next section.</p>
</section>
</section>
</section>
<section id="deriving-the-logistic-regression-model-using-the-graph-of-averages" class="level2" data-number="23.6">
<h2 data-number="23.6" class="anchored" data-anchor-id="deriving-the-logistic-regression-model-using-the-graph-of-averages"><span class="header-section-number">23.6</span> Deriving the Logistic Regression Model Using the Graph of Averages</h2>
<p>This section demonstrates an alternative approach to deriving the logistic regression model using the graph of averages. Our goal will be to use the mean radius of a tumor to predict whether a cancer is malignant (<span class="math inline">\(y = 1\)</span>) or not (<span class="math inline">\(y = 0\)</span>). Data from the study is plotted below, along with the overlayed least squares model that minimizes MSE.</p>
<p><img src="images/radius_vs_malig.png" alt="radius_vs_malig" width="500"></p>
<p>Rather than using a linear model, what if used a non-linear model that was a better fit of our data, and bounded predictions between [<span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>]? We can look to the Data 8 Graph of Averages to formulate this model.</p>
<!-- This way, we'd be able classify data as such: $\hat y \ge 0.5$ corresponds to class $1$, and $\hat y < 0.5$ corresponds to class $0$. This is similar to our coin flip example.-->
<section id="the-graph-of-averages" class="level3" data-number="23.6.1">
<h3 data-number="23.6.1" class="anchored" data-anchor-id="the-graph-of-averages"><span class="header-section-number">23.6.1</span> The Graph of Averages</h3>
<p>The Graph of Averages says</p>
<ul>
<li>For each input <span class="math inline">\(x\)</span>, compute the average value of <span class="math inline">\(y\)</span> for all nearby <span class="math inline">\(x\)</span>, and predict that.
<ul>
<li>“nearby” is a loose definition; we’ll say for example the nearest 5 points</li>
</ul></li>
</ul>
<p>Here, <span class="math inline">\(x\)</span> is <code>mean radius</code> and <span class="math inline">\(y\)</span> is <code>malignant</code>. By the definition of an average, the prediction for a given <span class="math inline">\(x\)</span> is thus:</p>
<p><span class="math display">\[\frac{\text{Sum of Nearby ``Malignant" Values}}{\text{Number of Points in Bin}}\]</span></p>
<p>You’ll notice that this is the proportion of malignant tumors in some point <span class="math inline">\(x\)</span>’s bin. In a similar light, we can think of this proportion as the probability that point <span class="math inline">\(x\)</span> belongs to the malignant class (derived from it’s nearest neighbors). That is, <span class="math inline">\(P(Y = 1|x)\)</span>. This notation reads “given a tumor with a mean radius of <span class="math inline">\(x\)</span>, it belongs to class 1 (equivalently, the malignant class) with some specified probability”.</p>
<p>You may recognize some similarities between <span class="math inline">\(P(Y = 1|x)\)</span> and <span class="math inline">\(\hat \theta\)</span> from our earlier coin toss example. <span class="math inline">\(P(Y = 1|x)\)</span> is a proportion of malignant tumors in a bin, whereas <span class="math inline">\(\hat \theta\)</span> was a proportion of heads in 10 coin tosses. Both also represent probabilities of some new point belonging to class <span class="math inline">\(1\)</span>.</p>
<p>Notice how the Graph of Averages is a much better fit of the data.</p>
<p><img src="images/average_y.png" alt="average_y" width="500"></p>
<p>Unfortunately, the Graph of Averages begins to degenerate as we add more features. The exact reason is out of scope, but this model becomes harder to use in higher dimensions. Instead, we use <strong>Logistic Regression</strong>: a probabilistic model that tries to model the Graph of Averages.</p>
<p>As a recap of what we’ve discussed so far:</p>
<ul>
<li>We want to fit this “S” shaped curve as best as possible</li>
<li>This curve models the probability of belonging to class <span class="math inline">\(1\)</span>
<ul>
<li><span class="math inline">\(P(Y = 1|x)\)</span></li>
</ul></li>
</ul>
</section>
<section id="transforming-the-graph-of-averages" class="level3" data-number="23.6.2">
<h3 data-number="23.6.2" class="anchored" data-anchor-id="transforming-the-graph-of-averages"><span class="header-section-number">23.6.2</span> Transforming the Graph of Averages</h3>
<p>Our goal here is to find a mathematical function that models the Graph of Averages, or <span class="math inline">\(P(Y = 1|x)\)</span>. For shorthand, let’s call this function <span class="math inline">\(p\)</span>. We will</p>
<ol type="1">
<li>Transform <span class="math inline">\(p\)</span> until it looks linear.</li>
<li>Then, use algebra to invert all transformations.</li>
</ol>
<p><strong>Note</strong>: <span class="math inline">\(P(Y = 1|x)\)</span> and <span class="math inline">\(p\)</span> are used interchangeably throughout the rest of the note.</p>
<section id="transform-p-until-it-looks-linear" class="level4" data-number="23.6.2.1">
<h4 data-number="23.6.2.1" class="anchored" data-anchor-id="transform-p-until-it-looks-linear"><span class="header-section-number">23.6.2.1</span> 1. Transform <span class="math inline">\(p\)</span> until it looks linear</h4>
<p>To transform this “S” shaped curve, we will define a quantity called the odds ratio. The <strong>odds of an event</strong> is the probability the event happens divided by the probability that it doesn’t happen. Remember, <span class="math inline">\(p\)</span> is shorthand for our event <span class="math inline">\(P(Y = 1|x)\)</span>.</p>
<p><span class="math display">\[\text{odds}(p) = \frac{p}{1 - p}\]</span></p>
<p>Here, we’ve applied the odds ratio to our Graph of Averages. The result is an exponential relationship.</p>
<p><img src="images/odds.png" alt="odds" width="500"></p>
<p>Applying a logarithmic transformation to the y-axis will linearize the data. We know this from the Tukey-Mosteller Buldge Diagram.</p>
<p><span class="math display">\[\text{log-odds}(p) = \text{log}(\frac{p}{1-p})\]</span></p>
<p><img src="images/log_odds.png" alt="log_odds" width="500"></p>
<p>We have a roughly linear relationship! Specifically, this is linear in our features <span class="math inline">\(x\)</span> and parameters <span class="math inline">\(\theta\)</span>. In our one dimensional feature space, <span class="math inline">\(x\)</span> is the <code>mean radius</code> and <span class="math inline">\(\theta\)</span> some constant.</p>
<p><span class="math display">\[\text{log}(\frac{p}{1-p}) = x^T\theta\]</span></p>
</section>
<section id="use-algebra-to-invert-all-transformations" class="level4" data-number="23.6.2.2">
<h4 data-number="23.6.2.2" class="anchored" data-anchor-id="use-algebra-to-invert-all-transformations"><span class="header-section-number">23.6.2.2</span> 2. Use algebra to invert all transformations</h4>
<p>Remember our original goal was to model the Graph of Averages, which we called <span class="math inline">\(p\)</span>. Here, we will algebraically solve for <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[\text{log}(\frac{p}{1-p}) = x^T\theta\]</span> <span class="math display">\[\frac{p}{1-p} = e^{x^T\theta}\]</span> <span class="math display">\[p = e^{x^T\theta} - pe^{x^T\theta}\]</span> <span class="math display">\[p = \frac{e^{x^T\theta}}{1 + e^{x^T\theta}}\]</span> <span class="math display">\[p = \frac{1}{1 + e^{-x^T\theta}}\]</span></p>
<p>We once again end up modeling <span class="math inline">\(p\)</span> with the sigmoid (logistic) function!</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sql_II/sql_II.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../logistic_regression_2/logistic_reg_2.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>