<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 22&nbsp; Logistic Regression I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../logistic_regression_2/logistic_reg_2.html" rel="next">
<link href="../sql_II/sql_II.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../logistic_regression_1/logistic_reg_1.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causal Inference and Confounding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#classification" id="toc-classification" class="nav-link active" data-scroll-target="#classification"><span class="header-section-number">22.1</span> Classification</a>
  <ul>
  <li><a href="#modeling-process" id="toc-modeling-process" class="nav-link" data-scroll-target="#modeling-process"><span class="header-section-number">22.1.1</span> Modeling Process</a></li>
  </ul></li>
  <li><a href="#deriving-the-logistic-regression-model" id="toc-deriving-the-logistic-regression-model" class="nav-link" data-scroll-target="#deriving-the-logistic-regression-model"><span class="header-section-number">22.2</span> Deriving the Logistic Regression Model</a>
  <ul>
  <li><a href="#graph-of-averages" id="toc-graph-of-averages" class="nav-link" data-scroll-target="#graph-of-averages"><span class="header-section-number">22.2.1</span> Graph of Averages</a></li>
  <li><a href="#handling-non-linear-output" id="toc-handling-non-linear-output" class="nav-link" data-scroll-target="#handling-non-linear-output"><span class="header-section-number">22.2.2</span> Handling Non-Linear Output</a>
  <ul>
  <li><a href="#step-1-linearize-the-relationship" id="toc-step-1-linearize-the-relationship" class="nav-link" data-scroll-target="#step-1-linearize-the-relationship"><span class="header-section-number">22.2.2.1</span> Step 1: Linearize the Relationship</a></li>
  <li><a href="#step-2-find-a-linear-combination-of-the-features" id="toc-step-2-find-a-linear-combination-of-the-features" class="nav-link" data-scroll-target="#step-2-find-a-linear-combination-of-the-features"><span class="header-section-number">22.2.2.2</span> Step 2: Find a Linear Combination of the Features</a></li>
  <li><a href="#step-3-transforming-back-to-original-variables" id="toc-step-3-transforming-back-to-original-variables" class="nav-link" data-scroll-target="#step-3-transforming-back-to-original-variables"><span class="header-section-number">22.2.2.3</span> Step 3: Transforming Back to Original Variables</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#the-logistic-regression-model" id="toc-the-logistic-regression-model" class="nav-link" data-scroll-target="#the-logistic-regression-model"><span class="header-section-number">22.3</span> The Logistic Regression Model</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss"><span class="header-section-number">22.4</span> Cross-Entropy Loss</a>
  <ul>
  <li><a href="#why-not-mse" id="toc-why-not-mse" class="nav-link" data-scroll-target="#why-not-mse"><span class="header-section-number">22.4.1</span> Why Not MSE?</a></li>
  <li><a href="#motivating-cross-entropy-loss" id="toc-motivating-cross-entropy-loss" class="nav-link" data-scroll-target="#motivating-cross-entropy-loss"><span class="header-section-number">22.4.2</span> Motivating Cross-Entropy Loss</a></li>
  </ul></li>
  <li><a href="#decision-boundaries" id="toc-decision-boundaries" class="nav-link" data-scroll-target="#decision-boundaries"><span class="header-section-number">22.5</span> Decision Boundaries</a></li>
  <li><a href="#bonus-maximum-likelihood-estimation" id="toc-bonus-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#bonus-maximum-likelihood-estimation"><span class="header-section-number">22.6</span> [BONUS] Maximum Likelihood Estimation</a>
  <ul>
  <li><a href="#building-intuition-the-coin-flip" id="toc-building-intuition-the-coin-flip" class="nav-link" data-scroll-target="#building-intuition-the-coin-flip"><span class="header-section-number">22.6.1</span> Building Intuition: The Coin Flip</a></li>
  <li><a href="#likelihood-of-data" id="toc-likelihood-of-data" class="nav-link" data-scroll-target="#likelihood-of-data"><span class="header-section-number">22.6.2</span> Likelihood of Data</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Understand the difference between regression and classification</li>
<li>Derive the logistic regression model for classifying data</li>
<li>Quantify the error of our logistic regression model with cross-entropy loss</li>
</ul>
</div>
</div>
</div>
<p>Up until this point in the class , we’ve focused on <strong>regression</strong> tasks - that is, predicting an <em>unbounded numerical quantity</em> from a given dataset. We discussed optimization, feature engineering, and regularization all in the context of performing regression to predict some quantity.</p>
<p>Now that we have this deep understanding of the modeling process, let’s expand our knowledge of possible modeling tasks.</p>
<section id="classification" class="level2" data-number="22.1">
<h2 data-number="22.1" class="anchored" data-anchor-id="classification"><span class="header-section-number">22.1</span> Classification</h2>
<p>In the next two lectures, we’ll tackle the task of <strong>classification</strong>. A classification problem aims to classify data into <em>categories</em>. Unlike in regression, where we predicted a numeric output, classification involves predicting some <strong>categorical variable</strong>, or <strong>response</strong>, <span class="math inline">\(y\)</span>. Examples of classification tasks include:</p>
<ul>
<li>Predicting which team won from its turnover percentage</li>
<li>Predicting the day of the week of a meal from the total restaurant bill</li>
<li>Predicting the model of car from its horsepower</li>
</ul>
<p>There are a couple of different types of classification:</p>
<ul>
<li><strong>Binary classification</strong>: classify data into two classes, and responses <span class="math inline">\(y\)</span> are either 0 or 1</li>
<li><strong>Multiclass classification</strong>: classify data into multiple classes (e.g., image labeling, next word in a sentence, etc.)</li>
</ul>
<p>We can further combine multiple related classfication predictions (e.g., translation, voice recognition, etc.) to tackle complex problems through structured prediction tasks.</p>
<p>In Data 100, we will mostly deal with <strong>binary classification</strong>, where we are attempting to classify data into one of two classes.</p>
<section id="modeling-process" class="level3" data-number="22.1.1">
<h3 data-number="22.1.1" class="anchored" data-anchor-id="modeling-process"><span class="header-section-number">22.1.1</span> Modeling Process</h3>
<p>To build a classification model, we need to modify our modeling workflow slightly. Recall that in regression we:</p>
<ol type="1">
<li>Created a design matrix of numeric features</li>
<li>Defined our model as a linear combination of these numeric features</li>
<li>Used the model to output numeric predictions</li>
</ol>
<p>In classification, however, we no longer want to output numeric predictions; instead, we want to predict the class to which a datapoint belongs. This means that we need to update our workflow. To build a classification model, we will:</p>
<ol type="1">
<li>Create a design matrix of numeric features.</li>
<li>Define our model as a linear combination of these numeric features, transformed by a non-linear <strong>sigmoid function</strong>. This outputs a numeric quantity.</li>
<li>Apply a <strong>decision rule</strong> to interpret the outputted quantity and decide a classification.</li>
<li>Output a predicted class.</li>
</ol>
<p>There are two key differences: as we’ll soon see, we need to incorporate a non-linear transformation to capture the non-linear relationships hidden in our data. We do so by applying the sigmoid function to a linear combination of the features. Secondly, we must apply a decision rule to convert the numeric quantities computed by our model into an actual class prediction. This can be as simple as saying that any datapoint with a feature greater than some number <span class="math inline">\(x\)</span> belongs to Class 1.</p>
<p><strong>Regression:</strong></p>
<center>
<img src="images/reg.png" alt="reg" width="750">
</center>
<p><strong>Classification:</strong></p>
<center>
<img src="images/class.png" alt="class" width="750">
</center>
<p>This was a very high-level overview. Let’s walk through the process in detail to clarify what we mean.</p>
</section>
</section>
<section id="deriving-the-logistic-regression-model" class="level2" data-number="22.2">
<h2 data-number="22.2" class="anchored" data-anchor-id="deriving-the-logistic-regression-model"><span class="header-section-number">22.2</span> Deriving the Logistic Regression Model</h2>
<p>Throughout this lecture, we will work with the <code>games</code> dataset, which contains information about games played in the NBA basketball league. Our goal will be to use a basketball team’s <code>"GOAL_DIFF"</code> to predict whether or not a given team won their game (<code>"WON"</code>). If a team wins their game, we’ll say they belong to Class 1. If they lose, they belong to Class 0.</p>
<p>For those who are curious, <code>"GOAL_DIFF"</code> represents the difference in successful field goal percentages between the two competing teams.</p>
<div id="a0291278" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.seterr(divide<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>games <span class="op">=</span> pd.read_csv(<span class="st">"data/games"</span>).dropna()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>games.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">GAME_ID</th>
<th data-quarto-table-cell-role="th">TEAM_NAME</th>
<th data-quarto-table-cell-role="th">MATCHUP</th>
<th data-quarto-table-cell-role="th">WON</th>
<th data-quarto-table-cell-role="th">GOAL_DIFF</th>
<th data-quarto-table-cell-role="th">AST</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>21701216</td>
<td>Dallas Mavericks</td>
<td>DAL vs. PHX</td>
<td>0</td>
<td>-0.251</td>
<td>20</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>21700846</td>
<td>Phoenix Suns</td>
<td>PHX @ GSW</td>
<td>0</td>
<td>-0.237</td>
<td>13</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>21700071</td>
<td>San Antonio Spurs</td>
<td>SAS @ ORL</td>
<td>0</td>
<td>-0.234</td>
<td>19</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>21700221</td>
<td>New York Knicks</td>
<td>NYK @ TOR</td>
<td>0</td>
<td>-0.234</td>
<td>17</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>21700306</td>
<td>Miami Heat</td>
<td>MIA @ NYK</td>
<td>0</td>
<td>-0.222</td>
<td>21</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Let’s visualize the relationship between <code>"GOAL_DIFF"</code> and <code>"WON"</code> using the Seaborn function <code>sns.stripplot</code>. A strip plot automatically introduces a small amount of random noise to <strong>jitter</strong> the data. Recall that all values in the <code>"WON"</code> column are either 1 (won) or 0 (lost) – if we were to directly plot them without jittering, we would see severe overplotting.</p>
<div id="dbda1d66" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>sns.stripplot(data<span class="op">=</span>games, x<span class="op">=</span><span class="st">"GOAL_DIFF"</span>, y<span class="op">=</span><span class="st">"WON"</span>, orient<span class="op">=</span><span class="st">"h"</span>, hue<span class="op">=</span><span class="st">'WON'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># By default, sns.stripplot plots 0, then 1. We invert the y axis to reverse this behavior</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-3-output-1.png" width="580" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This dataset is unlike anything we’ve seen before – our target variable contains only two unique values! (Remember that each y value is either 0 or 1; the plot above jitters the y data slightly for ease of reading.)</p>
<p>The regression models we have worked with always assumed that we were attempting to predict a continuous target. If we apply a linear regression model to this dataset, something strange happens.</p>
<div id="c2d93f13" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.linear_model <span class="im">as</span> lm</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> games[[<span class="st">"GOAL_DIFF"</span>]], games[<span class="st">"WON"</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>regression_model <span class="op">=</span> lm.LinearRegression()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>regression_model.fit(X, Y)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X.squeeze(), regression_model.predict(X), <span class="st">"k"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>sns.stripplot(data<span class="op">=</span>games, x<span class="op">=</span><span class="st">"GOAL_DIFF"</span>, y<span class="op">=</span><span class="st">"WON"</span>, orient<span class="op">=</span><span class="st">"h"</span>, hue<span class="op">=</span><span class="st">'WON'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-4-output-1.png" width="580" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The linear regression fit follows the data as closely as it can. However, this approach has a key flaw - the predicted output, <span class="math inline">\(\hat{y}\)</span>, can be outside the range of possible classes (there are predictions above 1 and below 0). This means that the output can’t always be interpreted (what does it mean to predict a class of -2.3?).</p>
<p>Our usual linear regression framework won’t work here. Instead, we’ll need to get more creative.</p>
<section id="graph-of-averages" class="level3" data-number="22.2.1">
<h3 data-number="22.2.1" class="anchored" data-anchor-id="graph-of-averages"><span class="header-section-number">22.2.1</span> Graph of Averages</h3>
<p>Back in <a href="https://inferentialthinking.com/chapters/08/1/Applying_a_Function_to_a_Column.html#example-prediction">Data 8</a>, you gradually built up to the concept of linear regression by using the <strong>graph of averages</strong>. Before you knew the mathematical underpinnings of the regression line, you took a more intuitive approach: you bucketed the <span class="math inline">\(x\)</span> data into bins of common values, then computed the average <span class="math inline">\(y\)</span> for all datapoints in the same bin. The result gave you the insight needed to derive the regression fit.</p>
<p>Let’s take the same approach as we grapple with our new classification task. In the cell below, we 1) bucket the <code>"GOAL_DIFF"</code> data into bins of similar values and 2) compute the average <code>"WON"</code> value of all datapoints in a bin.</p>
<div id="45e0f5b0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># bucket the GOAL_DIFF data into 20 bins</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> pd.cut(games[<span class="st">"GOAL_DIFF"</span>], <span class="dv">20</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>games[<span class="st">"bin"</span>] <span class="op">=</span> [(b.left <span class="op">+</span> b.right) <span class="op">/</span> <span class="dv">2</span> <span class="cf">for</span> b <span class="kw">in</span> bins]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>win_rates_by_bin <span class="op">=</span> games.groupby(<span class="st">"bin"</span>)[<span class="st">"WON"</span>].mean()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the graph of averages</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>sns.stripplot(data<span class="op">=</span>games, x<span class="op">=</span><span class="st">"GOAL_DIFF"</span>, y<span class="op">=</span><span class="st">"WON"</span>, orient<span class="op">=</span><span class="st">"h"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, hue<span class="op">=</span><span class="st">'WON'</span>) <span class="co"># alpha makes the points transparent</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.plot(win_rates_by_bin.index, win_rates_by_bin, c<span class="op">=</span><span class="st">"tab:red"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-5-output-1.png" width="580" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Interesting: our result is certainly not like the straight line produced by finding the graph of averages for a linear relationship. We can make two observations:</p>
<ul>
<li>All predictions on our line are between 0 and 1</li>
<li>The predictions are <strong>non-linear</strong>, following a rough “S” shape</li>
</ul>
<p>Let’s think more about what we’ve just done.</p>
<p>To find the average <span class="math inline">\(y\)</span> value for each bin, we computed:</p>
<p><span class="math display">\[\frac{1 \text{(\# Y = 1 in bin)} + 0 \text{(\# Y = 0 in bin)}}{\text{\# datapoints in bin}} = \frac{\text{\# Y = 1 in bin}}{\text{\# datapoints in bin}} = P(\text{Y = 1} | \text{bin})\]</span></p>
<p>This is simply the probability of a datapoint in that bin belonging to Class 1! This aligns with our observation from earlier: all of our predictions lie between 0 and 1, just as we would expect for a probability.</p>
<p>Our graph of averages was really modeling the probability, <span class="math inline">\(p\)</span>, that a datapoint belongs to Class 1, or essentially that <span class="math inline">\(\text{Y = 1}\)</span> for a particular value of <span class="math inline">\(\text{x}\)</span>.</p>
<p><span class="math display">\[ p = P(Y = 1 | \text{ x} )\]</span></p>
<p>In logistic regression, we have a new modeling goal. We want to model the <strong>probability that a particular datapoint belongs to Class 1</strong> by approximating the S-shaped curve we plotted above. However, we’ve only learned about linear modeling techniques like Linear Regression and OLS.</p>
</section>
<section id="handling-non-linear-output" class="level3" data-number="22.2.2">
<h3 data-number="22.2.2" class="anchored" data-anchor-id="handling-non-linear-output"><span class="header-section-number">22.2.2</span> Handling Non-Linear Output</h3>
<p>Fortunately for us, we’re already well-versed with a technique to model non-linear relationships – we can apply non-linear transformations like log or exponents to make a non-linear relationship more linear. Recall the steps we’ve applied previously:</p>
<ol type="1">
<li>Apply <strong>transformations to linearize</strong> the relationship.</li>
<li><strong>Fit a linear model</strong> to the transformed variables</li>
<li>“Undo” our transformations to identify the <strong>underlying relationship</strong> between the original variables</li>
</ol>
<p>In past examples, we used the bulge diagram to help us decide what transformations may be useful. The S-shaped curve we saw above, however, looks nothing like any relationship we’ve seen in the past. We’ll need to think carefully about what transformations will linearize this curve.</p>
<section id="step-1-linearize-the-relationship" class="level4" data-number="22.2.2.1">
<h4 data-number="22.2.2.1" class="anchored" data-anchor-id="step-1-linearize-the-relationship"><span class="header-section-number">22.2.2.1</span> Step 1: Linearize the Relationship</h4>
<p>Let’s consider our eventual goal: determining if we should predict a Class of 0 or 1 for each datapoint. Rephrased, we want to decide if it seems more “likely” that the datapoint belongs to Class 0 or to Class 1. One way of deciding this is to see which class has the higher predicted probability for a given datapoint. The <strong>odds</strong> is defined as the probability of a datapoint Y belonging to Class 1 divided by the probability of it belonging to Class 0.</p>
<p><span class="math display">\[\text{odds} = \frac{P(Y=1|x)}{P(Y=0|x)} = \frac{p}{1-p}\]</span></p>
<p>If we plot the odds for each input <code>"GOAL_DIFF"</code> (<span class="math inline">\(x\)</span>), we see something that looks more promising.</p>
<div id="1635c32b" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> win_rates_by_bin</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>odds <span class="op">=</span> p<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p) </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.plot(odds.index, odds)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"GOAL_DIFF"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Odds $= \frac</span><span class="sc">{p}</span><span class="vs">{1-p}$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-6-output-1.png" width="595" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It turns out that the relationship between our input <code>"GOAL_DIFF"</code> and the odds is roughly exponential! Let’s linearize the exponential by taking the logarithm (as suggested by the <a href="https://ds100.org/course-notes/visualization_2/visualization_2.html#tukey-mosteller-bulge-diagram">Tukey-Mosteller Bulge Diagram</a>). As a reminder, you should assume that any logarithm in Data 100 is the base <span class="math inline">\(e\)</span> natural logarithm unless told otherwise.</p>
<div id="86643327" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>log_odds <span class="op">=</span> np.log(odds)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.plot(odds.index, log_odds, c<span class="op">=</span><span class="st">"tab:green"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"GOAL_DIFF"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Log-Odds $= \log{\frac</span><span class="sc">{p}</span><span class="vs">{1-p</span><span class="sc">}}</span><span class="vs">$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-7-output-1.png" width="597" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="step-2-find-a-linear-combination-of-the-features" class="level4" data-number="22.2.2.2">
<h4 data-number="22.2.2.2" class="anchored" data-anchor-id="step-2-find-a-linear-combination-of-the-features"><span class="header-section-number">22.2.2.2</span> Step 2: Find a Linear Combination of the Features</h4>
<p>We see something promising in the graph above – the relationship between the log-odds and our input feature is approximately linear. We can represent this relationship as a linear combination of the features. In other words:</p>
<p><span class="math display">\[\begin{align}
z &amp;= x^{\top} \theta = \theta_0 + \theta_1 x_1 + ... + \theta_p x_p\\
\end{align}\]</span></p>
<p>Here, we use <span class="math inline">\(x^{\top}\)</span> to represent an observation in our dataset, stored as a row vector. You can imagine it as a single row in our design matrix. <span class="math inline">\(x^{\top} \theta\)</span> indicates a linear combination of the features for this observation (just as we used in multiple linear regression). Remember that our goal is to predict the probability of Class 1. This linear combination z isn’t our final prediction! Recall that z was the log-odds, so we can also write the following below.</p>
<p><span class="math display">\[z = \text{log(odds)} = \text{log} \left(\frac{p}{1-p}\right)\]</span></p>
</section>
<section id="step-3-transforming-back-to-original-variables" class="level4" data-number="22.2.2.3">
<h4 data-number="22.2.2.3" class="anchored" data-anchor-id="step-3-transforming-back-to-original-variables"><span class="header-section-number">22.2.2.3</span> Step 3: Transforming Back to Original Variables</h4>
<p>We’re in good shape! We have now derived the following relationship by setting the two values in Step 2 equal to each other:</p>
<p><span class="math display">\[\log{(\frac{p}{1-p})} = x^{\top} \theta\]</span></p>
<p>Remember that our goal is to predict the probability of a datapoint belonging to Class 1, <span class="math inline">\(p\)</span>. Let’s rearrange this relationship to uncover the original relationship between <span class="math inline">\(p\)</span> and our input data, <span class="math inline">\(x^{\top}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\log{(\frac{p}{1-p})} &amp;= x^T \theta\\
\frac{p}{1-p} &amp;= e^{x^T \theta}\\
p &amp;= (1-p)e^{x^T \theta}\\
p &amp;= e^{x^T \theta}- p e^{x^T \theta}\\
p(1 + e^{x^T \theta}) &amp;= e^{x^T \theta} \\
p &amp;= \frac{e^{x^T \theta}}{1+e^{x^T \theta}}\\
p &amp;= \frac{1}{1+e^{-x^T \theta}}\\
\end{align}\]</span></p>
<p>Phew, that was a lot of algebra. What we’ve uncovered is the <strong>logistic regression model</strong> to predict the probability of a datapoint <span class="math inline">\(x^{\top}\)</span> belonging to Class 1. If we plot this relationship for our data, we see the S-shaped curve from earlier!</p>
<div id="2135b8ab" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll discuss the `LogisticRegression` class next time</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.3</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>logistic_model <span class="op">=</span> lm.LogisticRegression(C<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>logistic_model.fit(X, Y)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>predicted_prob <span class="op">=</span> logistic_model.predict_proba(xs[:, np.newaxis])[:, <span class="dv">1</span>]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>sns.stripplot(data<span class="op">=</span>games, x<span class="op">=</span><span class="st">"GOAL_DIFF"</span>, y<span class="op">=</span><span class="st">"WON"</span>, orient<span class="op">=</span><span class="st">"h"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, hue<span class="op">=</span><span class="st">"WON"</span>, legend<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.plot(xs, predicted_prob, c<span class="op">=</span><span class="st">"k"</span>, lw<span class="op">=</span><span class="dv">3</span>, label<span class="op">=</span><span class="st">"Logistic regression model"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.plot(win_rates_by_bin.index, win_rates_by_bin, lw<span class="op">=</span><span class="dv">2</span>, c<span class="op">=</span><span class="st">"tab:red"</span>, label<span class="op">=</span><span class="st">"Graph of averages"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>plt.gca().invert_yaxis()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-8-output-1.png" width="576" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The S-shaped curve is formally known as the <strong>sigmoid function</strong> and is typically denoted by <span class="math inline">\(\sigma\)</span>.</p>
<p><span class="math display">\[\sigma(t) = \frac{1}{1+e^{-t}}\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of the Sigmoid
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Reflection/Symmetry: <span class="math display">\[1-\sigma(t) = \frac{e^{-t}}{1+e^{-t}}=\sigma(-t)\]</span></li>
<li>Inverse: <span class="math display">\[t=\sigma^{-1}(p)=\log{(\frac{p}{1-p})}\]</span></li>
<li>Derivative: <span class="math display">\[\frac{d}{dz} \sigma(t) = \sigma(t) (1-\sigma(t))=\sigma(t)\sigma(-t)\]</span></li>
<li>Domain: <span class="math inline">\(-\infty &lt; t &lt; \infty\)</span></li>
<li>Range: <span class="math inline">\(0 &lt; \sigma(t) &lt; 1\)</span></li>
</ul>
</div>
</div>
<p>In the context of our modeling process, the sigmoid is considered an <strong>activation function</strong>. It takes in a linear combination of the features and applies a non-linear transformation.</p>
</section>
</section>
</section>
<section id="the-logistic-regression-model" class="level2" data-number="22.3">
<h2 data-number="22.3" class="anchored" data-anchor-id="the-logistic-regression-model"><span class="header-section-number">22.3</span> The Logistic Regression Model</h2>
<p>To predict a probability using the logistic regression model, we:</p>
<ol type="1">
<li>Compute a linear combination of the features, <span class="math inline">\(x^{\top}\theta\)</span></li>
<li>Apply the sigmoid activation function, <span class="math inline">\(\sigma(x^{\top} \theta)\)</span>.</li>
</ol>
<p>Our predicted probabilities are of the form <span class="math inline">\(P(Y=1|x) = p = \frac{1}{1+e^{-x^T \theta}}  = \frac{1}{1+e^{-(\theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_p x_p)}}\)</span></p>
<p>An important note: despite its name, logistic regression is used for <em>classification</em> tasks, not regression tasks. In Data 100, we always apply logistic regression with the goal of classifying data.</p>
<p>Let’s summarize our logistic regression modeling workflow:</p>
<center>
<img src="images/log_reg.png" alt="log_reg" width="750">
</center>
<p>Our main takeaways from this section are:</p>
<ul>
<li>Model the <strong>probability <span class="math inline">\(P(Y = 1 | x)\)</span></strong> using a <strong>sigmoid transformation</strong> of a <strong>linear model</strong></li>
<li>The <strong>log-odds</strong> is a <strong>linear combination of <span class="math inline">\(x\)</span> and <span class="math inline">\(\theta\)</span></strong></li>
</ul>
<p>Putting this together, we know that the estimated probability that response is 1 given the features <span class="math inline">\(x\)</span> is equal to the logistic function <span class="math inline">\(\sigma()\)</span> at the value <span class="math inline">\(x^{\top}\theta\)</span>:</p>
<p><span class="math display">\[\begin{align}
\hat{P}_{\theta}(Y = 1 | x) = \frac{1}{1 + e^{-x^{\top}\theta}}
\end{align}\]</span></p>
<p>More commonly, the logistic regression model is written as:</p>
<p><span class="math display">\[\begin{align}
\hat{P}_{\theta}(Y = 1 | x) = \sigma(x^{\top}\theta)
\end{align}\]</span></p>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Calculation
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose we want to predict the probability that a team wins a game, given <code>"GOAL_DIFF"</code> (first feature) and the <strong>number of free throws</strong> (second feature). Let’s say we fit a logistic regression model (with no intercept) using the training data and estimate the optimal parameters. Now we want to predict the probability that a new team with a <code>"GOAL_DIFF"</code> of 15 and 1 free throw per game will win their game.</p>
<p><span class="math display">\[\begin{align}
\hat{\theta}^{\top} &amp;= \begin{matrix}[0.1 &amp; -0.5]\end{matrix}
\\x^{\top} &amp;= \begin{matrix}[15 &amp; 1]\end{matrix}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\hat{P}_{\hat{\theta}}(Y = 1 | x) = \sigma(x^{\top}\hat{\theta}) = \sigma(0.1 \cdot 15 + (-0.5) \cdot 1) = \sigma(1) = \frac{1}{1+e^{-1}} \approx 0.7311
\end{align}\]</span></p>
<p>We see that the response is more likely to be 1 than 0, indicating that a reasonable prediction is <span class="math inline">\(\hat{y} = 1\)</span>. We’ll dive deeper into this in the next lecture.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Properties of the Logistic Model
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a logistic regression model with one feature and an intercept term:</p>
<p><span class="math display">\[\begin{align}
p = P(Y = 1 | x) = \frac{1}{1+e^{-(\theta_0 + \theta_1 x)}}
\end{align}\]</span></p>
<p>Properties:</p>
<ul>
<li><span class="math inline">\(\theta_0\)</span> controls the position of the curve along the horizontal axis</li>
<li>The magnitude of <span class="math inline">\(\theta_1\)</span> controls the “steepness” of the sigmoid</li>
<li>The sign of <span class="math inline">\(\theta_1\)</span> controls the orientation of the curve</li>
</ul>
</div>
</div>
</section>
<section id="cross-entropy-loss" class="level2" data-number="22.4">
<h2 data-number="22.4" class="anchored" data-anchor-id="cross-entropy-loss"><span class="header-section-number">22.4</span> Cross-Entropy Loss</h2>
<p>To quantify the error of our logistic regression model, we’ll need to define a new loss function.</p>
<section id="why-not-mse" class="level3" data-number="22.4.1">
<h3 data-number="22.4.1" class="anchored" data-anchor-id="why-not-mse"><span class="header-section-number">22.4.1</span> Why Not MSE?</h3>
<p>You may wonder: why not use our familiar mean squared error? It turns out that the MSE is not well suited for logistic regression. To see why, let’s consider a simple, artificially generated <code>toy</code> dataset with just one feature (this will be easier to work with than the more complicated <code>games</code> data).</p>
<div id="fca39d60" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>toy_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>        <span class="st">"x"</span>: [<span class="op">-</span><span class="dv">4</span>, <span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>],</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"y"</span>: [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]})</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>toy_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>-4.0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>-2.0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>-0.5</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1.0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>3.0</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We’ll construct a basic logistic regression model with only one feature and no intercept term. Our predicted probabilities take the form:</p>
<p><span class="math display">\[p=\hat{P}_\theta(Y=1|x)= \sigma({x^{T}\theta}) = \frac{1}{1+e^{-\theta_1 x}}\]</span></p>
<p>In the cell below, we plot the MSE for our model on the data.</p>
<div id="557faffd" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.e<span class="op">**</span>(<span class="op">-</span>z))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mse_on_toy_data(theta):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    p_hat <span class="op">=</span> sigmoid(toy_df[<span class="st">'x'</span>] <span class="op">*</span> theta)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((toy_df[<span class="st">'y'</span>] <span class="op">-</span> p_hat)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">15</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, [mse_on_toy_data(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"MSE on toy classification data"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$\theta_1$'</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'MSE'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-10-output-1.png" width="589" height="451" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This looks nothing like the parabola we found when plotting the MSE of a linear regression model! In particular, we can identify two flaws with using the MSE for logistic regression:</p>
<ol type="1">
<li>The MSE loss surface is <em>non-convex</em>. There is both a global minimum and a (barely perceptible) local minimum in the loss surface above. This means that there is the risk of gradient descent converging on the local minimum of the loss surface, missing the true optimum parameter <span class="math inline">\(\theta_1\)</span>.
<center>
<img src="images/global_local_min.png" alt="reg" width="400">
</center></li>
<li>Squared loss is <em>bounded</em> for a classification task. MSE never gets very large, because both response and predicted probability are bounded by 1. Recall that each true <span class="math inline">\(y\)</span> has a value of either 0 or 1. This means that even if our model makes the worst possible prediction (e.g.&nbsp;predicting <span class="math inline">\(p=0\)</span> for <span class="math inline">\(y=1\)</span>), the squared loss for an observation will be no greater than 1: <span class="math display">\[(y-p)^2=(1-0)^2=1\]</span> The MSE does not strongly penalize poor predictions.
<center>
<img src="images/squared_loss.png" alt="reg" width="400">
</center></li>
</ol>
</section>
<section id="motivating-cross-entropy-loss" class="level3" data-number="22.4.2">
<h3 data-number="22.4.2" class="anchored" data-anchor-id="motivating-cross-entropy-loss"><span class="header-section-number">22.4.2</span> Motivating Cross-Entropy Loss</h3>
<p>Suffice to say, we don’t want to use the MSE when working with logistic regression. Instead, we’ll consider what kind of behavior we would <em>like</em> to see in a loss function.</p>
<p>Let <span class="math inline">\(y\)</span> be the binary label (it can either be 0 or 1), and <span class="math inline">\(p\)</span> be the model’s predicted probability of the label <span class="math inline">\(y\)</span> being 1.</p>
<ul>
<li>When the true <span class="math inline">\(y\)</span> is 1, we should incur <em>low loss</em> when the model predicts <em>large</em> <span class="math inline">\(p\)</span></li>
<li>When the true <span class="math inline">\(y\)</span> is 0, we should incur <em>high loss</em> when the model predicts <em>large</em> <span class="math inline">\(p\)</span></li>
</ul>
<p>In other words, our loss function should behave differently depending on the value of the true class, <span class="math inline">\(y\)</span>.</p>
<p>The <strong>cross-entropy loss</strong> incorporates this changing behavior. We will use it throughout our work on logistic regression. Below, we write out the cross-entropy loss for a <em>single</em> datapoint (no averages just yet). Again let <span class="math inline">\(y\)</span> be a binary label {0, 1}, and <span class="math inline">\(p\)</span> be the probability of the label being 1.</p>
<p><span class="math display">\[\text{Cross-Entropy Loss} = \begin{cases}
  -\log{(p)}  &amp; \text{if } y=1 \\
  -\log{(1-p)} &amp; \text{if } y=0
\end{cases}\]</span></p>
<p>Why does this (seemingly convoluted) loss function “work”? Let’s break it down.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>When <span class="math inline">\(y=1\)</span></th>
<th>When <span class="math inline">\(y=0\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="images/y=1.png" alt="cross-entropy loss when Y=1" width="300"></td>
<td><img src="images/y=0.png" alt="cross-entropy loss when Y=0" width="300"></td>
</tr>
<tr class="even">
<td>As <span class="math inline">\(p \rightarrow 0\)</span>, loss approches <span class="math inline">\(\infty\)</span></td>
<td>As <span class="math inline">\(p \rightarrow 0\)</span>, loss approches 0</td>
</tr>
<tr class="odd">
<td>As <span class="math inline">\(p \rightarrow 1\)</span>, loss approaches 0</td>
<td>As <span class="math inline">\(p \rightarrow 1\)</span>, loss approaches <span class="math inline">\(\infty\)</span></td>
</tr>
</tbody>
</table>
<!-- :::: {.columns}

::: {.column width="35%"}
When $y=1$
<center><img src="images/y=1.png" alt='cross-entropy loss when Y=1' width='300'></center>

* As $p \rightarrow 0$, loss approches $\infty$
* As $p \rightarrow 1$, loss approaches 0
  
:::

::: {.column width="20%"}
:::

::: {.column width="35%"}
When $y=0$
<center><img src="images/y=0.png" alt='cross-entropy loss when Y=0' width='300'></center>

* As $p \rightarrow 0$, loss approches 0
* As $p \rightarrow 1$, loss approaches $\infty$
  
:::

:::: -->
<p>All good – we are seeing the behavior we want for our logistic regression model.</p>
<p>The piecewise function we outlined above is difficult to optimize: we don’t want to constantly “check” which form of the loss function we should be using at each step of choosing the optimal model parameters. We can re-express cross-entropy loss in a more convenient way:</p>
<p><span class="math display">\[\text{Cross-Entropy Loss} = -\left(y\log{(p)}+(1-y)\log{(1-p)}\right)\]</span></p>
<p>By setting <span class="math inline">\(y\)</span> to 0 or 1, we see that this new form of cross-entropy loss gives us the same behavior as the original formulation. Another way to think about this is that in either scenario (y being equal to 0 or 1), only one of the cross-entropy loss terms is activated, which gives us a convenient way to combine the two independent loss functions.</p>
<div class="columns">
<div class="column" style="width:35%;">
<p>When <span class="math inline">\(y=1\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{CE} &amp;= -\left((1)\log{(p)}+(1-1)\log{(1-p)}\right)\\
&amp;= -\log{(p)}
\end{align}\]</span></p>
</div><div class="column" style="width:20%;">

</div><div class="column" style="width:35%;">
<p>When <span class="math inline">\(y=0\)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{CE} &amp;= -\left((0)\log{(p)}+(1-0)\log{(1-p)}\right)\\
&amp;= -\log{(1-p)}
\end{align}\]</span></p>
</div>
</div>
<p>The empirical risk of the logistic regression model is then the mean cross-entropy loss across all datapoints in the dataset. When fitting the model, we want to determine the model parameter <span class="math inline">\(\theta\)</span> that leads to the lowest mean cross-entropy loss possible. For logistic regression, the empirical risk over a sample of size n is:</p>
<p><span class="math display">\[
\begin{align}
R(\theta) &amp;= - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(p_i)}+(1-y_i)\log{(1-p_i)}\right) \\
&amp;= - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{\sigma(\mathbb{X}_i^{\top}\theta)}+(1-y_i)\log{(1-\sigma(\mathbb{X}_i^{\top}\theta))}\right)
\end{align}
\]</span></p>
<p>The optimization problem is therefore to find the estimate <span class="math inline">\(\hat{\theta}\)</span> that minimizes <span class="math inline">\(R(\theta)\)</span>:</p>
<p><span class="math display">\[
\hat{\theta} = \underset{\theta}{\arg\min} - \frac{1}{n} \sum_{i=1}^n \left(y_i\log{(\sigma(\mathbb{X}_i^{\top}\theta))}+(1-y_i)\log{(1-\sigma(\mathbb{X}_i^{\top}\theta))}\right)
\]</span></p>
<p>Plotting the cross-entropy loss surface for our <code>toy</code> dataset gives us a more encouraging result – our loss function is now convex. This means we can optimize it using gradient descent. Computing the gradient of the logistic model is fairly challenging, so we’ll let <code>sklearn</code> take care of this for us. You won’t need to compute the gradient of the logistic model in Data 100.</p>
<div id="ea2a037e" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cross_entropy(y, p_hat):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span> y <span class="op">*</span> np.log(p_hat) <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> y) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> p_hat)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_cross_entropy_on_toy_data(theta):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    p_hat <span class="op">=</span> sigmoid(toy_df[<span class="st">'x'</span>] <span class="op">*</span> theta)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(cross_entropy(toy_df[<span class="st">'y'</span>], p_hat))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, [mean_cross_entropy_on_toy_data(theta) <span class="cf">for</span> theta <span class="kw">in</span> thetas], color <span class="op">=</span> <span class="st">'green'</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r'Mean Cross-Entropy Loss($\theta$)'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r'$\theta$'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-11-output-1.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="decision-boundaries" class="level2" data-number="22.5">
<h2 data-number="22.5" class="anchored" data-anchor-id="decision-boundaries"><span class="header-section-number">22.5</span> Decision Boundaries</h2>
<p>In logistic regression, we model the <em>probability</em> that a datapoint belongs to Class 1.</p>
<center>
<img src="images/log_reg_summary.png" alt="log_reg_summary" width="800">
</center>
<p><br> In this lecture, we developed the logistic regression model to predict that probability, but we never actually made any <em>classifications</em> for whether our prediction <span class="math inline">\(y\)</span> belongs in Class 0 or Class 1.</p>
<p><span class="math display">\[ p = \hat{P}_\theta(Y=1 | x) = \frac{1}{1 + e^{-x^{\top}\theta}}\]</span></p>
<p>A <strong>decision rule</strong> tells us how to interpret the output of the model to make a decision on how to classify a datapoint. We commonly make decision rules by specifying a <strong>threshold</strong>, <span class="math inline">\(T\)</span>. If the predicted probability is greater than or equal to <span class="math inline">\(T\)</span>, predict Class 1. Otherwise, predict Class 0.</p>
<p><span class="math display">\[\hat y = \text{classify}(x) = \begin{cases}
        1, &amp; p \ge T\\
        0, &amp; p &lt; T
    \end{cases}\]</span></p>
<p>The threshold is often set to <span class="math inline">\(T = 0.5\)</span>, but <em>not always</em>. We’ll discuss why we might want to use other thresholds <span class="math inline">\(T \neq 0.5\)</span> later in this lecture and the next.</p>
<p>Using our decision rule, we can define a <strong>decision boundary</strong> as the “line” that splits the data into classes based on its features. For logistic regression, since we are working in <span class="math inline">\(p\)</span> dimensions, the decision boundary is a <strong>hyperplane</strong> – a linear combination of the features in <span class="math inline">\(p\)</span>-dimensions – and we can recover it from the final logistic regression model. For example, if we have a model with 2 features (2D), we have <span class="math inline">\(\theta = [\theta_0, \theta_1, \theta_2]\)</span> including the intercept term, and we can solve for the decision boundary like so:</p>
<p><span class="math display">\[
\begin{align}
T &amp;= \frac{1}{1 + e^{-(\theta_0 + \theta_1 * \text{feature1} +  \theta_2 * \text{feature2})}} \\
1 + e^{-(\theta_0 + \theta_1 \cdot \text{feature1} +  \theta_2  \cdot  \text{feature2})} &amp;= \frac{1}{T} \\
e^{-(\theta_0 + \theta_1  \cdot  \text{feature1} +  \theta_2  \cdot  \text{feature2})} &amp;= \frac{1}{T} - 1 \\
\theta_0 + \theta_1  \cdot  \text{feature1} +  \theta_2  \cdot  \text{feature2} &amp;= -\log(\frac{1}{T} - 1)
\end{align}
\]</span></p>
<p>For a model with 2 features, the decision boundary is a line in terms of its features. To make it easier to visualize, we’ve included an example of a 1-dimensional and a 2-dimensional decision boundary below. Notice how the decision boundary predicted by our logistic regression model perfectly separates the points into two classes. Here the color is the <em>predicted</em> class, rather than the true class.</p>
<center>
<img src="images/decision_boundary.png" alt="decision_boundary" width="800">
</center>
<p>In real life, however, that is often not the case, and we often see some overlap between points of different classes across the decision boundary. The <em>true</em> classes of the 2D data are shown below:</p>
<center>
<img src="images/decision_boundary_true.png" alt="decision_boundary_true" width="400">
</center>
<p>As you can see, the decision boundary predicted by our logistic regression does not perfectly separate the two classes. There’s a “muddled” region near the decision boundary where our classifier predicts the wrong class. What would the data have to look like for the classifier to make perfect predictions?</p>
</section>
<section id="bonus-maximum-likelihood-estimation" class="level2" data-number="22.6">
<h2 data-number="22.6" class="anchored" data-anchor-id="bonus-maximum-likelihood-estimation"><span class="header-section-number">22.6</span> [BONUS] Maximum Likelihood Estimation</h2>
<p>It may have seemed like we pulled cross-entropy loss out of thin air. How did we know that taking the negative logarithms of our probabilities would work so well? It turns out that cross-entropy loss is justified by probability theory.</p>
<p>The following section is out of scope, but is certainly an interesting read!</p>
<section id="building-intuition-the-coin-flip" class="level3" data-number="22.6.1">
<h3 data-number="22.6.1" class="anchored" data-anchor-id="building-intuition-the-coin-flip"><span class="header-section-number">22.6.1</span> Building Intuition: The Coin Flip</h3>
<p>To build some intuition for logistic regression, let’s look at an introductory example to classification: the coin flip. Suppose we observe some outcomes of a coin flip (1 = Heads, 0 = Tails).</p>
<div id="cb3a4c14" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>flips</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>[0, 0, 1, 1, 1, 1, 0, 0, 0, 0]</code></pre>
</div>
</div>
<p>A reasonable model is to assume all flips are IID (independent and identically distributed). In other words, each flip has the same probability of returning a 1 (or heads). Let’s define a parameter <span class="math inline">\(\theta\)</span>, the probability that the next flip is a heads. We will use this parameter to inform our decision for <span class="math inline">\(\hat y\)</span> (predicting either 0 or 1) of the next flip. If <span class="math inline">\(\theta \ge 0.5, \hat y = 1, \text{else } \hat y = 0\)</span>.</p>
<p>You may be inclined to say <span class="math inline">\(0.5\)</span> is the best choice for <span class="math inline">\(\theta\)</span>. However, notice that we made no assumption about the coin itself. The coin may be biased, so we should make our decision based only on the data. We know that exactly <span class="math inline">\(\frac{4}{10}\)</span> of the flips were heads, so we might guess <span class="math inline">\(\hat \theta = 0.4\)</span>. In the next section, we will mathematically prove why this is the best possible estimate.</p>
</section>
<section id="likelihood-of-data" class="level3" data-number="22.6.2">
<h3 data-number="22.6.2" class="anchored" data-anchor-id="likelihood-of-data"><span class="header-section-number">22.6.2</span> Likelihood of Data</h3>
<p>Let’s call the result of the coin flip a random variable <span class="math inline">\(Y\)</span>. This is a Bernoulli random variable with two outcomes. <span class="math inline">\(Y\)</span> has the following distribution:</p>
<p><span class="math display">\[P(Y = y) = \begin{cases}
        p, \text{if }  y=1\\
        1 - p, \text{if }  y=0
    \end{cases} \]</span></p>
<p><span class="math inline">\(p\)</span> is unknown to us. But we can find the <span class="math inline">\(p\)</span> that makes the data we observed the most <em>likely</em>.</p>
<p>The probability of observing 4 heads and 6 tails follows the binomial distribution.</p>
<p><span class="math display">\[\binom{10}{4} (p)^4 (1-p)^6\]</span></p>
<p>We define the <strong>likelihood</strong> of obtaining our observed data as a quantity <em>proportional</em> to the probability above. To find it, simply multiply the probabilities of obtaining each coin flip.</p>
<p><span class="math display">\[(p)^{4} (1-p)^6\]</span></p>
<p>The technique known as <strong>maximum likelihood estimation</strong> finds the <span class="math inline">\(p\)</span> that maximizes the above likelihood. You can find this maximum by taking the derivative of the likelihood, but we’ll provide a more intuitive graphical solution.</p>
<div id="14ab234c" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>thetas <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plt.plot(thetas, (thetas<span class="op">**</span><span class="dv">4</span>)<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>thetas)<span class="op">**</span><span class="dv">6</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Likelihood"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="logistic_reg_1_files/figure-html/cell-13-output-1.png" width="614" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>More generally, the likelihood for some Bernoulli(<span class="math inline">\(p\)</span>) random variable <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[P(Y = y) = \begin{cases}
        1, \text{with probability }  p\\
        0, \text{with probability }  1 - p
    \end{cases} \]</span></p>
<p>Equivalently, this can be written in a compact way:</p>
<p><span class="math display">\[P(Y=y) = p^y(1-p)^{1-y}\]</span></p>
<ul>
<li>When <span class="math inline">\(y = 1\)</span>, this reads <span class="math inline">\(P(Y=y) = p\)</span></li>
<li>When <span class="math inline">\(y = 0\)</span>, this reads <span class="math inline">\(P(Y=y) = (1-p)\)</span></li>
</ul>
<p>In our example, a Bernoulli random variable is analogous to a single data point (e.g., one instance of a basketball team winning or losing a game). All together, our <code>games</code> data consists of many IID Bernoulli(<span class="math inline">\(p\)</span>) random variables. To find the likelihood of independent events in succession, simply multiply their likelihoods.</p>
<p><span class="math display">\[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]</span></p>
<p>As with the coin example, we want to find the parameter <span class="math inline">\(p\)</span> that maximizes this likelihood. Earlier, we gave an intuitive graphical solution, but let’s take the derivative of the likelihood to find this maximum.</p>
<p>At a first glance, this derivative will be complicated! We will have to use the product rule, followed by the chain rule. Instead, we can make an observation that simplifies the problem.</p>
<p>Finding the <span class="math inline">\(p\)</span> that maximizes <span class="math display">\[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]</span> is equivalent to the <span class="math inline">\(p\)</span> that maximizes <span class="math display">\[\text{log}(\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i})\]</span></p>
<p>This is because <span class="math inline">\(\text{log}\)</span> is a strictly <em>increasing</em> function. It won’t change the maximum or minimum of the function it was applied to. From <span class="math inline">\(\text{log}\)</span> properties, <span class="math inline">\(\text{log}(a*b)\)</span> = <span class="math inline">\(\text{log}(a) + \text{log}(b)\)</span>. We can apply this to our equation above to get:</p>
<p><span class="math display">\[\underset{p}{\text{argmax}} \sum_{i=1}^{n} \text{log}(p^{y_i} (1-p)^{1-y_i})\]</span></p>
<p><span class="math display">\[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (\text{log}(p^{y_i}) + \text{log}((1-p)^{1-y_i}))\]</span></p>
<p><span class="math display">\[= \underset{p}{\text{argmax}} \sum_{i=1}^{n} (y_i\text{log}(p) + (1-y_i)\text{log}(1-p))\]</span></p>
<p>We can add a constant factor of <span class="math inline">\(\frac{1}{n}\)</span> out front. It won’t affect the <span class="math inline">\(p\)</span> that maximizes our likelihood.</p>
<p><span class="math display">\[=\underset{p}{\text{argmax}}  \frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)\]</span></p>
<p>One last “trick” we can do is change this to a minimization problem by negating the result. This works because we are dealing with a <em>concave</em> function, which can be made <em>convex</em>.</p>
<p><span class="math display">\[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)\]</span></p>
<p>Now let’s say that we have data that are independent with different probability <span class="math inline">\(p_i\)</span>. Then, we would want to find the <span class="math inline">\(p_1, p_2, \dots, p_n\)</span> that maximize <span class="math display">\[\prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}\]</span></p>
<p>Setting up and simplifying the optimization problems as we did above, we ultimately want to find:</p>
<p><span class="math display">\[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p_i) + (1-y_i)\text{log}(1-p_i)\]</span></p>
<p>For logistic regression, <span class="math inline">\(p_i = \sigma(x^{\top}\theta)\)</span>. Plugging that in, we get:</p>
<p><span class="math display">\[= \underset{p}{\text{argmin}} -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(\sigma(x^{\top}\theta)) + (1-y_i)\text{log}(1-\sigma(x^{\top}\theta))\]</span></p>
<p>This is exactly our average cross-entropy loss minimization problem from before!</p>
<p>Why did we do all this complicated math? We have shown that <em>minimizing</em> cross-entropy loss is equivalent to <em>maximizing</em> the likelihood of the training data.</p>
<ul>
<li>By minimizing cross-entropy loss, we are choosing the model parameters that are “most likely” for the data we observed.</li>
</ul>
<p>Note that this is under the assumption that all data is drawn independently from the same logistic regression model with parameter <span class="math inline">\(\theta\)</span>. In fact, many of the model + loss combinations we’ve seen can be motivated using MLE (e.g., OLS, Ridge Regression, etc.). In probability and ML classes, you’ll get the chance to explore MLE further.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../sql_II/sql_II.html" class="pagination-link  aria-label=" &lt;span="" ii&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../logistic_regression_2/logistic_reg_2.html" class="pagination-link" aria-label="<span class='chapter-number'>23</span>&nbsp; <span class='chapter-title'>Logistic Regression II</span>">
        <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>