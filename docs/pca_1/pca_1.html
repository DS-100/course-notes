<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 25&nbsp; PCA I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../pca_2/pca_2.html" rel="next">
<link href="../logistic_regression_2/logistic_reg_2.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="../Principles-and-Techniques-of-Data-Science.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Random Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_climate/case_study_climate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Case Study in Climate and Physical Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link active" data-scroll-target="#unsupervised-learning"><span class="toc-section-number">25.1</span>  Unsupervised Learning</a>
  <ul>
  <li><a href="#dimensionality-reductions" id="toc-dimensionality-reductions" class="nav-link" data-scroll-target="#dimensionality-reductions"><span class="toc-section-number">25.1.1</span>  Dimensionality Reductions</a></li>
  <li><a href="#dimensionality-of-data-linear-algebra" id="toc-dimensionality-of-data-linear-algebra" class="nav-link" data-scroll-target="#dimensionality-of-data-linear-algebra"><span class="toc-section-number">25.1.2</span>  Dimensionality of Data (Linear Algebra)</a></li>
  <li><a href="#matrix-decomposition" id="toc-matrix-decomposition" class="nav-link" data-scroll-target="#matrix-decomposition"><span class="toc-section-number">25.1.3</span>  Matrix Decomposition</a>
  <ul>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication"><span class="toc-section-number">25.1.3.1</span>  Matrix Multiplication</a></li>
  </ul></li>
  <li><a href="#limited-by-rank" id="toc-limited-by-rank" class="nav-link" data-scroll-target="#limited-by-rank"><span class="toc-section-number">25.1.4</span>  Limited by Rank</a></li>
  <li><a href="#automatic-factorization" id="toc-automatic-factorization" class="nav-link" data-scroll-target="#automatic-factorization"><span class="toc-section-number">25.1.5</span>  Automatic Factorization</a></li>
  </ul></li>
  <li><a href="#principle-component-analysis" id="toc-principle-component-analysis" class="nav-link" data-scroll-target="#principle-component-analysis"><span class="toc-section-number">25.2</span>  Principle Component Analysis</a>
  <ul>
  <li><a href="#capturing-total-variance" id="toc-capturing-total-variance" class="nav-link" data-scroll-target="#capturing-total-variance"><span class="toc-section-number">25.2.1</span>  Capturing Total Variance</a></li>
  <li><a href="#pca-vs.-svd" id="toc-pca-vs.-svd" class="nav-link" data-scroll-target="#pca-vs.-svd"><span class="toc-section-number">25.2.2</span>  PCA vs.&nbsp;SVD</a></li>
  <li><a href="#pca-procedure" id="toc-pca-procedure" class="nav-link" data-scroll-target="#pca-procedure"><span class="toc-section-number">25.2.3</span>  PCA Procedure</a></li>
  <li><a href="#svd" id="toc-svd" class="nav-link" data-scroll-target="#svd"><span class="toc-section-number">25.2.4</span>  SVD</a></li>
  </ul></li>
  <li><a href="#data-variance-and-centering" id="toc-data-variance-and-centering" class="nav-link" data-scroll-target="#data-variance-and-centering"><span class="toc-section-number">25.3</span>  Data Variance and Centering</a></li>
  <li><a href="#proof-of-component-score-out-of-scope" id="toc-proof-of-component-score-out-of-scope" class="nav-link" data-scroll-target="#proof-of-component-score-out-of-scope"><span class="toc-section-number">25.4</span>  Proof of component score (out of scope)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Introduce principles of unsupervised learning</li>
</ul>
</div>
</div>
</div>
<section id="unsupervised-learning" class="level2" data-number="25.1">
<h2 data-number="25.1" class="anchored" data-anchor-id="unsupervised-learning"><span class="header-section-number">25.1</span> Unsupervised Learning</h2>
<p>In “Supervised Learning”, our goal is to create a model that maps <em>inputs</em> to <em>outputs</em>.</p>
<p>Model is <strong>learned</strong> from example input/output pairs. Each pair consists of:</p>
<ul>
<li>Input vector.</li>
<li>Output value (label).</li>
</ul>
<p>We learned two supervised learning methods so far.</p>
<ol type="1">
<li><p>Regression: Output value is quantitative.</p></li>
<li><p>Classification: Output value is categorical.</p></li>
</ol>
<p>Now, we will move into “Unsupervised Learning”, where our goal is to identify patterns in <strong>unlabeled</strong> data. We <em>do not</em> have input/output pairs. Note: Sometimes we may have labels, but we choose to ignore them.</p>
<section id="dimensionality-reductions" class="level3" data-number="25.1.1">
<h3 data-number="25.1.1" class="anchored" data-anchor-id="dimensionality-reductions"><span class="header-section-number">25.1.1</span> Dimensionality Reductions</h3>
<p>Suppose we wanted to study how students worked through an introductory CS assignment, where they have to draw a pyramid using graphics and blocks.</p>
<p><img src="images/dim_reduc.png"></p>
<p>A researcher has already labeled all “assignment snapshots” as one of 16 different “milestones” of the program. How can we visualize how similar/close the snapshots are on a 2-D plane?</p>
<ul>
<li><p>Problem: Each image is 800 x 600 = 480k pixels. This is a lot of features, but visualizations are usually 2-D. Past 3 dimensions is hard to perceive meaningfully.</p></li>
<li><p>Solution: Dimensionality reduction to 2-D space.</p></li>
</ul>
<p>Principal Component Analysis (PCA) is a linear technique for dimensionality reduction.</p>
<p>PCA relies on a linear algebra algorithm called <em>Singular Value Decomposition</em>.</p>
</section>
<section id="dimensionality-of-data-linear-algebra" class="level3" data-number="25.1.2">
<h3 data-number="25.1.2" class="anchored" data-anchor-id="dimensionality-of-data-linear-algebra"><span class="header-section-number">25.1.2</span> Dimensionality of Data (Linear Algebra)</h3>
<p>Previously, we have been working with data tables with rows and columns. These rows and columns corresponded to observations and attributes about said observations. Now, we have to be a bit more clear with our wording to follow the language of linear algebra.</p>
<p>Suppose we have a dataset of: - N observations (datapoints/rows) - d attributes (features/columns).</p>
<p>In Linear Algebra, we think of data being a collection of vectors. Vectors have a <em>dimension</em>, meaning they have some number of unique elements. Row and column now denotes the direction a vector is written (horizontally, like a row, or vertically, like a column):</p>
<p>Linear Algebra views our data as a matrix: - N row vectors in a d-Dimensions, OR - d column vectors in an N-Dimensions.</p>
<p><strong>Dimensionality</strong> of data is a complex topic. Sometimes, it is clear from looking at the number of rows/columns, but other times it is not.</p>
<p><img src="images/dataset_dims.png"></p>
<p>For example, the dataset below we can see that while there is 4 columns, the <code>Weight (lbs)</code> actually just a transformation of the <code>Weight (kg)</code> column. Thus, no new information is captured</p>
<p><img src="images/dataset4.png"></p>
<p>Plotting the weight columns together reveals the key visual intuition. While the two columns visually span a 2-d space as a line, the data does not deviate at all from that singular line. This means that one of the weight columns are redundant! Even given the option to cover the whole 2d space, the data below does not. It might as well not have this dimension, which is why we still do not consider the data below to span more than 1 dimension.</p>
<p><img src="images/dataset3.png"></p>
</section>
<section id="matrix-decomposition" class="level3" data-number="25.1.3">
<h3 data-number="25.1.3" class="anchored" data-anchor-id="matrix-decomposition"><span class="header-section-number">25.1.3</span> Matrix Decomposition</h3>
<p>How can we get rid of the non-essential parts of our matrix? One overarching idea is to break the matrix into a smaller matrix through clever matrix multiplication. This is called matrix decomposition.</p>
<section id="matrix-multiplication" class="level4" data-number="25.1.3.1">
<h4 data-number="25.1.3.1" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">25.1.3.1</span> Matrix Multiplication</h4>
<p><img src="images/matmul.png" width="700"></p>
<p>The first view of matrix multiplication we saw when working with linear models is the data and operations view.</p>
<p><img src="images/matmul2.png" width="700"></p>
<p>However, another more linear algebra centric perspective is the columns and transformations view. The columns of the left matrix are being transformed according to the rules specified by the right matrix. We highly recommend watching this video for some <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">neat animations</a>.</p>
<p><img src="images/matmul3.png" width="700"></p>
<p>So now that we have a broader idea of what matrix multiplication does, we can formulate a plan with which to reduce our matrix down to its essential dimensions.</p>
<p><img src="images/matmul_dim.png" width="700"></p>
<p>Matrix decomposition (a.k.a. Matrix Factorization) is the opposite of matrix multiplication, i.e.&nbsp;taking a matrix and decomposing it into two separate matrices.</p>
<p>Just like with real numbers, there are infinitely many such decompositions.</p>
<p>Example: <span class="math inline">\(9.9 = 1.1 * 9 = 3.3 * 3.3 = 1 * 9.9 = ...\)</span></p>
<p>Matrix sizes introduce even more room for differences in decomposition.</p>
<p><img src="images/factor.png" width="700"></p>
</section>
</section>
<section id="limited-by-rank" class="level3" data-number="25.1.4">
<h3 data-number="25.1.4" class="anchored" data-anchor-id="limited-by-rank"><span class="header-section-number">25.1.4</span> Limited by Rank</h3>
<p>One key fact to remember is that the decomposition are not arbitrary. The <em>rank</em> of a matrix limits how small our inner dimensions can be if we want to perfectly recreate our matrix. The proof for this is out of scope.</p>
</section>
<section id="automatic-factorization" class="level3" data-number="25.1.5">
<h3 data-number="25.1.5" class="anchored" data-anchor-id="automatic-factorization"><span class="header-section-number">25.1.5</span> Automatic Factorization</h3>
<p>Even if we know we have to factorize our matrix using an inner dimension of R, that still leaves a large space of solutions to traverse. What if we have a procedure to automatically factorize a rank R matrix into an R dimensional representation with some transformation matrix?</p>
<ul>
<li><p>Lower dimensional representation avoids redundant features.</p></li>
<li><p>Imagine a 1000 dimensional dataset: If the rank is only 5, it’s much easier to do EDA after this mystery procedure.</p></li>
</ul>
<p>What if we wanted a 2-D representation? Its valuable to compress all of the data that is relevant onto as few dimensions as possible in order to plot it efficiently. Some 2D matrices yield better approximations than others. How well can we do?</p>
</section>
</section>
<section id="principle-component-analysis" class="level2" data-number="25.2">
<h2 data-number="25.2" class="anchored" data-anchor-id="principle-component-analysis"><span class="header-section-number">25.2</span> Principle Component Analysis</h2>
<p>Goal: Transform observations from high-dimensional data down to low dimensions (often 2) through linear transformations.</p>
<p>Related Goal: Low-dimension representation should capture the <em>variability</em> of the original data.</p>
<p><img src="images/PCA_1.png" width="700"></p>
<p>Dimensionality reductions can also aid in Exploratory Data Analysis:</p>
<ol type="1">
<li>Visually identify clusters of similar observations in high dimensions.</li>
<li>You are still exploring the data (if you already know what to model, you probably don’t need PCA).</li>
<li>You have reason to believe the data are inherently low rank, e.g., There are many attributes but only a few mostly determine the rest through linear associations.</li>
</ol>
<p>Why two dimensions? Most visualizations are 2-D, so it makes it easy to choose the two axes on which to plot datapoints. Furthermore, there is a <a href="https://en.wikipedia.org/wiki/Manifold_hypothesis">hypothesis</a> that most high dimensional data lives in low-dimensional manifolds.</p>
<section id="capturing-total-variance" class="level3" data-number="25.2.1">
<h3 data-number="25.2.1" class="anchored" data-anchor-id="capturing-total-variance"><span class="header-section-number">25.2.1</span> Capturing Total Variance</h3>
<p>Variability of the data is the most important aspect that we want to maintain. It is the bulk of what we care about when we conduct EDA and modeling. Thus, oversimplifying the data may lead to incorrect analyses.</p>
<p>Variance for a matrix is defined as the sum of the variances of the columns of a matrix.</p>
<p><img src="images/variance.png" width="700"></p>
<p>One approach is to simply keep the columns with the most variability. However, this leads to capturing of a small amount of variability present in our original dataset. Can we do better than just taking two columns from the original matrix?</p>
<p><img src="images/approach_1.png" width="700"></p>
<p>It turns out that PCA does a better job at preserving our variances. This is one of the best techniques for dimensionality reductions when it comes to preserving variances. Let’s dig into how it works!</p>
<p><img src="images/approach_2.png" width="700"></p>
</section>
<section id="pca-vs.-svd" class="level3" data-number="25.2.2">
<h3 data-number="25.2.2" class="anchored" data-anchor-id="pca-vs.-svd"><span class="header-section-number">25.2.2</span> PCA vs.&nbsp;SVD</h3>
<p>Principle Component Analysis PCA and Singular Value Decomposition can be easily mixed up, especially when you have to keep track of so many acronyms. Here is a quick summary:</p>
<p>PCA: a data science procedure used for dimensionality reduction that uses SVD as one of the steps.</p>
<p>SVD: a linear algebra algorithm that splits a matrix into 3 component parts.</p>
</section>
<section id="pca-procedure" class="level3" data-number="25.2.3">
<h3 data-number="25.2.3" class="anchored" data-anchor-id="pca-procedure"><span class="header-section-number">25.2.3</span> PCA Procedure</h3>
<ol type="1">
<li><p>Center the data matrix by subtracting the mean of each attribute column.</p></li>
<li><p>Use SVD to find all <span class="math inline">\(v_i \in \{1...k\}\)</span>, the principal components, which fulfills the following criteria:</p>
<ul>
<li><span class="math inline">\(v_i\)</span> is a unit vector that linearly combines the attributes.</li>
<li><span class="math inline">\(v_i\)</span> gives a one-dimensional projection of the data.</li>
<li><span class="math inline">\(v_i\)</span> is chosen to minimize the sum of squared distances between each point and its projection onto v.</li>
<li><span class="math inline">\(v_i\)</span> is orthogonal to all previous principal components.</li>
</ul></li>
</ol>
<p>references for SVD: <a href="https://eecs16b.org/notes/sp23/note14.pdf">EECS 16B Note 14</a>, <a href="https://eecs16b.org/notes/sp23/note15.pdf">EECS 16B Note 15</a></p>
</section>
<section id="svd" class="level3" data-number="25.2.4">
<h3 data-number="25.2.4" class="anchored" data-anchor-id="svd"><span class="header-section-number">25.2.4</span> SVD</h3>
<p>Singular value decomposition (SVD) is an important concept in linear algebra.</p>
<ul>
<li>We assume you have taken (or are taking) a linear algebra course.</li>
<li>We will not explain SVD in its entirety—in particular, we will not prove:
<ul>
<li>Why SVD is a valid decomposition of rectangular matrices</li>
<li>Why PCA is an application of SVD.</li>
</ul></li>
<li>We know SVD is not covered in EECS 16A, nor Math 54 this semester.</li>
</ul>
<p>We will not go much into the theory and details of SVD. Instead, we will only cover what is needed for a data science interpretation.</p>
<p><img src="images/svd.png" width="700"></p>
<p>In NumPy, this algorithm is already written and can be called with <code>np.linalg.svd</code>.</p>
</section>
</section>
<section id="data-variance-and-centering" class="level2" data-number="25.3">
<h2 data-number="25.3" class="anchored" data-anchor-id="data-variance-and-centering"><span class="header-section-number">25.3</span> Data Variance and Centering</h2>
<p>Formally, the i-th singular value tells us the <strong>component score</strong>, i.e., how much of the data variance is captured by the ith principal component. Supposing the number of datapoints is <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[\text{i-th component score} = \frac{(\text{i-th singular varlue}^2}{n}\]</span></p>
<p>Summing up the component scores is equivalent to computing total variance.</p>
<p><strong>Data Centering</strong>: PCA has a data centering step that precedes any singular value decomposition, where if implemented defines the component score as above.</p>
</section>
<section id="proof-of-component-score-out-of-scope" class="level2" data-number="25.4">
<h2 data-number="25.4" class="anchored" data-anchor-id="proof-of-component-score-out-of-scope"><span class="header-section-number">25.4</span> Proof of component score (out of scope)</h2>
<p>The proof defining component score out of scope for this class, but it is included below for your convenienve.</p>
<p><em>Setup</em>: Consider the design matrix <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span>, where the <span class="math inline">\(j\)</span>-th column (corresponding to the <span class="math inline">\(j\)</span>-th feature) is <span class="math inline">\(x_j \in \mathbb{R}^n\)</span> and the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> is <span class="math inline">\(x_{ij}\)</span>. Further define <span class="math inline">\(\tilde{X}\)</span> as the <strong>centered</strong> design matrix. The <span class="math inline">\(j\)</span>-th column is <span class="math inline">\(\tilde{x}_j \in \mathbb{R}^n\)</span> and the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> is <span class="math inline">\(\tilde{x}_{ij} = x_{ij} - \bar{x_j}\)</span>, where <span class="math inline">\(\bar{x_j}\)</span> is the mean of the <span class="math inline">\(x_j\)</span> column vector from the original <span class="math inline">\(X\)</span>.</p>
<p><em>Variance</em>: Construct the <strong>covariance matrix</strong>: <span class="math inline">\(\frac{1}{n} \tilde{X}^T \tilde{X} \in \mathbb{R}^{d \times d}\)</span>. The <span class="math inline">\(j\)</span>-th element along the diagonal is the <strong>variance</strong> of the <span class="math inline">\(j\)</span>-th column of the original design matrix <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} = \frac{1}{n} \tilde{x}_j ^T \tilde{x}_j = \frac{1}{n} \sum_{i=i}^n (\tilde{x}_{ij} )^2 = \frac{1}{n} \sum_{i=i}^n (x_{ij} - \bar{x_j})^2\]</span></p>
<p><em>SVD</em>: Suppose singular value decomposition of the <em>centered</em> design matrix <span class="math inline">\(\tilde{X}\)</span> yields <span class="math inline">\(\tilde{X} = U \Sigma V^T\)</span>, where $U ^{n d} $ and <span class="math inline">\(V \in \mathbb{R}^{d \times d}\)</span> are matrices with orthonormal columns, and <span class="math inline">\(\Sigma \in \mathbb{R}^{d \times d}\)</span> is a diagonal matrix with singular values of <span class="math inline">\(\tilde{X}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\tilde{X}^T \tilde{X} &amp;= (U \Sigma V^T )^T (U \Sigma V^T) \\
&amp;= V \Sigma U^T U \Sigma V^T  &amp; (\Sigma^T = \Sigma) \\
&amp;= V \Sigma^2 V^T &amp; (U^T U = I) \\
\frac{1}{n} \tilde{X}^T \tilde{X} &amp;= \frac{1}{n} V \Sigma V^T =V \left( \frac{1}{n} \Sigma \right) V^T \\
\frac{1}{n} \tilde{X}^T \tilde{X} V &amp;= V \left( \frac{1}{n} \Sigma \right) V^T V = V \left( \frac{1}{n} \Sigma \right) &amp; \text{(right multiply by }V \rightarrow V^T V = I \text{)} \\
V^T \frac{1}{n} \tilde{X}^T \tilde{X} V &amp;= V^T V \left( \frac{1}{n} \Sigma \right) = \frac{1}{n} \Sigma &amp; \text{(left multiply by }V^T \rightarrow V^T V = I \text{)} \\
\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} &amp;= \frac{1}{n}\sigma_j^2  &amp; \text{(Define }\sigma_j\text{ as the} j\text{-th singular value)} \\
\frac{1}{n} \sigma_j^2 &amp;= \frac{1}{n} \sum_{i=i}^n (x_{ij} - \bar{x_j})^2
\end{aligned}\]</span></p>
<p>The last line defines the <span class="math inline">\(j\)</span>-th component score.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../logistic_regression_2/logistic_reg_2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../pca_2/pca_2.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>