<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 27&nbsp; Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../pca_2/pca_2.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="../Principles-and-Techniques-of-Data-Science.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Random Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_climate/case_study_climate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Case Study in Climate and Physical Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#clustering" id="toc-clustering" class="nav-link active" data-scroll-target="#clustering"><span class="toc-section-number">28</span>  Clustering</a>
  <ul>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="toc-section-number">28.0.1</span>  Supervised Learning</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning"><span class="toc-section-number">28.0.2</span>  Unsupervised Learning</a>
  <ul>
  <li><a href="#clustering-examples" id="toc-clustering-examples" class="nav-link" data-scroll-target="#clustering-examples"><span class="toc-section-number">28.0.2.1</span>  Clustering Example(s)</a></li>
  </ul></li>
  <li><a href="#taxonomy-of-clustering-approaches" id="toc-taxonomy-of-clustering-approaches" class="nav-link" data-scroll-target="#taxonomy-of-clustering-approaches"><span class="toc-section-number">28.0.3</span>  Taxonomy of Clustering Approaches</a></li>
  <li><a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="toc-section-number">28.1</span>  K-Means Clustering</a>
  <ul>
  <li><a href="#note" id="toc-note" class="nav-link" data-scroll-target="#note"><span class="toc-section-number">28.1.0.1</span>  Note</a></li>
  </ul></li>
  <li><a href="#minimizing-inertia" id="toc-minimizing-inertia" class="nav-link" data-scroll-target="#minimizing-inertia"><span class="toc-section-number">28.2</span>  Minimizing Inertia</a></li>
  <li><a href="#hierarchical-agglomerative-clustering" id="toc-hierarchical-agglomerative-clustering" class="nav-link" data-scroll-target="#hierarchical-agglomerative-clustering"><span class="toc-section-number">28.3</span>  Hierarchical Agglomerative Clustering</a>
  <ul>
  <li><a href="#clustering-dendrograms-and-intuition" id="toc-clustering-dendrograms-and-intuition" class="nav-link" data-scroll-target="#clustering-dendrograms-and-intuition"><span class="toc-section-number">28.3.1</span>  Clustering, Dendrograms, and Intuition</a></li>
  </ul></li>
  <li><a href="#picking-k" id="toc-picking-k" class="nav-link" data-scroll-target="#picking-k"><span class="toc-section-number">28.4</span>  Picking K</a>
  <ul>
  <li><a href="#silhouette-scores" id="toc-silhouette-scores" class="nav-link" data-scroll-target="#silhouette-scores"><span class="toc-section-number">28.4.1</span>  Silhouette Scores</a></li>
  <li><a href="#silhouette-plot" id="toc-silhouette-plot" class="nav-link" data-scroll-target="#silhouette-plot"><span class="toc-section-number">28.4.2</span>  Silhouette Plot</a></li>
  <li><a href="#picking-k-real-world-metrics-example-via-andrew-ng" id="toc-picking-k-real-world-metrics-example-via-andrew-ng" class="nav-link" data-scroll-target="#picking-k-real-world-metrics-example-via-andrew-ng"><span class="toc-section-number">28.4.3</span>  Picking K: Real World Metrics (Example via Andrew Ng)</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">28.5</span>  Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="clustering" class="level1" data-number="28">
<h1 data-number="28"><span class="header-section-number">28</span> Clustering</h1>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Introduction to clustering</li>
<li>Taking a step back; assessing the taxonomy of clustering approaches</li>
<li>K-Means clustering</li>
<li>How to cluster if there is no explicit loss function? Minimize inertia.</li>
<li>Hierarchical Agglomerative Clustering</li>
<li>Picking K: a hyperparameter</li>
</ul>
</div>
</div>
</div>
<p>Last time, we began our journey into unsupervised learning by discussing about Principal Component Analysis (PCA).</p>
<p>In this lecture, we will explore another very popular unsupervised learning unsupervised learning concept: clustering. Clustering allows us to ‘group’ similar data together, without being given labels of what ‘class’ or where data explicitly comes from. We will discuss two clustering algorithms: K-Means clustering and hierarchichal agglomerative clustering, examining the assumptions, strengths, and drawbacks to each one.</p>
<section id="supervised-learning" class="level3" data-number="28.0.1">
<h3 data-number="28.0.1" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">28.0.1</span> Supervised Learning</h3>
<p>In “Supervised Learning”, our goal is to create a function that maps inputs to outputs. Each model is learned from example input/output pairs (training set), validated using input/output pairs, and eventually tested on more input/output pairs. Each pair consists of:</p>
<ul>
<li>Input vector.</li>
<li>Output value (label, value).</li>
</ul>
<p>In regression, our output value is quantitative and in classification, our output value is categorical.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ml_taxonomy.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ML taxonomy</figcaption><p></p>
</figure>
</div>
</section>
<section id="unsupervised-learning" class="level3" data-number="28.0.2">
<h3 data-number="28.0.2" class="anchored" data-anchor-id="unsupervised-learning"><span class="header-section-number">28.0.2</span> Unsupervised Learning</h3>
<p>In “Unsupervised Learning”, our goal is to identify patterns in unlabeled data. In this type of learning, we do not have input/output pairs. Sometimes we may have labels, but we’re just choosing to ignore them (e.g.&nbsp;PCA on labeled data). Instead, we are more interested in the inherent structure of the data we have, rather than trying to simply predict a label using that structure of data.</p>
<p>For example, if we are interested in dimensionality reduction, we can use PCA to reduce our data to a lower dimension.</p>
<p>Now let us consider a new problem: Clustering.</p>
<section id="clustering-examples" class="level4" data-number="28.0.2.1">
<h4 data-number="28.0.2.1" class="anchored" data-anchor-id="clustering-examples"><span class="header-section-number">28.0.2.1</span> Clustering Example(s)</h4>
<p>Consider the figure shown from Fall 2019 Midterm 2.</p>
<p><img src="images/blobs.png" alt="blobs" width="600"></p>
<p>Each point represents the 1st and 2nd principal component of how much time patrons spent at 8 different zoo exhibits. The original dataset was 8 dimensions, but we have used PCA to reduce our data down to 2 dimensions.</p>
<p>Visually and inuitively, we could potentially guess that this data belongs to 3 groups: one for each cluster. The goal of clustering is now to assign each point (in the 2 dimensional PCA representation) to a cluster.</p>
<p>This is an unsupervised task:</p>
<ul>
<li>We don’t have labels for each visitor.</li>
<li>Want to infer pattern even without labels.</li>
</ul>
<p>Now, consider the plot below:</p>
<p><img src="images/genes.png" alt="genes" width="600"></p>
<p>The rows of this plot are conditions (e.g.&nbsp;a row might be: “poured acid on the cells”) and the columns are genes. The green coloration indicates that the gene was ~off (red indicates the gene was ~on). For example, the ~9 genes in the top left corner of the plot were all turned off by the 6 experiments (rows) at the top.</p>
<p>In a clustering lens, we might be interested in clustering similar observations together based on the reactions (on/off) to certain experiments.</p>
<p>Note: Apologies if you can’t differentiate red from green by eye! Historical visualizations are not always the best.</p>
</section>
</section>
<section id="taxonomy-of-clustering-approaches" class="level3" data-number="28.0.3">
<h3 data-number="28.0.3" class="anchored" data-anchor-id="taxonomy-of-clustering-approaches"><span class="header-section-number">28.0.3</span> Taxonomy of Clustering Approaches</h3>
<p><img src="images/taxonomy.png" alt="taxonomy" width="600"></p>
<p>There are many types of clustering algorithms, all that have strengths, inherent weaknesses, and different use cases. We will first focus on a partitional approach: K-Means clustering.</p>
</section>
<section id="k-means-clustering" class="level2" data-number="28.1">
<h2 data-number="28.1" class="anchored" data-anchor-id="k-means-clustering"><span class="header-section-number">28.1</span> K-Means Clustering</h2>
<p>The most popular clustering approach is K-Means. The algorithm itself:</p>
<ul>
<li>Pick an arbitrary <span class="math inline">\(K\)</span>, and randomly place <span class="math inline">\(K\)</span> “centers”, each a different color.</li>
<li>Repeat until convergence:
<ul>
<li>Color points according to the closest center.</li>
<li>Move center for each color to center of points with that color.</li>
</ul></li>
</ul>
<p>Consider the following data with arbitrary <span class="math inline">\(K = 2\)</span>, with randomly placed “centers” denoted by the different (blue, orange) colors:</p>
<p><img src="images/init_cluster.png" alt="init_cluster" width="600"></p>
<p>Now, we will follow the rest of the algorithm. First, let us color each point according to the closest center:</p>
<p><img src="images/cluster_class.png" alt="cluster_class" width="600"></p>
<p>Next, we will move the center for each color to the center of points with that color. Notice how the centers are generally well centered amongst the data that shares its color.</p>
<p><img src="images/cluster_iter1.png" alt="cluster_iter1" width="600"></p>
<p>Assume this process (re-color, re-set centers) repeats for a few more iterations, and now we’ve ended up at this state.</p>
<p><img src="images/cluster_iter5.png" alt="cluster_iter5" width="600"></p>
<p>After this iteration, the center stays still and does not move at all. Thus, we have converged, and the clustering is completed.</p>
<section id="note" class="level4" data-number="28.1.0.1">
<h4 data-number="28.1.0.1" class="anchored" data-anchor-id="note"><span class="header-section-number">28.1.0.1</span> Note</h4>
<p>A quick note: K-Means is a totally different algorithm than “K-Nearest Neighbors”.</p>
<ul>
<li>K-Means: For clustering:
<ul>
<li>Assigns each point to one of K clusters.</li>
</ul></li>
<li>K-Nearest Neighbors: For Classification (or less often, Regression):
<ul>
<li>Prediction is the most common class among the k-nearest data points in the training set.</li>
<li>This is covered in Data 8.</li>
</ul></li>
</ul>
<p>The names may be similar, but there isn’t really anything in common.</p>
</section>
</section>
<section id="minimizing-inertia" class="level2" data-number="28.2">
<h2 data-number="28.2" class="anchored" data-anchor-id="minimizing-inertia"><span class="header-section-number">28.2</span> Minimizing Inertia</h2>
<p>Consider the following example where <span class="math inline">\(K = 4\)</span>:</p>
<p><img src="images/four_cluster.png" alt="four_cluster" width="600"></p>
<p>Due to the randomness where the <span class="math inline">\(K\)</span> centers initialize/start, you will get a different output/clustering every time you run K-Means. Consider three possible K-Means outputs (the algorithm has converged and the colors denote the final cluster they are clustered as)</p>
<p><img src="images/random_outputs.png" alt="random_outputs" width="600"></p>
<p>Which clustering output is the best? To evaluate different clustering results, we need a loss function.</p>
<p>Two common loss functions:</p>
<ul>
<li>Inertia: Sum of squared distances from each data point to its center.</li>
<li>Distortion: Weighted sum of squared distances from each data point to its center.</li>
</ul>
<p><img src="images/inertia_distortion.png" alt="inertia_distortion" width="600"></p>
<p>In the example above:</p>
<ul>
<li>Calculated inertia: <span class="math inline">\(0.472 + 0.192 + 0.342 + 0.252 + 0.582 + 0.362 + 0.442\)</span></li>
<li>Calculated distortion: <span class="math inline">\((0.472 + 0.192 + 0.342)/3 + (0.252 + 0.582 + 0.362 + 0.442)/4\)</span></li>
</ul>
<p>Switching back to the four cluster example at the beginning of this section, the <code>random.seed(25)</code> had an inertia of <code>44.96</code>, the <code>random.seed(29)</code> had an inertia of <code>45.95</code>, and the <code>random.seed(40)</code> had an inertia of <code>54.35</code>.</p>
<p>It seems that the best clustering output was <code>random.seed(25)</code> with an inertia of <code>44.96</code>.</p>
<p>It turns out that the function K-Means is trying to minimize is inertia… but often fails to find global optimum. Why does this happen?</p>
<p>We can think of K-means as a pair of optimizers that take turns:</p>
<ul>
<li>First optimizer:
<ul>
<li>Holds center positions constant.</li>
<li>Optimizes data colors.</li>
</ul></li>
<li>Second optimizer:
<ul>
<li>Holds data colors constant.</li>
<li>Optimizes center positions.</li>
</ul></li>
<li>Neither gets total control!</li>
</ul>
<p>This is a hard problem: Give an algorithm that optimizes inertia FOR A GIVEN <span class="math inline">\(K\)</span>. <span class="math inline">\(K\)</span> is picked in advance.</p>
<ul>
<li>Your algorithm should return the EXACT best centers and colors.</li>
<li>Don’t worry about runtime.</li>
</ul>
<p><em>Note: This is a bit of a CS61B/CS70/CS170 problem, so do not worry about completely understanding the tricky predicament we are in too much!</em></p>
<p>A potential algorithm:</p>
<ul>
<li>For all possible kn colorings:
<ul>
<li>Compute the k centers for that coloring.</li>
<li>Compute the inertia for the k centers.
<ul>
<li>If current inertia is better than best known, write down the current centers and coloring and call that the new best known.</li>
</ul></li>
</ul></li>
</ul>
<p>No better algorithm has been found for solving the problem of minimizing inertia exactly.</p>
</section>
<section id="hierarchical-agglomerative-clustering" class="level2" data-number="28.3">
<h2 data-number="28.3" class="anchored" data-anchor-id="hierarchical-agglomerative-clustering"><span class="header-section-number">28.3</span> Hierarchical Agglomerative Clustering</h2>
<p>Now, let us consider hierarchical agglomerative clustering.</p>
<p><img src="images/hierarchical_approach.png" alt="hierarchical_approach" width="600"></p>
<p>Consider the following results of a two K-Means clustering outputs:</p>
<p><img src="images/clustering_comparison.png" alt="clustering_comparison" width="600"></p>
<p>Which clustering result do you like better? It seems K-Means likes the one on the right better because it has lower inertia (the sum of squared distances from each data point to its center). This raises some questions:</p>
<ul>
<li>Why is the inertia on the right lower? K-Means optimizes for distance, not “blobbines”</li>
<li>Is clustering on the right “wrong”? Good question!</li>
</ul>
<p>Now, let us introduce Hierarchical Agglomerative Clustering:</p>
<ul>
<li><p>We start with every data point in a separate cluster.</p></li>
<li><p>We keep merging the most similar pairs of data points/clusters until we have one big cluster left.</p></li>
<li><p>This is called a <strong>bottom-up</strong> or <strong>agglomerative method</strong>.</p></li>
<li><p>There are various ways to decide the order of combining clusters called <strong>Linkage Criterion</strong>.</p></li>
<li><p><strong>Single linkage</strong>: Similarity of the most similar: the distance between two clusters as the <strong>minimum</strong> distance between a point in the first cluster and a point in the second.</p></li>
<li><p><strong>Complete linkage</strong>: Similarity of the least similar: the distance between two clusters as the <strong>maximum</strong> distance between a point in the first cluster and a point in the second.</p></li>
<li><p><strong>Average linkage</strong>: <strong>Average</strong> similarity of pairs of points in clusters.</p></li>
</ul>
<p><img src="images/linkage.png" alt="linkage" width="600"></p>
<p>When the algorithm starts, every data point is in its own cluster. In the plot below, there are 12 data points, so the algorithm starts with 12 clusters. As the clustering begins, it begins to assess which clusters are the closest together.</p>
<p><img src="images/agg1.png" alt="agg1" width="600"></p>
<p>The closest clusters are 10 and 11, so they are merged together.</p>
<p><img src="images/agg2.png" alt="agg2" width="600"></p>
<p>Next, points 0 and 4 are merged together because they are closest.</p>
<p><img src="images/agg3.png" alt="agg3" width="600"></p>
<p>At this point, we have 10 clusters: 8 with a single point (1, 2, 3, 4, 5, 6, 7, 8, 9) and 2 with 2 points (0, 10).</p>
<p>Although clusters 0 and 3 are not the closest, let us consider if we were trying to merge them.</p>
<p>A tricky question arises:</p>
<ul>
<li>What is the distance between clusters 0 and 3? We can use the <strong>Complete-Link</strong> approach that uses the <strong>max</strong> distance amongest all pairs of points between groups.</li>
</ul>
<p><img src="images/agg4.png" alt="agg4" width="600"></p>
<p>Let us assume the algorithm runs a little longer and we have reached the following state. Clusters 0 and 7 are up next; but why? The <strong>max line between any member of 0 and 6</strong> is longer than the <strong>max line between any member of 0 and 7</strong>.</p>
<p><img src="images/agg5.png" alt="agg5" width="600"></p>
<p>Thus, 0 and 7 are merged into 0.</p>
<p>After more iterations, we finally converge to the plot on the left. There are two clusters (0, 1), and the agglomerative algorithm has converged.</p>
<p><img src="images/agg6.png" alt="agg6" width="600"></p>
<p>Notice that on the full dataset, our agglomerative clustering algorithm gets the “correct” output.</p>
<section id="clustering-dendrograms-and-intuition" class="level3" data-number="28.3.1">
<h3 data-number="28.3.1" class="anchored" data-anchor-id="clustering-dendrograms-and-intuition"><span class="header-section-number">28.3.1</span> Clustering, Dendrograms, and Intuition</h3>
<p>Agglomerative clustering is one form of “hierarchical clustering.” It is interpretable because:</p>
<ul>
<li>We can keep track of when two clusters got merged.
<ul>
<li>Each cluster is a tree.</li>
</ul></li>
<li>Can visualize merging hierarchy, resulting in a “dendrogram.”
<ul>
<li>Won’t discuss any further, but you might see these in the wild.</li>
</ul></li>
</ul>
<p>Here are some examples:</p>
<p><img src="images/dendro_1.png" alt="dendro_1" width="600"></p>
<p><img src="images/dendro_2.png" alt="dendro_2" width="600"></p>
<p>Some professors use agglomerative clustering for grading bins.</p>
<ul>
<li>If there is a big gap between two people, draw a grading threshold there.</li>
<li>The idea: Grade clustering should be more like left figure, not right.</li>
</ul>
<p><img src="images/grading.png" alt="grading" width="600"></p>
</section>
</section>
<section id="picking-k" class="level2" data-number="28.4">
<h2 data-number="28.4" class="anchored" data-anchor-id="picking-k"><span class="header-section-number">28.4</span> Picking K</h2>
<p>The algorithms we’ve discussed require us to pick a <span class="math inline">\(K\)</span> before we start. But how do we pick <span class="math inline">\(K\)</span>?</p>
<p><img src="images/states.png" alt="states" width="600"></p>
<p>Often, the best <span class="math inline">\(K\)</span> is subjective:</p>
<ul>
<li>Example: State plot.</li>
<li>How many clusters are there here?</li>
</ul>
<p>For K-Means, one approach is to plot inertia versus many different <span class="math inline">\(K\)</span> values.</p>
<ul>
<li>Pick the <span class="math inline">\(K\)</span> in the “elbow”, where we get diminishing returns afterwards.</li>
<li>Note: Big complicated data often lacks an elbow, so this method is not fool proof.</li>
</ul>
<p><img src="images/elbow.png" alt="elbow" width="600"></p>
<p>Here, we would likely select <span class="math inline">\(K = 2\)</span>.</p>
<section id="silhouette-scores" class="level3" data-number="28.4.1">
<h3 data-number="28.4.1" class="anchored" data-anchor-id="silhouette-scores"><span class="header-section-number">28.4.1</span> Silhouette Scores</h3>
<p>To evaluate how “well-clustered” a specific data point is, we can use the “silhouette score”, a.k.a. the “silhouette width”.</p>
<ul>
<li>High score: Near the other points in its X’s cluster.</li>
<li>Low score: Far from the other points in its cluster.</li>
</ul>
<p><img src="images/high_low.png" alt="high_low" width="600"></p>
<p>For a data point <span class="math inline">\(X\)</span>, score <span class="math inline">\(S\)</span> is:</p>
<ul>
<li><span class="math inline">\(A\)</span> = avg distance to other points in cluster.</li>
<li><span class="math inline">\(B\)</span> = avg distance to points in closest cluster.</li>
<li><span class="math inline">\(S\)</span> = <span class="math inline">\(\frac{B - A}{max(A, B)}\)</span></li>
</ul>
<p>Consider what the highest possible value of <span class="math inline">\(S\)</span> is and how can that value occur? The highest possible value of <span class="math inline">\(S\)</span> is 1. This happens if every point in <span class="math inline">\(X\)</span>’s cluster is right on top of <span class="math inline">\(X\)</span>: The average distance to other points in <span class="math inline">\(X\)</span>’s cluster is <span class="math inline">\(0\)</span>, so <span class="math inline">\(A = 0\)</span>. Thus, <span class="math inline">\(\frac{B}{max(0, B)} = \frac{B}{B} = 1\)</span>. Additionally, another case where <span class="math inline">\(S = 1\)</span> could happen is if <span class="math inline">\(B &gt;&gt; A\)</span>.</p>
<p>Now, can <span class="math inline">\(S\)</span> be negative? The answer is yes. If the average distance to X’s clustermates is larger than distance to the closest cluster.</p>
<p>Example: The “low score” point on the right has <span class="math inline">\(S = -0.13\)</span></p>
</section>
<section id="silhouette-plot" class="level3" data-number="28.4.2">
<h3 data-number="28.4.2" class="anchored" data-anchor-id="silhouette-plot"><span class="header-section-number">28.4.2</span> Silhouette Plot</h3>
<p>We can plot the <strong>Silhouette Scores</strong> for all of our data points.</p>
<ul>
<li>Points with large silhouette widths are deeply embedded in their cluster.</li>
<li>Red dotted line shows the average.</li>
</ul>
<p><img src="images/high_low.png" alt="high_low" width="600"></p>
<p>The plot below is the silhouette score for the plot above:</p>
<p><img src="images/silhouette_2.png" alt="silhouette_2" width="600"></p>
<p>What do you notice?</p>
<p><img src="images/cluster_3.png" alt="cluster_3" width="600"></p>
<p>Similarly, the plot below is the silhouette score for the plot above:</p>
<p><img src="images/silhouette_scores.png" alt="silhouette_scores" width="600"></p>
<p>The average silhouette score is lower with 3 clusters, and thus <span class="math inline">\(K=2\)</span> is a better choice. This aligns with our visual intution as well.</p>
</section>
<section id="picking-k-real-world-metrics-example-via-andrew-ng" class="level3" data-number="28.4.3">
<h3 data-number="28.4.3" class="anchored" data-anchor-id="picking-k-real-world-metrics-example-via-andrew-ng"><span class="header-section-number">28.4.3</span> Picking K: Real World Metrics (Example via Andrew Ng)</h3>
<p>Sometimes you can rely on real world metrics to guide your choice of K.</p>
<p>Perform 2 clusterings:</p>
<ul>
<li>Cluster heights and weights of customers with <span class="math inline">\(K = 3\)</span> to design Small, Medium, and Large shirts.</li>
<li>Cluster heights and weights of customers with <span class="math inline">\(K = 5\)</span> to design XS, S, M, L, and XL shirts.</li>
</ul>
<p>To pick <span class="math inline">\(K\)</span>:</p>
<ul>
<li>Consider projected costs and sales for the 2 different <span class="math inline">\(K\)</span>s.</li>
<li>Pick the one that maximizes profit.</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2" data-number="28.5">
<h2 data-number="28.5" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">28.5</span> Conclusion</h2>
<p>We’ve now discussed a new machine learning goal: Clustering. We saw two solutions:</p>
<ul>
<li>K-Means
<ul>
<li>K-Means tries to optimize a loss function called inertia (no known algorithm to find the optimal answer in an efficient manner.)</li>
</ul></li>
<li>Hierarchical Agglomerative</li>
</ul>
<p>Our version of these algorithms required a hyperparameter <span class="math inline">\(K\)</span>.</p>
<ul>
<li>4 ways to pick <span class="math inline">\(K\)</span>: Intuitively, elbow method, silhouette scores, harnessing real world metrics.</li>
</ul>
<p>There are many Machine Learning problems.</p>
<ul>
<li>Each can be addressed by many different solution techniques.</li>
<li>Each has many metrics for evaluating success / loss.</li>
</ul>
<p>Many solution technique can be used for multiple problem types.</p>
<ul>
<li>Example: Linear models can be used for regression and classification.</li>
</ul>
<p>We’ve only scratched the surface. Haven’t discussed many important ideas.</p>
<ul>
<li>One hugely important solution technique: Neural Networks / Deep Learning.</li>
<li>Will provide some specific course recommendations in the last lecture.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../pca_2/pca_2.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>