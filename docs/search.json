[
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#logistic-regression-model-continued",
    "href": "logistic_regression_2/logistic_reg_2.html#logistic-regression-model-continued",
    "title": "24  Logistic Regression II",
    "section": "24.1 Logistic Regression Model (continued)",
    "text": "24.1 Logistic Regression Model (continued)\n\n24.1.1 Maximum Likelihood Estimation\nIn our earlier coin toss example, we had data on 10 coin flips, and wanted to estimate \\(\\hat \\theta\\), the probability of a heads.\n\nflips = [0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\nflips\n\n[0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n\n\n\\(\\hat \\theta = 0.4\\) is the most intuitive two reasons:\n\nIt is the frequency of heads in our data\nIt maximizes the likelihood of our data\n\n\\[\\hat \\theta = \\text{argmax}_\\theta (\\theta^4(1-\\theta)^6)\\]\nMore generally, we can apply this notion of likelihood to any random binary sample. For example, we can find the likelihood of the data observed in our breast cancer study. We will show how the likelihood is intrinsically related to cross-entropy loss.\nAs a quick refresher on likelihood:\nFor some Bernoulli(\\(p\\)) random variable \\(Y\\) the probability distribution, or likelihood is:\n\\[P(Y = y) = \\begin{cases}\n        1, \\text{with probability }  p\\\\\n        0, \\text{with probability }  1 - p\n    \\end{cases} \\]\nEquivalently, this can be written in a compact way:\n\\[P(Y=y) = p^y(1-p)^{1-y}\\]\n\nWhen \\(y = 1\\), this reads \\(P(Y=y) = p\\)\nWhen \\(y = 0\\), this reads \\(P(Y=y) = (1-p)\\)\n\nIn our example, a Bernoulli random variable is analagous to a single data point, or tumor (from the previous chapter). All together, our breast cancer study consist of multiple IID Bernoulli(\\(p\\)) random variables. To find the likelihood of independent events in succession, simply multiply their likelihoods.\n\\[\\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\\]\nAs with the coin example, we want to find the parameter \\(p\\) that maximizes this likelihood - this technique is known as maximum likelihood estimation. Earlier, we gave an intuitive graphical solution, but let’s take the derivative of the likelihood to find this maximum.\nFrom a first glance, this derivative will be complicated! We will have to use the product rule, followed by the chain rule. Instead, we can make an observation that simplifies the problem.\nFinding the \\(p\\) that maximizes \\[\\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\\] is equivalent to the \\(p\\) that maximizes \\[\\text{log}(\\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i})\\]\nThis is because \\(\\text{log}\\) is a strictly increasing function. It won’t change the maximum or minimum of the function it was applied to. From \\(\\text{log}\\) properties, \\(\\text{log}(a*b)\\) = \\(\\text{log}(a) + \\text{log}(b)\\). We can apply this to our equation above to get:\n\\[\\text{argmax}_p \\sum_{i=1}^{n} \\text{log}(p^{y_i} (1-p)^{1-y_i})\\]\n\\[= \\text{argmax}_p \\sum_{i=1}^{n} \\text{log}(p^{y_i}) + \\text{log}((1-p)^{1-y_i})\\]\n\\[= \\text{argmax}_p \\sum_{i=1}^{n} y_i\\text{log}(p) + (1-y_i)\\text{log}(1-p)\\]\nWe can add a constant factor of \\(\\frac{1}{n}\\) out front. It won’t affect the \\(p\\) that maximizes our likelihood.\n\\[=\\text{argmax}_p  \\frac{1}{n} \\sum_{i=1}^{n} y_i\\text{log}(p) + (1-y_i)\\text{log}(1-p)\\]\nOne last “trick” we can do is change this to a minimization problem by negating the result. This works because we are dealing with a concave function, which can be made convex.\n\\[= \\text{argmin}_p -\\frac{1}{n} \\sum_{i=1}^{n} y_i\\text{log}(p) + (1-y_i)\\text{log}(1-p)\\]\nThis is exactly our average cross-entropy loss minimization problem from before!\nWhy did we do all this complicated math? We have shown that minimizing cross-entropy loss is equivalent to maximizing the likelihood of the training data.\n\nBy minimizing cross-entropy loss, we are choosing the model parameters that are “most likely” for the data we observed."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#a-tangent-note",
    "href": "logistic_regression_2/logistic_reg_2.html#a-tangent-note",
    "title": "24  Logistic Regression II",
    "section": "24.2 A (Tangent) Note",
    "text": "24.2 A (Tangent) Note\nYou will study MLE further in probability and ML classes. But now you know it exists. It turns out that many of the model + loss combinations we’ve seen can be motivated using MLE (OLS, Ridge Regression, etc.)\nThe two important takeways from this section are\n\nFormulating the Logistic Regression Model\nMotivating the Cross-Entropy Loss\n\nWe will now continue to learn how to evaluate the strength of logistic regression models.\nAbove, we proved that minimizing cross-entropy loss is equivalent to maximizing likelihood of the training data.\nIntuitively, this means that the optimal \\(\\hat \\theta\\) that minimizes cross-entropy loss “pushes” all the probabilities in the direction of their true, underlying class.\n\nFor points that belong to the \\(0\\) class, \\(\\sigma(x^T\\theta) \\rightarrow 0\\)\nFor points that belong to the \\(1\\) class, \\(\\sigma(x^T\\theta) \\rightarrow 1\\)\n\nHowever, something interesting happens when our data is perfectly classifiable; in other words, linearly seperable.\n\n24.2.1 Linear Seperability and Regularization\nA classification dataset is said to be linearly separable if there exists a hyperplane among input features \\(x\\) that separates the two classes \\(y\\). For example, notice how the plot on the bottom left is linearly seperable along the vertical line \\(x=0\\). No such line perfectly seperates the two classes on the bottom right.\n\nLinear seperability in 1D can be found with a rugplot of a single feature.\n\n\nThis same definition holds in higher dimensions. If there are two features, the seperating hyperplane must exist in two dimensions (any line of the form \\(y=mx+b\\))\n\nLinear seperability among 2 features is evident from a two-dimensional visualization, or scatter plot.\n\n\nComplications may arise when data is linearly seperable. Consider the toy dataset with 2 points and only a single feature \\(x\\):\n\nThe optimal \\(\\theta\\) value that minimizes loss pushes the predicted probabilities of the data points to their true class.\n\n\\(P(Y = 1|x = -1) = \\frac{1}{1 + e^\\theta} \\rightarrow 1\\)\n\\(P(Y = 1|x = 1) = \\frac{1}{1 + e^{-\\theta}} \\rightarrow 0\\)\n\nThis happens when \\(\\theta = -\\infty\\). When \\(\\theta = -\\infty\\), we observe the following behavior for any input \\(x\\).\n\\[P(Y=1|x) = \\sigma(\\theta x) \\rightarrow \\begin{cases}\n        1, \\text{if }  x < 0\\\\\n        0, \\text{if }  x \\ge 0\n    \\end{cases}\\]\nThe diverging weights cause the model to be overconfident. For example, consider the new point \\((x, y) = (0.5, 1)\\). Following the behavior above, our model will incorrectly predict \\(p=0\\), and a thus, \\(\\hat y = 0\\).\n\nThe loss incurred by this misclassified point is infinite.\n\\[-(y\\text{ log}(p) + (1-y)\\text{ log}(1-p))\\]\n\\[=1\\text{log}(0)\\]\nThus, diverging weights (\\(|\\theta| \\rightarrow \\infty\\)) occur with lineary separable data. “Overconfidence” is a particularly dangerous version of overfitting.\nConsider the loss function with respect to the parameter \\(\\theta\\).\n\nAlthough impossible to see, the plateau for negative values of \\(\\theta\\) is slightly tilted downwards, meaning the loss approaches \\(0\\) as \\(\\theta\\) decreases and approaches \\(-\\infty\\).\n\n24.2.1.1 Regularized Logistic Regression\nTo avoid large weights, and thus, infinite loss (particularly on linearly seperable data), regularization is used. The same principles apply as with linear regression - make sure to standardize your features first.\nFor example, L2 (Ridge) Logistic Regression takes on the form\n\\[\\min_{\\theta} -\\frac{1}{n} \\sum_{i=1}^{n} (y_i \\text{log}(\\sigma(x_i^T\\theta)) + (1-y_i)\\text{log}(1-\\sigma(x_i^T\\theta))) + \\lambda \\sum_{i=1}^{d} \\theta_j^2\\]\nNow, let us compare the loss functions of un-regularized and regularized logistic regression.\n\n\nAs we can see, \\(L2\\) regularization helps us prevent diverging weights and deters against “overconfidence.”\n\n\n\n24.2.2 Logistic Regression Model Implementation\nThe implementation of logistic regression in sklearn is simple. We’ll begin by fitting a model on the breast cancer dataset from last lecture.\n\n\nCode\nimport pandas as pd\nimport sklearn.datasets\n\ndata_dict = sklearn.datasets.load_breast_cancer()\ndata = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\ndata['malignant'] = (data_dict['target'] == 0).astype(int)\n\nX = data[['mean radius']]\ny = data['malignant']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X, y);\n\nBy default, sklearn applies regularization to the logistic regression class. This is to avoid diverging weights with seperable data. The code above can be written more expliclty as follows.\n\n# sklearn defaults\nmodel = LogisticRegression(penalty='l2', C=1.0, fit_intercept=True)\nmodel.fit(X, y);\n\nThe parameter C controls the amount of regularization – C is the inverse of the regularization hyperparameter \\(\\lambda\\). Set C big for minimal regularization, and vice versa.\nThe .predict_proba method returns the predicted probabilities of belonging to each class. The first element corresponds to class \\(0\\), the second to class \\(1\\).\n\n# Here are the first 5 predicted probabilities\nmodel.predict_proba(X)[:5]\n\narray([[0.03507844, 0.96492156],\n       [0.00257756, 0.99742244],\n       [0.00632723, 0.99367277],\n       [0.96826813, 0.03173187],\n       [0.00343123, 0.99656877]])\n\n\nFrom here, the .predict function returns the predicted class \\(\\hat y\\) of the point. In the simple binary case,\n\\[\\hat y = \\begin{cases}\n        1, & P(Y=1|x) \\ge 0.5\\\\\n        0, & \\text{otherwise }\n    \\end{cases}\\]\n\n# Here are the first 5 predicted classes\nmodel.predict(X)[:5]\n\narray([1, 1, 1, 0, 1])"
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#performance-metrics",
    "href": "logistic_regression_2/logistic_reg_2.html#performance-metrics",
    "title": "24  Logistic Regression II",
    "section": "24.3 Performance Metrics",
    "text": "24.3 Performance Metrics\nNow that we have our classifier, let’s quantify how well it performs. The most basic evaluation metric is accuracy – the proportion of correctly classified points.\n\\[\\text{accuracy} = \\frac{\\# \\text{ of points classified correctly}}{\\# \\text{ of total points}}\\]\n\nmodel.score(X, y) # built-in accuracy function\n\n0.8787346221441125\n\n\nHowever, accuracy is not always a great metric for classification, particularily when the data has class imbalance.\nTo understand why, let’s consider a classification problem with 100 emails, 5 of which are spam. We’ll investigate two models where accuracy is a poor metric.\n\nModel 1: Our first model classifies every email as non-spam. The model’s accuracy is high (\\(\\frac{95}{100} = 0.95\\)), but it doesn’t detect any spam emails. Despite the high accuracy, this is a bad model.\nModel 2: The second model classifies every email as spam. The accuracy is low (\\(\\frac{5}{100} = 0.05\\)), but the model correctly labels every spam email. Unfortunately, it also misclassifies every non-spam email.\n\n\n24.3.1 The Confusion Matrix\nModel 1 from above has 5 false negatives (FN) – data points which were predicted to belong to class \\(0\\) (non-spam), but their true class was \\(1\\) (spam). In a similar vein, Model 2 has 95 false positives (FP) – that is, “false alarms” where we predict class \\(1\\), but the true class was \\(0\\). True positives (TP) and true negatives (TN) are when we correctly classify observations as being positive or negative, respectively.\nThese classifications can be concisely summarized in a confusion matrix.\n\nAn easy way to remember this terminology is as follows:\n\nLook at the second word in the phrase. Positive means a prediction of 1. Negative means a prediction of 0.\nLook at the first word in the phrase. True means our prediction was correct. False means it was incorrect.\n\nA confusion matrix for a particular classifier may be found programatically. For our breast cancer data, it looks like this:\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(X)\nconfusion_matrix(y, y_pred)\n\narray([[333,  24],\n       [ 45, 167]])\n\n\n\n\n24.3.2 Accuracy, Precision, and Recall\nThe purpose of our discussion of the confusion matrix was to motivate better performance metrics for classification problems with class imbalance - namely, precision and recall.\nPrecision is defined as\n\\[\\frac{\\text{TP}}{\\text{TP + FP}}\\]\nPrecision answers the question: “of all observations that were predicted to be \\(1\\), what proportion were actually \\(1\\)?” It measures how accurate the classifier is when its predictions are positive.\nRecall (or sensitivity) is defined as\n\\[\\frac{\\text{TP}}{\\text{TP + FN}}\\]\nRecall aims to answer: “of all observations that were actually \\(1\\), what proportion were predicted to be \\(1\\)?” It measures how many positive predictions were missed.\nHere’s a helpful graphic that summarizes our discussion above.\n\n\n24.3.2.1 Example Calculation\nIn this section, we will calculate the accuracy, precision, and recall performance metrics for our earlier spam classification example. As a reminder, we had a 100 emails, 5 of which were spam. We designed two models:\n\nModel 1: Predict that every email is non-spam\nModel 2: Predict that every email is spam\n\n\n24.3.2.1.1 Model 1\nFirst, let’s begin by creating the confusion matrix.\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTrue Negative: 95\nFalse Positive: 0\n\n\n1\nFalse Negative: 5\nTrue Positive: 0\n\n\n\nConvince yourself of why our confusion matrix looks like so.\n\\[\\text{accuracy} = \\frac{95}{100} = 0.95\\] \\[\\text{precision} = \\frac{0}{0 + 0} = \\text{undefined}\\] \\[\\text{recall} = \\frac{0}{0 + 5} = 0\\]\n\nNotice how our precision is undefined because we never predicted class \\(1\\)\nOur recall is 0 for the same reason – the numerator is 0 (we had no positive predictions)\n\n\n\n24.3.2.1.2 Model 2\nOur confusion matrix for Model 2 looks like so.\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTrue Negative: 0\nFalse Positive: 95\n\n\n1\nFalse Negative: 0\nTrue Positive: 5\n\n\n\n\\[\\text{accuracy} = \\frac{5}{100} = 0.05\\] \\[\\text{precision} = \\frac{5}{5 + 95} = 0.05\\] \\[\\text{recall} = \\frac{5}{5 + 0} = 1\\]\n\nOur precision is low because we have many false positives\nOur recall is perfect - we correctly classified all spam emails (we never predicted class \\(0\\))\n\n\n\n\n24.3.2.2 Precision vs Recall\nPrecision (\\(\\frac{\\text{TP}}{\\text{TP} + \\textbf{ FP}}\\)) penalizes false positives, while recall (\\(\\frac{\\text{TP}}{\\text{TP} + \\textbf{ FN}}\\)) penalizes false negatives.\nIn fact, precision and recall are inversely related. This is evident in our second model – we observed a high recall and low precision. Usually, there is a tradeoff in these two (most models can either minimize the number of FP or FN; and in rare cases, both).\nThe specific performance metric(s) to prioritize depends on the context. In many medical settings, there might be a much higher cost to missing positive cases. For instance, in our breast cancer example, it is more costly to misclassify malignant tumors (false negatives) than it is to incorrectly classify a benign tumor as malignant (false positives). In the case of the latter, pathologists can conduct further study to verify malignant tumors. As such, we should minimize the number of false negatives. This is equivalent to maximizing recall."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#adjusting-the-classification-threshold",
    "href": "logistic_regression_2/logistic_reg_2.html#adjusting-the-classification-threshold",
    "title": "24  Logistic Regression II",
    "section": "24.4 Adjusting the Classification Threshold",
    "text": "24.4 Adjusting the Classification Threshold\nOne way to minimize the number of FP vs. FN (equivalently, maximizing precision vs. recall) is by adjusting the classification threshold \\(T\\).\n\\[\\hat y = \\begin{cases}\n        1, & P(Y=1|x) \\ge T\\\\\n        0, & \\text{otherwise }\n    \\end{cases}\\]\nThe default threshold in sklearn is \\(T = 0.5\\). As we increase the threshold \\(T\\), we “raise the standard” of how confident our classifier needs to be to predict 1 (i.e., “positive”).\n\nAs you may notice, the choice of threshold \\(T\\) impacts our classifier’s performance.\n\nHigh \\(T\\): Most predictions are \\(0\\).\n\nLots of false negatives\nFewer false positives\n\nLow \\(T\\): Most predictions are \\(1\\).\n\nLots of false positives\nFewer false negatives\n\n\nIn fact, we can choose a threshold \\(T\\) based on our desired number, or proportion, of false positives and false negatives. We can do so using a few different tools. We’ll touch on two of the most important ones in Data 100.\n\nPrecision-Recall Curve (PR Curve). [Covered in Extra Content]\n“Receiver Operating Characteristic” Curve (ROC Curve)\n\nTo motivate the ROC Curve, let’s first consider two more metrics - true positive rate (TPR) and false positive rate (FPR).\n\n24.4.1 Two More Metrics\nThe True Positive Rate (TPR) is defined as\n\\[\\frac{\\text{TP}}{\\text{TP + FN}}\\]\nYou’ll notice this is equivalent to recall. In the context of our spam email classifier, it answers the question: “what proportion of spam did I mark correctly?”.\n\nWe’d like this to be close to \\(1\\)\n\nThe False Positive Rate (FPR) is defined as\n\\[\\frac{\\text{FP}}{\\text{FP + TN}}\\]\nAnother word for FPR is specificity. This answers the question: “what proportion of regular email did I mark as spam?”\n\nWe’d like this to be close to \\(0\\)\n\nAs we increase threshold \\(T\\), both TPR and FPR decrease. We’ve plotted this relationship below for some model on a toy dataset.\n\n\n\n24.4.2 The ROC Curve\nThe “Receiver Operating Characteristic” Curve (ROC Curve) plots the tradeoff between FPR and TPR. Notice how the far-left of the curve corresponds to higher threshold \\(T\\) values.\n\nThe “perfect” classifier is the one that has a TPR of 1, and FPR of 0. This is achieved at the top-left of the plot below. More generally, it’s ROC curve resembles the curve in orange.\n\nWe want our model to be as close to this orange curve as possible. How do we quantify “closeness”?\nWe can compute the area under curve (AUC) of the ROC curve. Notice how the perfect classifier has an AUC = 1. The closer our model’s AUC is to 1, the better it is. On the other hand, a terrible model will have an AUC closer to 0.5. This indicates the classifier is not able to distinguish between positive and negative classes, and thus, randomly predicts one of the two."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#extra-content",
    "href": "logistic_regression_2/logistic_reg_2.html#extra-content",
    "title": "24  Logistic Regression II",
    "section": "24.5 Extra Content",
    "text": "24.5 Extra Content\n\n24.5.1 Precision-Recall Curves\nA Precision-Recall Curve (PR Curve) is an alternative to the ROC curve that displays the relationship between precision and recall for various threshold values. It is constructed in a similar way as with the ROC curve.\nLet’s first consider how precision and recall change as a function of the threshold \\(T\\). We know this quite well from earlier – precision will generally increase, and recall will decrease.\n\nDisplayed below is the PR-Curve for the same toy dataset. Notice how threshold values increase as we move to the left.\n\nOnce again, the perfect classifier will resemble the orange curve, this time, facing the opposite direction.\n\nWe want our PR-Curve to be as close to the “top right” of this graph as possible. Again, we use the AUC to determine “closeness”, with the perfect classifier exhibiting an AUC = 1 (and the worst with an AUC = 0.5)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  }
]