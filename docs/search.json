[
  {
    "objectID": "pandas_1/pandas_1.html",
    "href": "pandas_1/pandas_1.html",
    "title": "Pandas I",
    "section": "",
    "text": "Tabular data is one of the most common data formats used in data science. We’ll primarily be looking at tabular data in Data 100.\nIn Data 8, you encountered the Table class of the datascience library. In Data 100, we’ll be using the DataFrame class of the pandas library to represent tabular data.\nHere is an example of a DataFrame containing election data.\n\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n      loss\n      1.073699\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n      win\n      51.311515\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n182 rows × 6 columns\n\n\n\nLet’s dissect the code above.\n\nWe first import the pandas library into our Python environment, using the alias pd.   import pandas as pd\nThere are a number of ways to read data into a DataFrame. In Data 100, our data are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.   pd.read_csv(\"elections.csv\")\n\nThis code stores our DataFrame object into the elections variable. Upon inspection, our elections DataFrame has 182 rows and 6 columns. Each row represents a single record - in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nThe API (application programming interface) for the DataFrame class is enormous. In the next section, we’ll discuss several methods of the DataFrame API that allow us to extract subsets of data."
  },
  {
    "objectID": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "href": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "title": "Pandas I",
    "section": "Slicing in DataFrames",
    "text": "Slicing in DataFrames\nOne of the most important tasks in manipulating a DataFrame is extracting a subset of rows and columns. This is called slicing. We can do so using three primary methods of the DataFrame class:\n\n.loc\n.iloc\n[]\n\n\nIndexing with .loc\nThe .loc operator selects rows and columns in a DataFrame by their row and column labels, respectively. The row label (commonly referred to as the index) is the bold text on the far left of a DataFrame, while the column label is the text found at the top of a DataFrame. By default, pandas assigns row labels as sequential integers beginning from 0. The column labels in our elections DataFrame are the columns Year, Candidate, Party, Popular Vote, Result, and %.\n.loc lets us specify the row and column labels to select from our DataFrame as the first and second arguments to the function. For example, to select the the row labeled 0 and the column labeled Candidate from our elections DataFrame we can write:\n\nelections.loc[0, 'Candidate']\n\n'Andrew Jackson'\n\n\nTo select multiple rows and columns, we can use Python slice notation. We can select the first four rows and first four columns.\n\nelections.loc[0:3, 'Year':'Popular vote']\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nSuppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : comes in great use.\n\nelections.loc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\nThere are a couple of things we should note. Unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.\nEquivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame.\n\nelections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nLastly, we can interchange list and slicing notation.\n\nelections.loc[[0, 1, 2, 3], :]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\nIndexing with .iloc\nSlicing with .iloc works similarily to .loc, although .iloc uses the integer positions of rows and columns rather the labels. The arguments for the .iloc function also behave similarly - single values, lists, indices, and any combination of these are permitted.\nWe can begin reproducing our results from above. Let’s begin by selecting for the first presedential candidate in our elections DataFrame:\n\n# elections.loc[0, \"Candidate\"] - Previous approach\nelections.iloc[0, 1]\n\n'Andrew Jackson'\n\n\nNotice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th (or first) position of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.\nHowever, when we select for the first four rows and columns using .iloc, we notice something.\n\n# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach\nelections.iloc[0:4, 0:4]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nSlicing is no longer inclusive in .iloc - it’s exclusive! Sad to say, this is one of Pandas syntatical subtleties. Don’t worry, you’ll get used to with practice.\nList behavior works just as expected.\n\n#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach\nelections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nThis discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.\n\n\nIndexing with []\nThe [] selection operator is the most baffling of all, yet it is the commonly used. It only takes a single argument, which may be one of the following:\n\nA slice of row numbers\nA list of column labels\nA single column label\n\nThat is, [] is context dependent. Let’s see some examples.\n\nA slice of row numbers\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\nA list of column labels\nSuppose we now want the first four columns.\n\nelections[[\"Year\", \"Candidate\", \"Party\", \"Popular vote\"]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n    \n  \n\n182 rows × 4 columns\n\n\n\n\n\nA single column label\nLastly, if we only want the Candidate column.\n\nelections[\"Candidate\"]\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThe output looks quite different - it’s no longer a DataFrame! This is a Series. We’ll talk about what a Series is in the next section."
  },
  {
    "objectID": "pandas_1/pandas_1.html#dataframes-series-and-indices",
    "href": "pandas_1/pandas_1.html#dataframes-series-and-indices",
    "title": "Pandas I",
    "section": "DataFrames, Series, and Indices",
    "text": "DataFrames, Series, and Indices\nWe saw that selecting a single column from a DataFrame using the [] operator outputted a new data format, called a Series. Let’s verify this claim.\n\ntype(elections)\n\npandas.core.frame.DataFrame\n\n\n\ntype(elections['Candidate'])\n\npandas.core.series.Series\n\n\nA Series is a one dimensional object that represents a single column of data. It has two components - an index and the data. A DataFrame is equivalent to a collection of multiple Series, which all share the same index. Notice how the index of a Series is equivalent to the index (or row labels) of a DataFrame.\n\nHowever, a DataFrame index doesn’t have to be an integer, nor does it have to be unique. For example, we can set our index to be the name of presedential candidates. Selecting a new Series from this modified DataFrame yields the following:\n\nTo retrieve the indices of a DataFrame, simple use the .index attribute of the DataFrame class.\n\nelections.set_index(\"Candidate\", inplace=True)\nelections.index\n\nIndex(['Andrew Jackson', 'John Quincy Adams', 'Andrew Jackson',\n       'John Quincy Adams', 'Andrew Jackson', 'Henry Clay', 'William Wirt',\n       'Hugh Lawson White', 'Martin Van Buren', 'William Henry Harrison',\n       ...\n       'Darrell Castle', 'Donald Trump', 'Evan McMullin', 'Gary Johnson',\n       'Hillary Clinton', 'Jill Stein', 'Joseph Biden', 'Donald Trump',\n       'Jo Jorgensen', 'Howard Hawkins'],\n      dtype='object', name='Candidate', length=182)\n\n\n\nelections.reset_index(inplace=True)\n\nEarlier, we mentioned that a Series was just a column of data. What if we wanted a single column as a DataFrame? To obtain this, we can pass in a list containing a single column to the [] selection operator.\n\nelections[[\"Party\"]] # [\"Party\"] is the argument - a list with a single element\n\n\n\n\n\n  \n    \n      \n      Party\n    \n  \n  \n    \n      0\n      Democratic-Republican\n    \n    \n      1\n      Democratic-Republican\n    \n    \n      2\n      Democratic\n    \n    \n      3\n      National Republican\n    \n    \n      4\n      Democratic\n    \n    \n      ...\n      ...\n    \n    \n      177\n      Green\n    \n    \n      178\n      Democratic\n    \n    \n      179\n      Republican\n    \n    \n      180\n      Libertarian\n    \n    \n      181\n      Green\n    \n  \n\n182 rows × 1 columns"
  },
  {
    "objectID": "pandas_1/pandas_1.html#conditional-selection",
    "href": "pandas_1/pandas_1.html#conditional-selection",
    "title": "Pandas I",
    "section": "Conditional Selection",
    "text": "Conditional Selection\nConditional selection allows us to select a subset of rows in a DataFrame if they follow some specified condition.\nTo understand how to use conditional selection, we must look at another input to the .loc and [] operators - a boolean array. This boolean array must have a length equal to the number of rows in the DataFrame. It will then return all rows with a corresponding True value in the array.\nHere, we will select all even-indexed rows in the first 10 rows of our DataFrame.\n\n# Why is :9 is the correct slice to select the first 10 rows?\nelections_first_10_rows = elections.loc[:9, :]\n\n# Notice how we have exactly 10 elements in our boolean array argument\nelections_first_10_rows[[True, False, True, False, True, \\\n                         False, True, False, True, False]]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      4\n      Andrew Jackson\n      1832\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      6\n      William Wirt\n      1832\n      Anti-Masonic\n      100715\n      loss\n      7.821583\n    \n    \n      8\n      Martin Van Buren\n      1836\n      Democratic\n      763291\n      win\n      52.272472\n    \n  \n\n\n\n\nUnfortunately, using this method to select multiple rows in a large DataFrame is infeasible. Instead, we can provide a logical condition as an input to .loc or [] that returns a boolean array with said length.\nFor example, to return all candidates affilliated with the Independent party:\n\nlogical_operator = elections['Party'] == \"Independent\"\nelections[logical_operator]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298\n    \n    \n      161\n      Ralph Nader\n      2004\n      Independent\n      465151\n      loss\n      0.380663\n    \n    \n      167\n      Ralph Nader\n      2008\n      Independent\n      739034\n      loss\n      0.563842\n    \n    \n      174\n      Evan McMullin\n      2016\n      Independent\n      732273\n      loss\n      0.539546\n    \n  \n\n\n\n\nHere, logical_operator evaluates to a Series of boolean values with length 182.\n\nlogical_operator\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n177    False\n178    False\n179    False\n180    False\n181    False\nName: Party, Length: 182, dtype: bool\n\n\nRows 121, 130, 143, 161, 167, and 174 evaluate to True and are thus returned in the DataFrame.\n\n\nCode\nprint(logical_operator.loc[[121, 130, 143, 161, 167, 174]])\n\n\n121    True\n130    True\n143    True\n161    True\n167    True\n174    True\nName: Party, dtype: bool\n\n\nPassing a Series as an argument to elections[] has the same affect as using in a boolean array. In fact, the [] selection operator can take a boolean Series, array, and list as arguments. These three are used interchangeably thoughout the course.\nSimilarly, we can use .loc to achieve similar results.\n\nelections.loc[elections['Party'] == \"Independent\"]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298\n    \n    \n      161\n      Ralph Nader\n      2004\n      Independent\n      465151\n      loss\n      0.380663\n    \n    \n      167\n      Ralph Nader\n      2008\n      Independent\n      739034\n      loss\n      0.563842\n    \n    \n      174\n      Evan McMullin\n      2016\n      Independent\n      732273\n      loss\n      0.539546\n    \n  \n\n\n\n\nBoolean conditions can be combined using various operators that allow us to filter results by multiple conditions. Some examples include the & (and) operator and | (or) operator.\nNote: When using logical operators, pay careful attention to surround each condition with a set of paranthesis (). If you forget, your code will throw an error.\nFor example, if we want to return data on all presidential candidates affiliated with the Independent Party before the 21st century, we can do so:\n\nelections[(elections['Party'] == \"Independent\") \\\n          & (elections['Year'] < 2000)]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298"
  },
  {
    "objectID": "pandas_1/pandas_1.html#handy-utility-functions",
    "href": "pandas_1/pandas_1.html#handy-utility-functions",
    "title": "Pandas I",
    "section": "Handy Utility Functions",
    "text": "Handy Utility Functions\nThere are a large number of operations supported by pandas Series and DataFrames that allow us to efficiently manipulate data . In this section, we’ll cover a few.\n\n.head and .tail\n.shape and .size\n.describe\n.sample\n.value_counts\n.unique\n.sort_values\n\n\n.head / .tail\n.head(n) and .tail(n) display the first n and last n rows of a DataFrame, respectively.\n\nelections.head(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n  \n\n\n\n\n\nelections.tail(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      179\n      Donald Trump\n      2020\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      Jo Jorgensen\n      2020\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      Howard Hawkins\n      2020\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n\n\n\n\n\n.shape / .size\n.shape returns a tuple with the number of rows and columns in a DataFrame.  .size returns the total number of data entries. This is the product of the number of rows and columns.\n\nelections.shape\n\n(182, 6)\n\n\n\nnum_rows, num_cols = elections.shape\nassert(elections.size == num_rows * num_cols)\nelections.size\n\n1092\n\n\n\n\n.describe\n.describe() returns a DataFrame of useful summary statistics for each numerical column.\n\nelections.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Popular vote\n      %\n    \n  \n  \n    \n      count\n      182.000000\n      1.820000e+02\n      182.000000\n    \n    \n      mean\n      1934.087912\n      1.235364e+07\n      27.470350\n    \n    \n      std\n      57.048908\n      1.907715e+07\n      22.968034\n    \n    \n      min\n      1824.000000\n      1.007150e+05\n      0.098088\n    \n    \n      25%\n      1889.000000\n      3.876395e+05\n      1.219996\n    \n    \n      50%\n      1936.000000\n      1.709375e+06\n      37.677893\n    \n    \n      75%\n      1988.000000\n      1.897775e+07\n      48.354977\n    \n    \n      max\n      2020.000000\n      8.126892e+07\n      61.344703\n    \n  \n\n\n\n\n\n\n.sample\n.sample(n) returns a random sample of n rows from the given DataFrame.\n\nelections.sample(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      48\n      James B. Weaver\n      1892\n      Populist\n      1041028\n      loss\n      8.645038\n    \n    \n      93\n      William Lemke\n      1936\n      Union\n      892378\n      loss\n      1.960733\n    \n    \n      83\n      Al Smith\n      1928\n      Democratic\n      15015464\n      loss\n      40.902853\n    \n  \n\n\n\n\n\n\n.value_counts\n.value_counts() is called on a column and returns a Series containing the total count of every unique value.\n\nelections['Candidate'].value_counts()\n\nNorman Thomas         5\nFranklin Roosevelt    4\nRalph Nader           4\nEugene V. Debs        4\nAndrew Jackson        3\n                     ..\nT. Coleman Andrews    1\nAlton B. Parker       1\nPat Buchanan          1\nJohn W. Davis         1\nWilliam Wirt          1\nName: Candidate, Length: 132, dtype: int64\nThis code tells us how many times each candidate ran for president of the United States.\n\n\n\n\n.unique\n.unique() is called on a Series and returns an array with the unique values contained in that Series.\n\n# For brevity, we have limited the results to 5 candidates \nelections['Candidate'].unique()[:5]\n\narray(['Andrew Jackson', 'John Quincy Adams', 'Henry Clay',\n       'William Wirt', 'Hugh Lawson White'], dtype=object)\n\n\n\n\n.sort_values\n.sort_values() returns the Series it was called on back in sorted order. Numerical values are in sorted magnitude, while text is sorted in alphabetical order. You may specify optional arguments to sort in ascending or descending order.\n\nelections['Candidate'].sort_values()\n\n75           Aaron S. Watkins\n27            Abraham Lincoln\n23            Abraham Lincoln\n108           Adlai Stevenson\n105           Adlai Stevenson\n                ...          \n19             Winfield Scott\n37     Winfield Scott Hancock\n74             Woodrow Wilson\n70             Woodrow Wilson\n16             Zachary Taylor\nName: Candidate, Length: 182, dtype: object\n\n\n\n\nParting Note\nThe pandas library is enormous and contains many useful functions. Here is a link to documentation.\nThis lecture and the next will cover important methods you should be fluent in. However, we want you to get familiar with the real world programming practice of …Googling! Answers to your questions can be found in documentation, Stack Overflow, etc.\nWith that, let’s move on to Pandas II."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Cleaning and EDA",
    "section": "",
    "text": "In the past few lectures, we’ve learned that pandas is a toolkit to restructure, modify, and explore a dataset. What we haven’t yet touched on is how to make these data transformation decisions. When we receive a new set of data from the “real world,” how do we know what processing we should do to convert this data into a usable form?\nData cleaning, also called data wrangling, is the process of transforming raw data to facilitate subsequent analysis. It is often used to address issues like:\nExploratory data analysis (EDA) is the process of understanding a new dataset. It is an open-ended, informal analysis that involves familiarizing ourselves with the variables present in the data, discovering potential hypotheses, and identifying potential issues with the data. This last point can often motivate further data cleaning to address any problems with the dataset’s format; because of this, EDA and data cleaning are often thought of as an “infinite loop,” with each process driving the other.\nIn this lecture, we will consider the key properties of data to consider when performing data cleaning and EDA. In doing so, we’ll develop a “checklist” of sorts for you to consider when approaching a new dataset. Throughout this process, we’ll build a deeper understanding of this early (but very important!) stage of the data science lifecycle."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "Data Cleaning and EDA",
    "section": "Structure",
    "text": "Structure\n\nFile Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for comma-separated values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (tab-separated values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\n﻿Year   Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\nVariable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nPrimary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request #\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "Data Cleaning and EDA",
    "section": "Granularity, Scope, and Temporality",
    "text": "Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "Data Cleaning and EDA",
    "section": "Faithfulness",
    "text": "Faithfulness\nAt this stage in our data wangling and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "visualization_1/visualization_1.html",
    "href": "visualization_1/visualization_1.html",
    "title": "Visualization I",
    "section": "",
    "text": "In our journey of the data science lifecycle, we have begun to explore the vast world of exploratory data analysis. More recently, we learned how to pre-process data using feature engineering and data manipulation techniques. As we work towards designing complex models, there is one key component missing in our arsenal - the ability to visualize and discern relationships in existing data.\nThese next two lectures will introduce you to various examples of data visualizations and their underlying theory. In doing so, we’ll motivate their usefulness in real-world examples with helpful plotting libraries.\n\nVisualizations in Data 8 and Data 100 (so far)\nYou’ve likely encountered several forms of data visualizations in your studies. You may remember two such examples from Data 8: line charts and histograms. Each of these served a unique purpose. For example, line charts displayed how numerical quantities changed over time, while histograms were useful in understanding a variable’s distribution.\n\n\nLine Chart\n\n\n\n\nHistogram\n\n\n\n\n\nGoals of Visualization\nVisualizations are useful for a number of reasons. In Data 100, we consider two areas in particular:\n\nTo broaden your understanding of the data\n\nKey part in exploratory data analysis\nUseful in investigating relationships between variables\n\nTo communicate your results to others\n\nVisualization theory is especially important here\n\n\nOne of the most common applications of visualizations - and the one that will be covered today - is in understanding a distribution of data.\n\n\nAn Overview of Distributions\nA distribution describes the frequency of unique values in a variable. Distributions must satisfy two properties:\n\nEach data point must belong to only one category.\nThe total frequency of all categories must sum to 100%. In other words, their total count should equal the number of values in consideration.\n\nLet’s look at a couple of examples.\n\n\nNot a Valid Distribution\n\n\n\n\nValid Distribution\n\n\n\nLeft Diagram: This is not a valid distribution. Individuals can belong to more than one category and the total frequency of all categories does not sum up to 100%.\nRight Diagram: This example satisfies the two properties of distributions, so it is a valid distribution.\n\n\nBar Plots\nAs we saw above, bar plots are one of the most common ways of displaying the distribution of a qualitative (categorical) variable. The length of a bar plot encodes the frequency of a category; the width encodes no useful information.\nLet’s contextualize this in an example. We will use the familiar births dataset from Data 8 in our analysis.\n\n\nCode\nimport pandas as pd\n\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nWe can visualize the distribution of the Maternal Smoker column using a bar plot. There are a few ways to do this.\n\nPlotting in Pandas\n\nbirths['Maternal Smoker'].value_counts().plot(kind = 'bar');\n\n\n\n\nRecall that .value_counts() returns a Series with the total count of each unique value. We call .plot(kind = 'bar') on this result to visualize these counts as a bar plot.\nPlotting methods in pandas are the least preferred and not supported in Data 100, as their functionality is limited. Instead, future examples will focus on other libaries built specifically for visualizing data. The most well-known library here is matplotlib.\n\n\nPlotting in Matplotlib\n\nimport matplotlib.pyplot as plt\n\nms = births['Maternal Smoker'].value_counts()\nplt.bar(ms.index, ms)\nplt.xlabel(\"Maternal Smoker\")\nplt.ylabel(\"Count\");\n\n\n\n\nWhile more code is required to achieve the same result, matplotlib is often used over pandas for its ability to plot more complex visualizations, some of which are discussed shortly.\nHowever, notice how the x-axis is a range of integers rather than the two categories, True and False. This is because matplotlib coerces True to a value of 1 and False to 0. Also, note how we needed to label the axes with plt.xlabel and plt.ylabel - matplotlib does not support automatic axis labeling. To get around these inconveniences, we can use a more effecient plotting library, seaborn.\n\n\nPlotting in Seaborn\n\nimport seaborn as sns\nsns.countplot(data = births, x = \"Maternal Smoker\");\n\n\n\n\nseaborn.countplot both counts and visualizes the number of unique values in a given column. This column is specified by the x argument to sns.countplot, while the DataFrame is specified by the data argument.\nFor the vast majority of visualizations, seaborn is far more concise and aesthetically pleasing than matplotlib. However, the color scheme of this particular bar plot is abritrary - it encodes no additional information about the categories themselves. This is not always true; color may signify meaningful detail in other visualizations. We’ll explore this more in-depth during the next lecture.\n\n\nPlotting in Plotly\n\nplotly is one of the most versatile plottling libraries and widely used in industry. However, plotly has various dependencies that make it difficult to support in Data 100. Therfore, we have intentionally excluded the code to generate the plot above.\nBy now, you’ll have noticed that each of these plotting libraries have a very different syntax. As with pandas, we’ll teach you the important methods in matplotlib and seaborn, but you’ll learn more through documentation.\n\nMatplotlib Documentation\nSeaborn Documentation\n\n\n\n\nHistograms\nHistograms are a natural extension to bar plots; they visualize the distribution of quantitative (numerical) data.\nRevisiting our example with the births DataFrame, let’s plot the distribution of the Maternal Pregnancy Weight column.\n\n\nCode\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nHow should we define our categories for this variable? In the previous example, these were the unique values of the Maternal Smoker column: True and False. If we use similar logic here, our categories are the different numerical weights contained in the Maternal Pregnancy Weight column.\nUnder this assumption, let’s plot this distribution using the seaborn.countplot function.\n\nsns.countplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis histogram clearly suffers from overplotting. This is somewhat expected for Maternal Pregnancy Weight - it is a quantitative variable that takes on a wide range of values.\nTo combat this problem, statisticians use bins to categorize numerical data. Luckily, seaborn provides a helpful plotting function that automatically bins our data.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\");\n\n\n\n\nThis diagram is known as a histogram. While it looks more reasonable, notice how we lose fine-grain information on the distribution of data contained within each bin. We can introduce rug plots to minimize this information loss. An overlaid “rug plot” displays the within-bin distribution of our data, as denoted by the thickness of the colored line on the x-axis.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\");\nsns.rugplot(data = births, x = \"Maternal Pregnancy Weight\", color = 'red');\n\n\n\n\nYou may have seen histograms drawn differently - perhaps with an overlaid density curve and normalized y-axis. We can display both with a few tweaks to our code.\nTo visualize a density curve, we can set the the kde = True argument of the sns.histplot. Setting the argument stat = \"density\" normalizes our histogram and displays densities, instead of counts, on the y-axis. You’ll notice that the area under the density curve is 1.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\", kde = True, \n             stat = \"density\")\nsns.rugplot(data = births, x = \"Maternal Pregnancy Weight\", color = 'red');\n\n\n\n\n\n\nEvaluating Histograms\nHistograms allow us to assess a distribution by their shape. There are a few properties of histograms we can analyze:\n\nSkewness and Tails\n\nSkewed left vs skewed right\nLeft tail vs right tail\n\nOutliers\n\nDefined arbitrarily for now\n\nModes\n\nMost commonly occuring data\n\n\n\nSkewness and Tails\nIf a distribution has a long right tail (such as Maternal Pregancy Weight), it is skewed right. In a right-skewed distribution, the few large outliers “pull” the mean to the right of the median.\nIf a distribution has a long left tail, it is skewed left. In a left-skewed distribution, the few small outliers “pull” the mean to the left of the median.\nIn the case where a distribution has equal-sized right and left tails, it is symmetric. The mean is approximately equal to the median.\n\n\nOutliers\nLoosely speaking, an outlier is defined as a data point that lies an abnormally large distance away from other values. We’ll define the statistical measure for this shortly.\nOutliers disproportionately influce the mean because their magnitude is directly involved in computing the average. However, the median is largely unaffected - the magnitude of an outlier is irrelevant; we only care that it is some non-zero distance away from the midpoint of the data.\n\n\nModes\nA mode of a distribution is a local or global maximum. A distribution with a single clear maximum is unimodal, distributions with two modes are bimodal, and those with 3 or more are multimodal.\nFor example, the distribution of birth weights for maternal smokers is (weakly) multimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight');\n\n\n\n\nOn the other hand, the distribution of birth weights for maternal non-smokers is unimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight');\n\n\n\n\n\n\n\nBox Plots and Violin Plots\n\nBoxplots\nBoxplots are an alternative to histograms that visualize numerical distributions. They are especially useful in graphicaly summarizing several characteristics of a distribution. These include:\n\nLower Quartile (\\(1\\)st Quartile)\nMedian (\\(2\\)nd Quartile)\nUpper Quartile (\\(3\\)rd Quartile)\nInterquartile Range (IQR)\nWhiskers\nOutliers\n\nThe lower quartile, median, and uper quartile are the \\(25\\)th, \\(50\\)th, and \\(75\\)th percentiles of data, respectively. The interquartile range measures the spread of the middle \\(50\\)% of the distribution, calculated as the (\\(3\\)rd Quartile \\(-\\) \\(1\\)st Quartile).\nThe whiskers of a box-plot are the two points that lie at the \\(1\\)st Quartile \\(-\\) (\\(1.5\\) * IQR), and the \\(3\\)rd Quartile \\(+\\) (\\(1.5\\) * IQR). They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than (\\(1.5\\) \\(*\\) IQR) from the extreme quartiles.\nLet’s visualize a box-plot of the Birth Weight column.\n\n\nCode\nimport numpy as np\n\nsns.boxplot(data = births, y = \"Birth Weight\");\n\nbweights = births['Birth Weight']\nq1 = np.percentile(bweights, 25)\nq2 = np.percentile(bweights, 50)\nq3 = np.percentile(bweights, 75)\niqr = q3 - q1\nwhisk1 = q1 - (1.5 * iqr)\nwhisk2 = q3 + (1.5 * iqr)\n\nprint(\"The first quartile is {}\".format(q1))\nprint(\"The second quartile is {}\".format(q2))\nprint(\"The third quartile is {}\".format(q3))\nprint(\"The interquartile range is {}\".format(iqr))\nprint(\"The whiskers are {} and {}\".format(whisk1, whisk2))\n\n\nThe first quartile is 108.0\nThe second quartile is 120.0\nThe third quartile is 131.0\nThe interquartile range is 23.0\nThe whiskers are 73.5 and 165.5\n\n\n\n\n\nHere is a helpful visual that summarizes our discussion above.\n\n\n\nViolin Plots\nAnother diagram that is useful in visualizing a variable’s distribution is the violin plot. A violin plot supplements a box-plot with a smoothed density curve on either side of the plot. These density curves highlight the relative frequency of variable’s possible values. If you look closely, you’ll be able to discern the quartiles, whiskers, and other hallmark features of the box-plot.\n\nsns.violinplot(data = births, y = 'Birth Weight');\n\n\n\n\n\n\n\nComparing Quantitative Distributions\nEarlier in our discussion of the mode, we visualized two histograms that described the distribution of birth weights for maternal smokers and non-smokers. However, comparing these histograms was difficult because they were displayed on seperate plots. Can we overlay the two to tell a more compelling story?\nIn seaborn, multiple calls to a plotting library in the same code cell will overlay the plots. For example:\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == True]\n\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight',\n             color = 'orange', label = 'smoker')\nsns.histplot(data = births_non_maternal_smoker, x= 'Birth Weight',\n             color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nHowever, notice how this diagram suffers from overplotting. We can fix this with a call to sns.kdeplot. This will remove the bins and overlay the histogram with a density curve that better summarizes the distribution.\n\nsns.kdeplot(data = births_maternal_smoker, x= 'Birth Weight', color = 'orange', label = 'smoker')\nsns.kdeplot(data = births_non_maternal_smoker, x= 'Birth Weight', color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nUnfortunately, we lose critical information in our distribution by removing small details. Therefore, we typically prefer to use box-plots and violin plots when comparing distributions. These are more concise and allow us to compare summary statistics across many distributions.\n\nsns.violinplot(data=births, x='Maternal Smoker', y='Birth Weight');\n# The following line of code plots a box-plot\n#sns.boxplot(data=births, x='Maternal Smoker', y='Birth Weight');\n\n\n\n\n\n\nRelationships Between Quantitative Variables\nUp until now, we’ve discussed how to visualize single-variable distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n\nScatter Plots\nScatter plots are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\nFor example, let’s plot a scatter plot comparing the Maternal Pregnancy Weight and Birth Weight colums, using both matplotlib and seaborn.\n\n# Matplotlib Example\nplt.scatter(births['Maternal Pregnancy Weight'], births['Birth Weight'])\n# For brevity, we have excluded code to label the axes\n\n<matplotlib.collections.PathCollection at 0x7fb6acdf5550>\n\n\n\n\n\n\n# Seaborn Example\nsns.scatterplot(data = births, x = \"Maternal Pregnancy Weight\", y = \"Birth Weight\",\n                hue = \"Maternal Smoker\")\n\n<AxesSubplot: xlabel='Maternal Pregnancy Weight', ylabel='Birth Weight'>\n\n\n\n\n\nThis is an example where color is used to add a third dimension to our plot. This is possible with the hue paramater in seaborn, which adds a categorical column encoding to an existing visualization. This way, we can look for relationships in Maternal Pregnancy Weight and Birth Weight in both maternal smokers and non-smokers. If we wish to see the relationship’s strength more clearly, we can use sns.lmplot.\n\nsns.lmplot(data = births, x = \"Maternal Pregnancy Weight\", y = \"Birth Weight\", \n           hue=\"Maternal Smoker\", ci=False);\n\n\n\n\nWe can make out a weak, positive relationship in pregnancy weight and birth weight for both maternal smokers and non-smokers (slightly more positive in maternal smokers).\n\n\nHex Plots and Contour Plots\nUnfortunately, our scatter plots above suffered from overplotting, which made them hard to interpret. And with a large number of points, jittering is unlikely to resolve the issue. Instead, we can look to hex plots and contour plots.\nHex Plots can be thought of as a two dimensional histogram that shows the joint distribution between two variables. This is particularly useful working with very dense data.\n\nsns.jointplot(data = births, x = \"Maternal Pregnancy Weight\", \n              y = \"Birth Weight\", kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x7fb6885efac0>\n\n\n\n\n\nThe axes are evidently binned into hexagons, which makes the linear relationship easier to decipher. Darker regions generally indicate a higher density of points.\nOn the other hand, contour plots are two dimensional versions of density curves with marginal distributions of each variable on the axes. We’ve used very similar code here to generate our contour plots, with the addition of the kind = 'kde' and fill = True arguments.\n\nsns.jointplot(data = births, x = \"Maternal Pregnancy Weight\", \n              y = \"Birth Weight\", kind = 'kde', fill = True)\n\n<seaborn.axisgrid.JointGrid at 0x7fb6acb10370>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data100-course-notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "pandas_2/pandas_2.html",
    "href": "pandas_2/pandas_2.html",
    "title": "Pandas II",
    "section": "",
    "text": "Last time, we introduced the Pandas library as a toolkit for processing data. We learned the DataFrame and Series data structures, familiarized ourselves with the basic syntax for manipulating tabular data, and began writing our first lines of Pandas code.\nIn this lecture, we’ll start to dive into some advanced Pandas syntax. You may find it helpful to follow along with a notebook of your own as we walk through these new pieces of code.\nWe’ll start by loading the babynames dataset."
  },
  {
    "objectID": "pandas_2/pandas_2.html#sorting-with-a-custom-key",
    "href": "pandas_2/pandas_2.html#sorting-with-a-custom-key",
    "title": "Pandas II",
    "section": "Sorting With a Custom Key",
    "text": "Sorting With a Custom Key\nIn the last lecture, we learned how to sort a DataFrame by the values in one or more of its columns using .sort_values. Pandas automatically sorted values in order according to numeric value (for number data) or alphabetical order (for string data).\n\n# Sort names by reverse-alphabetical order\n# Recall that `.head(5)` displays the first five rows in the DataFrame\nbabynames.sort_values(\"Name\", ascending=False).head(5) \n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      400761\n      CA\n      M\n      2021\n      Zyrus\n      5\n    \n    \n      197519\n      CA\n      F\n      2011\n      Zyrah\n      5\n    \n    \n      232144\n      CA\n      F\n      2020\n      Zyrah\n      5\n    \n    \n      217415\n      CA\n      F\n      2016\n      Zyrah\n      5\n    \n    \n      220674\n      CA\n      F\n      2017\n      Zyrah\n      6\n    \n  \n\n\n\n\nThis offers us a lot of functionality, but what if we need to sort by some other metric? For example, what if we wanted to find the longest names in the DataFrame?\nWe can do this by specifying the key parameter of .sort_values. The key parameter is assigned to a function of our choice. This function is then applied to each value in the specified column. Pandas will, finally, sort the DataFrame by the values outputted by the function.\n\n# Here, a lambda function is applied to find the length of each value, `x`, in the \"Name\" column\nbabynames.sort_values(\"Name\", key = lambda x: x.str.len(), ascending=False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10"
  },
  {
    "objectID": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "href": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "title": "Pandas II",
    "section": "Adding and Removing Columns",
    "text": "Adding and Removing Columns\nTo add a new column to a DataFrame, we use a syntax similar to that used when accessing an existing column. Specify the name of the new column by writing dataframe[\"new_column\"], then assign this to a Series or Array containing the values that will populate this column.\n\n# Add a column named \"Length\" that includes the length of each name\nbabynames[\"Length\"] = babynames[\"Name\"].str.len()\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      Length\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n      4\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n      5\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n      7\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n      8\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n      7\n    \n  \n\n\n\n\nIn the example above, we made use of an in-built function given to us by the str accessor. What if we had wanted to generate the values in our new column using a function of our own making?\nWe can do this using the Series .map method. .map takes in a function as input, and will apply this function to each value of a Series.\nFor example, say we wanted to find the number of occurrences of the sequence “dr” or “ea” in each name.\n\n# First, define a function to count the number of times \"dr\" or \"ea\" appear in each name\ndef dr_ea_count(string):\n    return string.count(\"dr\") + string.count(\"ea\")\n\n# Then, use `map` to apply `dr_ea_count` to each name in the \"Name\" column\nbabynames[\"dr_ea_count\"] = babynames[\"Name\"].map(dr_ea_count)\n\n# Sort the DataFrame by the new \"dr_ea_count\" column so we can see our handiwork\nbabynames.sort_values(by = \"dr_ea_count\", ascending = False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      Length\n      dr_ea_count\n    \n  \n  \n    \n      101969\n      CA\n      F\n      1986\n      Deandrea\n      6\n      8\n      3\n    \n    \n      115950\n      CA\n      F\n      1990\n      Deandrea\n      5\n      8\n      3\n    \n    \n      131022\n      CA\n      F\n      1994\n      Leandrea\n      5\n      8\n      3\n    \n    \n      304390\n      CA\n      M\n      1985\n      Deandrea\n      6\n      8\n      3\n    \n    \n      108723\n      CA\n      F\n      1988\n      Deandrea\n      5\n      8\n      3\n    \n  \n\n\n\n\nIf we want to remove a column or row of a DataFrame, we can call the .drop method. Use the axis parameter to specify whether a column or row should be dropped. Unless otherwise specified, Pandas will assume that we are dropping a row by default.\n\n# Drop the row of the DataFrame with label 2\nbabynames = babynames.drop(2, axis=\"rows\")\n\n# Drop our \"dr_ea_count\" and \"length\" columns from the DataFrame\nbabynames = babynames.drop([\"dr_ea_count\", \"Length\"], axis=\"columns\")\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      5\n      CA\n      F\n      1910\n      Ruth\n      128\n    \n  \n\n\n\n\nNotice that we reassigned babynames to the result of babynames.drop(...). This is a subtle, but important point: Pandas table operations do not occur in-place. Calling dataframe.drop(...) will output a copy of dataframe with the row/column of interest removed, without modifying the original dataframe table.\nIn other words, if we simply call:\n\n# This creates a copy of `babynames` and removes the row with label 3...\nbabynames.drop(3, axis=\"rows\")\n\n# ...but the original `babynames` is unchanged! \n# Notice that the row with label 3 is still present\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      5\n      CA\n      F\n      1910\n      Ruth\n      128\n    \n  \n\n\n\n\nOur original babynames DataFrame will remain unmodified."
  },
  {
    "objectID": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "href": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "title": "Pandas II",
    "section": "Aggregating Data with GroupBy",
    "text": "Aggregating Data with GroupBy\nUp until this point, we have been working with individual rows of DataFrames. As Data Scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use Pandas GroupBy objects.\nLet’s say we wanted to aggregate all rows in babynames for a given year.\n\nbabynames.groupby(\"Year\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f93f37739d0>\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from babynames that correspond to a particular year.\nThe diagram below shows a simplified view of babynames to help illustrate this idea.\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells Pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, Pandas will return a normal (now grouped) DataFrame.\nThe first aggregation method we’ll consider is .agg. The .agg method takes in a function as its argument; this function is then applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in babynames – this is equivalent to finding the number of babies born in each year.\n\nbabynames.groupby(\"Year\").agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      8943\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of babynames, which is why we see smaller values for the summed counts.\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original babynames DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, Pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell Pandas to only consider the \"Count\" column when summing\nbabynames.groupby(\"Year\")[[\"Count\"]].agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      8943\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\nThere is a whole host of aggregation methods we can use other than .agg. Some useful options are:\n\n.max: creates a new DataFrame with the maximum value of each group\n.mean: creates a new DataFrame with the mean value of each group\n.size: creates a new Series with the number of entries in each group\n.filter: creates a copy of the original DataFrame, keeping only the rows from subframes that obey a provided condition"
  },
  {
    "objectID": "pandas_2/pandas_2.html#aggregating-data-with-pivot-tables",
    "href": "pandas_2/pandas_2.html#aggregating-data-with-pivot-tables",
    "title": "Pandas II",
    "section": "Aggregating Data with Pivot Tables",
    "text": "Aggregating Data with Pivot Tables\nWe know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of columns names to .groupby.\nLet’s find the total number of baby names associated with each sex for each year in babynames. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\n# Find the total number of baby names associated with each sex for each year in the data\nbabynames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].agg(sum).head(6)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1910\n      F\n      5730\n    \n    \n      M\n      3213\n    \n    \n      1911\n      F\n      6602\n    \n    \n      M\n      3381\n    \n    \n      1912\n      F\n      9804\n    \n    \n      M\n      8142\n    \n  \n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multindex where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multindexes have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. You saw these back in Data 8. One set of values is used to create the index of the table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the Pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nbabynames.pivot_table(index = \"Year\", columns = \"Sex\", values = \"Count\", aggfunc = sum).head(5)\n\n\n\n\n\n  \n    \n      Sex\n      F\n      M\n    \n    \n      Year\n      \n      \n    \n  \n  \n    \n      1910\n      5730\n      3213\n    \n    \n      1911\n      6602\n      3381\n    \n    \n      1912\n      9804\n      8142\n    \n    \n      1913\n      11860\n      10234\n    \n    \n      1914\n      13815\n      13111\n    \n  \n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = sum tells Pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\""
  },
  {
    "objectID": "pandas_2/pandas_2.html#joining-tables",
    "href": "pandas_2/pandas_2.html#joining-tables",
    "title": "Pandas II",
    "section": "Joining Tables",
    "text": "Joining Tables\nWhen working on Data Science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world Data Scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can join two or more tables into a single DataFrame.\nTo put this into practice, we’ll revisit the elections dataset from last lecture.\n\n\nCode\nelections = pd.read_csv(\"data/elections.csv\")\nelections.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nSay we want to understand the 2023 popularity of the names of each presidential candidate. To do this, we’ll need the combined data of babynames and elections.\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in babynames.\n\n# This `str` operation splits each candidate's full name at each \n# blank space, then takes just the candidiate's first name\nelections[\"First Name\"] = elections[\"Candidate\"].str.split().str[0]\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n      John\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n      John\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n    \n  \n\n\n\n\nNow, we’re ready to join the two tables. pd.merge is the Pandas method used to join DataFrames together. The left and right parameters are used to specify the DataFrames to be joined. The left_on and right_on parameters are assigned to the string names of the columns to be used when performing the join. These two on parameters tell Pandas what values should act as pairing keys to determine which rows to merge across the DataFrames.\n\npd.merge(left = elections, right = babynames, left_on = \"First Name\", right_on = \"Name\")\n\n\n\n\n\n  \n    \n      \n      Year_x\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n      State\n      Sex\n      Year_y\n      Name\n      Count\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1963\n      Andrew\n      5\n    \n    \n      1\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1968\n      Andrew\n      7\n    \n    \n      2\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1970\n      Andrew\n      5\n    \n    \n      3\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1971\n      Andrew\n      13\n    \n    \n      4\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1974\n      Andrew\n      7\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      21780\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      1996\n      Jo\n      8\n    \n    \n      21781\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      1999\n      Jo\n      6\n    \n    \n      21782\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      2016\n      Jo\n      5\n    \n    \n      21783\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      M\n      1934\n      Jo\n      5\n    \n    \n      21784\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      M\n      1985\n      Jo\n      5\n    \n  \n\n21785 rows × 12 columns"
  },
  {
    "objectID": "quarto-documentation/Quarto_Guide.html",
    "href": "quarto-documentation/Quarto_Guide.html",
    "title": "Name of the Lecture",
    "section": "",
    "text": "Begin by installing Quarto.\nTexts can be authored in Quarto using JupyterLab or classic Jupyter Notebook. To start a new document, open Jupyter and create a new notebook file.\nTo set up the document, create a new Raw NBConvert cell. This will be used to set document-level YAML options. The Data 100 lecture notes are generated using the following YAML settings:\nNow, the notebook is ready for writing content. Quarto supports all the functionality of a standard ipynb file – code cells, markdown, and LaTeX. To begin writing lecture notes, it’s a good idea to first set out the main headings of the document. These typically correspond to the title slides of each lecture (example) and are written with the Markdown second headng level (##). Quarto will auto-populate the table of contents as these headings are created.\nTo view the Quarto file, open a terminal window (either within Jupyter or through your machine’s terminal) and navigate to the notebook’s directory. Running the command quarto preview notebook.ipynb will render the document and open it in a new web browser tab.\nWith the preview activated, the rendered view will update every time a change is saved in the notebook. When editing the document, it’s helpful to have side-by-side views of the notebook and preview so you can watch changes in real-time.\n\n\n\nA pdf view of how this notebook renders in Quarto can be found here.\n\n\nThe code-fold: true option in the YAML set-up will automatically collapse all code cells in the rendered document. If a particular code cell should be uncollapsed by default (e.g. to explicitly show a pandas example), a cell-specific YAML option can be specified:\n\nprint(\"this code is now visible\")\n\nthis code is now visible\n\n\n\n\n\nInserting images in a Quarto document is similar to the standard Markdown syntax. The difference is that Quarto will insert figure captions automatically. The syntax below will insert an image with an accompanying description.\nTODO (as of 12/23/22): Convert HTML Markdown images to Quarto-supported images\n\n\nCode\n#![The best class at Berkeley](data.png)\n\n\n\n\n\nThe best class at Berkeley\n\n\n\n\n\nEach lecture note should start with a brief list of intended student learning outcomes. These are formatted as collapsable call-out cells, which can be created in a Markdown cell using the syntax below.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\n\n\nGain familiarity with Quarto\nCreate your first Quarto document\nWrite A+ Data 100 lecture notes\n\n\n\n\n\n\n\n\nTo generate the final notebook as an HTML, run the terminal command quarto render notebook.ipynb. The HTML will be outputted in the same directory as the notebook.\n\n\n\nQuick Start Guide\nComprehensive Guide\nMarkdown in Quarto\n\n\n\nFollow the local installation set-up above. Then, clone the course-notes repository: https://github.com/DS-100/course-notes.git.\nTo begin working on a new lecture note, create a folder with the lecture title. Within this folder, you can create new sub-folders titlted images and data, where you’ll store these\nquarto convert lecture_title.ipynb converts the notebook to a .qmd file. The live set of lecture notes are generated from the .qmd files that lie in _quarto.yaml. If you’d like to change the current lecture notes, make sure you re-generate the .qmd files rather than just editing the notebook"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html",
    "href": "constant_model_loss_transformations/loss_transformations.html",
    "title": "Constant Model, Loss, and Transformations",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nDo stuff\nDo other stuff\nLast time, we introduced the modeling process. We set up a framework to predict target variables as functions of our features, following a set workflow:\nTo illustrate this process, we derived the optimal model parameters under simple linear regression with mean squared error as the empirical risk function. In this lecture, we’ll continue familiarizing ourselves with the modeling process by finding the best model parameters under a new model. We’ll also test out two different loss functions to understand how our choice of loss influences model design. Later on, we’ll consider what happens when a linear model isn’t the best choice to capture trends in our data – and what solutions there are to create better models."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Constant Model + MSE",
    "text": "Constant Model + MSE\nIn today’s lecture, our focus will be on the constant model. The constant model is slightly different from the simple linear regression model we’ve explored previously. Rather than generate predictions from an inputted feature variable, the constant model predicts the same constant number every time. We call this constant \\(\\theta\\).\n\\[\\hat{y} = \\theta\\]\n\\(\\theta\\) is the parameter of the constant model, just as \\(a\\) and \\(b\\) were the parameters in SLR. Our task now is to determine what value of \\(\\theta\\) represents the optimal model – in other words, what number should we guess each time to have the lowest possible average loss on our data?\nConsider the case where L2 (squared) loss is used as the loss function and mean squared error is used as the empirical risk. At this stage, we’re well into the modeling process:\n\nChoose a model: constant model\nChoose a loss function: L2 loss\nFit the model\nEvaluate model performance\n\nIn Homework 5, you will fit the constant model under MSE risk to find that the best choice of \\(\\theta\\) is the mean of the observed \\(y\\) values. In other words, \\(\\hat{\\theta} = \\bar{y}\\).\nLet’s take a moment to interpret this result. Our optimal model parameter is the value of the parameter that minimizes the empirical risk. This minimum value of risk can be expressed:\n\\[R(\\hat{\\theta}) = \\min_{\\theta} R(\\theta)\\]\nTo restate the above in plain English: we are looking at the value of the risk function when it takes the best parameter as input. This optimal model parameter, \\(\\hat{\\theta}\\), is the value of \\(\\theta\\) that minimizes the risk \\(R\\).\nFor modeling purposes, we care less about the minimum value of risk, \\(R(\\hat{\\theta})\\), and more about the value of \\(\\theta\\) that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n\\[\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)\\]\nThat is, we want to find the argument \\(\\theta\\) that minimizes the risk function."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Constant Model + MAE",
    "text": "Constant Model + MAE\nWe see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?\nThis time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the mean absolute error.\n\nChoose a model: constant model\nChoose a loss function: L1 loss\nFit the model\nEvaluate model performance\n\nTo fit the model and find the optimal parameter value \\(\\hat{\\theta}\\), follow the usual process of differentiating the risk function with respect to \\(\\theta\\), setting the derivative equal to zero, and solving for \\(\\theta\\). Writing this out in longhand:\n\\[\\begin{align}\nR(\\theta) &= \\frac{1}{n}\\sum^{n}_{i=1} |y_i - \\theta| \\\\\n\\frac{d}{d\\theta} R(\\theta) &= \\frac{d}{d\\theta} \\left(\\frac{1}{n} \\sum^{n}_{i=1} |y_i - \\theta| \\right) \\\\\n\\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta|\n\\end{align}\\]\nHere, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when \\(y_i = \\theta\\)). For now, we’ll ignore this issue. It turns out that ignoring this case doesn’t influence our final result.\nTo perform the derivative, consider two cases. When \\(\\theta\\) is less than \\(y_i\\), the term \\(y_i - \\theta\\) will be positive and the absolute value has no impact. When \\(\\theta\\) is greater than \\(y_i\\), the term \\(y_i - \\theta\\) will be negative. After applying the absolute value, we are left with \\(-(y_i - \\theta) = \\theta - y_i\\).\n\\[|y_i - \\theta| = \\begin{cases} y_i - \\theta && \\text{if}\\:\\theta \\leq y_i \\\\ \\theta - y_i && \\text{if}\\:\\theta \\geq y_i \\end{cases}\\]\nTaking derivatives:\n\\[\\frac{d}{d\\theta} |y_i - \\theta| = \\begin{cases} -1 && \\text{if}\\:\\theta \\leq y_i \\\\ 1 && \\text{if}\\:\\theta \\geq y_i \\end{cases}\\]\nThis means that we obtain a different value for the derivative for datapoints where \\(\\theta < y_i\\) and where \\(\\theta > y_i\\). We can summarize this by saying:\n\\[\\begin{align} \\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta| \\\\\n&= \\frac{1}{n} \\left[\\sum_{\\theta < y_i} (-1) + \\sum_{\\theta > y_i} (+1) \\right]\n\\end{align}\\]\nTo finish finding the best value of \\(\\theta\\), set this derivative equal to zero and solve for \\(\\theta\\). You’ll do this in Homework 5 to show that \\(\\hat{\\theta} = \\text{median}(y)\\)."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "href": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Comparing Loss Functions",
    "text": "Comparing Loss Functions\nNow, we’ve tried our hand at fitting a model under both MSE and MAE risk. How do the two results compare?\nLet’s consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We’ll fit a constant model to predict the number of drinks that will be sold tomorrow.\n\nimport numpy as np\ndrinks = np.array([20, 21, 22, 29, 33])\n\nFrom our derivations above, we know that the optimal model parameter under MSE risk is the mean of the dataset. Under MAE risk, the optimal parameter is the median of the dataset.\nIf we plot each empirical risk function across several possible values of \\(\\theta\\), we find that each \\(\\hat{\\theta}\\) does indeed correspond to the lowest value of error:"
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "href": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Linear Transformations",
    "text": "Linear Transformations"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html",
    "href": "intro_to_modeling/intro_to_modeling.html",
    "title": "Introduction to Modeling",
    "section": "",
    "text": "Up until this point in the semester, we’ve focused on analyzing datasets. We’ve looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.\nThis lecture marks a shift in focus. We will move away from examining datasets to actually using our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some prediction about the world around us. In this lecture, we’ll introduce the conceptual framework for setting up a modeling task. In the next few lectures, we’ll put this framework into practice by implementing several kinds of models."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "href": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "title": "Introduction to Modeling",
    "section": "The Modeling Process",
    "text": "The Modeling Process\nAt a high level, a model is some way of representing a system. In Data 100, we’ll treat a model as some mathematical rule we use to describe the relationship between variables.\nWhat variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset \\(\\mathbb{D}\\):\n\\[\\mathbb{D} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\]\nEach pair of values \\((x_i, y_i)\\) represents a datapoint. In a modeling setting, we call these observations. \\(y_i\\) is the dependent variable we are trying to model, also called an output or response. \\(x_i\\) is the independent variable inputted into the model to make predictions, also known as a feature.\nOur goal in modeling is to use the observed data \\(\\mathbb{D}\\) to predict the output variable \\(y_i\\). We denote each prediction as \\(\\hat{y}_i\\) (read: “y hat sub i”).\nHow do we generate these predictions? Some examples of models we’ll encounter in this class are given below:\n\n\\(\\hat{y} = \\theta\\)\n\\(\\hat{y} = a + bx\\)\n\\(\\hat{y} = \\frac{1}{1+\\exp(-x^{T}\\theta)}\\)\n\nThe examples above are known as parametric models. They relate the collected data, \\(x\\), to the prediction we make, \\(\\hat{y}\\). A few parameters (\\(\\theta\\), \\(a\\), \\(b\\)) are used to describe the relationship between \\(x\\) and \\(\\hat{y}\\).\nNotice that we don’t immediately know the values of these parameters. While the features, \\(x\\), are taken from our observed data, we need to decide what values to give \\(\\theta\\), \\(a\\), and \\(b\\) ourselves. This is the heart of parametric modeling: what parameter values should we choose so our model makes the best possible predictions?\nTo choose our model parameters, we’ll work through the modeling process.\n\nChoose a model: how should we represent the world?\nChoose a loss function: how do we quantify prediction error?\nFit the model: how do we choose the best parameters of our model given our data?\nEvaluate model performance: how do we evaluate whether this process gave rise to a good model?"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "title": "Introduction to Modeling",
    "section": "Choosing a Model",
    "text": "Choosing a Model\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, \\(x\\), and predictions \\(\\hat{y}\\).\nIn Data 8, you learned about the simple linear regression (SLR) model. This model takes the form: \\[\\hat{y} = a + bx\\]\nThe parameters of the SLR model are \\(a\\), the intercept term, and \\(b\\), the slope term. To create an effective model, we want to choose values for \\(a\\) and \\(b\\) that most accurately predict the output variable. The “best” fitting model parameters are given the special names \\(\\hat{a}\\) and \\(\\hat{b}\\) – they are the specific parameter values that allow our model to generate the best possible predictions.\nIn Data 8, you learned that the best SLR model parameters are: \\[\\hat{a} = \\bar{y} - \\hat{b} \\cdot \\bar{x} \\qquad \\qquad \\hat{b} = r \\cdot \\frac{\\sigma_y}{\\sigma_x}\\]\nA quick refresher on notation:\n\n\\(\\bar{y}\\) and \\(\\bar{x}\\) indicate the mean value of \\(y\\) and \\(x\\), respectively\n\\(\\sigma_y\\) and \\(\\sigma_x\\) indicate the standard deviations of \\(y\\) and \\(x\\)\n\\(r\\) is the correlation, defined as the average of the product of \\(x\\) and \\(y\\) measured in standard units: \\(\\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y})\\)\n\nIn Data 100, we want to understand how to derive these best model coefficients. To do so, we’ll introduce the concept of a loss function."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "title": "Introduction to Modeling",
    "section": "Choosing a Loss Function",
    "text": "Choosing a Loss Function\nWe’ve talked about the idea of creating the “best” possible predictions. This begs the question: how do we decide how “good” or “bad” our model’s predictions are?\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, \\(L(y, \\hat{y})\\), quantifies how “far off” a single prediction by our model is from a true, observed value in our collected data.\nThe choice of loss function for a particular model depends on the modeling task at hand. Regardless of the specific function used, a loss function should follow two basic principles:\n\nIf the prediction \\(\\hat{y}\\) is close to the actual value \\(y\\), loss should be low\nIf the prediction \\(\\hat{y}\\) is far from the actual value \\(y\\), loss should be high\n\nTwo common choices of loss function are squared loss and absolute loss.\nSquared loss, also known as L2 loss, computes loss as the square of the difference between the observed \\(y\\) and predicted \\(\\hat{y}\\): \\[L(y, \\hat{y}) = (y - \\hat{y})^2\\]\nAbsolute loss, also known as L1 loss, computes loss as the absolute difference between the observed \\(y\\) and predicted \\(\\hat{y}\\): \\[L(y, \\hat{y}) = |y - \\hat{y}|\\]\nL1 and L2 loss give us a tool for quantifying our model’s performance on a single datapoint. This is a good start, but ideally we want to understand how our model performs across our entire dataset. A natural way to do this is to compute the average loss across all datapoints in the dataset. This is known as the empirical risk, \\(\\hat{R}(\\theta)\\): \\[\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)\\]\nWe can substitute our L1 and L2 loss into the empirical risk definition. The mean squared error (MSE) is the average squared loss across a dataset: \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nThe mean absolute error (MAE) is the average absolute loss across a dataset: \\[\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\\]"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "href": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "title": "Introduction to Modeling",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nNow that we’ve established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model’s empirical risk on our dataset. This process is called fitting the model.\nWe know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the objective function (our objective is to find its minimum).\nTo find the optimal model parameter, we:\n\nTake the derivative of the empirical risk with respect to that parameter\nSet the derivative equal to 0\nSolve for the parameter\n\nWe repeat this process for each parameter present in the model. For now, we’ll disregard the second derivative condition.\nTo help us make sense of this process, let’s put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our empirical risk.\nStep 1: take the derivative of empirical risk with respect to each model parameter. We substitute the SLR model, \\(\\hat{y} = a+bx\\), into the definition of MSE above and differentiate with respect to \\(a\\) and \\(b\\). \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - a - bx_i)^2\\]\n\\[\\frac{\\partial}{\\partial a} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - a - bx_i\\]\n\\[\\frac{\\partial}{\\partial b} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - a - bx_i)x_i\\]\nStep 2: set the derivatives equal to 0. After simplifying terms, this produces two estimating equations. The best set of model parameters \\((a, b)\\) must satisfy these two optimality conditions. \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - a - bx_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{y}_i = 0\\] \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - a - bx_i)x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_i = 0\\]\nStep 3: solve the estimating equations to compute estimates for \\(\\hat{a}\\) and \\(\\hat{b}\\).\nTaking the first equation gives the estimate of \\(\\hat{a}\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n y_i - \\hat{a} - \\hat{b}x_i &= 0 \\\\\n\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{a} - \\hat{b}\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) &= 0 \\\\\n\\hat{a} &= \\bar{y} - \\hat{b} \\bar{x}\n\\end{align}\\]\nWith a bit more maneuvering, the second equation gives the estimate of \\(\\hat{b}\\). Start by multiplying the first estimating equation by \\(\\bar{x}\\), then subtracting the result from the second estimating equation. \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\bar{x} &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)(x_i - \\bar{x}) &= 0\n\\end{align}\\]\nNext, plug in \\(\\hat{y} = \\hat{a} + \\hat{b}x = (\\bar{y} - \\hat{b} \\bar{x}) + \\hat{b}x = \\bar{y} + \\hat{b}(x - \\bar{x})\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{b}(x - \\bar{x}))(x_i - \\bar{x}) &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) = \\hat{b} \\times \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{align}\\]\nBy using the definition of correlation \\(\\left(r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)\\) and standard deviation \\(\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)\\), we can conclude: \\[r \\sigma_x \\sigma_y = \\hat{b} \\times \\sigma_x^2\\] \\[\\hat{b} = r \\frac{\\sigma_y}{\\sigma_x}\\]\nJust as was given in Data 8!\nRemember, this derivation found the optimal model parameters for SLR when using MSE risk. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can always follow these three steps to fit the model."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "href": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "title": "Introduction to Modeling",
    "section": "Evaluating Performance",
    "text": "Evaluating Performance\nAt this point, we’ve:\n\nDefined our model\nDefined our loss function\nFit the model to identify the best model parameters\n\nThis leaves us with one final question – how “good” are the predictions made by this “best” fitted model?\n\n[TODO: Rework using Lecture 10 content]\nThere are a few ways to evaluate a model’s performance.\n\nCompute statistics: find the mean and standard deviation of all predicted values. If these are similar to the mean and standard deviation of the original observed \\(y\\)s, the model is likely performing well.\nCompute performance metrics: the root mean squared error (RMSE) is the square root of MSE. Taking the square root converts the value back into the original units of \\(y\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset. \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nVisualization: generate a plot of the model’s residuals, defined as the difference between the observed and predicted \\(y\\) value (\\(e_i = y_i - \\hat{y}_i\\)). This gives a high-level view of how “off” each prediction is from the true observed value."
  },
  {
    "objectID": "visualization_2/visualization_2.html",
    "href": "visualization_2/visualization_2.html",
    "title": "Visualization II",
    "section": "",
    "text": "In the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we’ll learn how to construct density curves using Kernel Density Estimation.\n\n\nKernel Density Estimation involves a technique called smoothing - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\nMany of the visualizations we learned during the last lecture are examples of smoothing. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the larger properties of our distribution.\n\n\n\nKernel Density Estimation is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\nPlace a kernel at each data point\nNormalize kernels to have total area of 1\nSum kernels together\n\nSuppose we have 5 data points: \\([2.2, 2.8, 3.7, 5.3, 5.7]\\). We wish to recreate the following Kernel Density Estimate:\n\n\nCode\nimport seaborn as sns\n\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n\n\n\n\n\nLet’s walk through each step to construct this density curve.\n\n\nTo begin generating a density curve, we need to choose a kernel and bandwidth value. What are these exactly? A kernel is a density curve itself, and the bandwidth is a measure of the kernel’s width. Recall that a valid density has an area of 1.\nAt each of our 5 points (depicted in the rug plot on the left), we’ve placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We’ll explore what these are in the next section.\n\n\nRugplot of Data\n\n\n\n\nKernelized Data\n\n\n\n\n\n\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by \\(\\frac{1}{5}\\).\n\n\nKernelized Data\n\n\n\n\nNormalized Kernels\n\n\n\n\n\n\nOur kernel density estimate (KDE) is the vertical sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n\nNormalized Kernels\n\n\n\n\nKernel Density Estimate\n\n\n\n\n\n\n\n\n\n\nA kernel (for our purposes) is a valid density function. This means it:\n\nMust be non-negative for all inputs.\nMust integrate to 1.\n\n\n\nThe most common kernel is the Gaussian kernel. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value \\(x_i\\) with a standard deviation of \\(\\alpha\\) (this is known as the bandwidth parameter).\n\\(K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2a^{2}}}\\)$\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Gaussian kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nIf you’ve taken a probability class, you’ll recognize that the mean of this Gaussian kernel is \\(x_i\\) and the standard deviation is \\(\\alpha\\). Increasing \\(\\alpha\\) - equivalently, the bandwidth - smoothens the density curve. Larger values of \\(\\alpha\\) are typically easier to understand; however, we begin to lose important distributional information.\nHere is how adjusting \\(\\alpha\\) affects a distribution in some variable from an arbitrary dataset.\n\n\nGaussian Kernel, Alpha = 0.1\n\n\n\n\nGaussian Kernel, Alpha = 1\n\n\n\n\n\nGaussian Kernel, Alpha = 2\n\n\n\n\nGaussian Kernel, Alpha = 10\n\n\n\n\n\n\nAnother example of a kernel is the Boxcar kernel. The boxcar kernel assigns a uniform density to points within a “window” of the observation, and a density of 0 elsewhere.\n\\(K_a(x, x_i) = \\begin{cases}  \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\  0, & \\text{else }  \\end{cases}\\)\n\n\nCode\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Boxcar kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth \\(\\alpha\\) = 1."
  },
  {
    "objectID": "visualization_2/visualization_2.html#visualization-theory",
    "href": "visualization_2/visualization_2.html#visualization-theory",
    "title": "Visualization II",
    "section": "Visualization Theory",
    "text": "Visualization Theory\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We’ll discuss the abstract nature of visualizations and analyze how they convey information.\nRemember, we had two goals for visualizing data. This section is particularly important in:\n\nHelping us understand the data and results\nCommunicating our results and conclusions with others\n\n\nInformation Channels\nVisualizations are able to convey information through various encodings. In the remainder of this lecture, we’ll look at the use of color, scale, and depth, to name a few.\nTODO Delete: Additionally, some visuals convey more detail than others. We learned how rugplots, KDEs, and histograms are all different representations of a distribution. One such example is in the rugplot.\n\nEncodings in Rugplots\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n\n\n\nMulti-Dimensional Encodings\nEncodings are also useful for representing multi-dimensional data. Notice how the following visual highlights four distinct “dimensions” of data:\n\nX-axis\nY-axis\nArea\nColor\n\n\nThe human visual perception sytem is only capable of visualizing data in a three-dimensional plane, but as you’ve seen, we can encode many more channels of information.\n\n\n\nHarnessing the Axes\n\nConsider Scale of the Data\nHowever, we should be careful to not misrepresent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n\nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not be intersecting.\nWhen there is a large difference in the magnitude of the data, it’s advised to analyze percentages instead of counts. The following diagrams correctly display the trends in cancer screening and abortion rates.\n\n\n\n\n\n\n\n\n\n\n\nReveal the Data\nGreat visualizations not only consider the scale of the data, but also utilize the axes in a way that best conveys information. For example, data scientists commonly set certain axes limits to highlight parts of the visualization they are most interested in.\n\n\n\n\n\n\n\n\n\nThe visualization on the right captures the trend in coronavirus cases during the month March in 2020. From only looking at the visualization on the left, a viewer may incorrectly believe that coronavirus began to skyrocket on March 4th, 2020. However, the second illustration tells a different story - cases rose closer to March 21th, 2020.\n\n\n\nHarnessing Color\nColor is another important feature in visualizations that does more than what meets the eye.\nLast lecture, we used color to encode a categorical variable in our scatter plot. In this section, we will discuss uses of color in novel visualizations like colormaps and heatmaps.\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors which work together is evidently a challenging task!\n\nColormaps\nColormaps are mappings from pixel data to color values, and they’re often used to highlight distinct parts of an image. Let’s investigate a few properties of colormaps.\n\n\nJet Colormap \n\n\n\nViridis Colormap \n\n\nThe jet colormap is infamous for being misleading. While it seems more vibrant than viridis, the aggressive colors poorly encode numerical data. To understand why, let’s analyze the following images.\n\n\n\n\n\n\n\n\n\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low intensity. These include the jet colormap (row a) and grayscale (row b). Notice how the grayscale images do the best job in smoothly transitioning between pixel data. The jet colormap is the worst at this - the four images in row (a) look like a conglomeration of individual colors.\nThe difference is also evident in the images labeled (a) and (b) on the left side. The grayscale image is better at preserving finer detail in the vertical line strokes. Additionally, grayscale is preferred in x-ray scans for being more neutral. The intensity of dark red color in the jet colormap is frightening and indicates something is wrong.\nWhy is the jet colormap so much worse? The answer lies in how its color composition is percieved to the human eye.\n\n\nJet Colormap Perception \n\n\n\nViridis Colormap Perception \n\n\nThe jet colormap is largely misleading because it is not perceptually uniform. Perceptually uniform colormaps have the property that if the pixel data goes from 0.1 to 0.2, the perceptual change is the same as when the data goes from 0.8 to 0.9.\nNotice how the said uniformity is present within the linear trend displayed in the viridis colormap. On the other hand, the jet colormap is largely non-linear - this is precisely why it’s considered a worse colormap.\n\n\n\nHarnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these “markings”.\n\nThere are a few key takeaways from this diagram\n\nLengths are easy to discern. Don’t use plots with jiggled baselines - keep everything axis-aligned.\nAvoid pie charts! Angle judgements are inaccurate.\nAreas and volumes are hard to distinguish (area charts, word clouds, etc)\n\n\n\nHarnessing Conditioning\nConditioning is the process of comparing data that belong to seperate groups. We’ve seen this before in overlayed distributions, side-by-side box-plots, and scatter plots with categorical encodings. Here, we’ll introduce terminology that formalizes these examples.\nConsider an example where we want to analyze income earnings for male and females with varying levels of education. There are multiple ways to compare this data.\n\n\n\n\n\n\n\n\n\nThe barplot is an example of juxtaposition: placing multiple plots side by side, with the same scale. The scatter plot is an example of superposition: placing multiple density curves, scatter plots on top of each other.\nWhich is better depends on the problem at hand. Here, superposition makes the precise wage difference very clear from a quick glance. But many sophisticated plots convey information that favors the use of juxtaposition. Below is one example.\n\n\n\nHarnessing Context\nThe last component to a great visualization is perhaps the most critical - the use of context. Adding informative titles, axis labels, and descriptive captions are all best practices that we’ve heard repeatedly in Data 8.\nA publication-ready plot (and every Data 100 plot) needs:\n\nInformative title (takeaway, not description)\nAxis labels\nReference lines, markers, etc\nLegends, if appropriate\nCaptions that describe data\n\nCaptions should be:\n\nComprehensive and self-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graphs"
  },
  {
    "objectID": "visualization_2/visualization_2.html#transformations",
    "href": "visualization_2/visualization_2.html#transformations",
    "title": "Visualization II",
    "section": "Transformations",
    "text": "Transformations\nThese last two lectures have covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\nMuch of this was done to uncover insights in data, which will prove necessary for the modeling process. A strong graphical correlation between two variables hinted an underlying relationship that has reason for further study. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\nTransformations are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that “transform” their range of possible values and highlight some previously hidden associations between data.\n\nTransforming a Distribution\nWhen a distribution has a large dynamic range, it can be useful to take the logarithm of the data. For example, computing the logarithm of the ticket prices on the Titanic reduces skeweness and yields a distribution that is more “spread” across the x-axis. While it makes individual observations harder to interpret, the distribution is more favorable for subsequent analysis.\n\n\n\n\n\n\n\n\n\n\n\nLinearizing a Relationship\nTransformations are perhaps most useful to linearize a relationship between variables. If we find a transformation to make a scatter plot of two variables linear, we can “backtrack” to find the exact relationship between the variables. Linear relationships are particularly simple to interpret, and we’ll be doing a lot of linear modeling in Data 100 - starting next lecture!\nSay we want to understand the relationship between healthcare and life expectancy. Intuitively there should be a positive correlation, but upon plotting values from a dataset, we find a non-linear relationship that is somewhat hard to understand. However, applying a logarithmic transformation to both variables - healthcare and life expectancy - results in a scatter plot with a linear trend that we can interpret.\n\n\n\n\n\n\n\n\n\nHow can we find the relationship between the original variables? We know that taking a log of both axes gave us a linear relationship, so we can say (roughly) that\n\\[\\log y= a*\\log x + b\\]\nSolving for \\(y\\) implies a power relationship in the original plot.\n\\[y= e^{a*\\log x + b}\\] \\[y= Ce^{a*\\log x}\\] \\[y= Cx^{a}\\]\nHow did we know that taking the logarithm of both sides would result in a linear relationship? The Tukey-Mosteller Bulge Diagram is helpful here. We can use the direction of the buldge in our original data to find the appropriate transformations that will linearize the relationship. These transformations are found on axes that are nearest to the buldge. The buldge in our earlier example lay in Quadrant 2, so the transformations \\(\\log x\\), \\(\\sqrt x\\), \\(y^{2}\\), or \\(y^{3}\\) are possible contenders. It’s important to note that this diagram is not perfect, and some transformations will work better than others. In our case, \\(\\log x\\) and \\(\\log y\\) (found in Quadrant 3) were the best."
  }
]