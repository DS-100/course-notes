[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#regression-vs.-classification",
    "href": "logistic_regression_1/logistic_reg_1.html#regression-vs.-classification",
    "title": "23  Logistic Regression I",
    "section": "23.1 Regression vs. Classification",
    "text": "23.1 Regression vs. Classification\nYou may have seen this diagram which outlines the taxonomy of machine learning.\n\n Regression and classification are both supervised learning problems, meaning they consist of models that learn from data where the \\(y\\) observations are known. You’ve likely seen examples of classification models before - for example, the k-nearest neighbors algorithm from Data 8. This lecture will focus on another such model known as Logistic Regression.\nMore generally, a classification problem aims to categorize data. Today, we will focus on binary classification, a classification problem where data may only belong to one of two groups. One such example is the result of a coin toss. Many of the principles we’ll learn today can extended to three or more groups - called multiclass classification. In the next few lectures, we will learn about models designed specifically for these problems."
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#intuition-the-coin-flip",
    "href": "logistic_regression_1/logistic_reg_1.html#intuition-the-coin-flip",
    "title": "23  Logistic Regression I",
    "section": "23.2 Intuition: The Coin Flip",
    "text": "23.2 Intuition: The Coin Flip\nTo build some intuition for logistic regression, let’s look at an introductory example to classification: the coin flip. Suppose we observe some outcomes of a coin flip (1 = Heads, 0 = Tails).\n\nflips = [0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\nflips\n\n[0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n\n\nFor the next flip, would you predict heads or tails?\nA reasonable model is to assume all flips are IID (independent and identitically distributed). In other words, each flip has the same probability of returning a 1 (or heads). Let’s define a parameter \\(\\theta\\), the probability that the next flip is a heads. We will use this parameter to inform our decision for \\(\\hat y\\), or our 0 – 1 prediction of the next flip. If \\(\\theta \\ge 0.5, \\hat y = 1, \\text{else } \\hat y = 0\\).\nYou may be inclined to say \\(0.5\\) is the best choice for \\(\\theta\\). However, notice that we made no assumption about the coin itself. The coin may be biased, so we should make our decision based only on the data. We know that exactly \\(\\frac{4}{10}\\) of the flips were heads, so we should think \\(\\hat \\theta = 0.4\\). In the next section, we will mathematically prove why this is the best possible estimate.\n\n23.2.1 Likelihood of Data\nLet’s call the result of the coin flip a random variable \\(Y\\). This is a Bernoulli random variable with two outcomes. \\(Y\\) has the following distribution:\n\\[P(Y = y) = \\begin{cases}\n        1, \\text{with probability }  \\theta\\\\\n        0, \\text{with probability }  1 - \\theta\n    \\end{cases} \\]\n\\(\\theta\\) is unknown to us. But we can find the \\(\\theta\\) that makes the data we observed the most likely, by looking towards the likelihood of the data. The likelihood is proportional to the probability of observing the data.\nThe probability of observing 4 Heads and 6 Tails follows the Binomial Distribution.\n\\[\\binom{10}{4} (\\theta)^4 (1-\\theta)^6\\]\nThe likelihood is proportional to the probability above. To find it, simply multiply the probabilities of obtaining each coin flip.\n\\[(\\theta)^{4} (1-\\theta)^6\\]\nThe technique known as maximum likelihood estimation finds the \\(\\theta\\) that maximizes the above likelihood. You can find this maximum by taking the derivative of the likelihood, but we’ll provide a more intuitive graphical solution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntheta = np.linspace(0, 1, 100)\nplt.plot(theta, theta**4 * (1-theta)**6);\n\n\n\n\nYou can see this function is maximized at the value \\(0.4\\). Thus, \\(\\hat\\theta = 0.4\\). We will revisit these methods in our derivation of the logistic regression model."
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#deriving-the-logistic-regression-model",
    "href": "logistic_regression_1/logistic_reg_1.html#deriving-the-logistic-regression-model",
    "title": "23  Logistic Regression I",
    "section": "23.3 Deriving the Logistic Regression Model",
    "text": "23.3 Deriving the Logistic Regression Model\n\n23.3.1 Probabalistic Model\nWhat if instead of a 0-1 prediction response, the model returns the probability of data belonging to binary classes? - If the model’s predicted probability is greater than 0.5, classify as class 1. - If the model’s predicted probability is less than 0.5, classify as class 0.\nThe model still uses the concept of classification activation as a way to aggregate multiple features using a weighted combination of features.\nThe probabilistic classifier learns to generate the following probabilities and with the decision threshold of 0.5, classifies datapoints into two categories:\n\n\n\n23.3.2 Odds Ratio\nOdds is defined as the ration of the probability of an event happening (\\(p\\)) vs. it not happening (\\(1-p\\)). - “9 to 1 against” means \\(p = 0.1\\) - “Even odds” means \\(p = 0.5\\) - “3 to 1 on” means \\(p = 0.75\\)\n\\[\\text{odds}(p) = \\frac{p}{1 - p}\\]\nDomain of odds function: \\([0, 1)\\)\nRange of odds function: \\([0, +\\infty)\\)\nThus, the odds function maps values on \\([0, 1)\\) to \\([1, \\infty)\\).\n\n\n\n23.3.3 Logit Function\nOur end goal is to use a classification activation to output a probability value based on some unbounded real number. Currently, the odds ratio takes a probability value and outputs a positive real number. If we further transform the odds function to obtain a function that maps a probability to an unbouded real number, the inverse of that function will be our desired result.\nThus, we first need a transformation takes the ouput of the odds function and maps it to an unbounded real number (i.e. maps \\([0, \\infty)\\) to \\((-\\infty, \\infty)\\)).\nTaking the log of a positive real number creates an unbounded real number!\nApplying this transformation to the odds ratio gives us the Logit Function:\n\\[\\text{log-odds}(p) = \\text{log}(\\frac{p}{1-p})\\]\n\n\n23.3.3.1 Logistic Function: Inverse of the Logit Function\n\\[\\text{log}(\\frac{p}{1-p}) = z\\] \\[\\frac{p}{1-p} = e^{z}\\] \\[p = e^{z} - pe^{z}\\] \\[p + pe^{z} = e^{z}\\] \\[(1 + e^{z})p = e^{z}\\] \\[p = \\frac{e^{z}}{1 + e^{z}}\\] \\[p = \\frac{1}{1 + e^{z}}\\]\n\n\n23.3.3.2 Logistic Regression Model Summary\n\nOptimize the model to find parameters \\(\\vec{\\hat{\\theta}}\\)\nUse parameters to calculate classification activation: \\(z = \\vec{\\hat{\\theta}}^{T}\\vec{x}\\)\nApply the logistic function to get the probabability of a given datapoint belonging to class 1.\n\n\n\n\n23.3.4 The Logistic Function\nWhile it may look daunting, the logistic regression model is just a composition of two simpler functions. One of these we know well from OLS – the linear model \\(x^T\\theta\\). The second is the logistic function – commonly referred to as the sigmoid function.\n\\[\\sigma(t) = \\frac{1}{1 + e^{-t}}\\]\n\nThis logistic function has various properties, although we won’t prove any of them.\n\nDefinition: \\(\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{e^t}{1 + e^{t}}\\)\nReflection/Symmetry: \\(1 - \\sigma(t) = \\frac{e^{-t}}{1 + e^{-t}} = \\sigma(-t)\\)\nInverse: \\(t = \\sigma^{-1}(p) = \\log(\\frac{p}{1-p})\\)\nDomain: \\(-\\infty < t < \\infty\\)\nRange: \\(0 < \\sigma(t) < 1\\)\n\nYou’ll notice the logistic regression model is just the logistic function evaluated on the linear model \\(x^T\\theta\\). The logistic function squashes the result of \\(x^T\\theta\\) to a probability between \\(0\\) and \\(1\\). Larger values of \\(x^T\\theta\\) will be mapped to probabilities closer to \\(1\\), and vice versa. Putting it all together, we have:\n\\[P(Y = 1|x) = \\sigma(x^T\\theta) = \\frac{1}{1 + e^{-x^T\\theta}}\\]"
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#the-logistic-regression-model",
    "href": "logistic_regression_1/logistic_reg_1.html#the-logistic-regression-model",
    "title": "23  Logistic Regression I",
    "section": "23.4 The Logistic Regression Model",
    "text": "23.4 The Logistic Regression Model\nThe figure below summarizes the logistic regression process:\n\n\n23.4.1 Example Calcualtion\nSuppose we want to predict the probability that a tumor is malignant, given it’s mean radius and mean smoothness. Assume that we fit a logistic regression model (with no intercept), and somehow estimate the optimal parameters: \\(\\hat \\theta = [0.1, -0.5]\\).\nSay we encounter a new breast tumor with data: \\(x = [15, 1]\\). We want to answer two questions:\n\nWhat is the probability that this tumor is malignant?\nShould we predict the tumor is malignant or benign?\n\n\\[P(Y = 1|x) = \\sigma(x^T\\hat\\theta)\\] \\[= \\sigma(0.1 * 15 + (-0.5) * 1) = \\sigma(1)\\] \\[= \\frac{1}{1 + e^{-1}}\\] \\[\\approx 0.7311\\]\nBecause the tumor has a \\(73\\%\\) chance of being malignant, we should predict \\(\\hat y = 1\\).\n\n\n23.4.2 Parameter Interpretation\nIn our derivation of logistic regression, we showed that \\(\\frac{p}{1-p} = e^{x^T\\theta}\\). Equivalently, \\[\\frac{P(Y=1|x)}{P(Y=0|x)} = e^{x^T\\theta}\\]\nImagine our linear component has just a single feature, along with an intercept term.\n\\[\\frac{P(Y=1|x)}{P(Y=0|x)} = e^{\\theta_0 + \\theta_1 x}\\]\nWhat happens when you increase \\(x\\) by one unit?\n\nOdds is multiplied by \\(e^{\\theta_1}\\)\nIf \\(\\theta_1 > 0\\), then odds increase.\nIf \\(\\theta_1 < 0\\), then odds decrease.\n\nRememer, the odds ratio can be interpreted as the “number of successes for each failure.” When odds increase, the proportion of successes likewise increases.\n\n\n23.4.3 Comparison to Linear Regression\n\n23.4.3.1 Linear Regression\n\n\n\n23.4.3.2 Logistic Regression"
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#parameter-estimation",
    "href": "logistic_regression_1/logistic_reg_1.html#parameter-estimation",
    "title": "23  Logistic Regression I",
    "section": "23.5 Parameter Estimation",
    "text": "23.5 Parameter Estimation\nHaving derived the logistic regression model, the next step is to estimate the optimal parameters \\(\\hat\\theta\\).\n\n23.5.1 Pitfalls of Squared Error Loss\nIn linear regression, we found these optimal parameters by minimizing a cost function. The same applies to logistic regression. Let’s begin by evaluating the MSE loss function on the logistic regression model.\n\\[L(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\sigma(x_i^T\\theta))^2\\]\nHowever, there are 3 flaws with this approach.\n\n23.5.1.1 1. Non-convex\nThe MSE loss surface for logistic regression is non-convex. In the following example, you can see the function rises above the secant line, a clear violation of convexity.\n\nDepending on the initialization points, gradient descent may find multiple non-optimal solutions.\n\n\n23.5.1.2 2. Bounded\nA good loss function should penalize incorrect predictions; the MSE doesn’t do this. Consider the following scenario.\n\nThe predicted probability that some tumor is malignant is \\(0.99\\) (\\(\\hat y = 0.99\\)). However, this tumor is truly benign (\\(y = 0\\)). The loss incurred by this misprediction is only \\((0 - 0.99)^2 = 0.98\\).\n\nIn fact, loss is always bounded \\(< 1\\).\n\n\n23.5.1.3 3. Conceptually Questionable\nThe MSE loss function itself is conceptually questionable.\n\n\\(y\\) is a class label\n\\(\\hat y\\) is a probability\n\nWhy are we squaring the difference of a class label and probability?\n\n\n\n23.5.2 Cross-Entropy Loss\nThis section will introduce a more intuitive loss function - cross-entropy loss. The cross-entropy loss is defined as\n\\[L(\\theta) = -(y\\text{ log}(p) + (1-y)\\text{log}(1-p))\\]\n\\[\\text{where } p = \\sigma(x^T\\theta)\\]\nThe average risk over an entire dataset is\n\\[R(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n}(y_i\\text{ log}(p) + (1-y_i)\\text{log}(1-p))\\]\nCross-entropy loss addresses the 3 pitfalls of squared loss.\n\nConvex. No local minima for logistic regression\nA good measure of model error. Strongly penalizes bad predictions.\nConceptually sound.\n\n\n23.5.2.1 1. Convex\nTo prove that cross-entropy loss is always convex, we’ll need to take its derivative, which becomes difficult and out of scope. Instead, in Data 100, we will show a proof by picture.\nPlotted below is the cross-entropy loss function applied to the same toy dataset as before. As you’ll notice, this is a convex function - any line connecting two points lies entirely above the function.\n\n\n\n23.5.2.2 2. Strong Error Penalization\nTo understand why cross-entropy loss has strong penalization, we need to take a step back. Let’s decompose the loss function into two components.\n\\[L(\\theta) = -(y\\text{ log}(p) + (1-y)\\text{log}(1-p))\\]\n\n\\(-(1-y)\\text{ log}(1 - p)\\)\n\nFor \\(y = 0\\), only this term stays in cross-entropy loss\nSee the plot on the bottom left.\n\n\\(p \\rightarrow 0\\): zero loss\n\\(p \\rightarrow 1\\): infinite loss\n\n\n\\(- y\\text{ log}(p)\\)\n\nFor \\(y = 1\\), only this term stays in cross-entropy loss\nSee the plot on the bottom right.\n\n\\(p \\rightarrow 0\\): infinite loss\n\\(p \\rightarrow 1\\): zero loss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23.5.2.3 3. Conceptually Sound\nTo understand why cross-entropy loss makes a great intuitive loss function, we will look towards maximum likelihood estimation in the next section."
  },
  {
    "objectID": "logistic_regression_1/logistic_reg_1.html#deriving-the-logistic-regression-model-using-the-graph-of-averages",
    "href": "logistic_regression_1/logistic_reg_1.html#deriving-the-logistic-regression-model-using-the-graph-of-averages",
    "title": "23  Logistic Regression I",
    "section": "23.6 Deriving the Logistic Regression Model Using the Graph of Averages",
    "text": "23.6 Deriving the Logistic Regression Model Using the Graph of Averages\nThis section demonstrates an alternative approach to deriving the logistic regression model using the graph of averages. Our goal will be to use the mean radius of a tumor to predict whether a cancer is malignant (\\(y = 1\\)) or not (\\(y = 0\\)). Data from the study is plotted below, along with the overlayed least squares model that minimizes MSE.\n\nRather than using a linear model, what if used a non-linear model that was a better fit of our data, and bounded predictions between [\\(0\\), \\(1\\)]? We can look to the Data 8 Graph of Averages to formulate this model.\n\n\n23.6.1 The Graph of Averages\nThe Graph of Averages says\n\nFor each input \\(x\\), compute the average value of \\(y\\) for all nearby \\(x\\), and predict that.\n\n“nearby” is a loose definition; we’ll say for example the nearest 5 points\n\n\nHere, \\(x\\) is mean radius and \\(y\\) is malignant. By the definition of an average, the prediction for a given \\(x\\) is thus:\n\\[\\frac{\\text{Sum of Nearby ``Malignant\" Values}}{\\text{Number of Points in Bin}}\\]\nYou’ll notice that this is the proportion of malignant tumors in some point \\(x\\)’s bin. In a similar light, we can think of this proportion as the probability that point \\(x\\) belongs to the malignant class (derived from it’s nearest neighbors). That is, \\(P(Y = 1|x)\\). This notation reads “given a tumor with a mean radius of \\(x\\), it belongs to class 1 (equivalently, the malignant class) with some specified probability”.\nYou may recognize some similarities between \\(P(Y = 1|x)\\) and \\(\\hat \\theta\\) from our earlier coin toss example. \\(P(Y = 1|x)\\) is a proportion of malignant tumors in a bin, whereas \\(\\hat \\theta\\) was a proportion of heads in 10 coin tosses. Both also represent probabilities of some new point belonging to class \\(1\\).\nNotice how the Graph of Averages is a much better fit of the data.\n\nUnfortunately, the Graph of Averages begins to degenerate as we add more features. The exact reason is out of scope, but this model becomes harder to use in higher dimensions. Instead, we use Logistic Regression: a probabilistic model that tries to model the Graph of Averages.\nAs a recap of what we’ve discussed so far:\n\nWe want to fit this “S” shaped curve as best as possible\nThis curve models the probability of belonging to class \\(1\\)\n\n\\(P(Y = 1|x)\\)\n\n\n\n\n23.6.2 Transforming the Graph of Averages\nOur goal here is to find a mathematical function that models the Graph of Averages, or \\(P(Y = 1|x)\\). For shorthand, let’s call this function \\(p\\). We will\n\nTransform \\(p\\) until it looks linear.\nThen, use algebra to invert all transformations.\n\nNote: \\(P(Y = 1|x)\\) and \\(p\\) are used interchangeably throughout the rest of the note.\n\n23.6.2.1 1. Transform \\(p\\) until it looks linear\nTo transform this “S” shaped curve, we will define a quantity called the odds ratio. The odds of an event is the probability the event happens divided by the probability that it doesn’t happen. Remember, \\(p\\) is shorthand for our event \\(P(Y = 1|x)\\).\n\\[\\text{odds}(p) = \\frac{p}{1 - p}\\]\nHere, we’ve applied the odds ratio to our Graph of Averages. The result is an exponential relationship.\n\nApplying a logarithmic transformation to the y-axis will linearize the data. We know this from the Tukey-Mosteller Buldge Diagram.\n\\[\\text{log-odds}(p) = \\text{log}(\\frac{p}{1-p})\\]\n\nWe have a roughly linear relationship! Specifically, this is linear in our features \\(x\\) and parameters \\(\\theta\\). In our one dimensional feature space, \\(x\\) is the mean radius and \\(\\theta\\) some constant.\n\\[\\text{log}(\\frac{p}{1-p}) = x^T\\theta\\]\n\n\n23.6.2.2 2. Use algebra to invert all transformations\nRemember our original goal was to model the Graph of Averages, which we called \\(p\\). Here, we will algebraically solve for \\(p\\).\n\\[\\text{log}(\\frac{p}{1-p}) = x^T\\theta\\] \\[\\frac{p}{1-p} = e^{x^T\\theta}\\] \\[p = e^{x^T\\theta} - pe^{x^T\\theta}\\] \\[p = \\frac{e^{x^T\\theta}}{1 + e^{x^T\\theta}}\\] \\[p = \\frac{1}{1 + e^{-x^T\\theta}}\\]\nWe once again end up modeling \\(p\\) with the sigmoid (logistic) function!"
  }
]