[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "intro_lec/introduction.html#data-science-lifecycle",
    "href": "intro_lec/introduction.html#data-science-lifecycle",
    "title": "1  Introduction",
    "section": "1.1 Data Science Lifecycle",
    "text": "1.1 Data Science Lifecycle\nThe data science lifecycle is a high-level overview of the data science workflow. It’s a cycle of stages that a data scientist should explore as they conduct a thorough analysis of a data-driven problem.\nThere are many variations of the key ideas present in the data science lifecycle. In Data 100, we visualize the stages of the lifecycle using a flow diagram. Notice how there are two entry points.\n\n\n1.1.1 Ask a Question\nWhether by curiosity or necessity, data scientists will constantly ask questions. For example, in the business world, data scientists may be interested in predicting the profit generated by a certain investment. In the field of medicine, they may ask whether some patients are more likely than others to benefit from a treatment.\nPosing questions is one of the primary ways the data science lifecycle begins. It helps to fully define the question. Here are some things you should ask yourself before framing a question.\n\nWhat do we want to know?\n\nA question that is too ambiguous may lead to confusion.\n\nWhat problems are we trying to solve?\n\nThe goal of asking a question should be clear in order to justify your efforts to stakeholders.\n\nWhat are the hypotheses we want to test?\n\nThis gives a clear perspective from which to analyze final results.\n\nWhat are the metrics for our success?\n\nThis gives a clear point to know when to finish the project.\n\n\n\n\n\n1.1.2 Obtain Data\nThe second entry point to the lifecycle is by obtaining data. A careful analysis of any problem requires the use of data. Data may be readily available to us, or we may have to embark on a process to collect it. When doing so, its crucial to ask the following:\n\nWhat data do we have and what data do we need?\n\nDefine the units of the data (people, cities, points in time, etc.) and what features to measure.\n\nHow will we sample more data?\n\nScrape the web, collect manually, etc.\n\nIs our data representative of the population we want to study?\n\nIf our data is not representative of our population of interest, then we can come to incorrect conclusions.\n\n\nKey procedures: data acquisition, data cleaning\n\n\n\n1.1.3 Understand the Data\nRaw data itself is not inherently useful. It’s impossible to discern all the patterns and relationships between variables without carefully investigating them. Therefore, translating pure data to actionable insights is a key job of a data scientist. For example, we may choose to ask:\n\nHow is our data organized and what does it contain?\n\nKnowing what the data says about the world helps us better understand the world.\n\nDo we have relevant data?\n\nIf the data we have collected is not useful to the question at hand, then we must collected more data.\n\nWhat are the biases, anomalies, or other issues with the data?\n\nThese can lead to many false conclusions if ignored, so data scientists must always be aware of these issues.\n\nHow do we transform the data to enable effective analysis?\n\nData is not always easy to interpret at first glance, so a data scientist should reveal these hidden insights.\n\n\nKey procedures: exploratory data analysis, data visualization.\n\n\n\n1.1.4 Understand the World\nAfter observing the patterns in our data, we can begin answering our question. This may require that we predict a quantity (machine learning), or measure the effect of some treatment (inference).\nFrom here, we may choose to report our results, or possibly conduct more analysis. We may not be satisfied by our findings, or our initial exploration may have brought up new questions that require a new data.\n\nWhat does the data say about the world?\n\nGiven our models, the data will lead us to certain conclusions about the real world.\n\n\nDoes it answer our questions or accurately solve the problem?\n\nIf our model and data can not accomplish our goals, then we must reform our question, model, or both.\n\n\nHow robust are our conclusions and can we trust the predictions?\n\nInaccurate models can lead to untrue conclusions.\n\n\nKey procedures: model creation, prediction, inference."
  },
  {
    "objectID": "intro_lec/introduction.html#conclusion",
    "href": "intro_lec/introduction.html#conclusion",
    "title": "1  Introduction",
    "section": "1.2 Conclusion",
    "text": "1.2 Conclusion\nThe data science lifecycle is meant to be a set of general guidelines rather than a hard list of requirements. In our journey exploring the lifecycle, we’ll cover both the underlying theory and technologies used in data science, and we hope you’ll build an appreciation for the field.\nWith that, let’s begin by introducing one of the most important tools in exploratory data analysis: pandas."
  },
  {
    "objectID": "pandas_1/pandas_1.html#introduction-to-exploratory-data-analysis",
    "href": "pandas_1/pandas_1.html#introduction-to-exploratory-data-analysis",
    "title": "2  Pandas I",
    "section": "2.1 Introduction to Exploratory Data Analysis",
    "text": "2.1 Introduction to Exploratory Data Analysis\nImagine you collected, or have been given a box of data. What do you do next?\n\nThe first step is to clean your data. Data cleaning often corrects issues in the structure and formatting of data, including missing values and unit conversions.\nData scientists have coined the term exploratory data analysis (EDA) to describe the process of transforming raw data to insightful observations. EDA is an open-ended analysis of transforming, visualizing, and summarizing patterns in data. In order to conduct EDA, we first need to familiarize ourselves with pandas – an important programming tool."
  },
  {
    "objectID": "pandas_1/pandas_1.html#introduction-to-pandas",
    "href": "pandas_1/pandas_1.html#introduction-to-pandas",
    "title": "2  Pandas I",
    "section": "2.2 Introduction to Pandas",
    "text": "2.2 Introduction to Pandas\npandas is a data analysis library to make data cleaning and analysis fast and convenient in Python.\nThe pandas library adopts many coding idioms from NumPy. The biggest difference is that pandas is designed for working with tabular data, one of the most common data formats (and the focus of Data 100).\nBefore writing any code, we must import pandas into our Python environment.\n\n# `pd` is the conventional alias for Pandas, as `np` is for NumPy\nimport pandas as pd"
  },
  {
    "objectID": "pandas_1/pandas_1.html#series-dataframes-and-indices",
    "href": "pandas_1/pandas_1.html#series-dataframes-and-indices",
    "title": "2  Pandas I",
    "section": "2.3 Series, DataFrames, and Indices",
    "text": "2.3 Series, DataFrames, and Indices\nThere are three fundamental data structures in pandas:\n\nSeries: 1D labeled array data; best thought of as columnar data\nDataFrame: 2D tabular data with rows and columns\nIndex: A sequence of row/column labels\n\nDataFrames, Series, and Indices can be represented visually in the following diagram.\n\nNotice how the DataFrame is a two dimensional object – it contains both rows and columns. The Series above is a singular column of this DataFrame, namely the Candidate column. Both contain an Index, or a shared list of row labels (the integers from 0 to 5, inclusive).\n\n2.3.1 Series\nA Series represents a column of a DataFrame; more generally, it can be any 1-dimensional array-like object containing values of the same type with associated data labels, called its index.\n\nimport pandas as pd\n\ns = pd.Series([-1, 10, 2])\nprint(s)\n\n0    -1\n1    10\n2     2\ndtype: int64\n\n\n\ns.array # Data contained within the Series\n\n<PandasArray>\n[-1, 10, 2]\nLength: 3, dtype: int64\n\n\n\ns.index # The Index of the Series\n\nRangeIndex(start=0, stop=3, step=1)\n\n\nBy default, row indices in pandas are a sequential list of integers beginning from 0. Optionally, a list of desired indices can be passed to the index argument.\n\ns = pd.Series([-1, 10, 2], index = [\"a\", \"b\", \"c\"])\nprint(s)\n\na    -1\nb    10\nc     2\ndtype: int64\n\n\nIndices can also be changed after initialization.\n\ns.index = [\"first\", \"second\", \"third\"]\nprint(s)\n\nfirst     -1\nsecond    10\nthird      2\ndtype: int64\n\n\n\n2.3.1.1 Selection in Series\nSimilar to an array, we can select a single value or a set of values from a Series. There are 3 primary methods of selecting data.\n\nA single index label\nA list of index labels\nA filtering condition\n\nLet’s define the following Series ser.\n\nser = pd.Series([4, -2, 0, 6], index = [\"a\", \"b\", \"c\", \"d\"])\nprint(ser)\n\na    4\nb   -2\nc    0\nd    6\ndtype: int64\n\n\n\n2.3.1.1.1 A Single Index Label\n\nprint(ser[\"a\"]) # Notice how the return value is a single array element\n\n4\n\n\n\n\n2.3.1.1.2 A List of Index Labels\n\nser[[\"a\", \"c\"]] # Notice how the return value is another Series\n\na    4\nc    0\ndtype: int64\n\n\n\n\n2.3.1.1.3 A Filtering Condition\nPerhaps the most interesting (and useful) method of selecting data from a Series is with a filtering condition.\nWe first must apply a vectorized boolean operation to our Series that encodes the filter conditon.\n\nser > 0 # Filter condition: select all elements greater than 0\n\na     True\nb    False\nc    False\nd     True\ndtype: bool\n\n\nUpon “indexing” in our Series with this condition, pandas selects only the rows with True values.\n\nser[ser > 0] \n\na    4\nd    6\ndtype: int64\n\n\n\n\n\n\n2.3.2 DataFrames\nIn Data 8, you encountered the Table class of the datascience library, which represented tabular data. In Data 100, we’ll be using the DataFrame class of the pandas library.\nHere is an example of a DataFrame that contains election data.\n\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n      loss\n      1.073699\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n      win\n      51.311515\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n182 rows × 6 columns\n\n\n\nLet’s dissect the code above.\n\nWe first import the pandas library into our Python environment, using the alias pd.   import pandas as pd\nThere are a number of ways to read data into a DataFrame. In Data 100, our data are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.   pd.read_csv(\"elections.csv\")\n\nThis code stores our DataFrame object in the elections variable. Upon inspection, our elections DataFrame has 182 rows and 6 columns (Year, Candidate, Party, Popular Vote, Result, %). Each row represents a single record – in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nIn the example above, we constructed a DataFrame object using data from a CSV file. As we’ll explore in the next section, we can create a DataFrame with data of our own.\n\n2.3.2.1 Creating a DataFrame\nThere are many ways to create a DataFrame. Here, we will cover the most popular approaches.\n\nUsing a list and column names\nFrom a dictionary\nFrom a Series\n\n\n2.3.2.1.1 Using a List and Column Names\nConsider the following examples.\n\ndf_list = pd.DataFrame([1, 2, 3], columns=[\"Numbers\"])\ndf_list\n\n\n\n\n\n  \n    \n      \n      Numbers\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n  \n\n\n\n\nThe first code cell creates a DataFrame with a single column Numbers, while the second creates a DataFrame with an additional column Description. Notice how a 2D list of values is required to initialize the second DataFrame – each nested list represents a single row of data.\n\ndf_list = pd.DataFrame([[1, \"one\"], [2, \"two\"]], columns = [\"Number\", \"Description\"])\ndf_list\n\n\n\n\n\n  \n    \n      \n      Number\n      Description\n    \n  \n  \n    \n      0\n      1\n      one\n    \n    \n      1\n      2\n      two\n    \n  \n\n\n\n\n\n\n2.3.2.1.2 From a Dictionary\nA second (and more common) way to create a DataFrame is with a dictionary. The dictionary keys represent the column names, and the dictionary values represent the column values.\n\ndf_dict = pd.DataFrame({\"Fruit\": [\"Strawberry\", \"Orange\"], \"Price\": [5.49, 3.99]})\ndf_dict\n\n\n\n\n\n  \n    \n      \n      Fruit\n      Price\n    \n  \n  \n    \n      0\n      Strawberry\n      5.49\n    \n    \n      1\n      Orange\n      3.99\n    \n  \n\n\n\n\n\n\n2.3.2.1.3 From a Series\nEarlier, we explained how a Series was synonymous to a column in a DataFrame. It follows then, that a DataFrame is equivalent to a collection of Series, which all share the same index.\nIn fact, we can initialize a DataFrame by merging two or more Series.\n\n# Notice how our indices, or row labels, are the same\n\ns_a = pd.Series([\"a1\", \"a2\", \"a3\"], index = [\"r1\", \"r2\", \"r3\"])\ns_b = pd.Series([\"b1\", \"b2\", \"b3\"], index = [\"r1\", \"r2\", \"r3\"])\n\npd.DataFrame({\"A-column\": s_a, \"B-column\": s_b})\n\n\n\n\n\n  \n    \n      \n      A-column\n      B-column\n    \n  \n  \n    \n      r1\n      a1\n      b1\n    \n    \n      r2\n      a2\n      b2\n    \n    \n      r3\n      a3\n      b3\n    \n  \n\n\n\n\n\n\n\n\n2.3.3 Indices\nThe major takeaway: we can think of a DataFrame as a collection of Series that all share the same Index.\nOn a more technical note, an Index doesn’t have to be an integer, nor does it have to be unique. For example, we can set the index of the elections Dataframe to be the name of presedential candidates. Selecting a new Series from this modified DataFrame yields the following.\n\n# This sets the index to the \"Candidate\" column\nelections.set_index(\"Candidate\", inplace=True)\n\n\nTo retrieve the indices of a DataFrame, simply use the .index attribute of the DataFrame class.\n\nelections.index\n\nIndex(['Andrew Jackson', 'John Quincy Adams', 'Andrew Jackson',\n       'John Quincy Adams', 'Andrew Jackson', 'Henry Clay', 'William Wirt',\n       'Hugh Lawson White', 'Martin Van Buren', 'William Henry Harrison',\n       ...\n       'Darrell Castle', 'Donald Trump', 'Evan McMullin', 'Gary Johnson',\n       'Hillary Clinton', 'Jill Stein', 'Joseph Biden', 'Donald Trump',\n       'Jo Jorgensen', 'Howard Hawkins'],\n      dtype='object', name='Candidate', length=182)\n\n\n\n# This resets the index to be the default list of integers\nelections.reset_index(inplace=True)"
  },
  {
    "objectID": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "href": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "title": "2  Pandas I",
    "section": "2.4 Slicing in DataFrames",
    "text": "2.4 Slicing in DataFrames\nNow that we’ve learned how to create DataFrames, let’s dive deeper into their capabilities.\nThe API (application programming interface) for the DataFrame class is enormous. In this section, we’ll discuss several methods of the DataFrame API that allow us to extract subsets of data.\nThe simplest way to manipulate a DataFrame is to extract a subset of rows and columns, known as slicing. We will do so with three primary methods of the DataFrame class:\n\n.loc\n.iloc\n[]\n\n\n2.4.1 Indexing with .loc\nThe .loc operator selects rows and columns in a DataFrame by their row and column label(s), respectively. The row labels (commonly referred to as the indices) are the bold text on the far left of a DataFrame, while the column labels are the column names found at the top of a DataFrame.\nTo grab data with .loc, we must specify the row and column label(s) where the data exists. The row labels are the first argument to the .loc function; the column labels are the second. For example, we can select the the row labeled 0 and the column labeled Candidate from the elections DataFrame.\n\nelections.loc[0, 'Candidate']\n\n'Andrew Jackson'\n\n\nTo select multiple rows and columns, we can use Python slice notation. Here, we select both the first four rows and columns.\n\nelections.loc[0:3, 'Year':'Popular vote']\n\n\n\n\n\n  \n    \n      \n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nSuppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : is useful for this.\n\nelections.loc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\nThere are a couple of things we should note. Unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.\nEquivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame.\n\nelections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nLastly, we can interchange list and slicing notation.\n\nelections.loc[[0, 1, 2, 3], :]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\n2.4.2 Indexing with .iloc\nSlicing with .iloc works similarily to .loc, although .iloc uses the integer positions of rows and columns rather the labels. The arguments to the .iloc function also behave similarly - single values, lists, indices, and any combination of these are permitted.\nLet’s begin reproducing our results from above. We’ll begin by selecting for the first presedential candidate in our elections DataFrame:\n\n# elections.loc[0, \"Candidate\"] - Previous approach\nelections.iloc[0, 1]\n\n1824\n\n\nNotice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th (or first) position of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.\nHowever, when we select the first four rows and columns using .iloc, we notice something.\n\n# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach\nelections.iloc[0:4, 0:4]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nSlicing is no longer inclusive in .iloc - it’s exclusive. This is one of Pandas syntatical subtleties; you’ll get used to with practice.\nList behavior works just as expected.\n\n#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach\nelections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nThis discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.\n\n\n2.4.3 Indexing with []\nThe [] selection operator is the most baffling of all, yet the commonly used. It only takes a single argument, which may be one of the following:\n\nA slice of row numbers\nA list of column labels\nA single column label\n\nThat is, [] is context dependent. Let’s see some examples.\n\n2.4.3.1 A slice of row numbers\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\n2.4.3.2 A list of column labels\nSuppose we now want the first four columns.\n\nelections[[\"Year\", \"Candidate\", \"Party\", \"Popular vote\"]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n    \n  \n\n182 rows × 4 columns\n\n\n\n\n\n2.4.3.3 A single column label\nLastly, if we only want the Candidate column.\n\nelections[\"Candidate\"]\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThe output looks like a Series! In this course, we’ll become very comfortable with [], especially for selecting columns. In practice, [] is much more common than .loc."
  },
  {
    "objectID": "pandas_1/pandas_1.html#parting-note",
    "href": "pandas_1/pandas_1.html#parting-note",
    "title": "2  Pandas I",
    "section": "2.5 Parting Note",
    "text": "2.5 Parting Note\nThe pandas library is enormous and contains many useful functions. Here is a link to documentation.\nThe introductory pandas lectures will cover important data structures and methods you should be fluent in. However, we want you to get familiar with the real world programming practice of …Googling! Answers to your questions can be found in documentation, Stack Overflow, etc.\nWith that, let’s move on to Pandas II."
  },
  {
    "objectID": "pandas_2/pandas_2.html#conditional-selection",
    "href": "pandas_2/pandas_2.html#conditional-selection",
    "title": "3  Pandas II",
    "section": "3.1 Conditional Selection",
    "text": "3.1 Conditional Selection\nConditional selection allows us to select a subset of rows in a DataFrame if they follow some specified condition.\nTo understand how to use conditional selection, we must look at another possible input of the .loc and [] methods – a boolean array, which is simply an array where each element is either True or False. This boolean array must have a length equal to the number of rows in the DataFrame. It will return all rows in the position of a corresponding True value in the array.\nTo see this in action, let’s select all even-indexed rows in the first 10 rows of our DataFrame.\n\n# Ask yourself: why is :9 is the correct slice to select the first 10 rows?\nbabynames_first_10_rows = babynames.loc[:9, :]\n\n# Notice how we have exactly 10 elements in our boolean array argument\nbabynames_first_10_rows[[True, False, True, False, True, False, True, False, True, False]]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      6\n      CA\n      F\n      1910\n      Evelyn\n      126\n    \n    \n      8\n      CA\n      F\n      1910\n      Virginia\n      101\n    \n  \n\n\n\n\nWe can perform a similar operation using .loc.\n\nbabynames_first_10_rows.loc[[True, False, True, False, True, False, True, False, True, False], :]\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      6\n      CA\n      F\n      1910\n      Evelyn\n      126\n    \n    \n      8\n      CA\n      F\n      1910\n      Virginia\n      101\n    \n  \n\n\n\n\nThese techniques worked well in this example, but you can imagine how tedious it might be to list out Trues and Falses for every row in a larger DataFrame. To make things easier, we can instead provide a logical condition as an input to .loc or [] that returns a boolean array with the necessary length.\nFor example, to return all names associated with F sex:\n\n# First, use a logical condition to generate a boolean array\nlogical_operator = (babynames[\"Sex\"] == \"F\")\n\n# Then, use this boolean array to filter the DataFrame\nbabynames[logical_operator].head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n  \n\n\n\n\nHere, logical_operator evaluates to a Series of boolean values with length 400762.\n\nprint(\"There are a total of {} values in 'logical_operator'\".format(len(logical_operator)))\n\nThere are a total of 400762 values in 'logical_operator'\n\n\nRows starting at row 0 and ending at row 235790 evaluate to True and are thus returned in the DataFrame.\n\n\nCode\nprint(\"The 0th item in this 'logical_operator' is: {}\".format(logical_operator.iloc[0]))\nprint(\"The 235790th item in this 'logical_operator' is: {}\".format(logical_operator.iloc[235790]))\nprint(\"The 235791th item in this 'logical_operator' is: {}\".format(logical_operator.iloc[235791]))\n\n\nThe 0th item in this 'logical_operator' is: True\nThe 235790th item in this 'logical_operator' is: True\nThe 235791th item in this 'logical_operator' is: False\n\n\nPassing a Series as an argument to babynames[] has the same affect as using a boolean array. In fact, the [] selection operator can take a boolean Series, array, and list as arguments. These three are used interchangeably thoughout the course.\nWe can also use .loc to achieve similar results.\n\nbabynames.loc[babynames[\"Sex\"] == \"F\"].head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n  \n\n\n\n\nBoolean conditions can be combined using various operators that allow us to filter results by multiple conditions. Some examples include the & (and) operator and the | (or) operator.\nNote: When combining multiple conditions with logical operators, be sure to surround each condition with a set of parenthesis (). If you forget, your code will throw an error.\nFor example, if we want to return data on all females born before the 21st century, we can write:\n\nbabynames[(babynames[\"Sex\"] == \"F\") & (babynames[\"Year\"] < 2000)].head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n  \n\n\n\n\nBoolean array selection is a useful tool, but can lead to overly verbose code for complex conditions. Pandas provide many alternatives:\n\n(\n    babynames[(babynames[\"Name\"] == \"Bella\") | \n              (babynames[\"Name\"] == \"Alex\") |\n              (babynames[\"Name\"] == \"Ani\") |\n              (babynames[\"Name\"] == \"Lisa\")]\n).head()\n# Note: The parentheses surrounding the code make it possible to break the code on to multiple lines for readability\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      6289\n      CA\n      F\n      1923\n      Bella\n      5\n    \n    \n      7512\n      CA\n      F\n      1925\n      Bella\n      8\n    \n    \n      12368\n      CA\n      F\n      1932\n      Lisa\n      5\n    \n    \n      14741\n      CA\n      F\n      1936\n      Lisa\n      8\n    \n    \n      17084\n      CA\n      F\n      1939\n      Lisa\n      5\n    \n  \n\n\n\n\nThe .isin function can be used to filter dataframes. The method helps in selecting rows with having a particular (or multiple) value in a particular column.\n\nnames = [\"Bella\", \"Alex\", \"Ani\", \"Lisa\"]\nbabynames[babynames[\"Name\"].isin(names)].head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      6289\n      CA\n      F\n      1923\n      Bella\n      5\n    \n    \n      7512\n      CA\n      F\n      1925\n      Bella\n      8\n    \n    \n      12368\n      CA\n      F\n      1932\n      Lisa\n      5\n    \n    \n      14741\n      CA\n      F\n      1936\n      Lisa\n      8\n    \n    \n      17084\n      CA\n      F\n      1939\n      Lisa\n      5\n    \n  \n\n\n\n\nThe function str.startswith can be used to define a filter based on string values in a Series object.\n\nbabynames[babynames[\"Name\"].str.startswith(\"N\")].head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      76\n      CA\n      F\n      1910\n      Norma\n      23\n    \n    \n      83\n      CA\n      F\n      1910\n      Nellie\n      20\n    \n    \n      127\n      CA\n      F\n      1910\n      Nina\n      11\n    \n    \n      198\n      CA\n      F\n      1910\n      Nora\n      6\n    \n    \n      310\n      CA\n      F\n      1911\n      Nellie\n      23"
  },
  {
    "objectID": "pandas_2/pandas_2.html#handy-utility-functions",
    "href": "pandas_2/pandas_2.html#handy-utility-functions",
    "title": "3  Pandas II",
    "section": "3.2 Handy Utility Functions",
    "text": "3.2 Handy Utility Functions\npandas contains an extensive library of functions that can help shorten the process of setting and getting information from its data structures. In the following section, we will give overviews of each of the main utility functions that will help us in Data 100.\n\nNumpy and built-in function support\n.shape\n.size\n.describe()\n.sample()\n.value_counts()\n.unique()\n.sort_values()\n\n\n3.2.1 Numpy\n\nbella_counts = babynames[babynames[\"Name\"] == \"Bella\"][\"Count\"]\n\n\n# Average number of babies named Bella each year\nnp.mean(bella_counts)\n\n270.1860465116279\n\n\n\n# Max number of babies named Bella born on a given year\nmax(bella_counts)\n\n902\n\n\n\n\n3.2.2 .shape and .size\n.shape and .size are attributes of Series and DataFrames that measure the “amount” of data stored in the structure. Calling .shape returns a tuple containing the number of rows and columns present in the DataFrame or Series. .size is used to find the total number of elements in a structure, equivalent to the number of rows times the number of columns.\nMany functions strictly require the dimensions of the arguments along certain axes to match. Calling these dimension-finding functions is much faster than counting all of the items by hand.\n\nbabynames.shape\n\n(400762, 5)\n\n\n\nbabynames.size\n\n2003810\n\n\n\n\n3.2.3 .describe()\nIf many statistics are required from a DataFrame (minimum value, maximum value, mean value, etc.), then .describe() can be used to compute all of them at once.\n\nbabynames.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Count\n    \n  \n  \n    \n      count\n      400762.000000\n      400762.000000\n    \n    \n      mean\n      1985.131287\n      79.953781\n    \n    \n      std\n      26.821004\n      295.414618\n    \n    \n      min\n      1910.000000\n      5.000000\n    \n    \n      25%\n      1968.000000\n      7.000000\n    \n    \n      50%\n      1991.000000\n      13.000000\n    \n    \n      75%\n      2007.000000\n      38.000000\n    \n    \n      max\n      2021.000000\n      8262.000000\n    \n  \n\n\n\n\nA different set of statistics will be reported if .describe() is called on a Series.\n\nbabynames[\"Sex\"].describe()\n\ncount     400762\nunique         2\ntop            F\nfreq      235791\nName: Sex, dtype: object\n\n\n\n\n3.2.4 .sample()\nAs we will see later in the semester, random processes are at the heart of many data science techniques (for example, train-test splits, bootstrapping, and cross-validation). .sample() lets us quickly select random entries (a row if called from a DataFrame, or a value if called from a Series).\n\nbabynames.sample()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      286168\n      CA\n      M\n      1974\n      Ronaldo\n      5\n    \n  \n\n\n\n\n\nbabynames.sample(5).iloc[:, 2:]\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      54728\n      1966\n      Shane\n      6\n    \n    \n      293861\n      1980\n      Jesus\n      1034\n    \n    \n      8378\n      1926\n      Elna\n      5\n    \n    \n      289337\n      1977\n      Marcos\n      155\n    \n    \n      377790\n      2014\n      Brayden\n      492\n    \n  \n\n\n\n\n\nbabynames[babynames[\"Year\"] == 2000].sample(4, replace = True).iloc[:, 2:]\n\n\n\n\n\n  \n    \n      \n      Year\n      Name\n      Count\n    \n  \n  \n    \n      150197\n      2000\n      Corrina\n      24\n    \n    \n      150494\n      2000\n      Inez\n      17\n    \n    \n      340908\n      2000\n      Bakari\n      5\n    \n    \n      151071\n      2000\n      Alejandro\n      10\n    \n  \n\n\n\n\n\n\n3.2.5 .value_counts()\nWhen we want to know the distribution of the items in a Series (for example, what items are most/least common), we use .value-counts() to get a breakdown of the unique values and their counts. In the example below, we can determine the name with the most years in which at least one person has taken that name by counting the number of times each name appears in the \"Name\" column of babynames.\n\nbabynames[\"Name\"].value_counts().head()\n\nJean         221\nFrancis      219\nGuadalupe    216\nJessie       215\nMarion       213\nName: Name, dtype: int64\n\n\n\n\n3.2.6 .unique()\nIf we have a Series with many repeated values, then .unique() can be used to identify only the unique values. Here we can get a list of all the names in babynames.\nExercise: what function can we call on the Series below to get the number of unique names?\n\nbabynames[\"Name\"].unique()\n\narray(['Mary', 'Helen', 'Dorothy', ..., 'Zyire', 'Zylo', 'Zyrus'],\n      dtype=object)\n\n\n\n\n3.2.7 .sort_values()\nOrdering a DataFrame can be useful for isolating extreme values. For example, the first 5 entries of a row sorted in descending order (that is, from highest to lowest) are the largest 5 values. .sort_values allows us to order a DataFrame or Series by a specified rule. For DataFrames, we must specify the column by which we want to compare the rows and the function will return such rows. We can choose to either receive the rows in ascending order (default) or descending order.\n\nbabynames.sort_values(by = \"Count\", ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      263272\n      CA\n      M\n      1956\n      Michael\n      8262\n    \n    \n      264297\n      CA\n      M\n      1957\n      Michael\n      8250\n    \n    \n      313644\n      CA\n      M\n      1990\n      Michael\n      8247\n    \n    \n      278109\n      CA\n      M\n      1969\n      Michael\n      8244\n    \n    \n      279405\n      CA\n      M\n      1970\n      Michael\n      8197\n    \n  \n\n\n\n\nWe do not need to explicitly specify the column used for sorting when calling .value_counts() on a Series. We can still specify the ordering paradigm – that is, whether values are sorted in ascending or descending order.\n\nbabynames[\"Name\"].sort_values(ascending=True).head()\n\n380256      Aadan\n362255      Aadan\n365374      Aadan\n394460    Aadarsh\n366561      Aaden\nName: Name, dtype: object\n\n\n\n3.2.7.1 Sorting With a Custom Key\nUsing .sort_values can be useful in many situations, but it many not cover all use cases. This is because pandas automatically sorts values in order according to numeric value (for number data) or alphabetical order (for string data). The following code finds the top 5 most popular names in California in 2021.\n\n# Sort names by count in year 2021\n# Recall that `.head(5)` displays the first five rows in the DataFrame\nbabynames[babynames[\"Year\"] == 2021].sort_values(\"Count\", ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      397909\n      CA\n      M\n      2021\n      Noah\n      2591\n    \n    \n      397910\n      CA\n      M\n      2021\n      Liam\n      2469\n    \n    \n      232145\n      CA\n      F\n      2021\n      Olivia\n      2395\n    \n    \n      232146\n      CA\n      F\n      2021\n      Emma\n      2171\n    \n    \n      397911\n      CA\n      M\n      2021\n      Mateo\n      2108\n    \n  \n\n\n\n\nThis offers us a lot of functionality, but what if we need to sort by some other metric? For example, what if we wanted to find the longest names in the DataFrame?\nWe can do this by specifying the key parameter of .sort_values. The key parameter is assigned to a function of our choice. This function is then applied to each value in the specified column. pandas will, finally, sort the DataFrame by the values outputted by the function.\n\n# Here, a lambda function is applied to find the length of each value, `x`, in the \"Name\" column\nbabynames.sort_values(\"Name\", key=lambda x: x.str.len(), ascending=False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10"
  },
  {
    "objectID": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "href": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "title": "3  Pandas II",
    "section": "3.3 Adding and Removing Columns",
    "text": "3.3 Adding and Removing Columns\nTo add a new column to a DataFrame, we use a syntax similar to that used when accessing an existing column. Specify the name of the new column by writing dataframe[\"new_column\"], then assign this to a Series or Array containing the values that will populate this column.\n\n# Add a column named \"name_lengths\" that includes the length of each name\nbabynames[\"name_lengths\"] = babynames[\"Name\"].str.len()\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      name_lengths\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n      4\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n      5\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n      7\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n      8\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n      7\n    \n  \n\n\n\n\n\n# Sort by the temporary column\nbabynames = babynames.sort_values(by = \"name_lengths\", ascending=False)\nbabynames.head()\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      name_lengths\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n      15\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n      15\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n      15\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n      15\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10\n      15\n    \n  \n\n\n\n\nIn the example above, we made use of an in-built function given to us by the str accessor for getting the length of names. Then we used name_length column to sort the dataframe. What if we had wanted to generate the values in our new column using a function of our own making?\nWe can do this using the Series .map method. .map takes in a function as input, and will apply this function to each value of a Series.\nFor example, say we wanted to find the number of occurrences of the sequence “dr” or “ea” in each name.\n\n# First, define a function to count the number of times \"dr\" or \"ea\" appear in each name\ndef dr_ea_count(string):\n    return string.count(\"dr\") + string.count(\"ea\")\n\n# Then, use `map` to apply `dr_ea_count` to each name in the \"Name\" column\nbabynames[\"dr_ea_count\"] = babynames[\"Name\"].map(dr_ea_count)\n\n# Sort the DataFrame by the new \"dr_ea_count\" column so we can see our handiwork\nbabynames.sort_values(by = \"dr_ea_count\", ascending = False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      name_lengths\n      dr_ea_count\n    \n  \n  \n    \n      101969\n      CA\n      F\n      1986\n      Deandrea\n      6\n      8\n      3\n    \n    \n      304390\n      CA\n      M\n      1985\n      Deandrea\n      6\n      8\n      3\n    \n    \n      131022\n      CA\n      F\n      1994\n      Leandrea\n      5\n      8\n      3\n    \n    \n      115950\n      CA\n      F\n      1990\n      Deandrea\n      5\n      8\n      3\n    \n    \n      108723\n      CA\n      F\n      1988\n      Deandrea\n      5\n      8\n      3\n    \n  \n\n\n\n\nIf we want to remove a column or row of a DataFrame, we can call the .drop method. Use the axis parameter to specify whether a column or row should be dropped. Unless otherwise specified, pandas will assume that we are dropping a row by default.\n\n# Drop our \"dr_ea_count\" and \"length\" columns from the DataFrame\nbabynames = babynames.drop([\"dr_ea_count\", \"name_lengths\"], axis=\"columns\")\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10\n    \n  \n\n\n\n\nNotice that we reassigned babynames to the result of babynames.drop(...). This is a subtle, but important point: pandas table operations do not occur in-place. Calling dataframe.drop(...) will output a copy of dataframe with the row/column of interest removed, without modifying the original dataframe table.\nIn other words, if we simply call:\n\n# This creates a copy of `babynames` and removes the row with label 3...\nbabynames.drop(3, axis=\"rows\")\n\n# ...but the original `babynames` is unchanged! \n# Notice that the row with label 3 is still present\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10"
  },
  {
    "objectID": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "href": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "title": "3  Pandas II",
    "section": "3.4 Aggregating Data with GroupBy",
    "text": "3.4 Aggregating Data with GroupBy\nUp until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.\nLet’s say we wanted to aggregate all rows in babynames for a given year.\n\nbabynames.groupby(\"Year\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001FB1594CC40>\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from babynames that correspond to a particular year.\nThe diagram below shows a simplified view of babynames to help illustrate this idea.\n\n\n\nCreating a GroupBy object\n\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.\nThe first aggregation method we’ll consider is .agg. The .agg method takes in a function as its argument; this function is then applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in babynames – this is equivalent to finding the number of babies born in each year.\n\nbabynames.groupby(\"Year\").agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      9163\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of babynames, which is why we see smaller values for the summed counts.\n\n\n\nPerforming an aggregation\n\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original babynames DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell Pandas to only consider the \"Count\" column when summing\nbabynames.groupby(\"Year\")[[\"Count\"]].agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      9163\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\n\n3.4.1 Parting note\nManipulating DataFrames is a skill that is not mastered in just one day. Due to the flexibility of pandas, there are many different ways to get from a point A to a point B. We recommend trying multiple different ways to solve the same problem to gain even more practice and reach that point of mastery sooner.\nNext, we will start digging deeper into the mechanics behind grouping data."
  },
  {
    "objectID": "pandas_3/pandas_3.html#more-on-agg-function",
    "href": "pandas_3/pandas_3.html#more-on-agg-function",
    "title": "4  Pandas III",
    "section": "4.1 More on agg() Function",
    "text": "4.1 More on agg() Function\nLast time, we introduced the concept of aggregating data – we familiarized ourselves with GroupBy objects and used them as tools to consolidate and summarize a DataFrame. In this lecture, we will explore some advanced .groupby methods to show just how powerful of a resource they can be for understanding our data. We will also introduce other techniques for data aggregation to provide flexibility in how we manipulate our tables."
  },
  {
    "objectID": "pandas_3/pandas_3.html#groupby-continued",
    "href": "pandas_3/pandas_3.html#groupby-continued",
    "title": "4  Pandas III",
    "section": "4.2 GroupBy(), Continued",
    "text": "4.2 GroupBy(), Continued\nAs we learned last lecture, a groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.\nFor some arbitrary DataFrame df below, the code df.groupby(\"year\").agg(sum) does the following:\n\nOrganizes all rows with the same year into a subframe for that year.\nCreates a new DataFrame with one row representing each subframe year.\nCombines all integer rows in each subframe using the sum function.\n\n\n\n4.2.1 Aggregation with lambda Functions\nThroughout this note, we’ll work with the elections DataFrame.\n\n\nCode\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nWhat if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.\nLet’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.\nA naive approach may be to group by the Party column and aggregate by the maximum.\n\nelections.groupby(\"Party\").agg(max).head(10)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Popular vote\n      Result\n      %\n    \n    \n      Party\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      American\n      1976\n      Thomas J. Anderson\n      873053\n      loss\n      21.554001\n    \n    \n      American Independent\n      1976\n      Lester Maddox\n      9901118\n      loss\n      13.571218\n    \n    \n      Anti-Masonic\n      1832\n      William Wirt\n      100715\n      loss\n      7.821583\n    \n    \n      Anti-Monopoly\n      1884\n      Benjamin Butler\n      134294\n      loss\n      1.335838\n    \n    \n      Citizens\n      1980\n      Barry Commoner\n      233052\n      loss\n      0.270182\n    \n    \n      Communist\n      1932\n      William Z. Foster\n      103307\n      loss\n      0.261069\n    \n    \n      Constitution\n      2016\n      Michael Peroutka\n      203091\n      loss\n      0.152398\n    \n    \n      Constitutional Union\n      1860\n      John Bell\n      590901\n      loss\n      12.639283\n    \n    \n      Democratic\n      2020\n      Woodrow Wilson\n      81268924\n      win\n      61.344703\n    \n    \n      Democratic-Republican\n      1824\n      John Quincy Adams\n      151271\n      win\n      57.210122\n    \n  \n\n\n\n\nThis approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.\nWhy is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:\n\nThe most recent Year a Democratic candidate ran for president (2020)\nThe Candidate with the alphabetically “largest” name (“Woodrow Wilson”)\nThe Result with the alphabetically “largest” outcome (“win”)\n\nInstead, let’s try a different approach. We will:\n\nSort the DataFrame so that rows are in descending order of %\nGroup by Party and select the first row of each groupby object\n\nWhile it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.\n\nelections_sorted_by_percent = elections.sort_values(\"%\", ascending=False)\nelections_sorted_by_percent.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      114\n      1964\n      Lyndon Johnson\n      Democratic\n      43127041\n      win\n      61.344703\n    \n    \n      91\n      1936\n      Franklin Roosevelt\n      Democratic\n      27752648\n      win\n      60.978107\n    \n    \n      120\n      1972\n      Richard Nixon\n      Republican\n      47168710\n      win\n      60.907806\n    \n    \n      79\n      1920\n      Warren Harding\n      Republican\n      16144093\n      win\n      60.574501\n    \n    \n      133\n      1984\n      Ronald Reagan\n      Republican\n      54455472\n      win\n      59.023326\n    \n  \n\n\n\n\n\nelections_sorted_by_percent.groupby(\"Party\").agg(lambda x : x.iloc[0]).head(10)\n\n# Equivalent to the below code\n# elections_sorted_by_percent.groupby(\"Party\").agg('first').head(10)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Popular vote\n      Result\n      %\n    \n    \n      Party\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      American\n      1856\n      Millard Fillmore\n      873053\n      loss\n      21.554001\n    \n    \n      American Independent\n      1968\n      George Wallace\n      9901118\n      loss\n      13.571218\n    \n    \n      Anti-Masonic\n      1832\n      William Wirt\n      100715\n      loss\n      7.821583\n    \n    \n      Anti-Monopoly\n      1884\n      Benjamin Butler\n      134294\n      loss\n      1.335838\n    \n    \n      Citizens\n      1980\n      Barry Commoner\n      233052\n      loss\n      0.270182\n    \n    \n      Communist\n      1932\n      William Z. Foster\n      103307\n      loss\n      0.261069\n    \n    \n      Constitution\n      2008\n      Chuck Baldwin\n      199750\n      loss\n      0.152398\n    \n    \n      Constitutional Union\n      1860\n      John Bell\n      590901\n      loss\n      12.639283\n    \n    \n      Democratic\n      1964\n      Lyndon Johnson\n      43127041\n      win\n      61.344703\n    \n    \n      Democratic-Republican\n      1824\n      Andrew Jackson\n      151271\n      loss\n      57.210122\n    \n  \n\n\n\n\nNotice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.\nMore generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.\nIn fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.\nNote: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.\n\n# Using the idxmax function\nbest_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]\nbest_per_party.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      22\n      1856\n      Millard Fillmore\n      American\n      873053\n      loss\n      21.554001\n    \n    \n      115\n      1968\n      George Wallace\n      American Independent\n      9901118\n      loss\n      13.571218\n    \n    \n      6\n      1832\n      William Wirt\n      Anti-Masonic\n      100715\n      loss\n      7.821583\n    \n    \n      38\n      1884\n      Benjamin Butler\n      Anti-Monopoly\n      134294\n      loss\n      1.335838\n    \n    \n      127\n      1980\n      Barry Commoner\n      Citizens\n      233052\n      loss\n      0.270182\n    \n  \n\n\n\n\n\n# Using the .drop_duplicates function\nbest_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')\nbest_per_party2.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      148\n      1996\n      John Hagelin\n      Natural Law\n      113670\n      loss\n      0.118219\n    \n    \n      164\n      2008\n      Chuck Baldwin\n      Constitution\n      199750\n      loss\n      0.152398\n    \n    \n      110\n      1956\n      T. Coleman Andrews\n      States' Rights\n      107929\n      loss\n      0.174883\n    \n    \n      147\n      1996\n      Howard Phillips\n      Taxpayers\n      184656\n      loss\n      0.192045\n    \n    \n      136\n      1988\n      Lenora Fulani\n      New Alliance\n      217221\n      loss\n      0.237804\n    \n  \n\n\n\n\n\n\n4.2.2 Other GroupBy Features\nThere are many aggregation methods we can use with .agg. Some useful options are:\n\n.max: creates a new DataFrame with the maximum value of each group\n.mean: creates a new DataFrame with the mean value of each group\n.size: creates a new Series with the number of entries in each group\n\nIn fact, these (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.\nFor example, the following are equivalent:\n\nelections.groupby(\"Candidate\").agg(mean)\nelections.groupby(\"Candidate\").mean()\n\n\n\n4.2.3 The groupby.filter() function\nAnother common use for GroupBy objects is to filter data by group.\ngroupby.filter takes an argument \\(\\text{f}\\), where \\(\\text{f}\\) is a function that:\n\nTakes a GroupBy object as input\nReturns a single True or False for the entire subframe\n\nGroupBy objects that correspond to True are returned in the final result, whereas those with a False value are not. Importantly, groupby.filter is different from groupby.agg in that the entire subframe is returned in the final DataFrame, not just a single row.\nTo illustrate how this happens, consider the following .filter function applied on some arbitrary data. Say we want to identify “tight” election years – that is, we want to find all rows that correspond to elections years where all candidates in that year won a similar portion of the total vote. Specifically, let’s find all rows corresponding to a year where no candidate won more than 45% of the total vote.\nAn equivalent way of framing this goal is to say:\n\nFind the years where the maximum % in that year is less than 45%\nReturn all DataFrame rows that correspond to these years\n\nFor each year, we need to find the maximum % among all rows for that year. If this maximum % is lower than 45%, we will tell pandas to keep all rows corresponding to that year.\n\nelections.groupby(\"Year\").filter(lambda sf: sf[\"%\"].max() < 45).head(9)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      23\n      1860\n      Abraham Lincoln\n      Republican\n      1855993\n      win\n      39.699408\n    \n    \n      24\n      1860\n      John Bell\n      Constitutional Union\n      590901\n      loss\n      12.639283\n    \n    \n      25\n      1860\n      John C. Breckinridge\n      Southern Democratic\n      848019\n      loss\n      18.138998\n    \n    \n      26\n      1860\n      Stephen A. Douglas\n      Northern Democratic\n      1380202\n      loss\n      29.522311\n    \n    \n      66\n      1912\n      Eugene V. Debs\n      Socialist\n      901551\n      loss\n      6.004354\n    \n    \n      67\n      1912\n      Eugene W. Chafin\n      Prohibition\n      208156\n      loss\n      1.386325\n    \n    \n      68\n      1912\n      Theodore Roosevelt\n      Progressive\n      4122721\n      loss\n      27.457433\n    \n    \n      69\n      1912\n      William Taft\n      Republican\n      3486242\n      loss\n      23.218466\n    \n    \n      70\n      1912\n      Woodrow Wilson\n      Democratic\n      6296284\n      win\n      41.933422\n    \n  \n\n\n\n\nWhat’s going on here? In this example, we’ve defined our filtering function, \\(\\text{f}\\), to be lambda sf: sf[\"%\"].max() < 45. This filtering function will find the maximum \"%\" value among all entries in the grouped subframe, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped subframe will appear in the final output DataFrame.\nExamine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.\nYou may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections[\"%\"] < 45] will check the \"%\" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output."
  },
  {
    "objectID": "pandas_3/pandas_3.html#aggregating-data-with-pivot-tables",
    "href": "pandas_3/pandas_3.html#aggregating-data-with-pivot-tables",
    "title": "4  Pandas III",
    "section": "4.3 Aggregating Data with Pivot Tables",
    "text": "4.3 Aggregating Data with Pivot Tables\nWe know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of columns names to .groupby.\nLet’s consider the babynames dataset from last lecture. In this problem, we will find the total number of baby names associated with each sex for each year. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\n\nCode\nimport urllib.request\nimport os.path\n\n# Download data from the web directly\ndata_url = \"https://www.ssa.gov/oact/babynames/names.zip\"\nlocal_filename = \"data/babynames.zip\"\nif not os.path.exists(local_filename): # if the data exists don't download again\n    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n        f.write(resp.read())\n\n        \n# Load data without unzipping the file\nimport zipfile\nbabynames = [] \nwith zipfile.ZipFile(local_filename, \"r\") as zf:\n    data_files = [f for f in zf.filelist if f.filename[-3:] == \"txt\"]\n    def extract_year_from_filename(fn):\n        return int(fn[3:7])\n    for f in data_files:\n        year = extract_year_from_filename(f.filename)\n        with zf.open(f) as fp:\n            df = pd.read_csv(fp, names=[\"Name\", \"Sex\", \"Count\"])\n            df[\"Year\"] = year\n            babynames.append(df)\nbabynames = pd.concat(babynames)\n\n\n\nbabynames.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Mary\n      F\n      7065\n      1880\n    \n    \n      1\n      Anna\n      F\n      2604\n      1880\n    \n    \n      2\n      Emma\n      F\n      2003\n      1880\n    \n    \n      3\n      Elizabeth\n      F\n      1939\n      1880\n    \n    \n      4\n      Minnie\n      F\n      1746\n      1880\n    \n  \n\n\n\n\n\n# Find the total number of baby names associated with each sex for each year in the data\nbabynames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].agg(sum).head(6)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1880\n      F\n      90994\n    \n    \n      M\n      110490\n    \n    \n      1881\n      F\n      91953\n    \n    \n      M\n      100737\n    \n    \n      1882\n      F\n      107847\n    \n    \n      M\n      113686\n    \n  \n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multindex where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multindexes have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. You saw these back in Data 8. One set of values is used to create the index of the table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nimport numpy as np\nbabynames.pivot_table(index = \"Year\", columns = \"Sex\", values = \"Count\", aggfunc = np.sum).head(5)\n\n\n\n\n\n  \n    \n      Sex\n      F\n      M\n    \n    \n      Year\n      \n      \n    \n  \n  \n    \n      1880\n      90994\n      110490\n    \n    \n      1881\n      91953\n      100737\n    \n    \n      1882\n      107847\n      113686\n    \n    \n      1883\n      112319\n      104625\n    \n    \n      1884\n      129019\n      114442\n    \n  \n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = np.sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\"\n\nWe can even include multiple values in the index or columns of our pivot tables.\n\nbabynames_pivot = babynames.pivot_table(\n    index=\"Year\",     # the rows (turned into index)\n    columns=\"Sex\",    # the column values\n    values=[\"Count\", \"Name\"], \n    aggfunc=max,   # group operation\n)\nbabynames_pivot.head(6)\n\n\n\n\n\n  \n    \n      \n      Count\n      Name\n    \n    \n      Sex\n      F\n      M\n      F\n      M\n    \n    \n      Year\n      \n      \n      \n      \n    \n  \n  \n    \n      1880\n      7065\n      9655\n      Zula\n      Zeke\n    \n    \n      1881\n      6919\n      8769\n      Zula\n      Zeb\n    \n    \n      1882\n      8148\n      9557\n      Zula\n      Zed\n    \n    \n      1883\n      8012\n      8894\n      Zula\n      Zeno\n    \n    \n      1884\n      9217\n      9388\n      Zula\n      Zollie\n    \n    \n      1885\n      9128\n      8756\n      Zula\n      Zollie"
  },
  {
    "objectID": "pandas_3/pandas_3.html#joining-tables",
    "href": "pandas_3/pandas_3.html#joining-tables",
    "title": "4  Pandas III",
    "section": "4.4 Joining Tables",
    "text": "4.4 Joining Tables\nWhen working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can join two or more tables into a single DataFrame.\nTo put this into practice, we’ll revisit the elections dataset.\n\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nSay we want to understand the 2020 popularity of the names of each presidential candidate. To do this, we’ll need the combined data of babynames and elections.\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in babynames.\n\n# This `str` operation splits each candidate's full name at each \n# blank space, then takes just the candidiate's first name\nelections[\"First Name\"] = elections[\"Candidate\"].str.split().str[0]\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n      John\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n      John\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n    \n  \n\n\n\n\n\n# Here, we'll only consider `babynames` data from 2020\nbabynames_2020 = babynames[babynames[\"Year\"]==2020]\nbabynames_2020.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Olivia\n      F\n      17641\n      2020\n    \n    \n      1\n      Emma\n      F\n      15656\n      2020\n    \n    \n      2\n      Ava\n      F\n      13160\n      2020\n    \n    \n      3\n      Charlotte\n      F\n      13065\n      2020\n    \n    \n      4\n      Sophia\n      F\n      13036\n      2020\n    \n  \n\n\n\n\nNow, we’re ready to join the two tables. pd.merge is the pandas method used to join DataFrames together. The left and right parameters are used to specify the DataFrames to be joined. The left_on and right_on parameters are assigned to the string names of the columns to be used when performing the join. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.\n\nmerged = pd.merge(left = elections, right = babynames_2020, \\\n                  left_on = \"First Name\", right_on = \"Name\")\nmerged.head()\n# Notice that pandas automatically specifies `Year_x` and `Year_y` \n# when both merged DataFrames have the same column name to avoid confusion\n\n\n\n\n\n  \n    \n      \n      Year_x\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n      Name\n      Sex\n      Count\n      Year_y\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      Andrew\n      F\n      12\n      2020\n    \n    \n      1\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      Andrew\n      M\n      6036\n      2020\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n      Andrew\n      F\n      12\n      2020\n    \n    \n      3\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n      Andrew\n      M\n      6036\n      2020\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n      Andrew\n      F\n      12\n      2020"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "5  Data Cleaning and EDA",
    "section": "",
    "text": "6 EDA Demo: Tuberculosis in the United States\nNow, let’s follow this data-cleaning and EDA workflow to see what can we say about the presence of Tuberculosis in the United States!\nWe will examine the data included in the original CDC article published in 2021."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "5  Data Cleaning and EDA",
    "section": "5.1 Structure",
    "text": "5.1 Structure\n\n5.1.1 File Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for Comma-Separated Values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the first few rows of the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (Tab-Separated Values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\nï»¿Year Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\n5.1.2 Variable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nClassification of variable types\n\n\n\n\n5.1.3 Primary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "5  Data Cleaning and EDA",
    "section": "5.2 Granularity, Scope, and Temporality",
    "text": "5.2 Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is what a single row represents. You can also think of it as the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "5  Data Cleaning and EDA",
    "section": "5.3 Faithfulness",
    "text": "5.3 Faithfulness\nAt this stage in our data cleaning and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "eda/eda.html#csvs-and-field-names",
    "href": "eda/eda.html#csvs-and-field-names",
    "title": "5  Data Cleaning and EDA",
    "section": "6.1 CSVs and Field Names",
    "text": "6.1 CSVs and Field Names\nSuppose Table 1 was saved as a CSV file located in data/cdc_tuberculosis.csv.\nWe can then explore the CSV (which is a text file, and does not contain binary-encoded data) in many ways: 1. Using a text editor like emacs, vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only), Excel, Google Sheets, etc. 3. The Python file object 4. pandas, using pd.read_csv()\n1, 2. Let’s start with the first two so we really solidify the idea of a CSV as rectangular data (i.e., tabular data) stored as comma-separated values.\n\nNext, let’s try using the Python file object. Let’s check out the first three lines:\n\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(row)\n        i += 1\n        if i > 3:\n            break\n\n\n,No. of TB cases,,,TB incidence,,\n\nU.S. jurisdiction,2019,2020,2021,2019,2020,2021\n\nTotal,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\n\nAlabama,87,72,92,1.77,1.43,1.83\n\n\n\nWhoa, why are there blank lines interspaced between the lines of the CSV?\nYou may recall that all line breaks in text files are encoded as the special newline character \\n. Python’s print() prints each string (including the newline), and an additional newline on top of that.\nIf you’re curious, we can use the repr() function to return the raw string with all special characters:\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(repr(row)) # print raw strings\n        i += 1\n        if i > 3:\n            break\n\n\n',No. of TB cases,,,TB incidence,,\\n'\n'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\\n'\n'Total,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\\n'\n'Alabama,87,72,92,1.77,1.43,1.83\\n'\n\n\n\nFinally, let’s see the tried-and-true Data 100 approach: pandas.\n\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\")\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      No. of TB cases\n      Unnamed: 2\n      Unnamed: 3\n      TB incidence\n      Unnamed: 5\n      Unnamed: 6\n    \n  \n  \n    \n      0\n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.00\n      2020.00\n      2021.00\n    \n    \n      1\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      2\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      3\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      4\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n  \n\n\n\n\nWait, what’s up with the “Unnamed” column names? And the first row, for that matter?\nCongratulations – you’re ready to wrangle your data. Because of how things are stored, we’ll need to clean the data a bit to name our columns better.\nA reasonable first step is to identify the row with the right header. The pd.read_csv() function (documentation) has the convenient header parameter:\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1) # row index\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.1\n      2020.1\n      2021.1\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nWait…but now we can’t differentiate betwen the “Number of TB cases” and “TB incidence” year columns. pandas has tried to make our lives easier by automatically adding “.1” to the latter columns, but this doesn’t help us as humans understand the data.\nWe can do this manually with df.rename() (documentation):\n\n\nCode\nrename_dict = {'2019': 'TB cases 2019',\n               '2020': 'TB cases 2020',\n               '2021': 'TB cases 2021',\n               '2019.1': 'TB incidence 2019',\n               '2020.1': 'TB incidence 2020',\n               '2021.1': 'TB incidence 2021'}\ntb_df = tb_df.rename(columns=rename_dict)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28"
  },
  {
    "objectID": "eda/eda.html#record-granularity",
    "href": "eda/eda.html#record-granularity",
    "title": "5  Data Cleaning and EDA",
    "section": "6.2 Record Granularity",
    "text": "6.2 Record Granularity\nYou might already be wondering: What’s up with that first record?\nRow 0 is what we call a rollup record, or summary record. It’s often useful when displaying tables to humans. The granularity of record 0 (Totals) vs the rest of the records (States) is different.\nOkay, EDA step two. How was the rollup record aggregated?\nLet’s check if Total TB cases is the sum of all state TB cases. If we sum over all rows, we should get 2x the total cases in each of our TB cases by year (why?).\n\n\nCode\ntb_df.sum(axis=0)\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019        8,9008758183642,111666718245583029973261085237...\nTB cases 2020        7,1737258136591,706525417194122219282169239376...\nTB cases 2021        7,8609258129691,750585443194992281064255127494...\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nWhoa, what’s going on? Check out the column types:\n\n\nCode\ntb_df.dtypes\n\n\nU.S. jurisdiction     object\nTB cases 2019         object\nTB cases 2020         object\nTB cases 2021         object\nTB incidence 2019    float64\nTB incidence 2020    float64\nTB incidence 2021    float64\ndtype: object\n\n\nLooks like those commas are causing all TB cases to be read as the object datatype, or storage type (close to the Python string datatype), so pandas is concatenating strings instead of adding integers.\nFortunately read_csv also has a thousands parameter (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):\n\n\nCode\n# improve readability: chaining method calls with outer parentheses/line breaks\ntb_df = (\n    pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1, thousands=',')\n    .rename(columns=rename_dict)\n)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ntb_df.sum()\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019                                                    17800\nTB cases 2020                                                    14346\nTB cases 2021                                                    15720\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nThe Total TB cases look right. Phew!\nLet’s just look at the records with state-level granularity:\n\n\nCode\nstate_tb_df = tb_df[1:]\nstate_tb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n    \n      5\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46"
  },
  {
    "objectID": "eda/eda.html#gather-more-data-census",
    "href": "eda/eda.html#gather-more-data-census",
    "title": "5  Data Cleaning and EDA",
    "section": "6.3 Gather More Data: Census",
    "text": "6.3 Gather More Data: Census\nU.S. Census population estimates source (2019), source (2020-2021).\nRunning the below cells cleans the data. There are a few new methods here: * df.convert_dtypes() (documentation) conveniently converts all float dtypes into ints and is out of scope for the class. * df.drop_na() (documentation) will be explained in more detail next time.\n\n\nCode\n# 2010s census data\ncensus_2010s_df = pd.read_csv(\"data/nst-est2019-01.csv\", header=3, thousands=\",\")\ncensus_2010s_df = (\n    census_2010s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Census\", \"Estimates Base\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2010s_df['Geographic Area'] = census_2010s_df['Geographic Area'].str.strip('.')\n\n# with pd.option_context('display.min_rows', 30): # shows more rows\n#     display(census_2010s_df)\n    \ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      1\n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      2\n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      3\n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      4\n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nOccasionally, you will want to modify code that you have imported. To reimport those modifications you can either use the python importlib library:\nfrom importlib import reload\nreload(utils)\nor use iPython magic which will intelligently import code when files change:\n%load_ext autoreload\n%autoreload 2\n\n\nCode\n# census 2020s data\ncensus_2020s_df = pd.read_csv(\"data/NST-EST2022-POP.csv\", header=3, thousands=\",\")\ncensus_2020s_df = (\n    census_2020s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Unnamed: 1\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2020s_df['Geographic Area'] = census_2020s_df['Geographic Area'].str.strip('.')\n\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      1\n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      2\n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      3\n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      4\n      West\n      78650958\n      78589763\n      78743364"
  },
  {
    "objectID": "eda/eda.html#joining-data-on-primary-keys",
    "href": "eda/eda.html#joining-data-on-primary-keys",
    "title": "5  Data Cleaning and EDA",
    "section": "6.4 Joining Data on Primary Keys",
    "text": "6.4 Joining Data on Primary Keys\nTime to merge! Here we use the DataFrame method df1.merge(right=df2, ...) on DataFrame df1 (documentation). Contrast this with the function pd.merge(left=df1, right=df2, ...) (documentation). Feel free to use either.\n\n\nCode\n# merge TB dataframe with two US census dataframes\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .merge(right=census_2020s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      Geographic Area_x\n      2010\n      2011\n      ...\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      Geographic Area_y\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      Alabama\n      4785437\n      4799069\n      ...\n      4841799\n      4852347\n      4863525\n      4874486\n      4887681\n      4903185\n      Alabama\n      5031362\n      5049846\n      5074296\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      Alaska\n      713910\n      722128\n      ...\n      736283\n      737498\n      741456\n      739700\n      735139\n      731545\n      Alaska\n      732923\n      734182\n      733583\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      Arizona\n      6407172\n      6472643\n      ...\n      6730413\n      6829676\n      6941072\n      7044008\n      7158024\n      7278717\n      Arizona\n      7179943\n      7264877\n      7359197\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      Arkansas\n      2921964\n      2940667\n      ...\n      2967392\n      2978048\n      2989918\n      3001345\n      3009733\n      3017804\n      Arkansas\n      3014195\n      3028122\n      3045637\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      California\n      37319502\n      37638369\n      ...\n      38596972\n      38918045\n      39167117\n      39358497\n      39461588\n      39512223\n      California\n      39501653\n      39142991\n      39029342\n    \n  \n\n5 rows × 22 columns\n\n\n\nThis is a little unwieldy. We could either drop the unneeded columns now, or just merge on smaller census DataFrames. Let’s do the latter.\n\n\nCode\n# try merging again, but cleaner this time\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"Geographic Area\", \"2019\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n    .merge(right=census_2020s_df[[\"Geographic Area\", \"2020\", \"2021\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991"
  },
  {
    "objectID": "eda/eda.html#reproducing-data-compute-incidence",
    "href": "eda/eda.html#reproducing-data-compute-incidence",
    "title": "5  Data Cleaning and EDA",
    "section": "6.5 Reproducing Data: Compute Incidence",
    "text": "6.5 Reproducing Data: Compute Incidence\nLet’s recompute incidence to make sure we know where the original CDC numbers came from.\nFrom the CDC report: TB incidence is computed as “Cases per 100,000 persons using mid-year population estimates from the U.S. Census Bureau.”\nIf we define a group as 100,000 people, then we can compute the TB incidence for a given state population as\n\\[\\text{TB incidence} = \\frac{\\text{TB cases in population}}{\\text{groups in population}} = \\frac{\\text{TB cases in population}}{\\text{population}/100000} \\]\n\\[= \\frac{\\text{TB cases in population}}{\\text{population}} \\times 100000\\]\nLet’s try this for 2019:\n\n\nCode\ntb_census_df[\"recompute incidence 2019\"] = tb_census_df[\"TB cases 2019\"]/tb_census_df[\"2019\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n    \n  \n\n\n\n\nAwesome!!!\nLet’s use a for-loop and Python format strings to compute TB incidence for all years. Python f-strings are just used for the purposes of this demo, but they’re handy to know when you explore data beyond this course (Python documentation).\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.27864\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n      4.318807\n      4.470788\n    \n  \n\n\n\n\nThese numbers look pretty close!!! There are a few errors in the hundredths place, particularly in 2021. It may be useful to further explore reasons behind this discrepancy.\n\n\nCode\ntb_census_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      count\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      5.100000e+01\n      5.100000e+01\n      5.100000e+01\n      51.000000\n      51.000000\n      51.000000\n    \n    \n      mean\n      174.509804\n      140.647059\n      154.117647\n      2.102549\n      1.782941\n      1.971961\n      6.436069e+06\n      6.500226e+06\n      6.510423e+06\n      2.104969\n      1.784655\n      1.969928\n    \n    \n      std\n      341.738752\n      271.055775\n      286.781007\n      1.498745\n      1.337414\n      1.478468\n      7.360660e+06\n      7.408168e+06\n      7.394300e+06\n      1.500236\n      1.338263\n      1.474929\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.170000\n      0.000000\n      0.210000\n      5.787590e+05\n      5.776050e+05\n      5.794830e+05\n      0.172783\n      0.000000\n      0.210049\n    \n    \n      25%\n      25.500000\n      29.000000\n      23.000000\n      1.295000\n      1.210000\n      1.235000\n      1.789606e+06\n      1.820311e+06\n      1.844920e+06\n      1.297485\n      1.211433\n      1.233905\n    \n    \n      50%\n      70.000000\n      67.000000\n      69.000000\n      1.800000\n      1.520000\n      1.700000\n      4.467673e+06\n      4.507445e+06\n      4.506589e+06\n      1.808606\n      1.521612\n      1.694502\n    \n    \n      75%\n      180.500000\n      139.000000\n      150.000000\n      2.575000\n      1.990000\n      2.220000\n      7.446805e+06\n      7.451987e+06\n      7.502811e+06\n      2.577577\n      1.993607\n      2.219482\n    \n    \n      max\n      2111.000000\n      1706.000000\n      1750.000000\n      7.910000\n      7.920000\n      7.920000\n      3.951222e+07\n      3.950165e+07\n      3.914299e+07\n      7.928425\n      7.913519\n      7.899949"
  },
  {
    "objectID": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "href": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "title": "5  Data Cleaning and EDA",
    "section": "6.6 Bonus EDA: Reproducing the reported statistic",
    "text": "6.6 Bonus EDA: Reproducing the reported statistic\nHow do we reproduce that reported statistic in the original CDC report?\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nThis is TB incidence computed across the entire U.S. population! How do we reproduce this * We need to reproduce the “Total” TB incidences in our rolled record. * But our current tb_census_df only has 51 entries (50 states plus Washington, D.C.). There is no rolled record. * What happened…?\nLet’s get exploring!\nBefore we keep exploring, we’ll set all indexes to more meaningful values, instead of just numbers that pertained to some row at some point. This will make our cleaning slightly easier.\n\n\nCode\ntb_df = tb_df.set_index(\"U.S. jurisdiction\")\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ncensus_2010s_df = census_2010s_df.set_index(\"Geographic Area\")\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\ncensus_2020s_df = census_2020s_df.set_index(\"Geographic Area\")\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\nIt turns out that our merge above only kept state records, even though our original tb_df had the “Total” rolled record:\n\n\nCode\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nRecall that merge by default does an inner merge by default, meaning that it only preserves keys that are present in both DataFrames.\nThe rolled records in our census dataframes have different Geographic Area fields, which was the key we merged on:\n\n\nCode\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nThe Census DataFrame has several rolled records. The aggregate record we are looking for actually has the Geographic Area named “United States”.\nOne straightforward way to get the right merge is to rename the value itself. Because we now have the Geographic Area index, we’ll use df.rename() (documentation):\n\n\nCode\n# rename rolled record for 2010s\ncensus_2010s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\n# same, but for 2020s rename rolled record\ncensus_2020s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      Total\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\n\nNext let’s rerun our merge. Note the different chaining, because we are now merging on indexes (df.merge() documentation).\n\n\nCode\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"2019\"]],\n           left_index=True, right_index=True)\n    .merge(right=census_2020s_df[[\"2020\", \"2021\"]],\n           left_index=True, right_index=True)\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n  \n\n\n\n\n\nFinally, let’s recompute our incidences:\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n      2.711435\n      2.163726\n      2.367245\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.27864\n    \n  \n\n\n\n\nWe reproduced the total U.S. incidences correctly!\nWe’re almost there. Let’s revisit the quote:\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nRecall that percent change from \\(A\\) to \\(B\\) is computed as \\(\\text{percent change} = \\frac{B - A}{A} \\times 100\\).\n\n\nCode\nincidence_2020 = tb_census_df.loc['Total', 'recompute incidence 2020']\nincidence_2020\n\n\n2.1637257652759883\n\n\n\n\nCode\nincidence_2021 = tb_census_df.loc['Total', 'recompute incidence 2021']\nincidence_2021\n\n\n2.3672448914298068\n\n\n\n\nCode\ndifference = (incidence_2021 - incidence_2020)/incidence_2020 * 100\ndifference\n\n\n9.405957511804143"
  },
  {
    "objectID": "regex/regex.html#why-work-with-text",
    "href": "regex/regex.html#why-work-with-text",
    "title": "6  Regular Expressions",
    "section": "6.1 Why Work with Text?",
    "text": "6.1 Why Work with Text?\nLast lecture, we learned of the difference between quantitative and qualitative variable types. The latter includes string data - the primary focus of today’s lecture. In this note, we’ll discuss the necessary tools to manipulate text: Python string manipulation and regular expressions.\nThere are two main reasons for working with text.\n\nCanonicalization: Convert data that has multiple formats into a standard form.\n\nBy manipulating text, we can join tables with mismatched string labels\n\nExtract information into a new feature.\n\nFor example, we can extract date and time features from text"
  },
  {
    "objectID": "regex/regex.html#python-string-methods",
    "href": "regex/regex.html#python-string-methods",
    "title": "6  Regular Expressions",
    "section": "6.2 Python String Methods",
    "text": "6.2 Python String Methods\nFirst, we’ll introduce a few methods useful for string manipulation. The following table includes a number of string operations supported by Python and pandas. The Python functions operate on a single string, while their equivalent in pandas are vectorized - they operate on a Series of string data.\n\n\n\n\n\n\n\n\nOperation\nPython\nPandas (Series)\n\n\n\n\nTransformation\n\ns.lower(_)\ns.upper(_)\n\n\nser.str.lower(_)\nser.str.upper(_)\n\n\n\nReplacement + Deletion\n\ns.replace(_)\n\n\nser.str.replace(_)\n\n\n\nSplit\n\ns.split(_)\n\n\nser.str.split(_)\n\n\n\nSubstring\n\ns[1:4]\n\n\nser.str[1:4]\n\n\n\nMembership\n\n'_' in s\n\n\nser.str.contains(_)\n\n\n\nLength\n\nlen(s)\n\n\nser.str.len()\n\n\n\n\nWe’ll discuss the differences between Python string functions and pandas Series methods in the following section on canonicalization.\n\n6.2.1 Canonicalization\nAssume we want to merge the given tables.\n\n\nCode\nimport pandas as pd\n\nwith open('data/county_and_state.csv') as f:\n    county_and_state = pd.read_csv(f)\n    \nwith open('data/county_and_population.csv') as f:\n    county_and_pop = pd.read_csv(f)\n\n\n\ndisplay(county_and_state), display(county_and_pop);\n\n\n\n\n\n  \n    \n      \n      County\n      State\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n    \n    \n      1\n      Lac qui Parle County\n      MN\n    \n    \n      2\n      Lewis and Clark County\n      MT\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n    \n    \n      1\n      Lac Qui Parle\n      8067\n    \n    \n      2\n      Lewis & Clark\n      55716\n    \n    \n      3\n      St. John the Baptist\n      43044\n    \n  \n\n\n\n\nLast time, we used a primary key and foreign key to join two tables. While neither of these keys exist in our DataFrames, the County columns look similar enough. Can we convert these columns into one standard, canonical form to merge the two tables?\n\n6.2.1.1 Canonicalization with Python String Manipulation\nThe following function uses Python string manipulation to convert a single county name into canonical form. It does so by eliminating whitespace, punctuation, and unnecessary text.\n\ndef canonicalize_county(county_name):\n    return (\n        county_name\n            .lower()\n            .replace(' ', '')\n            .replace('&', 'and')\n            .replace('.', '')\n            .replace('county', '')\n            .replace('parish', '')\n    )\n\ncanonicalize_county(\"St. John the Baptist\")\n\n'stjohnthebaptist'\n\n\nWe will use the pandas map function to apply the canonicalize_county function to every row in both DataFrames. In doing so, we’ll create a new column in each called clean_county_python with the canonical form.\n\ncounty_and_pop['clean_county_python'] = county_and_pop['County'].map(canonicalize_county)\ncounty_and_state['clean_county_python'] = county_and_state['County'].map(canonicalize_county)\n\n\ndisplay(county_and_state), display(county_and_pop);\n\n\n\n\n\n  \n    \n      \n      County\n      State\n      clean_county_python\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n      dewitt\n    \n    \n      1\n      Lac qui Parle County\n      MN\n      lacquiparle\n    \n    \n      2\n      Lewis and Clark County\n      MT\n      lewisandclark\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n      clean_county_python\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n      dewitt\n    \n    \n      1\n      Lac Qui Parle\n      8067\n      lacquiparle\n    \n    \n      2\n      Lewis & Clark\n      55716\n      lewisandclark\n    \n    \n      3\n      St. John the Baptist\n      43044\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n6.2.1.2 Canonicalization with Pandas Series Methods\nAlternatively, we can use pandas Series methods to create this standardized column. To do so, we must call the .str attribute of our Series object prior to calling any methods, like .lower and .replace. Notice how these method names match their equivalent built-in Python string functions.\nChaining multiple Series methods in this manner eliminates the need to use the map function (as this code is vectorized).\n\ndef canonicalize_county_series(county_series):\n    return (\n        county_series\n            .str.lower()\n            .str.replace(' ', '')\n            .str.replace('&', 'and')\n            .str.replace('.', '')\n            .str.replace('county', '')\n            .str.replace('parish', '')\n    )\n\ncounty_and_pop['clean_county_pandas'] = canonicalize_county_series(county_and_pop['County'])\ncounty_and_state['clean_county_pandas'] = canonicalize_county_series(county_and_state['County'])\n\nC:\\Users\\phanm\\AppData\\Local\\Temp\\ipykernel_11612\\837021704.py:3: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\nC:\\Users\\phanm\\AppData\\Local\\Temp\\ipykernel_11612\\837021704.py:3: FutureWarning:\n\nThe default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n\n\n\n\ndisplay(county_and_pop), display(county_and_state);\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n      clean_county_python\n      clean_county_pandas\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n      dewitt\n      dewitt\n    \n    \n      1\n      Lac Qui Parle\n      8067\n      lacquiparle\n      lacquiparle\n    \n    \n      2\n      Lewis & Clark\n      55716\n      lewisandclark\n      lewisandclark\n    \n    \n      3\n      St. John the Baptist\n      43044\n      stjohnthebaptist\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      State\n      clean_county_python\n      clean_county_pandas\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n      dewitt\n      dewitt\n    \n    \n      1\n      Lac qui Parle County\n      MN\n      lacquiparle\n      lacquiparle\n    \n    \n      2\n      Lewis and Clark County\n      MT\n      lewisandclark\n      lewisandclark\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n      stjohnthebaptist\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n\n6.2.2 Extraction\nExtraction explores the idea of obtaining useful information from text data. This will be particularily important in model building, which we’ll study in a few weeks.\nSay we want to read some data from a .txt file.\n\nwith open('data/log.txt', 'r') as f:\n    log_lines = f.readlines()\n\n\nlog_lines\n\n['169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n',\n '193.205.203.3 - - [2/Feb/2005:17:23:6 -0800] \"GET /stat141/Notes/dim.html HTTP/1.0\" 404 302 \"http://eeyore.ucdavis.edu/stat141/Notes/session.html\"\\n',\n '169.237.46.240 - \"\" [3/Feb/2006:10:18:37 -0800] \"GET /stat141/homework/Solutions/hw1Sol.pdf HTTP/1.1\"\\n']\n\n\nSuppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Unfortunately, these items are not in a fixed position from the beginning of the string, so slicing by some fixed offset won’t work.\nInstead, we can use some clever thinking. Notice how the relevant information is contained within a set of brackets, further seperated by / and :. We can hone in on this region of text, and split the data on these characters. Python’s built-in .split function makes this easy.\n\nfirst = log_lines[0] # Only considering the first row of data\n\npertinent = first.split(\"[\")[1].split(']')[0]\nday, month, rest = pertinent.split('/')\nyear, hour, minute, rest = rest.split(':')\nseconds, time_zone = rest.split(' ')\nday, month, year, hour, minute, seconds, time_zone\n\n('26', 'Jan', '2014', '10', '47', '58', '-0800')\n\n\nThere are two problems with this code:\n\nPython’s built-in functions limit us to extract data one record at a time\n\nThis can be resolved using a map function or Pandas Series methods.\n\nThe code is quite verbose\n\nThis is a larger issue that is trickier to solve\n\n\nIn the next section, we’ll introduce regular expressions - a tool that solves problem 2."
  },
  {
    "objectID": "regex/regex.html#regex-basics",
    "href": "regex/regex.html#regex-basics",
    "title": "6  Regular Expressions",
    "section": "6.3 Regex Basics",
    "text": "6.3 Regex Basics\nA regular expression (“regex”) is a sequence of characters that specifies a search pattern. They are written to extract specific information from text. Regular expressions are essentially part of a smaller programming language embedded in Python, made available through the re module. As such, they have a stand-alone syntax and methods for various capabilities.\nRegular expressions are useful in many applications beyond data science. For example, Social Security Numbers (SSNs) are often validated with regular expresions.\n\nr\"[0-9]{3}-[0-9]{2}-[0-9]{4}\" # Regular Expression Syntax\n\n# 3 of any digit, then a dash,\n# then 2 of any digit, then a dash,\n# then 4 of any digit\n\n'[0-9]{3}-[0-9]{2}-[0-9]{4}'\n\n\n\n\nThere are a ton of resources to learn and experiment with regular expressions. A few are provided below:\n\nOfficial Regex Guide\nData 100 Reference Sheet\nRegex101.com\n\nBe sure to check Python under the category on the left.\n\n\n\n6.3.1 Basics Regex Syntax\nThere are four basic operations with regular expressions.\n\n\n\n\n\n\n\n\n\n\nOperation\nOrder\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nOr: |\n4\nAA|BAAB\nAA BAAB\nevery other string\n\n\nConcatenation\n3\nAABAAB\nAABAAB\nevery other string\n\n\nClosure: *  (zero or more)\n2\nAB*A\nAA ABBBBBBA\nAB  ABABA\n\n\nGroup: ()  (parenthesis)\n1\nA(A|B)AAB    (AB)*A\nAAAAB ABAAB    A  ABABABABA\nevery other string    AA  ABBA\n\n\n\nNotice how these metacharacter operations are ordered. Rather than being literal characters, these metacharacters manipulate adjacent characters. () takes precedence, followed by *, and finally |. This allows us to differentiate between very different regex commands like AB* and (AB)*. The former reads “A then zero or more copies of B”, while the latter specifies “zero or more copies of AB”.\n\n6.3.1.1 Examples\nQuestion 1: Give a regular expression that matches moon, moooon, etc. Your expression should match any even number of os except zero (i.e. don’t match mn).\nAnswer 1: moo(oo)*n\n\nHardcoding oo before the capture group ensures that mn is not matched.\nA capture group of (oo)* ensures the number of o’s is even.\n\nQuestion 2: Using only basic operations, formulate a regex that matches muun, muuuun, moon, moooon, etc. Your expression should match any even number of us or os except zero (i.e. don’t match mn).\nAnswer 2: m(uu(uu)*|oo(oo)*)n\n\nThe leading m and trailing n ensures that only strings beginning with m and ending with n are matched.\nNotice how the outer capture group surrounds the |.\n\nConsider the regex m(uu(uu)*)|(oo(oo)*)n. This incorrectly matches muu and oooon.\n\nEach OR clause is everything to the left and right of |. The incorrect solution matches only half of the string, and ignores either the beginning m or trailing n.\nA set of paranthesis must surround |. That way, each OR clause is everything to the left and right of | within the group. This ensures both the beginning m and trailing n are matched."
  },
  {
    "objectID": "regex/regex.html#regex-expanded",
    "href": "regex/regex.html#regex-expanded",
    "title": "6  Regular Expressions",
    "section": "6.4 Regex Expanded",
    "text": "6.4 Regex Expanded\nProvided below are more complex regular expression functions.\n\n\n\n\n\n\n\n\n\nOperation\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nAny Character: .  (except newline)\n.U.U.U.\nCUMULUS  JUGULUM\nSUCCUBUS TUMULTUOUS\n\n\nCharacter Class: []  (match one character in [])\n[A-Za-z][a-z]*\nword  Capitalized\ncamelCase 4illegal\n\n\nRepeated \"a\" Times: {a}\nj[aeiou]{3}hn\njaoehn  jooohn\njhn  jaeiouhn\n\n\nRepeated \"from a to b\" Times: {a, b}\nj[0u]{1,2}hn\njohn  juohn\njhn  jooohn\n\n\nAt Least One: +\njo+hn\njohn  joooooohn\njhn  jjohn\n\n\nZero or One: ?\njoh?n\njon  john\nany other string\n\n\n\nA character class matches a single character in it’s class. These characters can be hardcoded – in the case of [aeiou] – or shorthand can be specified to mean a range of characters. Examples include:\n\n[A-Z]: Any capitalized letter\n[a-z]: Any lowercase letter\n[0-9]: Any single digit\n[A-Za-z]: Any capitalized of lowercase letter\n[A-Za-z0-9]: Any capitalized or lowercase letter or single digit\n\n\n6.4.0.1 Examples\nLet’s analyze a few examples of complex regular expressions.\n\n\n\n\n\n\n\nMatches\nDoes Not Match\n\n\n\n\n\n.*SPB.*\n\n\n\n\nRASPBERRY  SPBOO\nSUBSPACE  SUBSPECIES\n\n\n\n[0-9]{3}-[0-9]{2}-[0-9]{4}\n\n\n\n\n231-41-5121  573-57-1821\n231415121  57-3571821\n\n\n\n[a-z]+@([a-z]+\\.)+(edu|com)\n\n\n\n\nhorse@pizza.com  horse@pizza.food.com\nfrank_99@yahoo.com  hug@cs\n\n\n\nExplanations\n\n.*SPB.* only matches strings that contain the substring SPB.\n\nThe .* metacharacter matches any amount of non-negative characters. Newlines do not count.\n\n\nThis regular expression matches 3 of any digit, then a dash, then 2 of any digit, then a dash, then 4 of any digit\n\nYou’ll recognize this as the familiar Social Security Number regular expression\n\nMatches any email with a com or edu domain, where all characters of the email are letters.\n\nAt least one . must preceed the domain name. Including a backslash \\ before any metacharacter (in this case, the .) tells regex to match that character exactly."
  },
  {
    "objectID": "regex/regex.html#convenient-regex",
    "href": "regex/regex.html#convenient-regex",
    "title": "6  Regular Expressions",
    "section": "6.5 Convenient Regex",
    "text": "6.5 Convenient Regex\nHere are a few more convenient regular expressions.\n\n\n\n\n\n\n\n\n\nOperation\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nbuilt in character class\n\\w+  \\d+ \\s+ \nFawef_03  231123  whitespace\nthis person 423 people non-whitespace\n\n\ncharacter class negation: [^] (everything except the given characters)\n[^a-z]+.\nPEPPERS3982 17211!↑å\nporch  CLAmS\n\n\nescape character: \\  (match the literal next character)\ncow\\.com\ncow.com\ncowscom\n\n\nbeginning of line: ^\n^ark\nark two ark o ark\ndark\n\n\nend of line: $\nark$\ndark  ark o ark\nark two\n\n\nlazy version of zero or more : *?\n5.*?5\n5005  55\n5005005\n\n\n\n\n6.5.0.1 Examples\nLet’s revist our earlier problem of extracting date/time data from the given .txt files. Here is how the data looked.\n\nlog_lines[0]\n\n'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n'\n\n\nQuestion: Give a regular expression that matches everything contained within and including the brackets - the day, month, year, hour, minutes, seconds, and timezone.\nAnswer: \\[.*\\]\n\nNotice how matching the literal [ and ] is necessary. Therefore, an escape character \\ is required before both [ and ] - otherwise these metacharacters will match character classes.\nWe need to match a particular format between [ and ]. For this example, .* will suffice.\n\nAlternative Solution: \\[\\w+/\\w+/\\w+:\\w+:\\w+:\\w+\\s-\\w+\\]\n\nThis solution is much safer.\n\nImagine the data between [ and ] was garbage - .* will still match that.\nThe alternate solution will only match data that follows the correct format."
  },
  {
    "objectID": "regex/regex.html#regex-in-python-and-pandas-regex-groups",
    "href": "regex/regex.html#regex-in-python-and-pandas-regex-groups",
    "title": "6  Regular Expressions",
    "section": "6.6 Regex in Python and Pandas (Regex Groups)",
    "text": "6.6 Regex in Python and Pandas (Regex Groups)\n\n6.6.1 Canonicalization\n\n6.6.1.1 Canonicalization with Regex\nEarlier in this note, we examined the process of canonicalization using Python string manipulation and pandas Series methods. However, we mentioned this approach had a major flaw: our code was unnecessarily verbose. Equipped with our knowledge of regular expressions, let’s fix this.\nTo do so, we need to understand a few functions in the re module. The first of these is the substitute function: re.sub(pattern, rep1, text). It behaves similarily to Python’s built-in .replace function, and returns text with all instances of pattern replaced by rep1.\nThe regular expression here removes text surrounded by <> (also known as HTML tags).\nIn order, the pattern matches … 1. a single < 2. any character that is not a > : div, td valign…, /td, /div 3. a single >\nAny substring in text that fulfills all three conditions will be replaced by ''.\n\nimport re\n\ntext = \"<div><td valign='top'>Moo</td></div>\"\npattern = r\"<[^>]+>\"\nre.sub(pattern, '', text) \n\n'Moo'\n\n\nNotice the r preceeding the regular expression pattern; this specifies the regular expression is a raw string. Raw strings do not recognize escape sequences (ie the Python newline metacharacter \\n). This makes them useful for regular expressions, which often contain literal \\ characters.\nIn other words, don’t forget to tag your regex with a r.\n\n\n6.6.1.2 Canonicalization with Pandas\nWe can also use regular expressions with Pandas Series methods. This gives us the benefit of operating on an entire column of data as opposed to a single value. The code is simple:  ser.str.replace(pattern, repl, regex=True).\nConsider the following DataFrame html_data with a single column.\n\n\nCode\ndata = {\"HTML\": [\"<div><td valign='top'>Moo</td></div>\", \\\n                 \"<a href='http://ds100.org'>Link</a>\", \\\n                 \"<b>Bold text</b>\"]}\nhtml_data = pd.DataFrame(data)\n\n\n\nhtml_data\n\n\n\n\n\n  \n    \n      \n      HTML\n    \n  \n  \n    \n      0\n      <div><td valign='top'>Moo</td></div>\n    \n    \n      1\n      <a href='http://ds100.org'>Link</a>\n    \n    \n      2\n      <b>Bold text</b>\n    \n  \n\n\n\n\n\npattern = r\"<[^>]+>\"\nhtml_data['HTML'].str.replace(pattern, '', regex=True)\n\n0          Moo\n1         Link\n2    Bold text\nName: HTML, dtype: object\n\n\n\n\n\n6.6.2 Extraction\n\n6.6.2.1 Extraction with Regex\nJust like with canonicalization, the re module provides capability to extract relevant text from a string:  re.findall(pattern, text). This function returns a list of all matches to pattern.\nUsing the familiar regular expression for Social Security Numbers:\n\ntext = \"My social security number is 123-45-6789 bro, or maybe it’s 321-45-6789.\"\npattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\nre.findall(pattern, text)  \n\n['123-45-6789', '321-45-6789']\n\n\n\n\n6.6.2.2 Extraction with Pandas\nPandas similarily provides extraction functionality on a Series of data: ser.str.findall(pattern)\nConsider the following DataFrame ssn_data.\n\n\nCode\ndata = {\"SSN\": [\"987-65-4321\", \"forty\", \\\n                \"123-45-6789 bro or 321-45-6789\",\n               \"999-99-9999\"]}\nssn_data = pd.DataFrame(data)\n\n\n\nssn_data\n\n\n\n\n\n  \n    \n      \n      SSN\n    \n  \n  \n    \n      0\n      987-65-4321\n    \n    \n      1\n      forty\n    \n    \n      2\n      123-45-6789 bro or 321-45-6789\n    \n    \n      3\n      999-99-9999\n    \n  \n\n\n\n\n\nssn_data[\"SSN\"].str.findall(pattern)\n\n0                 [987-65-4321]\n1                            []\n2    [123-45-6789, 321-45-6789]\n3                 [999-99-9999]\nName: SSN, dtype: object\n\n\nThis function returns a list for every row containing the pattern matches in a given string.\n\n\n\n6.6.3 Regular Expression Capture Groups\nEarlier we used parentheses ( ) to specify the highest order of operation in regular expressions. However, they have another meaning; paranthesis are often used to represent capture groups. Capture groups are essentially, a set of smaller regular expressions that match multiple substrings in text data.\nLet’s take a look at an example.\n\n6.6.3.1 Example 1\n\ntext = \"Observations: 03:04:53 - Horse awakens. \\\n        03:05:14 - Horse goes back to sleep.\"\n\nSay we want to capture all occurences of time data (hour, minute, and second) as seperate entities.\n\npattern_1 = r\"(\\d\\d):(\\d\\d):(\\d\\d)\"\nre.findall(pattern_1, text)\n\n[('03', '04', '53'), ('03', '05', '14')]\n\n\nNotice how the given pattern has 3 capture groups, each specified by the regular expression (\\d\\d). We then use re.findall to return these capture groups, each as tuples containing 3 matches.\nThese regular expression capture groups can be different. We can use the (\\d{2}) shorthand to extract the same data.\n\npattern_2 = r\"(\\d\\d):(\\d\\d):(\\d{2})\"\nre.findall(pattern_2, text)\n\n[('03', '04', '53'), ('03', '05', '14')]\n\n\n\n\n6.6.3.2 Example 2\nWith the notion of capture groups, convince yourself how the following regular expression works.\n\nfirst = log_lines[0]\nfirst\n\n'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n'\n\n\n\npattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\nday, month, year, hour, minute, second, time_zone = re.findall(pattern, first)[0]\nprint(day, month, year, hour, minute, second, time_zone)\n\n26 Jan 2014 10 47 58 -0800"
  },
  {
    "objectID": "regex/regex.html#limitations-of-regular-expressions",
    "href": "regex/regex.html#limitations-of-regular-expressions",
    "title": "6  Regular Expressions",
    "section": "6.7 Limitations of Regular Expressions",
    "text": "6.7 Limitations of Regular Expressions\nToday, we explored the capabilities of regular expressions in data wrangling with text data. However, there are a few things to be wary of.\nWriting regular expressions is like writing a program.\n\nNeed to know the syntax well.\nCan be easier to write than to read.\nCan be difficult to debug.\n\nRegular expressions are terrible at certain types of problems:\n\nFor parsing a hierarchical structure, such as JSON, use the json.load() parser, not regex!\nComplex features (e.g. valid email address).\nCounting (same number of instances of a and b). (impossible)\nComplex properties (palindromes, balanced parentheses). (impossible)"
  },
  {
    "objectID": "visualization_1/visualization_1.html#visualizations-in-data-8-and-data-100-so-far",
    "href": "visualization_1/visualization_1.html#visualizations-in-data-8-and-data-100-so-far",
    "title": "7  Visualization I",
    "section": "7.1 Visualizations in Data 8 and Data 100 (so far)",
    "text": "7.1 Visualizations in Data 8 and Data 100 (so far)\nYou’ve likely encountered several forms of data visualizations in your studies. You may remember two such examples from Data 8: line charts and histograms. Each of these served a unique purpose. For example, line charts displayed how numerical quantities changed over time, while histograms were useful in understanding a variable’s distribution.\n\n\nLine Chart\n\n\n\n\nHistogram"
  },
  {
    "objectID": "visualization_1/visualization_1.html#goals-of-visualization",
    "href": "visualization_1/visualization_1.html#goals-of-visualization",
    "title": "7  Visualization I",
    "section": "7.2 Goals of Visualization",
    "text": "7.2 Goals of Visualization\nVisualizations are useful for a number of reasons. In Data 100, we consider two areas in particular:\n\nTo broaden your understanding of the data\n\nKey part in exploratory data analysis.\nUseful in investigating relationships between variables.\n\nTo communicate results/conclusions to others\n\nVisualization theory is especially important here.\n\n\nOne of the most common applications of visualizations is in understanding a distribution of data."
  },
  {
    "objectID": "visualization_1/visualization_1.html#an-overview-of-distributions",
    "href": "visualization_1/visualization_1.html#an-overview-of-distributions",
    "title": "7  Visualization I",
    "section": "7.3 An Overview of Distributions",
    "text": "7.3 An Overview of Distributions\nA distribution describes the frequency of unique values in a variable. Distributions must satisfy two properties:\n\nEach data point must belong to only one category.\nThe total frequency of all categories must sum to 100%. In other words, their total count should equal the number of values in consideration.\n\n\n\nNot a Valid Distribution\n\n\n\n\nValid Distribution\n\n\n\nLeft Diagram: This is not a valid distribution since individuals can be associated to more than one category and the bar values demonstrate values in minutes and not probability\nRight Diagram: This example satisfies the two properties of distributions, so it is a valid distribution."
  },
  {
    "objectID": "visualization_1/visualization_1.html#bar-plots",
    "href": "visualization_1/visualization_1.html#bar-plots",
    "title": "7  Visualization I",
    "section": "7.4 Bar Plots",
    "text": "7.4 Bar Plots\nAs we saw above, a bar plot is one of the most common ways of displaying the distribution of a qualitative (categorical) variable. The length of a bar plot encodes the frequency of a category; the width encodes no useful information.\nLet’s contextualize this in an example. We will use the familiar births dataset from Data 8 in our analysis.\n\n\nCode\nimport pandas as pd\n\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nWe can visualize the distribution of the Maternal Smoker column using a bar plot. There are a few ways to do this.\n\n7.4.1 Plotting in Pandas\n\nbirths['Maternal Smoker'].value_counts().plot(kind = 'bar');\n\n\n\n\nRecall that .value_counts() returns a Series with the total count of each unique value. We call .plot(kind = 'bar') on this result to visualize these counts as a bar plot.\nPlotting methods in pandas are the least preferred and not supported in Data 100, as their functionality is limited. Instead, future examples will focus on other libaries built specifically for visualizing data. The most well-known library here is matplotlib.\n\n\n7.4.2 Plotting in Matplotlib\n\nimport matplotlib.pyplot as plt\n\nms = births['Maternal Smoker'].value_counts()\nplt.bar(ms.index.astype('string'), ms)\nplt.xlabel('Maternal Smoker')\nplt.ylabel('Count');\n\n\n\n\nWhile more code is required to achieve the same result, matplotlib is often used over pandas for its ability to plot more complex visualizations, some of which are discussed shortly.\nHowever, notice how we need to explicitly specify the type of the value for the x-axis to string. In absence of conversion, the x-axis will be a range of integers rather than the two categories, True and False. This is because matplotlib coerces True to a value of 1 and False to 0. Also, note how we needed to label the axes with plt.xlabel and plt.ylabel - matplotlib does not support automatic axis labeling. To get around these inconveniences, we can use a more effecient plotting library, seaborn.\n\n\n7.4.3 Plotting in Seaborn\n\nimport seaborn as sns\nsns.countplot(data = births, x = 'Maternal Smoker');\n\n\n\n\nseaborn.countplot both counts and visualizes the number of unique values in a given column. This column is specified by the x argument to sns.countplot, while the DataFrame is specified by the data argument.\nFor the vast majority of visualizations, seaborn is far more concise and aesthetically pleasing than matplotlib. However, the color scheme of this particular bar plot is abritrary - it encodes no additional information about the categories themselves. This is not always true; color may signify meaningful detail in other visualizations. We’ll explore this more in-depth during the next lecture.\n\n\n7.4.4 Plotting in Plotly\n\nplotly is one of the most versatile plottling libraries and widely used in industry. However, plotly has various dependencies that make it difficult to support in Data 100. Therfore, we have intentionally excluded the code to generate the plot above.\nBy now, you’ll have noticed that each of these plotting libraries have a very different syntax. As with pandas, we’ll teach you the important methods in matplotlib and seaborn, but you’ll learn more through documentation.\n\nMatplotlib Documentation\nSeaborn Documentation\n\nExample Questions:\n\nWhat colors should we use?\nHow wide should the bars be?\nShould the legend exist?\nShould the bars and axes have dark borders?\n\nTo accomplish goal 2, here are some ways we can improve plot:\n\nIntroducing different colors for each bar\nIncluding a legend\nIncluding a title\nLabeling the y-axis\nUsing color-blind friendly palettes\nRe-orienting the labels\nIncrease the font size"
  },
  {
    "objectID": "visualization_1/visualization_1.html#histograms",
    "href": "visualization_1/visualization_1.html#histograms",
    "title": "7  Visualization I",
    "section": "7.5 Histograms",
    "text": "7.5 Histograms\nHistograms are a natural extension to bar plots; they visualize the distribution of quantitative (numerical) data.\nRevisiting our example with the births DataFrame, let’s plot the distribution of the Maternal Pregnancy Weight column.\n\n\nCode\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nHow should we define our categories for this variable? In the previous example, these were the unique values of the Maternal Smoker column: True and False. If we use similar logic here, our categories are the different numerical weights contained in the Maternal Pregnancy Weight column.\nUnder this assumption, let’s plot this distribution using the seaborn.countplot function.\n\nsns.countplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis histogram clearly suffers from overplotting. This is somewhat expected for Maternal Pregnancy Weight - it is a quantitative variable that takes on a wide range of values.\nTo combat this problem, statisticians use bins to categorize numerical data. Luckily, seaborn provides a helpful plotting function that automatically bins our data.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis diagram is known as a histogram. While it looks more reasonable, notice how we lose fine-grain information on the distribution of data contained within each bin. We can introduce rug plots to minimize this information loss. An overlaid “rug plot” displays the within-bin distribution of our data, as denoted by the thickness of the colored line on the x-axis.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\nsns.rugplot(data = births, x = 'Maternal Pregnancy Weight', color = 'red');\n\n\n\n\nYou may have seen histograms drawn differently - perhaps with an overlaid density curve and normalized y-axis. We can display both with a few tweaks to our code.\nTo visualize a density curve, we can set the the kde = True argument of the sns.histplot. Setting the argument stat = 'density' normalizes our histogram and displays densities, instead of counts, on the y-axis. You’ll notice that the area under the density curve is 1.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight', kde = True, \n             stat = 'density')\nsns.rugplot(data = births, x = 'Maternal Pregnancy Weight', color = 'red');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#evaluating-histograms",
    "href": "visualization_1/visualization_1.html#evaluating-histograms",
    "title": "7  Visualization I",
    "section": "7.6 Evaluating Histograms",
    "text": "7.6 Evaluating Histograms\nHistograms allow us to assess a distribution by their shape. There are a few properties of histograms we can analyze:\n\nSkewness and Tails\n\nSkewed left vs skewed right\nLeft tail vs right tail\n\nOutliers\n\nDefined arbitrarily for now\n\nModes\n\nMost commonly occuring data\n\n\n\n7.6.1 Skewness and Tails\nIf a distribution has a long right tail (such as Maternal Pregancy Weight), it is skewed right. In a right-skewed distribution, the few large outliers “pull” the mean to the right of the median.\nIf a distribution has a long left tail, it is skewed left. In a left-skewed distribution, the few small outliers “pull” the mean to the left of the median.\nIn the case where a distribution has equal-sized right and left tails, it is symmetric. The mean is approximately equal to the median. Think of mean as the balancing point of the distribution\n\nimport numpy as np\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\ndf_mean = np.mean(births['Maternal Pregnancy Weight'])\ndf_median = np.median(births['Maternal Pregnancy Weight'])\nprint(\"The mean is: {} and the median is {}\".format(df_mean,df_median))\n\nThe mean is: 128.4787052810903 and the median is 125.0\n\n\n\n\n\n\n\n7.6.2 Outliers\nLoosely speaking, an outlier is defined as a data point that lies an abnormally large distance away from other values. We’ll define the statistical measure for this shortly.\nOutliers disproportionately influce the mean because their magnitude is directly involved in computing the average. However, the median is largely unaffected - the magnitude of an outlier is irrelevant; we only care that it is some non-zero distance away from the midpoint of the data.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\n## Where do we draw the line of outlier? \nplt.axvline(df_mean*1.75, color = 'red');\n\n\n\n\n\n\n7.6.3 Modes\nA mode of a distribution is a local or global maximum. A distribution with a single clear maximum is unimodal, distributions with two modes are bimodal, and those with 3 or more are multimodal. You need to distinguish between modes and random noise.\nFor example, the distribution of birth weights for maternal smokers is (weakly) multimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\nsns.histplot(data = births_maternal_smoker, x = 'Maternal Pregnancy Weight')\\\n            .set(title = 'Maternal Smoker histogram');\n\n\n\n\nOn the other hand, the distribution of birth weights for maternal non-smokers is weakly bi-modal.\n\nbirths_maternal_non_smoker = births[births['Maternal Smoker'] == False]\nsns.histplot(data = births_maternal_non_smoker, x = 'Maternal Pregnancy Weight')\\\n            .set(title = 'Maternal Non-Smoker histogram');\n\n\n\n\nHowever, changing the bins reveals that the data is not bi-modal.\n\nsns.histplot(data = births_maternal_non_smoker, x = 'Maternal Pregnancy Weight',\\\n             bins = 20);"
  },
  {
    "objectID": "visualization_1/visualization_1.html#density-curves",
    "href": "visualization_1/visualization_1.html#density-curves",
    "title": "7  Visualization I",
    "section": "7.7 Density Curves",
    "text": "7.7 Density Curves\nInstead of a discrete histogram, we can visualize what a continuous distribution corresponding to that same data could look like using a curve. - The smooth curve drawn on top of the histogram here is called a density curve.\nIn lecture 8, we will study how exactly to compute these density curves (using a technique is called Kernel Density Estimation).\nIf we plot birth weights of babies of smoking mothers, we get a histogram that appears bimodal.\n\nDensity curve reinforces belief in this bimodality.\n\nHowever, if we plot birth weights of babies of non-smoking mothers, we get a histogram that appears unimodal.\nFrom a goal 1 perspective, this is EDA which tells us there may be something interesting here worth pursuing.\n\nDeeper analysis necessary!\nIf we found something truly interesting, we’d have to cautiously write up an argument and create goal 2 level visualizations.\n\n\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\n \nsns.histplot(data = births_maternal_smoker , x = 'Birth Weight',\\\n             kde = True);\n\n\n\n\n\nsns.histplot(data = births_non_maternal_smoker , x = 'Birth Weight',\\\n             kde = True);\n\n\n\n\n\n7.7.1 Histograms and Density\nRather than labeling by counts, we can instead plot the density, as shown below. Density gives us a measure that is invariant to the total number of observed units. The numerical values on the Y-axis for a sample of 100 units would be the same for when we observe a sample of 10000 units instead. We can still calculate the absolute number of observed units using density.\nExample: There are 1174 observations total. - Total area of this bin should be: 120/1174 = ~10% - Density of this bin is therefore: 10% / (115 - 110) = 0.02"
  },
  {
    "objectID": "visualization_1/visualization_1.html#box-plots-and-violin-plots",
    "href": "visualization_1/visualization_1.html#box-plots-and-violin-plots",
    "title": "7  Visualization I",
    "section": "7.8 Box Plots and Violin Plots",
    "text": "7.8 Box Plots and Violin Plots\n\n7.8.1 Boxplots\nBoxplots are an alternative to histograms that visualize numerical distributions. They are especially useful in graphicaly summarizing several characteristics of a distribution. These include:\n\nLower Quartile (\\(1^{st}\\) Quartile)\nMedian (\\(2^{nd}\\) Quartile)\nUpper Quartile (\\(3^{rd}\\) Quartile)\nInterquartile Range (IQR)\nWhiskers\nOutliers\n\nThe lower quartile, median, and uper quartile are the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentiles of data, respectively. The interquartile range measures the spread of the middle \\(50\\)% of the distribution, calculated as the (\\(3^{rd}\\) Quartile \\(-\\) \\(1^{st}\\) Quartile).\nThe whiskers of a box-plot are the two points that lie at the [\\(1^{st}\\) Quartile \\(-\\) (\\(1.5\\times\\) IQR)], and the [\\(3^{rd}\\) Quartile \\(+\\) (\\(1.5\\times\\) IQR)]. They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than (\\(1.5 \\times\\) IQR) from the extreme quartiles.\nLet’s visualize a box-plot of the Birth Weight column.\n\n\nCode\nsns.boxplot(data = births, y = 'Birth Weight');\n\nbweights = births['Birth Weight']\nq1 = np.percentile(bweights, 25)\nq2 = np.percentile(bweights, 50)\nq3 = np.percentile(bweights, 75)\niqr = q3 - q1\nwhisk1 = q1 - (1.5 * iqr)\nwhisk2 = q3 + (1.5 * iqr)\n\nprint(\"The first quartile is {}\".format(q1))\nprint(\"The second quartile is {}\".format(q2))\nprint(\"The third quartile is {}\".format(q3))\nprint(\"The interquartile range is {}\".format(iqr))\nprint(\"The whiskers are {} and {}\".format(whisk1, whisk2))\n\n\nThe first quartile is 108.0\nThe second quartile is 120.0\nThe third quartile is 131.0\nThe interquartile range is 23.0\nThe whiskers are 73.5 and 165.5\n\n\n\n\n\nHere is a helpful visual that summarizes our discussion above.\n\n\n\n7.8.2 Violin Plots\nAnother diagram that is useful in visualizing a variable’s distribution is the violin plot. A violin plot supplements a box-plot with a smoothed density curve on either side of the plot. These density curves highlight the relative frequency of variable’s possible values. If you look closely, you’ll be able to discern the quartiles, whiskers, and other hallmark features of the box-plot.\n\nsns.violinplot(data = births, y = 'Birth Weight');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#comparing-quantitative-distributions",
    "href": "visualization_1/visualization_1.html#comparing-quantitative-distributions",
    "title": "7  Visualization I",
    "section": "7.9 Comparing Quantitative Distributions",
    "text": "7.9 Comparing Quantitative Distributions\nEarlier in our discussion of the mode, we visualized two histograms that described the distribution of birth weights for maternal smokers and non-smokers. However, comparing these histograms was difficult because they were displayed on seperate plots. Can we overlay the two to tell a more compelling story?\nIn seaborn, multiple calls to a plotting library in the same code cell will overlay the plots. For example:\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == True]\n\nsns.histplot(data = births_maternal_smoker, x = 'Birth Weight',\n             color = 'orange', label = 'smoker')\nsns.histplot(data = births_non_maternal_smoker, x = 'Birth Weight',\n             color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nHowever, notice how this diagram suffers from overplotting. We can fix this with a call to sns.kdeplot. This will remove the bins and overlay the histogram with a density curve that better summarizes the distribution.\n\nsns.kdeplot(data = births_maternal_smoker, x = 'Birth Weight', color = 'orange', label = 'smoker')\nsns.kdeplot(data = births_non_maternal_smoker, x = 'Birth Weight', color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nUnfortunately, we lose critical information in our distribution by removing small details. Therefore, we typically prefer to use box-plots and violin plots when comparing distributions. These are more concise and allow us to compare summary statistics across many distributions.\n\nsns.violinplot(data = births, x = 'Maternal Smoker', y = 'Birth Weight');\n\n\n\n\n\nsns.boxplot(data=births, x = 'Maternal Smoker', y = 'Birth Weight');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#ridge-plots",
    "href": "visualization_1/visualization_1.html#ridge-plots",
    "title": "7  Visualization I",
    "section": "7.10 Ridge Plots",
    "text": "7.10 Ridge Plots\nRidge plots show many density curves offset from one another with minimal overlap. They are useful when the specific shape of each curve is important."
  },
  {
    "objectID": "visualization_2/visualization_2.html#kernel-density-functions",
    "href": "visualization_2/visualization_2.html#kernel-density-functions",
    "title": "8  Visualization II",
    "section": "8.1 Kernel Density Functions",
    "text": "8.1 Kernel Density Functions\n\n8.1.1 KDE Mechanics\nIn the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we’ll learn how to construct density curves using Kernel Density Estimation (KDE).\n\n8.1.1.1 Smoothing\nKernel Density Estimation involves a technique called smoothing - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\nMany of the visualizations we learned during the last lecture are examples of smoothing. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the larger properties of our distribution.\n\n\nCode\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\nsns.rugplot(titanic['age'],height = 0.5);\n\n\n\n\n\n\nsns.histplot(titanic['age']);\n\n\n\n\n\n\n8.1.1.2 Kernel Density Estimation\nKernel Density Estimation is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\nPlace a kernel at each data point\nNormalize kernels to have total area of 1 (across all kernels)\nSum kernels together\n\nSuppose we have 5 data points: \\([2.2, 2.8, 3.7, 5.3, 5.7]\\). We wish to recreate the following Kernel Density Estimate:\n\n\nCode\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n\n\n\n\n\nLet’s walk through each step to construct this density curve.\n\n8.1.1.2.1 Step 1 - Place a Kernel at Each Data Point\nTo begin generating a density curve, we need to choose a kernel and bandwidth value (\\(\\alpha\\)). What are these exactly? A kernel is a density curve itself, and the bandwidth (\\(\\alpha\\)) is a measure of the kernel’s width. Recall that a valid density has an area of 1.\nAt each of our 5 points (depicted in the rug plot on the left), we’ve placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We’ll explore what these are in the next section.\n\n\nRugplot of Data\n\n\n\n\nKernelized Data\n\n\n\n\n\n8.1.1.2.2 Step 2 - Normalize Kernels to Have Total Area of 1\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by \\(\\frac{1}{5}\\).\n\n\nKernelized Data\n\n\n\n\nNormalized Kernels\n\n\n\n\n\n8.1.1.2.3 Step 3 - Sum Kernels Together\nOur kernel density estimate (KDE) is the sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n\nNormalized Kernels\n\n\n\n\nKernel Density Estimate\n\n\n\n\n\n\n\n8.1.2 Kernel Functions and Bandwidth\n\n8.1.2.1 Kernels\nA kernel (for our purposes) is a valid density function. This means it:\n\nMust be non-negative for all inputs.\nMust integrate to 1.\n\n\nA general “KDE formula” function is given above.\n\n\\(K_{\\alpha}(x, xi)\\) is the kernel centered on the observation i.\n\nEach kernel individually has area 1.\nx represents any number on the number line. It is the input to our function.\n\n\\(n\\) is the number of observed data points that we have.\n\nWe multiply by \\(\\frac{1}{n}\\) so that the total area of the KDE is still 1.\n\nEach \\(x_i \\in \\{x_1, x_2, \\dots, x_n\\}\\) represents an observed data point.\n\nThese are what we use to create our KDE by summing multiple shifted kernels centered at these points.\n\n\n*\\(\\alpha\\) (alpha) is the bandwidth or smoothing parameter.\n\n8.1.2.1.1 Gaussian Kernel\nThe most common kernel is the Gaussian kernel. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value \\(x_i\\) with a standard deviation of \\(\\alpha\\) (this is known as the bandwidth parameter).\n\\(K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2\\alpha^{2}}}\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Gaussian kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nIf you’ve taken a probability class, you’ll recognize that the mean of this Gaussian kernel is \\(x_i\\) and the standard deviation is \\(\\alpha\\). Increasing \\(\\alpha\\) - equivalently, the bandwidth - smoothens the density curve. Larger values of \\(\\alpha\\) are typically easier to understand; however, we begin to lose important distributional information.\nHere is how adjusting \\(\\alpha\\) affects a distribution in some variable from an arbitrary dataset.\n\n\nGaussian Kernel, \\(\\alpha\\) = 0.1\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 1\n\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 2\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 10\n\n\n\n\n\n8.1.2.1.2 Boxcar Kernel\nAnother example of a kernel is the Boxcar kernel. The boxcar kernel assigns a uniform density to points within a “window” of the observation, and a density of 0 elsewhere. The equation below is a Boxcar kernel with the center at \\(x_i\\) and the bandwidth of \\(\\alpha\\).\n\\(K_a(x, x_i) = \\begin{cases}  \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\  0, & \\text{else }  \\end{cases}\\)\n\n\nCode\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Boxcar kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Relationships Between Quantitative Variables\nUp until now, we’ve discussed how to visualize single-variable distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n\n8.1.3.1 Scatter Plots\nScatter plots are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\nFor example, let’s plot a scatter plot comparing the Maternal Height and Birth Weight colums, using both matplotlib and seaborn.\n\nimport pandas as pd\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n# Matplotlib Example\nplt.scatter(births['Maternal Height'], births['Birth Weight'])\nplt.xlabel('Maternal Height')\nplt.ylabel('Birth Weight');\n\n\n\n\n\n# Seaborn Example\nsns.scatterplot(data = births, x = 'Maternal Height', y = 'Birth Weight',\n                hue = 'Maternal Smoker')\n\n<AxesSubplot:xlabel='Maternal Height', ylabel='Birth Weight'>\n\n\n\n\n\nThis is an example where color is used to add a third dimension to our plot. This is possible with the hue paramater in seaborn, which adds a categorical column encoding to an existing visualization. This way, we can look for relationships in Maternal Height and Birth Weight in both maternal smokers and non-smokers. If we wish to see the relationship’s strength more clearly, we can use sns.lmplot.\n\nsns.lmplot(data = births, x = 'Maternal Height', y = 'Birth Weight', \n           hue = 'Maternal Smoker', ci = False);\n\n\n\n\nWe can make out a weak, positive relationship in the mother’s height and birth weight for both maternal smokers and non-smokers (the baseline is slightly lower in maternal smokers).\n\n\n\n8.1.4 Overplotting\nAs you may have noticed, the scatterplots of Maternal Height vs. Birth Weight have many densely plotted areas. Many of the points are on top of one other! This makes it difficult to tell exactly how many babies are plotted in each the more densely populated regions of the graph. This can arise when the tools used for measuring data have low granularity, many different values are rounded to the same value, or if the ranges of the two variables differ greatly in scale.\nWe can overcome this by introducing a small amount of uniform random noise to our data. This is called jittering. Let’s see what happens when we introduce noise to the Maternal Height.\n\nbirths[\"Maternal Height (jittered)\"] = births[\"Maternal Height\"] + np.random.uniform(-0.2, 0.2, len(births))\nsns.lmplot(data = births, x = 'Maternal Height (jittered)', y = 'Birth Weight', \n           hue = 'Maternal Smoker', ci = False);\n\n\n\n\nThis plot more clearly shows that most of the data is clustered tightly around the point (62.5,120) and gradually becomes more loose further away from the center. It is much easier for us and others to see how the data is distributed. In conclusion, jittering helps us better understand our own data (Goal 1) and communicate results to others (Goal 2).\n\n8.1.4.1 Hex Plots and Contour Plots\nUnfortunately, our scatter plots above suffered from overplotting, which made them hard to interpret. And with a large number of points, jittering is unlikely to resolve the issue. Instead, we can look to hex plots and contour plots.\nHex Plots can be thought of as a two dimensional histogram that shows the joint distribution between two variables. This is particularly useful working with very dense data.\n\nsns.jointplot(data = births, x = 'Maternal Pregnancy Weight', \n              y = 'Birth Weight', kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x236e8a4a8b0>\n\n\n\n\n\nThe axes are evidently binned into hexagons, which makes the linear relationship easier to decipher. Darker regions generally indicate a higher density of points.\nOn the other hand, contour plots are two dimensional versions of density curves with marginal distributions of each variable on the axes. We’ve used very similar code here to generate our contour plots, with the addition of the kind = 'kde' and fill = True arguments.\n\nsns.jointplot(data = births, x = 'Maternal Height', y = 'Birth Weight',\\\n              kind = 'kde', fill = True)\n\n<seaborn.axisgrid.JointGrid at 0x236e7c11c40>"
  },
  {
    "objectID": "visualization_2/visualization_2.html#transformations",
    "href": "visualization_2/visualization_2.html#transformations",
    "title": "8  Visualization II",
    "section": "8.2 Transformations",
    "text": "8.2 Transformations\nThese last two lectures have covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\nMuch of this was done to uncover insights in data, which will prove necessary for the modeling process. A strong graphical correlation between two variables hinted an underlying relationship that has reason for further study. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\nTransformations are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that “transform” their range of possible values and highlight some previously hidden associations between data.\n\n8.2.0.1 Transforming a Distribution\nWhen a distribution has a large dynamic range, it can be useful to take the logarithm of the data. For example, computing the logarithm of the ticket prices on the Titanic reduces skeweness and yields a distribution that is more “spread” across the x-axis. While it makes individual observations harder to interpret, the distribution is more favorable for subsequent analysis.\n\n\n\n\n\n\n\n\n\n\n\n8.2.0.2 Linearizing a Relationship\nTransformations are perhaps most useful to linearize a relationship between variables. If we find a transformation to make a scatter plot of two variables linear, we can “backtrack” to find the exact relationship between the variables. Linear relationships are particularly simple to interpret, and we’ll be doing a lot of linear modeling in Data 100 - starting next week!\nSay we want to understand the relationship between healthcare and life expectancy. Intuitively there should be a positive correlation, but upon plotting values from a dataset, we find a non-linear relationship that is somewhat hard to understand. However, applying a logarithmic transformation to both variables - healthcare and life expectancy - results in a scatter plot with a linear trend that we can interpret.\n\n\n\n\n\n\n\n\n\nHow can we find the relationship between the original variables? We know that taking a log of both axes gave us a linear relationship, so we can say (roughly) that\n\\[\\log y= a\\times\\log x + b\\]\nSolving for \\(y\\) implies a power relationship in the original plot.\n\\[y= e^{a\\times\\log x + b}\\] \\[y= Ce^{a\\times\\log x}\\] \\[y= Cx^{a}\\]\nHow did we know that taking the logarithm of both sides would result in a linear relationship? The Tukey-Mosteller Bulge Diagram is helpful here. We can use the direction of the buldge in our original data to find the appropriate transformations that will linearize the relationship. These transformations are found on axes that are nearest to the buldge. The buldge in our earlier example lay in Quadrant 2, so the transformations \\(\\log x\\), \\(\\sqrt x\\), \\(y^{2}\\), or \\(y^{3}\\) are possible contenders. It’s important to note that this diagram is not perfect, and some transformations will work better than others. In our case, \\(\\log x\\) and \\(\\log y\\) (found in Quadrant 3) were the best.\n\n\n\n8.2.0.3 Additional Remarks\nVisualization requires a lot of thought! - There are many tools for visualizing distributions. - Distribution of a single variable: 1. rug plot 2. histogram 3. density plot 4. box plot 5. violin plot - Joint distribution of two quantitative variables: 1. scatter plot 2. hex plot 3. contour plot.\nThis class primarily uses seaborn and matplotlib, but Pandas also has basic built-in plotting methods. Many other visualization libraries exist, and plotly is one of them. - plotly creates very easily creates interactive plots. - plotly will occasionally appear in lecture code, labs, and assignments!\nNext, we’ll go deeper into the theory behind visualization."
  },
  {
    "objectID": "visualization_2/visualization_2.html#visualization-theory",
    "href": "visualization_2/visualization_2.html#visualization-theory",
    "title": "8  Visualization II",
    "section": "8.3 Visualization Theory",
    "text": "8.3 Visualization Theory\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We’ll discuss the abstract nature of visualizations and analyze how they convey information.\nRemember, we had two goals for visualizing data. This section is particularly important in:\n\nHelping us understand the data and results\nCommunicating our results and conclusions with others\n\n\n8.3.1 Information Channels\nVisualizations are able to convey information through various encodings. In the remainder of this lecture, we’ll look at the use of color, scale, and depth, to name a few.\n\n8.3.1.1 Encodings in Rugplots\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n\n\n\n8.3.1.2 Multi-Dimensional Encodings\nEncodings are also useful for representing multi-dimensional data. Notice how the following visual highlights four distinct “dimensions” of data:\n\nX-axis\nY-axis\nArea\nColor\n\n\nThe human visual perception sytem is only capable of visualizing data in a three-dimensional plane, but as you’ve seen, we can encode many more channels of information.\n\n\n\n8.3.2 Harnessing the Axes\n\n8.3.2.1 Consider Scale of the Data\nHowever, we should be careful to not misrepresent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n\nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not be intersecting.\nWhen there is a large difference in the magnitude of the data, it’s advised to analyze percentages instead of counts. The following diagrams correctly display the trends in cancer screening and abortion rates.\n\n\n\n\n\n\n\n\n\n\n\n8.3.2.2 Reveal the Data\nGreat visualizations not only consider the scale of the data, but also utilize the axes in a way that best conveys information. For example, data scientists commonly set certain axes limits to highlight parts of the visualization they are most interested in.\n\n\n\n\n\n\n\n\n\nThe visualization on the right captures the trend in coronavirus cases during the month March in 2020. From only looking at the visualization on the left, a viewer may incorrectly believe that coronavirus began to skyrocket on March 4th, 2020. However, the second illustration tells a different story - cases rose closer to March 21th, 2020.\n\n\n\n8.3.3 Harnessing Color\nColor is another important feature in visualizations that does more than what meets the eye.\nLast lecture, we used color to encode a categorical variable in our scatter plot. In this section, we will discuss uses of color in novel visualizations like colormaps and heatmaps.\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors which work together is evidently a challenging task!\n\n8.3.3.1 Colormaps\nColormaps are mappings from pixel data to color values, and they’re often used to highlight distinct parts of an image. Let’s investigate a few properties of colormaps.\n\n\nJet Colormap \n\n\n\nViridis Colormap \n\n\nThe jet colormap is infamous for being misleading. While it seems more vibrant than viridis, the aggressive colors poorly encode numerical data. To understand why, let’s analyze the following images.\n\n\n\n\n\n\n\n\n\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low intensity. These include the jet colormap (row a) and grayscale (row b). Notice how the grayscale images do the best job in smoothly transitioning between pixel data. The jet colormap is the worst at this - the four images in row (a) look like a conglomeration of individual colors.\nThe difference is also evident in the images labeled (a) and (b) on the left side. The grayscale image is better at preserving finer detail in the vertical line strokes. Additionally, grayscale is preferred in x-ray scans for being more neutral. The intensity of dark red color in the jet colormap is frightening and indicates something is wrong.\nWhy is the jet colormap so much worse? The answer lies in how its color composition is percieved to the human eye.\n\n\nJet Colormap Perception \n\n\n\nViridis Colormap Perception \n\n\nThe jet colormap is largely misleading because it is not perceptually uniform. Perceptually uniform colormaps have the property that if the pixel data goes from 0.1 to 0.2, the perceptual change is the same as when the data goes from 0.8 to 0.9.\nNotice how the said uniformity is present within the linear trend displayed in the viridis colormap. On the other hand, the jet colormap is largely non-linear - this is precisely why it’s considered a worse colormap.\n\n\n\n8.3.4 Harnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these “markings”.\n\nThere are a few key takeaways from this diagram\n\nLengths are easy to discern. Don’t use plots with jiggled baselines - keep everything axis-aligned.\nAvoid pie charts! Angle judgements are inaccurate.\nAreas and volumes are hard to distinguish (area charts, word clouds, etc)\n\n\n\n8.3.5 Harnessing Conditioning\nConditioning is the process of comparing data that belong to seperate groups. We’ve seen this before in overlayed distributions, side-by-side box-plots, and scatter plots with categorical encodings. Here, we’ll introduce terminology that formalizes these examples.\nConsider an example where we want to analyze income earnings for male and females with varying levels of education. There are multiple ways to compare this data.\n\n\n\n\n\n\n\n\n\nThe barplot is an example of juxtaposition: placing multiple plots side by side, with the same scale. The scatter plot is an example of superposition: placing multiple density curves, scatter plots on top of each other.\nWhich is better depends on the problem at hand. Here, superposition makes the precise wage difference very clear from a quick glance. But many sophisticated plots convey information that favors the use of juxtaposition. Below is one example.\n\n\n\n8.3.6 Harnessing Context\nThe last component to a great visualization is perhaps the most critical - the use of context. Adding informative titles, axis labels, and descriptive captions are all best practices that we’ve heard repeatedly in Data 8.\nA publication-ready plot (and every Data 100 plot) needs:\n\nInformative title (takeaway, not description)\nAxis labels\nReference lines, markers, etc\nLegends, if appropriate\nCaptions that describe data\n\nCaptions should be:\n\nComprehensive and self-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graphs"
  },
  {
    "objectID": "sampling/sampling.html#censuses-and-surveys",
    "href": "sampling/sampling.html#censuses-and-surveys",
    "title": "9  Sampling",
    "section": "9.1 Censuses and Surveys",
    "text": "9.1 Censuses and Surveys\nIn general: a census is “an official count or survey of a population, typically recording various details of individuals.”\n\nExample: The U.S. Decennial Census was held in April 2020, and it counts every person living in all 50 states, DC, and US territories. (Not just citizens.) Participation is required by law (it is mandated by the U.S. Constitution). Important uses include the allocation of Federal funds, congressional representation, and drawing congressional and state legislative districts. The census is composed of a survey mailed to different housing addresses in the United States.\nIndividuals in a population are not always people. Other populations include: bacteria in your gut (sampled using DNA sequencing); trees of a certain species; small businesses receiving a microloan; or published results in an academic journal / field.\n\nA survey is a set of questions. An example is workers sampling individuals and households. What is asked, and how it is asked, can affect how the respondent answers, or even whether the respondent answers in the first place.\nWhile censuses are great, it is often difficult and expensive to survey everyone in a population. Thus, we usually survey a subset of the population instead.\nA sample is often used to make inferences about the population. That being said, how the sample is drawn will affect the reliability of such inferences. Two common source of error in sampling are chance error, where random samples can vary from what is expected, in any direction; and bias, which is a a systematic error in one direction.\nBecause of how surveys and samples are drawn, it turns out that samples are usually—but not always—a subset of the population: * Population: The group that you want to learn something about. * Sampling Frame: The list from which the sample is drawn. For example, if sampling people, then the sampling frame is the set of all people that could possibly end up in your sample. * Sample: Who you actually end up sampling. The sample is therefore a subset of your sampling frame.\nWhile ideally these three sets would be exactly the same, in practice they usually aren’t. For example, there may be individuals in your sampling frame (and hence, your sample) that are not in your population. And generally, sample sizes are much smaller than population sizes.\n\n\n\nSampling_Frames"
  },
  {
    "objectID": "sampling/sampling.html#bias-a-case-study",
    "href": "sampling/sampling.html#bias-a-case-study",
    "title": "9  Sampling",
    "section": "9.2 Bias: A Case Study",
    "text": "9.2 Bias: A Case Study\nThe following case study is adapted from Statistics by Freedman, Pisani, and Purves, W.W. Norton NY, 1978.\nIn 1936, President Franklin D. Roosevelt (D) went up for re-election against Alf Landon (R) . As is usual, polls were conducted in the months leading up to the election to try and predict the outcome. The Literary Digest was a magazine that had successfully predicted the outcome of 5 general elections coming into 1936. In their polling for the 1936 election, they sent out their survey to 10 million individuals, who they found from phone books, lists of magazine subscribers, and lists of country club members. Of the roughly 2.4 million people who filled out the survey, only 43% reported they would vote for Roosevelt; thus the Digest predicted that Landon would win.\nOn election day, Roosevelt won in a landslide, winning 61% of the popular vote of about 45 million voters. How could the Digest have been so wrong with their polling?\nIt turns out that the Literary Digest sample was not representative of the population. Their sampling frame inherently skewed towards more affluent voters, who tended to vote Republican, and they completely overlooked the lion’s share of voters who were still suffering through the Great Depression. Furthermore, they had a dismal response rate (about 24%); who knows how the other non-respondents would have polled? The Digest folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. His estimate (56% Roosevelt) was much closer despite having a smaller sample size of “only” 50,000 (still more than necessary; more when we cover the Central Limit Theorem). Gallup also predicted the Digest’s prediction within 1%, with a sample size of only 3000 people. He did so by anticipating the Digest’s affluent sampling frame and subsampled those individuals. The Gallup Poll today is one of the leading polls for election results.\nSo what’s the moral of the story? Samples, while convenient, are subject to chance error and bias. Election polling, in particular, can involve many sources of bias. To name a few: * Selection bias systematically excludes (or favors) particular groups. * Response bias occurs because people don’t always respond truthfully. Survey designers pay special detail to the nature and wording of questions to avoid this type of bias. * Non-response bias occurs because people don’t always respond to survey requests, which can skew responses. For example, the Gallup poll is conducted through landline phone calls, but many different populations in the U.S. do not pay for a landline, and still more do not always answer the phone. Surveyers address this bias by staying persistent and keeping surveys short."
  },
  {
    "objectID": "sampling/sampling.html#probability-samples",
    "href": "sampling/sampling.html#probability-samples",
    "title": "9  Sampling",
    "section": "9.3 Probability Samples",
    "text": "9.3 Probability Samples\nWhen sampling, it is essential to focus on the quality of the sample rather than the quantity of the sample. A huge sample size does not fix a bad sampling method. Our main goal is to gather a sample that is representative of the population it came from. The most common way to accomplish this is by randomly sampling from the population.\n\nA convenience sample is whatever you can get ahold of. Note that haphazard sampling is not necessarily random sampling; there are many potential sources of bias.\nIn a probability sample, we know the chance any given set of individuals will be in the sample.\n\nProbability samples allow us to estimate the bias and chance error, which helps us quantify uncertainty (more in a future lecture).\nNote that this does not imply that all individuals in the population need have the same chance of being selected (see: stratified random samples).\nFurther note that the real world is usually more complicated. For example, we do not generally know the probability that a given bacterium is in a microbiome sample, or whether people will answer when Gallup calls landlines. That being said, we try to model probability sampling where possible if the sampling or measurement process is not fully under our control.\n\n\nA few common random sampling schemes: * A random sample with replacement is a sample drawn uniformly at random with replacement. * Random doesn’t always mean “uniformly at random,” but in this specific context, it does. * Some individuals in the population might get picked more than once\n\nA simple random sample (SRS) is a sample drawn uniformly at random without replacement.\n\nEvery individual (and subset of individuals) has the same chance of being selected.\nEvery pair has the same chance as every other pair.\nEvery triple has the same chance as every other triple.\nAnd so on.\n\nA stratified random sample, where random sampling is performed on strata (specific groups), and the groups together compose a sample.\n\n\n9.3.1 Example: Stratified random sample\nSuppose that we are trying to run a poll to predict the mayoral election in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose we try a stratified random sample to select 100 voters as follows: 1. First, we take a simple random sample and obtain 50 voters that are above the median city income (“above-median-income”), i.e., in the upper 50-th percentile of income in the city. 2. We then take a simple random sample of the other 50 from voters that are below the median city income.\nThis is a probability sample: For any group of 100 people, if there are not exactly 50 “above-median-income” voters, then that group has zero probability of being chosen. For any other group (which has exactly 50 “above-median-income” voters), then the chance of it being chosen is 1/ # of such groups.\nNote that even if we replace the group counts with 80/20 (80 “above-median-income” voters, 20 others), then it is still a probability sample, because we can compute the precise probability of each group being chosen. However, the sampling scheme (and thus the modeling of voter preferences) becomes biased towards voters with income above the median."
  },
  {
    "objectID": "sampling/sampling.html#approximating-simple-random-sampling",
    "href": "sampling/sampling.html#approximating-simple-random-sampling",
    "title": "9  Sampling",
    "section": "9.4 Approximating Simple Random Sampling",
    "text": "9.4 Approximating Simple Random Sampling\nThe following is a very common situation in data science: - We have an enormous population. - We can only afford to sample a relatively small number of individuals. If the population is huge compared to the sample, then random sampling with and without replacement are pretty much the same.\nExample : Suppose there are 10,000 people in a population. Exactly 7,500 of them like Snack 1; the other 2,500 like Snack 2. What is the probability that in a random sample of 20, all people like Snack 1?\n\nMethod 1: SRS (Random Sample Without Replacement): \\(\\prod\\limits_{k=0}^{19} \\dfrac{7500 - k}{10000 - k} \\approx 0.003151\\)\nMethod 2: Random Sample with Replacement: \\((0.75)^{20} \\approx 0.003171\\)\n\nAs seen here, when the population size is large, probabilities of sampling with replacement are much easier to compute and lead to a reasonable approximation.\n\n9.4.1 Multinomial Probabilities\nThe approximation discussed above suggests the convenience of multinomial probabilities, which arise from sampling a categorical distribution at random **with replacement*.\nSuppose that we have a bag of marbles with the following distribution: 60% are blue, 30% are green, and 10% are red. If we then proceed to draw 100 marbles from this bag, at random with replacement, then the resulting 100-size sample is modeled as a multinomial distribution using np.random.multinomial:\n\nimport numpy as np\nnp.random.multinomial(100, [0.60, 0.30, 0.10])\n\narray([62, 31,  7])\n\n\nThis method allows us to generate, say, 10 samples of size 100 using the size parameter:\n\nnp.random.multinomial(100, [0.60, 0.30, 0.10], size=10)\n\narray([[61, 31,  8],\n       [56, 27, 17],\n       [61, 27, 12],\n       [61, 26, 13],\n       [57, 32, 11],\n       [58, 33,  9],\n       [52, 40,  8],\n       [59, 30, 11],\n       [66, 24, 10],\n       [59, 30, 11]])"
  },
  {
    "objectID": "sampling/sampling.html#comparing-convenience-sample-and-srs",
    "href": "sampling/sampling.html#comparing-convenience-sample-and-srs",
    "title": "9  Sampling",
    "section": "9.5 Comparing Convenience Sample and SRS",
    "text": "9.5 Comparing Convenience Sample and SRS\nSuppose that we are trying to run a poll to predict the mayoral election in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose we took a sample to predict the election outcome by polling all retirees. Even if they answer truthfully, we have a convenience sample. How biased would this sample be in predicting the results? While we will not numerically quantify the bias, in this demo we’ll visually show that because of the voter population distribution, any error in our prediction from a retiree sample cannot be simply due to chance:\nFirst, let’s grab a data set that has every single voter in the Bearkeley (again, this is a fake dataset) and how they actually voted in the election. For the purposes of this example, assume: * “high income” indicates a voter is above the median household income, which is $97,834 (actual Berkeley number). * There are only two mayoral candidates: one Democrat and one Republican. * Every registered voter votes in the election for the candidate under their registered party (Dem or Rep).\n\nimport pandas as pd\nimport numpy as np\nbearkeley = pd.read_csv(\"data/bearkeley.csv\")\n\n# create a 1/0 int that indicates democratic vote\nbearkeley['vote.dem'] = (bearkeley['vote'] == 'Dem').astype(int)\nbearkeley.head()\n\n\n\n\n\n  \n    \n      \n      age\n      high_income\n      vote\n      vote.dem\n    \n  \n  \n    \n      0\n      35\n      False\n      Dem\n      1\n    \n    \n      1\n      42\n      True\n      Rep\n      0\n    \n    \n      2\n      55\n      False\n      Dem\n      1\n    \n    \n      3\n      77\n      True\n      Rep\n      0\n    \n    \n      4\n      31\n      False\n      Dem\n      1\n    \n  \n\n\n\n\n\nbearkeley.shape\n\n(1300000, 4)\n\n\n\nactual_vote = np.mean(bearkeley[\"vote.dem\"])\nactual_vote\n\n0.5302792307692308\n\n\nThis is the actual outcome of the election. Based on this result, the Democratic candidate would win. However, if we were to only consider retiree voters (a retired person is anyone age 65 and up):\n\nconvenience_sample = bearkeley[bearkeley['age'] >= 65]\nnp.mean(convenience_sample[\"vote.dem\"])\n\n0.3744755089093924\n\n\nBased on this result, we would have predicted that the Republican candidate would win! This error is not due to the sample being too small to yield accurate predictions, because there are 359,396 retirees (about 27% of the 1.3 million Bearkeley voters). Instead, there seems to be something larger happening. Let’s visualize the voter preferences of the entire population to see how retirees trend:\nLet us aggregate all voters by age and visualize the fraction of Democratic voters, split by income.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nsns.set_theme(style='darkgrid', font_scale = 1.5,\n              rc={'figure.figsize':(7,5)})\n\n# aggregate all voters by age\nvotes_by_demo = bearkeley.groupby([\"age\",\"high_income\"]).agg(\"mean\").reset_index()\n\nfig = plt.figure();\nred_blue = [\"#bf1518\", \"#397eb7\"]\nwith sns.color_palette(sns.color_palette(red_blue)):\n    ax = sns.pointplot(data=votes_by_demo, x = \"age\", y = \"vote.dem\", hue = \"high_income\")\n\nax.set_title(\"Voting preferences by demographics\")\nfig.canvas.draw()\nnew_ticks = [i.get_text() for i in ax.get_xticklabels()];\nplt.xticks(range(0, len(new_ticks), 10), new_ticks[::10]);\n\n\n\n\nFrom the plot above, we see that retirees in the imaginary city of Bearkeley tend to vote less Democrat, which skewed our predictions from our sample. We also note that high-income voters tend to vote less Democrat (and more Republican).\nLet’s compare our biased convenience sample to a simple random sample. Supposing we took an SRS the same size as our retiree sample, we see that we get a result very close to the actual vote:\n\n## By default, replace = False\nn = len(convenience_sample)\nrandom_sample = bearkeley.sample(n, replace = False)\n\nnp.mean(random_sample[\"vote.dem\"])\n\n0.5304343954857594\n\n\nThis is very close to the actual vote!\nWe could even get pretty close with a much smaller sample size, say 800:\nIt turns out that we are pretty close, much smaller sample size, say, 800 (we’ll learn how to choose this number when we introduce the Central Limit Theorem):\n\nn = 800\nrandom_sample = bearkeley.sample(n, replace = False)\nnp.mean(random_sample[\"vote.dem\"])\n\n0.54\n\n\nTo visualize the chance error in an SRS, let’s simulate 1000 samples of the 800-size Simple Random Sample:\n\npoll_result = []\nnrep = 1000   # number of simulations\nn = 800       # size of our sample\nfor i in range(0,nrep):\n    random_sample = bearkeley.sample(n, replace = False)\n    poll_result.append(np.mean(random_sample[\"vote.dem\"]))\nsns.histplot(poll_result, stat='density', kde=True)\n\n# What fraction of these simulated samples would have predicted Democrat?\npoll_result = pd.Series(poll_result)\nnp.sum(poll_result >= 0.5)/1000\n\n0.957\n\n\n\n\n\nA few observations: First, the KDE looks roughly Gaussian. Second, supposing that we predicted a Democratic winner if 50% of our sample voted Democrat, then just about 4% of our simulated samples would have predicted the election result incorrectly. This visualization further justifies why our convenience sample had error that was not entirely just due to chance. We’ll revisit this notion later in the course."
  },
  {
    "objectID": "sampling/sampling.html#summary",
    "href": "sampling/sampling.html#summary",
    "title": "9  Sampling",
    "section": "9.6 Summary",
    "text": "9.6 Summary\nUnderstanding the sampling process is what lets us go from describing the data to understanding the world. Without knowing / assuming something about how the data were collected, there is no connection between the sample and the population. Ultimately, the dataset doesn’t tell us about the world behind the data."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#what-is-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#what-is-a-model",
    "title": "10  Introduction to Modeling",
    "section": "10.1 What is a Model?",
    "text": "10.1 What is a Model?\nA model is an idealized representation of a system. A system is a set of principles or procedures according to which something functions. We live in a world full of systems: the procedure of turning on a light happens according to a specific set of rules dictating the flow of electricity. The truth behind how any event occurs are usually complex, and many times the specifics are unknown. The workings of the world can be viewed is its own giant procedure. Models seek to simplify the world and distill them it into workable pieces.\nExample: We model the fall of an object on Earth as subject to a constant acceleration of \\(9.81 \\frac{m}{s^2}\\) due to gravity.\n\nWhile this describes the behavior of our system, it is merely an approximation.\nIt doesn’t account for the effects of air resistance, local variations in gravity, etc.\nIn practice, it’s accurate enough to be useful!\n\n\n10.1.1 Reasons for building models\nOften times, (1) we care about creating models that are simple and interpretable, allowing us to understand what the relationships between our variables are. Other times, (2) we care more about making extremely accurate predictions, at the cost of having an uninterpretable model. These are sometimes called black-box models, and are common in fields like deep learning.\n\nTo understand complex phenomena occurring in the world we live in.\n\nWhat factors play a role in the growth of COVID-19?\nHow do an object’s velocity and acceleration impact how far it travels? (Physics: \\(d = d_0 + vt + \\frac{1}{2}at^2\\))\n\nTo make accurate predictions about unseen data.\n\nCan we predict if an email is spam or not?\nCan we generate a one-sentence summary of this 10-page long article?\n\n\n\n\n10.1.2 Common Types of Models\nIn general, models can be split into two categories:\nNote: These specific models are not in the scope of Data 100 and exist to serve as motivation.\n\nDeterministic physical (mechanistic) models: Laws that govern how the world works.\n\nKepler’s Third Law of Planetary Motion (1619): The ratio of the square of an object’s orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.\n\n\\(T^2 \\propto R^3\\)\n\nNewton’s Laws: motion and gravitation (1687): Newton’s second law of motion models the relationship between the mass of an object and the force required to accelerate it.\n\n\\(F = ma\\)\n\\(F_g = G \\frac{m_1 m_2}{r^2}\\)\n\n\nProbabilistic models: models that attempt to understand how random processes evolve. These are more general and can be used describe many phenomena in the real world. These models commonly make simplifying assumption about the nature of the world.\n\nPoisson Process models: Used to model random events that can happen with some probability at any point in time and are strictly increasing in count, such as the arrival of customers at a store."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#simple-linear-regression",
    "href": "intro_to_modeling/intro_to_modeling.html#simple-linear-regression",
    "title": "10  Introduction to Modeling",
    "section": "10.2 Simple Linear Regression",
    "text": "10.2 Simple Linear Regression\nThe regression line is the unique straight line that minimizes the mean squared error of estimation among all straight lines. As with any straight line, it can be defined by a slope and a y-intercept:\n\nslope: \\(r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}\\)\ny-intercept: \\(\\text{average of y} - \\text{slope}\\cdot\\text{average of x}\\)\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Set random seed for consistency \nnp.random.seed(43)\nplt.style.use('default') \n\n#Generate random noise for plotting\nx = np.linspace(-3, 3, 100)\ny = x * 0.5 - 1 + np.random.randn(100) * 0.3\n\n#plot regression line\nsns.regplot(x=x,y=y);\n\n\n\n\n\n\n10.2.1 Definitions\nFor a random variable x:\n\nMean: \\(\\bar{x}\\)\nStandard Deviation: \\(\\sigma_x\\)\nPredicted value: \\(\\hat{x}\\)\n\n\n10.2.1.1 Standard Units\nA random variable is represented in standard units if the following are true:\n\n0 in standard units is the mean (\\(\\bar{x}\\)) in the original variable’s units.\nAn increase of 1 standard unit is an increase of 1 standard deviation(\\(\\sigma_x\\)) in the original variable’s units\n\n\n\n10.2.1.2 Correlation\nThe correlation (\\(r\\)) is the average of the product of \\(x\\) and \\(y\\), both measured in standard units. Correlation measures the strength of a linear association between two variables.\n\n\\(r = \\frac{1}{n} \\sum_1^n (\\frac{x_i - \\bar{x}}{\\sigma_x})(\\frac{y_i - \\bar{y}}{\\sigma_y})\\)\nCorrelations are between -1 and 1: \\(|r| < 1\\)\n\n\n\nCode\ndef plot_and_get_corr(ax, x, y, title):\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.scatter(x, y, alpha = 0.73)\n    r = np.corrcoef(x, y)[0, 1]\n    ax.set_title(title + \" (corr: {})\".format(r.round(2)))\n    return r\n\nfig, axs = plt.subplots(2, 2, figsize = (10, 10))\n\n# Just noise\nx1, y1 = np.random.randn(2, 100)\ncorr1 = plot_and_get_corr(axs[0, 0], x1, y1, title = \"noise\")\n\n# Strong linear\nx2 = np.linspace(-3, 3, 100)\ny2 = x2 * 0.5 - 1 + np.random.randn(100) * 0.3\ncorr2 = plot_and_get_corr(axs[0, 1], x2, y2, title = \"strong linear\")\n\n# Unequal spread\nx3 = np.linspace(-3, 3, 100)\ny3 = - x3/3 + np.random.randn(100)*(x3)/2.5\ncorr3 = plot_and_get_corr(axs[1, 0], x3, y3, title = \"strong linear\")\nextent = axs[1, 0].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n\n# Strong non-linear\nx4 = np.linspace(-3, 3, 100)\ny4 = 2*np.sin(x3 - 1.5) + np.random.randn(100) * 0.3\ncorr4 = plot_and_get_corr(axs[1, 1], x4, y4, title = \"strong non-linear\")\n\nplt.show()\n\n\n\n\n\n\n\n\n10.2.2 Alternate Form\nWhen the variables \\(y\\) and \\(x\\) are measured in standard units, the regression line for predicting \\(y\\) based on \\(x\\) has slope \\(r\\) and passes through the origin.\n\\[\\hat{y}_{su} = r \\cdot x_{su}\\]\n\n\n\n\nIn the original units, this becomes\n\n\\[\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}\\]\n\n\n\n\n\n10.2.3 Derivation\nStarting from the top, we have our claimed form of the regression line and we want to show that its equivalent to the optimal linear regression line: \\(\\hat{y} = \\hat{a} + \\hat{b}x\\)\nRecall:\n\n\\(\\hat{b}\\): \\(r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}\\)\n\\(\\hat{a}\\): \\(\\text{average of y} - \\text{slope}\\cdot\\text{average of x}\\)\n\n\n\n\n\n\n\nProof:\n\\[\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}\\]\nMultiply by \\(\\sigma_y\\) and add \\(\\bar{y}\\) on both sides.\n\\[\\hat{y} = \\sigma_y \\cdot r \\cdot \\frac{x - \\bar{x}}{\\sigma_x} + \\bar{y}\\]\nDistribute coefficient \\(\\sigma_{y}\\cdot r\\) to the \\(\\frac{x - \\bar{x}}{\\sigma_x}\\) term\n\\[\\hat{y} = (\\frac{r\\sigma_y}{\\sigma_x} ) \\cdot x + (\\bar{y} - (\\frac{r\\sigma_y}{\\sigma_x} ) \\bar{x})\\]\nWe now see that we have a line that matches our claim:\n\nslope: \\(r\\cdot\\frac{\\text{SD of x}}{\\text{SD of y}} = r\\cdot\\frac{\\sigma_x}{\\sigma_y}\\)\nintercept: \\(\\bar{y} - \\text{slope}\\cdot x\\)"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "href": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "title": "10  Introduction to Modeling",
    "section": "10.3 The Modeling Process",
    "text": "10.3 The Modeling Process\nAt a high level, a model is some way of representing a system. In Data 100, we’ll treat a model as some mathematical rule we use to describe the relationship between variables.\nWhat variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset \\(\\mathbb{D}\\):\n\\[\\mathbb{D} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\]\nEach pair of values \\((x_i, y_i)\\) represents a datapoint. In a modeling setting, we call these observations. \\(y_i\\) is the dependent variable we are trying to model, also called an output or response. \\(x_i\\) is the independent variable inputted into the model to make predictions, also known as a feature.\nOur goal in modeling is to use the observed data \\(\\mathbb{D}\\) to predict the output variable \\(y_i\\). We denote each prediction as \\(\\hat{y}_i\\) (read: “y hat sub i”).\nHow do we generate these predictions? Some examples of models we’ll encounter in the next few lectures are given below:\n\\[\\hat{y}_i = \\theta\\] \\[\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\]\nThe examples above are known as parametric models. They relate the collected data, \\(x_i\\), to the prediction we make, \\(\\hat{y}_i\\). A few parameters (\\(\\theta\\), \\(\\theta_0\\), \\(\\theta_1\\)) are used to describe the relationship between \\(x_i\\) and \\(\\hat{y}_i\\).\nNotice that we don’t immediately know the values of these parameters. While the features, \\(x_i\\), are taken from our observed data, we need to decide what values to give \\(\\theta\\), \\(\\theta_0\\), and \\(\\theta_1\\) ourselves. This is the heart of parametric modeling: what parameter values should we choose so our model makes the best possible predictions?\nTo choose our model parameters, we’ll work through the modeling process.\n\nChoose a model: how should we represent the world?\nChoose a loss function: how do we quantify prediction error?\nFit the model: how do we choose the best parameters of our model given our data?\nEvaluate model performance: how do we evaluate whether this process gave rise to a good model?"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "title": "10  Introduction to Modeling",
    "section": "10.4 Choosing a Model",
    "text": "10.4 Choosing a Model\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, \\(x_i\\), and predictions \\(\\hat{y}_i\\).\nIn Data 8, you learned about the Simple Linear Regression (SLR) model. You learned that the model takes the form: \\[\\hat{y}_i = a + bx_i\\]\nIn Data 100, we’ll use slightly different notation: we will replace \\(a\\) with \\(\\theta_0\\) and \\(b\\) with \\(\\theta_1\\). This will allow us to use the same notation when we explore more complex models later on in the course.\n\\[\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\]\nThe parameters of the SLR model are \\(\\theta_0\\), also called the intercept term, and \\(\\theta_1\\), also called the slope term. To create an effective model, we want to choose values for \\(\\theta_0\\) and \\(\\theta_1\\) that most accurately predict the output variable. The “best” fitting model parameters are given the special names \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\) – they are the specific parameter values that allow our model to generate the best possible predictions.\nIn Data 8, you learned that the best SLR model parameters are: \\[\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} \\qquad \\qquad \\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nA quick reminder on notation:\n\n\\(\\bar{y}\\) and \\(\\bar{x}\\) indicate the mean value of \\(y\\) and \\(x\\), respectively\n\\(\\sigma_y\\) and \\(\\sigma_x\\) indicate the standard deviations of \\(y\\) and \\(x\\)\n\\(r\\) is the correlation coefficient, defined as the average of the product of \\(x\\) and \\(y\\) measured in standard units: \\(\\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y})\\)\n\nIn Data 100, we want to understand how to derive these best model coefficients. To do so, we’ll introduce the concept of a loss function."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "title": "10  Introduction to Modeling",
    "section": "10.5 Choosing a Loss Function",
    "text": "10.5 Choosing a Loss Function\nWe’ve talked about the idea of creating the “best” possible predictions. This begs the question: how do we decide how “good” or “bad” our model’s predictions are?\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, \\(L(y, \\hat{y})\\), quantifies how “far off” a single prediction by our model is from a true, observed value in our collected data.\nThe choice of loss function for a particular model depends on the modeling task at hand. Regardless of the specific function used, a loss function should follow two basic principles:\n\nIf the prediction \\(\\hat{y}_i\\) is close to the actual value \\(y_i\\), loss should be low\nIf the prediction \\(\\hat{y}_i\\) is far from the actual value \\(y_i\\), loss should be high\n\nTwo common choices of loss function are squared loss and absolute loss.\nSquared loss, also known as L2 loss, computes loss as the square of the difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\\]\nAbsolute loss, also known as L1 loss, computes loss as the absolute difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = |y_i - \\hat{y}_i|\\]\nL1 and L2 loss give us a tool for quantifying our model’s performance on a single datapoint. This is a good start, but ideally we want to understand how our model performs across our entire dataset. A natural way to do this is to compute the average loss across all datapoints in the dataset. This is known as the cost function, \\(\\hat{R}(\\theta)\\): \\[\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)\\]\nThe cost function has many names in statistics literature. You may also encounter the terms:\n\nEmpirical risk (this is why we give the cost function the name \\(R\\))\nError function\nAverage loss\n\nWe can substitute our L1 and L2 loss into the cost function definition. The Mean Squared Error (MSE) is the average squared loss across a dataset: \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nThe Mean Absolute Error (MAE) is the average absolute loss across a dataset: \\[\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\\]"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "href": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "title": "10  Introduction to Modeling",
    "section": "10.6 Fitting the Model",
    "text": "10.6 Fitting the Model\nNow that we’ve established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model’s cost on our dataset. This process is called fitting the model.\nWe know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the objective function (our objective is to find its minimum).\nTo find the optimal model parameter, we:\n\nTake the derivative of the cost function with respect to that parameter\nSet the derivative equal to 0\nSolve for the parameter\n\nWe repeat this process for each parameter present in the model. For now, we’ll disregard the second derivative condition.\nTo help us make sense of this process, let’s put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!\nStep 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, \\(\\hat{y}_i = \\theta_0+\\theta_1 x_i\\), into the definition of MSE above and differentiate with respect to \\(\\theta_0\\) and \\(\\theta_1\\). \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)^2\\]\n\\[\\frac{\\partial}{\\partial \\theta_0} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i\\]\n\\[\\frac{\\partial}{\\partial \\theta_1} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i\\]\nStep 2: set the derivatives equal to 0. After simplifying terms, this produces two estimating equations. The best set of model parameters \\((\\theta_0, \\theta_1)\\) must satisfy these two optimality conditions. \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{y}_i = 0\\] \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_i = 0\\]\nStep 3: solve the estimating equations to compute estimates for \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\).\nTaking the first equation gives the estimate of \\(\\hat{\\theta}_0\\): \\[\n\\frac{1}{n} \\sum_{i=1}^n y_i - \\hat{\\theta}_0 - \\hat{\\theta}_1 x_i = 0 \\\\\n\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{\\theta}_0 - \\hat{\\theta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) = 0 \\\\\n\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1 \\bar{x}\n\\]\nWith a bit more maneuvering, the second equation gives the estimate of \\(\\hat{\\theta}_1\\). Start by multiplying the first estimating equation by \\(\\bar{x}\\), then subtracting the result from the second estimating equation. \\[\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\bar{x} = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)(x_i - \\bar{x}) = 0\n\\]\nNext, plug in \\(\\hat{y}_i = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_i = \\bar{y} + \\hat{\\theta}_1(x_i - \\bar{x})\\): \\[\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{\\theta}_1(x - \\bar{x}))(x_i - \\bar{x}) = 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) = \\hat{\\theta}_1 \\times \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nBy using the definition of correlation \\(\\left(r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)\\) and standard deviation \\(\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)\\), we can conclude: \\[r \\sigma_x \\sigma_y = \\hat{\\theta}_1 \\times \\sigma_x^2\\] \\[\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nJust as was given in Data 8!\nRemember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can always follow these three steps to fit the model."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "href": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "title": "10  Introduction to Modeling",
    "section": "10.7 Evaluating Performance",
    "text": "10.7 Evaluating Performance\nAt this point, we’ve:\n\nDefined our model\nDefined our loss function\nFit the model to identify the best model parameters\n\nNow, what are some ways to determine if our model was a good fit to our data? We will delve into this more in the next chapter, but there are three main ways for evaluating a model.\n\nStatistics:\n\nPlot original data\nCompute column means\nCompute standard deviations\nIf we want to fit a linear model, compute correlation (r)\n\nPerformance metrics:\n\nRoot Mean Square Error (RMSE). It is the square root of MSE, which is the average loss that we’ve been minimizing to determine optimal model parameters.\nRMSE is in the same units as \\(y\\).\nA lower RMSE indicates more “accurate” predictions (lower “average loss” across data)\n\nVisualization:\n\nLook at a residual plot of \\(e_i = y_i - \\hat{y_i}\\) to visualize the difference between actual and predicted \\(y\\) values."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.1 Constant Model + MSE",
    "text": "11.1 Constant Model + MSE\nIn today’s lecture, our focus will be on the constant model. The constant model is slightly different from the simple linear regression model we’ve explored previously. Rather than generate predictions from an inputted feature variable, the constant model predicts the same constant number every time. We call this constant \\(\\theta\\).\n\\[\\hat{y}_i = \\theta\\]\n\\(\\theta\\) is the parameter of the constant model, just as \\(\\theta_0\\) and \\(\\theta_1\\) were the parameters in SLR. Our task now is to determine what value of \\(\\theta\\) represents the optimal model – in other words, what number should we guess each time to have the lowest possible average loss on our data?\nConsider the case where L2 (squared) loss is used as the loss function and mean squared error is used as the cost function. At this stage, we’re well into the modeling process:\n\nChoose a model: constant model\nChoose a loss function: L2 loss\nFit the model\nEvaluate model performance\n\nIn Homework 5, you will fit the constant model under MSE cost to find that the best choice of \\(\\theta\\) is the mean of the observed \\(y\\) values. In other words, \\(\\hat{\\theta} = \\bar{y}\\).\nLet’s take a moment to interpret this result. Our optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:\n\\[R(\\hat{\\theta}) = \\min_{\\theta} R(\\theta)\\]\nTo restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, \\(\\hat{\\theta}\\), is the value of \\(\\theta\\) that minimizes the cost \\(R\\).\nFor modeling purposes, we care less about the minimum value of cost, \\(R(\\hat{\\theta})\\), and more about the value of \\(\\theta\\) that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n\\[\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)\\]\nThat is, we want to find the argument \\(\\theta\\) that minimizes the cost function."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.2 Constant Model + MAE",
    "text": "11.2 Constant Model + MAE\nWe see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?\nThis time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the mean absolute error.\n\nChoose a model: constant model\nChoose a loss function: L1 loss\nFit the model\nEvaluate model performance\n\nTo fit the model and find the optimal parameter value \\(\\hat{\\theta}\\), follow the usual process of differentiating the cost function with respect to \\(\\theta\\), setting the derivative equal to zero, and solving for \\(\\theta\\). Writing this out in longhand:\n\\[\nR(\\theta) = \\frac{1}{n}\\sum^{n}_{i=1} |y_i - \\theta| \\\\\n\\frac{d}{d\\theta} R(\\theta) = \\frac{d}{d\\theta} \\left(\\frac{1}{n} \\sum^{n}_{i=1} |y_i - \\theta| \\right) \\\\\n\\frac{d}{d\\theta} R(\\theta) = \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta|\n\\]\nHere, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when \\(y_i = \\theta\\)). For now, we’ll ignore this issue. It turns out that disregarding this case doesn’t influence our final result.\nTo perform the derivative, consider two cases. When \\(\\theta\\) is less than \\(y_i\\), the term \\(y_i - \\theta\\) will be positive and the absolute value has no impact. When \\(\\theta\\) is greater than \\(y_i\\), the term \\(y_i - \\theta\\) will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying \\(-(y_i - \\theta) = \\theta - y_i\\).\n\\[|y_i - \\theta| = \\begin{cases} y_i - \\theta \\quad \\text{ if: } \\theta < y_i \\\\ \\theta - y_i \\quad \\text{if: }\\theta > y_i \\end{cases}\\]\nTaking derivatives:\n\\[\\frac{d}{d\\theta} |y_i - \\theta| = \\begin{cases} \\frac{d}{d\\theta} (y_i - \\theta) = -1 \\quad \\text{if: }\\theta < y_i \\\\ \\frac{d}{d\\theta} (\\theta - y_i) = 1 \\quad \\text{if: }\\theta > y_i \\end{cases}\\]\nThis means that we obtain a different value for the derivative for datapoints where \\(\\theta < y_i\\) and where \\(\\theta > y_i\\). We can summarize this by saying:\n\\[\\frac{d}{d\\theta} R(\\theta) = \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta| \\\\\n= \\frac{1}{n} \\left[\\sum_{\\theta < y_i} (-1) + \\sum_{\\theta > y_i} (+1) \\right]\n\\]\nTo finish finding the best value of \\(\\theta\\), set this derivative equal to zero and solve for \\(\\theta\\). You’ll do this in Homework 5 to show that \\(\\hat{\\theta} = \\text{median}(y)\\)."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "href": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.3 Comparing Loss Functions",
    "text": "11.3 Comparing Loss Functions\nNow, we’ve tried our hand at fitting a model under both MSE and MAE cost functions. How do the two results compare?\nLet’s consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We’ll fit a constant model to predict the number of drinks that will be sold tomorrow.\n\nimport numpy as np\ndrinks = np.array([20, 21, 22, 29, 33])\ndrinks\n\narray([20, 21, 22, 29, 33])\n\n\nFrom our derivations above, we know that the optimal model parameter under MSE cost is the mean of the dataset. Under MAE cost, the optimal parameter is the median of the dataset.\n\nnp.mean(drinks), np.median(drinks)\n\n(25.0, 22.0)\n\n\nIf we plot each empirical risk function across several possible values of \\(\\theta\\), we find that each \\(\\hat{\\theta}\\) does indeed correspond to the lowest value of error:\n\nNotice that the MSE above is a smooth function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE, in contrast, is not differentiable at each of its “kinks.” We’ll explore how the smoothness of the cost function can impact our ability to apply numerical optimization in a few weeks.\nHow do outliers affect each cost function? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected.\n\ndrinks_with_outlier = np.append(drinks, 1000)\ndisplay(drinks_with_outlier)\nnp.mean(drinks_with_outlier), np.median(drinks_with_outlier)\n\narray([  20,   21,   22,   29,   33, 1000])\n\n\n(187.5, 25.5)\n\n\nThis means that under the MSE, the optimal model parameter \\(\\hat{\\theta}\\) is strongly affected by the presence of outliers. Under the MAE, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is sensitive to outliers, while the MAE is robust to outliers.\nLet’s try another experiment. This time, we’ll add an additional, non-outlying datapoint to the data.\n\ndrinks_with_additional_observation = np.append(drinks, 35)\ndrinks_with_additional_observation\n\narray([20, 21, 22, 29, 33, 35])\n\n\nWhen we again visualize the cost functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are infinitely many optimal values for the model parameter: any value \\(\\hat{\\theta} \\in [22, 29]\\) will minimize the MAE. In contrast, the MSE still has a single best value for \\(\\hat{\\theta}\\). In other words, the MSE has a unique solution for \\(\\hat{\\theta}\\); the MAE is not guaranteed to have a single unique solution."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#evaluating-models",
    "href": "constant_model_loss_transformations/loss_transformations.html#evaluating-models",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.4 Evaluating Models",
    "text": "11.4 Evaluating Models\nThis leaves us with one final question – how “good” are the predictions made by this “best” fitted model?\nOne way we might want to evaluate our model’s performance is by computing summary statistics. If the mean and standard deviation of our predictions are close to those of the original observed \\(y_i\\)s, we might be inclined to say that our model has done well. A large magnitude for the correlation coefficient between the feature and response variables might also support this conclusion. However, we should be cautious with this approach. To see why, we’ll consider a classic dataset called Anscombe’s quartet.\n\nIt turns out that the four sets of points shown here all have identical means, standard deviations, and correlation coefficients. However, it only makes sense to model the first of these four sets of data using SLR! It is important to visualize your data before starting to model to confirm that your choice of model makes sense for the data.\nAnother way of evaluating model performance is by using performance metrics. A common choice of metric is the Root Mean Squared Error, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of \\(y_i\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset. \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nWe may also wish to visualize the model’s residuals, defined as the difference between the observed and predicted \\(y_i\\) value (\\(e_i = y_i - \\hat{y}_i\\)). This gives a high-level view of how “off” each prediction is from the true observed value. Recall that you explored this concept in Data 8: a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe’s quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "href": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.5 Linear Transformations",
    "text": "11.5 Linear Transformations\nAt this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.\nA key word above is linear. When we computed parameter estimates earlier, we assumed that \\(x_i\\) and \\(y_i\\) shared roughly a linear relationship.\nData in the real world isn’t always so straightforward. Consider the dataset below, which contains information about the ages and lengths of dugongs.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndugong = pd.read_csv(\"data/dugongs.txt\", delimiter=\"\\t\").sort_values(\"Length\")\nx, y = dugong[\"Length\"], dugong[\"Age\"]\n\n# `corrcoef` computes the correlation coefficient between two variables\n# `std` finds the standard deviation\nr = np.corrcoef(x, y)[0, 1]\ntheta_1 = r*np.std(y)/np.std(x)\ntheta_0 = np.mean(y) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, y)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(\"Age\")\n\nax[1].scatter(x, y)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(\"Age\");\n\n\n\n\n\nLooking at the plot on the left, we see that there is a slight curvature to the data points. Plotting the SLR curve on the right results in a poor fit.\nFor SLR to perform well, we’d like there to be a rough linear trend relating \"Age\" and \"Length\". What is making the raw data deviate from a linear relationship? Notice that the data points with \"Length\" greater than 2.6 have disproportionately high values of \"Age\" relative to the rest of the data. If we could manipulate these data points to have lower \"Age\" values, we’d “shift” these points downwards and reduce the curvature in the data. Applying a logarithmic transformation to \\(y_i\\) (that is, taking \\(\\log(\\) \"Age\" \\()\\) ) would achieve just that.\nAn important word on \\(\\log\\): in Data 100 (and most upper-division STEM courses), \\(\\log\\) denotes the natural logarithm with base \\(e\\). The base-10 logarithm, where relevant, is indicated by \\(\\log_{10}\\).\n\n\nCode\nz = np.log(y)\n\nr = np.corrcoef(x, z)[0, 1]\ntheta_1 = r*np.std(z)/np.std(x)\ntheta_0 = np.mean(z) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, z)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(r\"$\\log{(Age)}$\")\n\nax[1].scatter(x, z)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(r\"$\\log{(Age)}$\")\n\nplt.subplots_adjust(wspace=0.3);\n\n\n\n\n\nOur SLR fit looks a lot better! We now have a new target variable: the SLR model is now trying to predict the log of \"Age\", rather than the untransformed \"Age\". In other words, we are applying the transformation \\(z_i = \\log{(y_i)}\\). The SLR model becomes:\n\\[\\log{\\hat{(y_i)}} = \\theta_0 + \\theta_1 x_i\\] \\[\\hat{z}_i = \\theta_0 + \\theta_1 x_i\\]\nIt turns out that this linearized relationship can help us understand the underlying relationship between \\(x_i\\) and \\(y_i\\). If we rearrange the relationship above, we find: \\[\n\\log{(y_i)} = \\theta_0 + \\theta_1 x_i \\\\\ny_i = e^{\\theta_0 + \\theta_1 x_i} \\\\\ny_i = (e^{\\theta_0})e^{\\theta_1 x_i} \\\\\ny_i = C e^{k x_i}\n\\]\nFor some constants \\(C\\) and \\(k\\).\n\\(y_i\\) is an exponential function of \\(x_i\\). Applying an exponential fit to the untransformed variables corroborates this finding.\n\n\nCode\nplt.figure(dpi=120, figsize=(4, 3))\n\nplt.scatter(x, y)\nplt.plot(x, np.exp(theta_0)*np.exp(theta_1*x), \"tab:red\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Age\");\n\n\n\n\n\nYou may wonder: why did we choose to apply a log transformation specifically? Why not some other function to linearize the data?\nPractically, many other mathematical operations that modify the relative scales of \"Age\" and \"Length\" could have worked here. The Tukey-Mosteller Bulge Diagram is a useful tool for summarizing what transformations can linearize the relationship between two variables. To determine what transformations might be appropriate, trace the shape of the “bulge” made by your data. Find the quadrant of the diagram that matches this bulge. The transformations shown on the vertical and horizontal axes of this quadrant can help improve the fit between the variables."
  },
  {
    "objectID": "ols/ols.html#linearity",
    "href": "ols/ols.html#linearity",
    "title": "12  Ordinary Least Squares",
    "section": "12.1 Linearity",
    "text": "12.1 Linearity\nAn expression is linear in \\(\\theta\\) (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms: a vector of \\(\\theta\\) s, and a matrix/vector not involving \\(\\theta\\).\nExample: \\(\\theta = [\\theta_1, \\theta_2, ... \\theta_p]\\)\n\nLinear in theta: \\(\\hat{y} = \\theta_0 + 2\\theta_1 + 3\\theta_2\\)\n\n\\[\\hat{y} = \\begin{bmatrix} 1 \\space 2 \\space 3 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}\\]\n\nNot linear in theta: \\(\\hat{y} = \\theta_0\\theta_1 + 2\\theta_1^2 + 3log(\\theta_2)\\)"
  },
  {
    "objectID": "ols/ols.html#multiple-linear-regression",
    "href": "ols/ols.html#multiple-linear-regression",
    "title": "12  Ordinary Least Squares",
    "section": "12.2 Multiple Linear Regression",
    "text": "12.2 Multiple Linear Regression\nMultiple Linear regression is an extension of simple linear regression that adds additional features into the model. Say we collect information on several variables when making an observation. For example, we may record the age, height, and weekly hours of sleep for a student in Data 100. This single observation now contains data for multiple features. To accommodate for the fact that we now consider several feature variables, we’ll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of \\(p\\) features.\n\nThe multiple Linear regression model takes the form:\n\\[\\hat{y}_i = \\theta_0\\:+\\:\\theta_1x_{i1}\\:+\\:\\theta_2 x_{i2}\\:+\\:...\\:+\\:\\theta_p x_{ip}\\]\nOur \\(i\\)th prediction, \\(\\hat{y}_i\\), is a linear combination of the parameters, \\(\\theta_i\\). Because we are now dealing with many parameter values, we’ll collect them all into a parameter vector with dimensions \\((p+1) \\times 1\\) to keep things tidy.\n\\[\\theta = \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix}\\]\nWe are now working with two vectors: a row vector representing the observed data, and a column vector containing the model parameters. The multiple Linear regression model given above is equivalent to the dot (scalar) product of the observation vector and parameter vector.\n\\[[1,\\:x_{i1},\\:x_{i2},\\:x_{i3},\\:...,\\:x_{ip}] \\theta = [1,\\:x_{i1},\\:x_{i2},\\:x_{i3},\\:...,\\:x_{ip}] \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix} = \\theta_0\\:+\\:\\theta_1x_{i1}\\:+\\:\\theta_2 x_{i2}\\:+\\:...\\:+\\:\\theta_p x_{ip}\\]\nNotice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with \\(\\theta_0\\) to give the intercept of the regression model. We call this 1 entry the intercept or bias term."
  },
  {
    "objectID": "ols/ols.html#linear-algebra-approach",
    "href": "ols/ols.html#linear-algebra-approach",
    "title": "12  Ordinary Least Squares",
    "section": "12.3 Linear Algebra Approach",
    "text": "12.3 Linear Algebra Approach\nWe now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple Linear regression. We want to use the tools of linear algebra.\nLet’s think carefully about what we did to generate the single prediction above. To make a prediction from the first observation in the data, we took the scalar product of the parameter vector and first observation vector. To make a prediction from the second observation, we would repeat this process to find the scalar product of the parameter vector and the second observation vector. If we wanted to find the model predictions for each observation in the dataset, we’d repeat this process for all \\(n\\) observations in the data.\n\\[\\hat{y}_1 = [1,\\:x_{11},\\:x_{12},\\:x_{13},\\:...,\\:x_{1p}] \\theta\\] \\[\\hat{y}_2 = [1,\\:x_{21},\\:x_{22},\\:x_{23},\\:...,\\:x_{2p}] \\theta\\] \\[\\vdots\\] \\[\\hat{y}_n = [1,\\:x_{n1},\\:x_{n2},\\:x_{n3},\\:...,\\:x_{np}] \\theta\\]\nOur observed data is represented by \\(n\\) row vectors, each with dimension \\((p+1)\\). We can collect them all into a single matrix, which we call \\(\\mathbb{X}\\).\n\nThe matrix \\(\\mathbb{X}\\) is known as the design matrix. It contains all observed data for each of our \\(p\\) features. It often (but not always) contains an additional column of all ones to represent the intercept or bias column.\nTo review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.\n\nThe multiple Linear regression model can then be restated in terms of matrices: \\[\\mathbb{\\hat{Y}} = \\mathbb{X} \\theta\\]\nHere, \\(\\mathbb{\\hat{Y}}\\) is the prediction vector with dimensions \\((n \\times 1)\\). It contains the prediction made by the model for each of \\(n\\) input observations.\nWe now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of cost functions and model fitting.\nRecall our definition of MSE: \\[R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nAt its heart, the MSE is a measure of distance – it gives an indication of how “far away” the predictions are from the true values, on average.\nWhen working with vectors, this idea of “distance” is represented by the norm. More precisely, the distance between two vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) can be expressed as: \\[||\\vec{a} - \\vec{b}||_2 = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\ldots + (a_n - b_n)^2} = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}\\]\nThe double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.\nLooks pretty familiar! We can rewrite the MSE to express it as a squared L2 norm in terms of the prediction vector, \\(\\hat{\\mathbb{Y}}\\), and true target vector, \\(\\mathbb{Y}\\):\n\\[R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} ||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2^2\\]\nHere, the superscript 2 outside of the norm double bars means that we are squaring the norm. If we plug in our linear model \\(\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta\\), we find the MSE cost function in vector notation:\n\\[R(\\theta) = \\frac{1}{n} ||\\mathbb{Y} - \\mathbb{X} \\theta||_2^2\\]\nUnder the linear algebra perspective, our new task is to fit the optimal parameter vector \\(\\theta\\) such that the cost function is minimized. Equivalently, we wish to minimize the norm \\[||\\mathbb{Y} - \\mathbb{X} \\theta||_2 = ||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2\\].\nWe can restate this goal in two ways:\n\nMinimize the distance between the vector of true values, \\(\\mathbb{Y}\\), and the vector of predicted values, \\(\\mathbb{\\hat{Y}}\\)\nMinimize the length of the residual vector, defined as: \\[e = \\mathbb{Y} - \\mathbb{\\hat{Y}} = \\begin{bmatrix}\n         y_1 - \\hat{y}_1 \\\\\n         y_2 - \\hat{y}_2 \\\\\n         \\vdots \\\\\n         y_n - \\hat{y}_n\n       \\end{bmatrix}\\]"
  },
  {
    "objectID": "ols/ols.html#geometric-perspective",
    "href": "ols/ols.html#geometric-perspective",
    "title": "12  Ordinary Least Squares",
    "section": "12.4 Geometric Perspective",
    "text": "12.4 Geometric Perspective\nTo derive the best parameter vector to meet this goal, we can turn to the geometric properties of our modeling set-up.\nUp until now, we’ve mostly thought of our model as a scalar product between horizontally-stacked observations and the parameter vector. We can also think of \\(\\hat{\\mathbb{Y}}\\) as a linear combination of feature vectors, scaled by the parameters. We use the notation \\(\\mathbb{X}_{:, i}\\) to denote the \\(i\\)th column of the design matrix. You can think of this as following the same convention as used when calling .iloc and .loc. “:” means that we are taking all entries in the \\(i\\)th column.\n\n\\[\n\\hat{\\mathbb{Y}} =\n\\theta_0 \\begin{bmatrix}\n           1 \\\\\n           1 \\\\\n           \\vdots \\\\\n           1\n         \\end{bmatrix} + \\theta_1 \\begin{bmatrix}\n           x_{11} \\\\\n           x_{21} \\\\\n           \\vdots \\\\\n           x_{n1}\n         \\end{bmatrix} + \\ldots + \\theta_p \\begin{bmatrix}\n           x_{1p} \\\\\n           x_{2p} \\\\\n           \\vdots \\\\\n           x_{np}\n         \\end{bmatrix}\n         = \\theta_0 \\mathbb{X}_{:,\\:1} + \\theta_1 \\mathbb{X}_{:,\\:2} + \\ldots + \\theta_p \\mathbb{X}_{:,\\:p+1}\\]\nThis new approach is useful because it allows us to take advantage of the properties of linear combinations.\nRecall that the span or column space of a matrix is the set of all possible linear combinations of the matrix’s columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns.\nBecause the prediction vector, \\(\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta\\), is a linear combination of the columns of \\(\\mathbb{X}\\), we know that the predictions are contained in the span of \\(\\mathbb{X}\\). That is, we know that \\(\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})\\).\nThe diagram below is a simplified view of \\(\\text{Span}(\\mathbb{X})\\), assuminh that each column of \\(\\mathbb{X}\\) has length \\(n\\). Notice that the columns of \\(\\mathbb{X}\\) define a subspace of \\(\\mathbb{R}^n\\), where each point in the subspace can be reached by a linear combination of \\(\\mathbb{X}\\)’s columns. The prediction vector \\(\\mathbb{\\hat{Y}}\\) lies somewhere in this subspace.\n\nExamining this diagram, we find a problem. The vector of true values, \\(\\mathbb{Y}\\), could theoretically lie anywhere in \\(\\mathbb{R}^n\\) space – its exact location depends on the data we collect out in the real world. However, our multiple Linear regression model can only make predictions in the subspace of \\(\\mathbb{R}^n\\) spanned by \\(\\mathbb{X}\\). Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, \\(\\mathbb{Y}\\), and the vector of predicted values, \\(\\mathbb{\\hat{Y}}\\), is minimized. This means that we want \\(\\mathbb{\\hat{Y}}\\) to be the vector in \\(\\text{Span}(\\mathbb{X})\\) that is closest to \\(\\mathbb{Y}\\).\nAnother way of rephrasing this goal is to say that we wish to minimize the length of the residual vector \\(e\\), as measured by its \\(L_2\\) norm.\n\nThe vector in \\(\\text{Span}(\\mathbb{X})\\) that is closest to \\(\\mathbb{Y}\\) is always the orthogonal projection of \\(\\mathbb{Y}\\) onto \\(\\text{Span}(\\mathbb{X})\\). Thus, we should choose the parameter vector \\(\\theta\\) that makes the residual vector orthogonal to any vector in \\(\\text{Span}(\\mathbb{X})\\). You can visualize this as the vector created by dropping a perpendicular line from \\(\\mathbb{Y}\\) onto the span of \\(\\mathbb{X}\\).\nHow does this help us identify the optimal parameter vector, \\(\\hat{\\theta}\\)? Recall that two vectors are orthogonal if their dot product is zero. A vector \\(\\vec{v}\\) is orthogonal to the span of a matrix \\(M\\) if \\(\\vec{v}\\) is orthogonal to each column in \\(M\\). Put together, a vector \\(\\vec{v}\\) is orthogonal to \\(\\text{Span}(M)\\) if:\n\\[M^T \\vec{v} = \\vec{0}\\]\nBecause our goal is to find \\(\\hat{\\theta}\\) such that the residual vector \\(e = \\mathbb{Y} - \\mathbb{X} \\theta\\) is orthogonal to \\(\\text{Span}(\\mathbb{X})\\), we can write:\n\\[\\mathbb{X}^T e = \\vec{0}\\] \\[\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\vec{0}\\] \\[\\mathbb{X}^T \\mathbb{Y} - \\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\vec{0}\\] \\[\\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}\\]\nThis last line is known as the normal equation. Any vector \\(\\theta\\) that minimizes MSE on a dataset must satisfy this equation.\nIf \\(\\mathbb{X}^T \\mathbb{X}\\) is invertible, we can conclude: \\[\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^T \\mathbb{Y}\\]\nThis is called the least squares estimate of \\(\\theta\\): it is the value of \\(\\theta\\) that minimizes the squared loss.\nNote that the least squares estimate was derived under the assumption that \\(\\mathbb{X}^T \\mathbb{X}\\) is invertible. This condition holds true when \\(\\mathbb{X}^T \\mathbb{X}\\) is full column rank, which, in turn, happens when \\(\\mathbb{X}\\) is full column rank. We will explore the consequences of this fact in lab and homework."
  },
  {
    "objectID": "ols/ols.html#evaluating-model-performance",
    "href": "ols/ols.html#evaluating-model-performance",
    "title": "12  Ordinary Least Squares",
    "section": "12.5 Evaluating Model Performance",
    "text": "12.5 Evaluating Model Performance\nOur geometric view of multiple Linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features.\nNow, we want to understand how well our fitted model performs. One measure of model performance is the Root Mean Squared Error, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of \\(y_i\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset.\n\\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nWhen working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple Linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple Linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple Linear model performs well if its residual plot shows no patterns.\n\nFor SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple Linear setting, we will need a performance metric that can account for multiple features at once. Multiple \\(R^2\\), also called the coefficient of determination, is the ratio of the variance of the predicted values \\(\\hat{y}_i\\) to the variance of the true values \\(y_i\\). It can be interpreted as the proportion of variance in the observations that is explained by the model.\n\\[R^2 = \\frac{\\text{variance of } \\hat{y}_i}{\\text{variance of } y_i} = \\frac{\\sigma^2_{\\hat{y}}}{\\sigma^2_y}\\]\nAs we add more features, our fitted values tend to become closer and closer to our actual values. Thus, \\(\\mathbb{R}^2\\) increases."
  },
  {
    "objectID": "ols/ols.html#ols-properties",
    "href": "ols/ols.html#ols-properties",
    "title": "12  Ordinary Least Squares",
    "section": "12.6 OLS Properties",
    "text": "12.6 OLS Properties\n\nWhen using the optimal parameter vector, our residuals \\(e = \\mathbb{Y} - \\hat{\\mathbb{Y}}\\) are orthogonal to \\(span(\\mathbb{X})\\)\n\n\\[\\mathbb{X}^Te = 0 \\]\n\n\n\n\n\n\nProof:\nThe optimal parameter vector, \\(\\hat{\\theta}\\), solves the normal equations \\(\\implies \\hat{\\theta} = \\mathbb{X}^T\\mathbb{X}^{-1}\\mathbb{X}^T\\mathbb{Y}\\)\n\\[\\mathbb{X}^Te = \\mathbb{X}^T (\\mathbb{Y} - \\mathbb{\\hat{Y}}) \\]\n\\[\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{X}\\hat{\\theta}\\]\nAny matrix multiplied with its own inverse is the identity matrix \\(\\mathbb{I}\\)\n\\[\\mathbb{X}^T\\mathbb{Y} - (\\mathbb{X}^T\\mathbb{X})(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y} = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{Y} = 0\\]\n\n\n\n\nFor all linear models with an intercept term, the sum of residuals is zero.\n\n\\[\\sum_i^n e_i = 0\\]\n\n\n\n\n\n\nProof:\nFor all linear models with an intercept term, the average of the predicted \\(y\\) values is equal to the average of the true \\(y\\) values.\n\\[\\bar{y} = \\bar{\\hat{y}}\\]\nRewriting the sum of residuals as two separate sums,\n\\[\\sum_i^n e_i = \\sum_i^n y_i - \\sum_i^n\\hat{y}_i\\]\nEach respective sum is a multiple of the average of the sum.\n\\[\\sum_i^n e_i = n\\bar{y} - n\\bar{y} = n(\\bar{y} - \\bar{y}) = 0\\]\n\n\n\n\nThe Least Squares estimate \\(\\hat{\\theta}\\) is unique if and only if \\(\\mathbb{X}\\) is full column rank.\n\n\n\n\n\n\n\nProof:\nWe know the solution to the normal equation \\(\\mathbb{X}^T\\mathbb{X}\\hat{\\theta} = \\mathbb{Y}\\) is the least square estimate that fulfills the prior equality.\n\\(\\hat{\\theta}\\) has a unique solution \\(\\iff\\) the square matrix \\(\\mathbb{X}^T\\mathbb{X}\\) is invertible.\nThe rank of a square matrix is the maximum number of of linearly independent columns it contains. \\(\\mathbb{X}^T\\mathbb{X}\\) has shape \\((p + 1) \\times (p + 1)\\), and therefore has max rank p + 1.\n\\(rank(\\mathbb{X}^T\\mathbb{X})\\) = \\(rank(\\mathbb{X})\\) (proof out of scope).\nTherefore \\(\\mathbb{X}^T\\mathbb{X}\\) has rank p + 1 \\(\\iff\\) \\(\\mathbb{X}\\) has rank p + 1 \\(\\iff \\mathbb{X}\\) is full column rank."
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#sklearn-implementing-derived-formulas-in-code",
    "href": "gradient_descent/gradient_descent.html#sklearn-implementing-derived-formulas-in-code",
    "title": "13  Gradient Descent",
    "section": "13.1 sklearn: Implementing Derived Formulas in Code",
    "text": "13.1 sklearn: Implementing Derived Formulas in Code\nThroughout this lecture, we’ll refer to the penguins dataset.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\npenguins = sns.load_dataset(\"penguins\")\npenguins = penguins[penguins[\"species\"] == \"Adelie\"].dropna()\npenguins.head(5)\n\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n    \n    \n      5\n      Adelie\n      Torgersen\n      39.3\n      20.6\n      190.0\n      3650.0\n      Male\n    \n  \n\n\n\n\nSuppose our goal is to predict the value of the 'bill depth' for a particular penguin given its 'flipper length'.\n\n# Define the design matrix, X...\nX = penguins[[\"flipper_length_mm\"]]\n\n# ...as well as the target variable, y\ny = penguins[[\"bill_depth_mm\"]]\n\n\n13.1.1 Simple Linear Regression (SLR)\nIn the SLR framework we learned last week, this means we are saying our model for bill depth, \\(y\\), is a linear function of flipper length, \\(x\\):\n\\[\\hat{y} = \\theta_0 + \\theta_1 x\\]\nLet’s do some EDA first.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.xlabel(\"flipper length (mm)\")\nplt.ylabel(\"bill depth (mm)\")\nplt.scatter(data = penguins, x = \"flipper_length_mm\", y = \"bill_depth_mm\")\n\n\n<matplotlib.collections.PathCollection at 0x19566590550>\n\n\n\n\n\nBased on our EDA, there is a linear relationship, though it is somewhat weak.\n\n13.1.1.1 SLR w/ Derived Analytical Formulas\nLet \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\) be the choices that minimize the Mean Squared Error.\nOne approach to compute \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\) is analytically, using the equations we derived in a previous lecture:\n\\[\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1 \\bar{x}\\]\n\\[\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\n\\[r = \\frac{1}{n} \\sum_{i=1}^{n}\\left( \\frac{x_i - x}{\\sigma_x} \\right) \\left( \\frac{y_i - y}{\\sigma_y} \\right) \\]\nLet’s implement these using the base numpy library, which provides many important functions such as .mean and .std .\n\n\nCode\nx = penguins[\"flipper_length_mm\"]\ny = penguins[\"bill_depth_mm\"]\n\nx_bar, sigma_x = np.mean(x), np.std(x)\ny_bar, sigma_y = np.mean(y), np.std(y)\nr = np.sum((x - x_bar) / sigma_x * (y - y_bar) / sigma_y) / len(x)\n\ntheta1_hat = r * sigma_y / sigma_x\n\ntheta0_hat = y_bar - theta1_hat * x_bar\n\nprint(f\"bias parameter: {theta0_hat}, \\nslope parameter: {theta1_hat}\".format(theta0_hat,theta1_hat))\n\n\nbias parameter: 7.297305899612297, \nslope parameter: 0.058126223695067675\n\n\n\n\n13.1.1.2 SLR Analytical Approach Performance\nLet’s first assess how “good” this model is using a performance metric. For this exercise, let’s use the MSE. As a review:\nMean Squared Error: We can compute this explicitly by averaging the square of the residuals \\(e_i\\):\n\\[\\large MSE  = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^n (e_i)^2 = \\frac{1}{n}\\]\n\n\nCode\n# using our estimated parameter values to create a column containing our \n# SLR predictions and errors\npenguins[\"analytical_preds_slr\"] = theta0_hat + theta1_hat * penguins[\"flipper_length_mm\"]\n\npenguins[\"residual\"] = penguins[\"bill_depth_mm\"] - penguins[\"analytical_preds_slr\"]\n                                          \npenguins\n\nprint(\"MSE: \", np.mean(penguins[\"residual\"]**2))\n\n\nMSE:  1.3338778799806363\n\n\nLet’s plot plot our results.\n\n\nCode\nsns.scatterplot(data = penguins, x = \"flipper_length_mm\", y = \"bill_depth_mm\")\nplt.plot(penguins[\"flipper_length_mm\"], penguins[\"analytical_preds_slr\"], 'r') # a line\nplt.legend([r'$\\hat{y}$', '$y$']);\n\n\n\n\n\n\n\n13.1.1.3 SLR w/ sklearn\nWe’ve already saved a lot of time (and avoided tedious calculations) by translating our derived formulas into code. However, we still had to go through the process of writing out the linear algebra ourselves.\nTo make life even easier, we can turn to the sklearn Python library. sklearn is a robust library of machine learning tools used extensively in research and industry. It gives us a wide variety of in-built modeling frameworks and methods, so we’ll keep returning to sklearn techniques as we progress through Data 100.\nRegardless of the specific type of model being implemented, sklearn follows a standard set of steps for creating a model.\n\nCreate a model object. This generates a new instance of the model class. You can think of it as making a new copy of a standard “template” for a model. In pseudocode, this looks like: my_model = ModelName()\nFit the model to the X design matrix and Y target vector. This calculates the optimal model parameters “behind the scenes” without us explicitly working through the calculations ourselves. The fitted parameters are then stored within the model for use in future predictions: my_model.fit(X, Y)\nAnalyze the fitted parameters using .coef_ or .intercept_, or use the fitted model to make predictions on the X input data using .predict.\nmy_model.coef_\n\nmy_model.intercept_\n\nmy_model.predict(X)\n\nLet’s put this into action with our multiple regression task. First, initialize an instance of the LinearRegression class.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n\nNext, fit the model instance to the design matrix X and target vector Y by calling .fit.\n\nmodel.fit(X, y)\n\nLinearRegression()\n\n\nAnd, lastly, generate predictions for \\(\\hat{Y}\\) using the .predict method. Here are the first 5 penguins in our dataset. How close are our analytical solution’s predictions to our sklearn model’s predictions?\n\n# Like before, show just the first 5 predictions. The output of predict is usually a np.array\npenguins[\"sklearn_preds_slr\"] = model.predict(X)\nsklearn_5 = penguins[\"sklearn_preds_slr\"][:5].to_numpy()\nprint(\"Sklearn solution: \", sklearn_5)\nanalytical_5 = penguins[\"analytical_preds_slr\"][:5].to_numpy()\nprint(\"Analytical solution: \", analytical_5)\n\nSklearn solution:  [17.81815239 18.10878351 18.63191952 18.51566707 18.3412884 ]\nAnalytical solution:  [17.81815239 18.10878351 18.63191952 18.51566707 18.3412884 ]\n\n\nYou can also use the model to predict what the bill depth of a hypothetical penguin with flipper length of 185mm would have.\n\n\nCode\n# this produces a warning since we\n# did not specify what X this refers\n# to, but since we only \n# have one input it is negligible\n\nmodel.predict([[185]]) \n\n\narray([18.05065728])\n\n\nWe can also check if the fitted parameters,\\(\\hat{\\theta}_0,\\hat{\\theta}_1\\), themselves are similar to our analytical solution. Note that since we can have at most 1 intercept in a SLR or OLS model, so we always get back a scalar value from .intercept. However, when OLS can have multiple coefficient values, so .coef returns an array.\n\n\nCode\ntheta0 = model.intercept_      # this a scalar\nprint(\"analytical bias term: \", theta0_hat)\nprint(\"sklearn bias term: \", theta0)\n\n\ntheta1 = model.coef_           # this an array\nprint(\"analytical coefficient terms: \", theta1_hat)\nprint(\"sklearn coefficient terms: \", theta1)\n\n\nanalytical bias term:  7.297305899612297\nsklearn bias term:  7.297305899612306\nanalytical coefficient terms:  0.058126223695067675\nsklearn coefficient terms:  [0.05812622]\n\n\n\n\n13.1.1.4 SLR sklearn Performance\nThe sklearn package also provides a function that computes the MSE from a list of observations and predictions. This avoids us having to manually compute MSE by first computing residuals.\ndocumentation\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\nMSE_sklearn = mean_squared_error(penguins[\"bill_depth_mm\"], penguins[\"sklearn_preds_slr\"])\nprint(\"MSE: \", MSE_sklearn)\n\n\nMSE:  1.3338778799806374\n\n\nWe’ve generated the exact same predictions and error as before, but without any need for manipulating matrices ourselves!\n\n\n\n13.1.2 Multiple Linear Regression\nIn the previous lecture, we expressed multiple linear regression using matrix notation.\n\\[\\hat{\\mathbb{Y}} = \\mathbb{X}\\theta\\]\n\n13.1.2.1 OLS w/ Derived Analytical Formulas\nWe used a geometric approach to derive the following expression for the optimal model parameters under MSE error, also called Ordinary Least Squares (OLS):\n\\[\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1}\\mathbb{X}^T \\mathbb{Y}\\]\nThat’s a whole lot of matrix manipulation. How do we implement it in Python?\nThere are three operations we need to perform here: multiplying matrices, taking transposes, and finding inverses.\n\nTo perform matrix multiplication, use the @ operator\nTo take a transpose, call the .T attribute of an array or DataFrame\nTo compute an inverse, use numpy’s in-built method np.linalg.inv\n\n\nX = penguins[[\"flipper_length_mm\", \"body_mass_g\"]].copy()\n\nX[\"bias\"] = np.ones(len(X))\ny = penguins[\"bill_depth_mm\"]\n\n\ntheta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\ntheta_hat  \n\n0     0.009828\n1     0.001477\n2    11.002995\ndtype: float64\n\n\nNote that since we added “bias” last, theta_hat[2] is our estimated value for the \\(\\theta_0\\). To make predictions using our newly-fitted model coefficients, matrix-multiply X and theta_hat.\n\ny_hat = X.to_numpy() @ theta_hat\n\n# Show just the first 5 predictions to save space on the page\ny_hat[:5]\npenguins[\"analytical_preds_ols\"] = y_hat\n\nNote, this technique doesn’t work if our X is not invertible.\n\n\n13.1.2.2 OLS w/ sklearn\nWe can actually compute the optimal parameters very easily using sklearn, using the exact code that we wrote earlier. Note: sklearn does NOT use the normal equations. Instead it uses gradient descent, a technique we will learn about soon, which can minimize ANY function, not just the MSE.\n\n\nCode\n# creating our design matrix.\n# Note:\n#  - no bias term needed here bc of sklearn automatically includes one\n#  - to remove the intercept term, set the fit_intercept = true in LinearRegression constructor.\n\nX_2d = penguins[[\"flipper_length_mm\", \"body_mass_g\"]]\ny = penguins[\"bill_depth_mm\"]\nmodel_2d = LinearRegression() # note fit_intercept=True by default\nmodel_2d.fit(X_2d, y)\n\n\nLinearRegression()\n\n\nNow we again have a model with which we can use to make predictions. For example, we can ask our model about a penguin’s bill depth if they have 185-mm flipper length and 3750 g body mass.\n\n\nCode\npenguins[\"sklearn_predictions_ols\"] = model_2d.predict(X_2d)\nmodel_2d.predict([[185, 3750]]) \n# since we have a 2d data matrix, we maintain the same \n# row-column expectation for our inputs. \n\n\narray([18.36187501])\n\n\nJust like with SLR, we cam also extract the coefficient estimates using .coef_ and .intercept. The reason why .intercept returns an array should now be more clear.\n\n\nCode\nprint(f\"(sklearn) theta0: {model_2d.intercept_}\")\nprint(f\"(analytical) theta0 {theta_hat[2]}\")\nprint(f\"(sklearn) theta1: {model_2d.coef_[0]}\")\nprint(f\"(analytical) theta1 {theta_hat[0]}\")\nprint(f\"(sklearn) theta2: {model_2d.coef_[1]}\")\nprint(f\"(analytical) theta2 {theta_hat[1]}\")\n\n\n(sklearn) theta0: 11.002995277447068\n(analytical) theta0 11.002995277447535\n(sklearn) theta1: 0.009828486885248712\n(analytical) theta1 0.0098284868852512\n(sklearn) theta2: 0.0014774959083212896\n(analytical) theta2 0.0014774959083213644\n\n\n\n\n13.1.2.3 Visualizing Our 2D Linear Model Predictions\nWhen we have two axis with which we can change an input, moving along this input plan creates a 2d plane with which we can get model outputs for. For example, for every single penguin with a flipper length , we also must specify a body mass. These two values in combination will help us predict the bill depth. Thus, we see that the predictions all lie in a 2d-plane. In higher dimensions, they all lie in a “hyperplane”.\n\n\nCode\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(penguins[\"flipper_length_mm\"], penguins[\"body_mass_g\"], penguins[\"bill_depth_mm\"])\nplt.xlabel('flipper_length_mm')\nplt.ylabel('body_mass_g')\n\n\nText(0.5, 0.5, 'body_mass_g')\n\n\n\n\n\n\n\nCode\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(penguins[\"flipper_length_mm\"], penguins[\"body_mass_g\"], penguins[\"bill_depth_mm\"])\nxx, yy = np.meshgrid(range(170, 220, 10), range(2500, 4500, 100))\nzz = ( 11.0029 + 0.00982 * xx + 0.001477 * yy) # thetas_using_sklearn\nax.plot_surface(xx, yy, zz, alpha=0.2)\nplt.xlabel('flipper_length_mm')\nplt.ylabel('body_mass_g')\nplt.gcf().savefig(\"plane.png\", dpi = 300, bbox_inches = \"tight\")\n\n\n\n\n\n\n\n\n13.1.3 Loss Terminology\nWe use the word “loss” in two different (but very related) contexts in this course.\n\nIn general, loss is the cost function that measures how far off model’s prediction(s) is(are) from the actual value(s).\n\nPer-datapoint loss is a cost function that measures the cost of \\(y\\) vs \\(\\hat{y}\\) for a particular datapoint.\nLoss (without any adjectives) is generally a cost function measured across all datapoints. Often times, empirical risk is average per-datapoint loss.\n\nWe prioritize using the latter term, because we don’t particularly look at a given datapoint’s loss when optimizing a model.\n\nIn other words, the dataset-level loss is the objective function that we’d like to minimize using gradient descent.\nWe achieve this minimization by using per-datapoint loss values."
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#gradient-descent",
    "href": "gradient_descent/gradient_descent.html#gradient-descent",
    "title": "13  Gradient Descent",
    "section": "13.2 Gradient Descent",
    "text": "13.2 Gradient Descent\nAt this point, we’re fairly comfortable with fitting a regression model under MSE risk (indeed, we’ve done it three times now!). It’s important to remember, however, that the results we’ve found previously apply to one very specific case: the equations we used above are only relevant to a linear regression model using MSE as the cost function. In reality, we’ll be working with a wide range of model types and objective functions, not all of which are as straightforward as the scenario we’ve discussed previously. This means that we need some more generalizable way of fitting a model to minimize loss.\nTo do this, we’ll introduce the technique of gradient descent.\n\n13.2.1 Minimizing a 1D Function\nLet’s shift our focus away from MSE to consider some new, arbitrary cost function. You can think of this function as outputting the empirical risk associated with some parameter theta.\n\n\n13.2.1.1 The Naive Approach: Guess and Check\nAbove, we saw that the minimum is somewhere around 5.3ish. Let’s see if we can figure out how to find the exact minimum algorithmically from scratch. One way very slow and terrible way would be manual guess-and-check.\n\n\nCode\ndef arbitrary(x):\n    return (x**4 - 15*x**3 + 80*x**2 - 180*x + 144)/10\n\ndef simple_minimize(f, xs):\n    # Takes in a function f and a set of values xs. \n    # Calculates the value of the function f at all values x in xs\n    # Takes the minimum value of f(x) and returns the corresponding value x \n    y = [f(x) for x in xs]  \n    return xs[np.argmin(y)]\n\nsimple_minimize(arbitrary, np.linspace(1, 7, 20))\n\n\n5.421052631578947\n\n\n\n\n13.2.1.2 Scipy.optimize.minimize\nOne way to minimize this mathematical function is to use the scipy.optimize.minimize function. It takes a function and a starting guess and tries to find the minimum.\n\n\nCode\nfrom scipy.optimize import minimize\n\n# takes a function f and a starting point x0 and returns a readout \n# with the optimal input value of x which minimizes f\nminimize(arbitrary, x0 = 3.5)\n\n\n      fun: -0.13827491292966557\n hess_inv: array([[0.73848255]])\n      jac: array([6.48573041e-06])\n  message: 'Optimization terminated successfully.'\n     nfev: 20\n      nit: 3\n     njev: 10\n   status: 0\n  success: True\n        x: array([2.39275266])\n\n\nOur choice of start point can affect the outcome. For example if we start to the left, we get stuck in the local minimum on the left side.\n\n\nCode\nminimize(arbitrary, x0 = 1)\n\n#here we see the optimal value is different from before\n\n\n      fun: -0.13827491294422317\n hess_inv: array([[0.74751575]])\n      jac: array([-3.7997961e-07])\n  message: 'Optimization terminated successfully.'\n     nfev: 16\n      nit: 7\n     njev: 8\n   status: 0\n  success: True\n        x: array([2.3927478])\n\n\nscipy.optimize.minimize is great. It may also seem a bit magical. How could you write a function that can find the minimum of any mathematical function? There are a number of ways to do this, which we’ll explore in today’s lecture, eventually arriving at the important idea of gradient descent, which is the principle that scipy.optimize.minimize uses.\nIt turns out that under the hood, the fit method for LinearRegression models uses gradient descent. Gradient descent is also how much of machine learning works, including even advanced neural network models.\nIn Data 100, the gradient descent process will usually be invisible to us, hidden beneath an abstraction layer. However, to be good data scientists, it’s important that we know the basic principles beyond the optimization functions that harness to find optimal parmaeters.\n\n\n\n13.2.2 Digging into Gradient Descent\nLooking at the function across this domain, it is clear that the function’s minimum value occurs around \\(\\theta = 5.3\\). Let’s pretend for a moment that we couldn’t see the full view of the cost function. How would we guess the value of \\(\\theta\\) that minimizes the function?\nIt turns out that the first derivative of the function can give us a clue. In the plots below, the line indicates the value of the derivative of each value of \\(\\theta\\). The derivative is negative where it is red and positive where it is green.\nSay we make a guess for the minimizing value of \\(\\theta\\). Remember that we read plots from left to right, and assume that our starting \\(\\theta\\) value is to the left of the optimal \\(\\hat{\\theta}\\). If the guess “undershoots” the true minimizing value – our guess for \\(\\theta\\) is not quite at the value of the \\(\\hat{\\theta}\\) that truly minimizes the function – the derivative will be negative in value. This means that if we increase \\(\\theta\\) (move further to the right), then we can decrease our loss function further. If this guess “overshoots” the true minimizing value, the derivative will be positive in value, implying the converse.\n\nWe can use this pattern to help formulate our next guess for the optimal \\(\\hat{\\theta}\\). Consider the case where we’ve undershot \\(\\theta\\) by guessing too low of a value. We’ll want our next guess to be greater in value than the previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope “downhill” to the function’s minimum value.\n\nIf we’ve overshot \\(\\hat{\\theta}\\) by guessing too high of a value, we’ll want our next guess to be lower in value – we want to shift our guess for \\(\\hat{\\theta}\\) to the left."
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#gradient-descent-in-1-dimension",
    "href": "gradient_descent/gradient_descent.html#gradient-descent-in-1-dimension",
    "title": "13  Gradient Descent",
    "section": "13.3 Gradient Descent in 1 Dimension",
    "text": "13.3 Gradient Descent in 1 Dimension\nThese observations lead us to the gradient descent update rule: \\[\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{d}{d\\theta}L(\\theta^{(t)})\\]\nBegin with our guess for \\(\\hat{\\theta}\\) at timestep \\(t\\). To find our guess for \\(\\hat{\\theta}\\) at the next timestep, \\(t+1\\), subtract the objective function’s derivative evaluted at \\(\\theta^{(t)}\\), \\(\\frac{d}{d\\theta} L(\\theta^{(t)})\\), scaled by a positive value \\(\\alpha\\). We’ve replaced the generic function \\(f\\) with \\(L\\) to indicate that we are minimizing loss.\nSupposing that any local minima is a global minimum (see convexity at the end of this page):\n\nIf our guess \\(\\theta^{(t)}\\) is to the left of \\(\\hat{\\theta}\\) (undershooting), the first derivative will be negative. Subtracting a negative number from \\(\\theta^{(t)}\\) will increase the value of the next guess, \\(\\theta^{(t+1)}\\) and move our loss function down. The guess will shift to the right.\nIf our guess \\(\\theta^{(t)}\\) was too high (overshooting \\(\\hat{\\theta}\\)), the first derivative will be positive. Subtracting a positive number from \\(\\theta^{(t)}\\) will decrease the value of the next guess, \\(\\theta^{(t+1)}\\) and move our loss function down. The guess will shift to the left.\n\nPut together, this captures the same behavior we reasoned through above. We repeatedly update our guess for the optimal \\(\\theta\\) until we’ve completed a set number of updates, or until each additional update iteration does not change the value of \\(\\theta\\). In this second case, we say that gradient descent has converged on a solution.\nThe \\(\\alpha\\) term in the update rule is known as the learning rate. It is a positive value represents the size of each gradient descent update step – in other words, how “far” should we step to the left or right with each updated guess? A high value of \\(\\alpha\\) will lead to large differences in value between consecutive guesses for \\(\\hat{\\theta}\\); a low value of \\(\\alpha\\) will result in smaller differences in value between consecutive guesses. This is the first example of a hyperparameter, a parameter that is hand picked by the data scientist that changes the model’s behavior, in the course.\n\n\nCode\n# define the derivative of the arbitrary function we want to minimize\ndef derivative_arbitrary(x):\n    return (4*x**3 - 45*x**2 + 160*x - 180)/10\n\ndef gradient_descent(df, initial_guess, alpha, n):\n    \"\"\"Performs n steps of gradient descent on df using learning rate alpha starting\n       from initial_guess. Returns a numpy array of all guesses over time.\"\"\"\n    guesses = [initial_guess]\n    current_guess = initial_guess\n    while len(guesses) < n:\n        current_guess = current_guess - alpha * df(current_guess)\n        guesses.append(current_guess)\n        \n    return np.array(guesses)\n\n# calling our function gives us the path that gradient descent takes for 20 steps \n# with a learning rate of 0.3 starting at theta = 4\ntrajectory = gradient_descent(derivative_arbitrary, 4, 0.3, 20)\ntrajectory\n\n\narray([4.        , 4.12      , 4.26729664, 4.44272584, 4.64092624,\n       4.8461837 , 5.03211854, 5.17201478, 5.25648449, 5.29791149,\n       5.31542718, 5.3222606 , 5.32483298, 5.32578765, 5.32614004,\n       5.32626985, 5.32631764, 5.32633523, 5.3263417 , 5.32634408])\n\n\nAbove, we’ve simply run our algorithm a fixed number of times. More sophisticated implementations will stop based on a variety of different stopping criteria, e.g. error getting too small, error getting too large, etc. We will not discuss these in our course.\n\n13.3.1 Application of 1D Gradient Descent\nWe’ve seen how to find the optimal parameters for a 1D linear model for the penguin dataset:\n\nUsing the derived equations from Data 8.\nUsing sklearn.\n\nUses gradient descent under the hood!\n\n\nIn real practice in this course, we’ll usually use sklearn. But for now, let’s see how we can do the gradient descent ourselves.\nLet’s consider a case where we have a linear model with no offset.\n\\[\\hat{y} = \\theta_1 x\\]\nWe want to find the parameter \\(\\theta_1\\) such that the L2 loss is minimized. In sklearn, this is easy. To avoid fitting an intercept, we set fit_intercept to false.\n\n\nCode\nmodel = LinearRegression(fit_intercept = False)\ndf = sns.load_dataset(\"tips\")\nmodel.fit(df[[\"total_bill\"]], df[\"tip\"])\nmodel.coef_ # the optimal tip percentage is 14.37%\n\n\narray([0.1437319])\n\n\n\n\n13.3.2 Creating an Explicit MSE Function\nTo employ gradient descent and do this ourselves, we need to define a function upon which we can use gradient descent. Suppose we select the L2 loss as our loss function. In this case, our goal will be to minimize the mean squared error.\nLet’s start by writing a function that computes the MSE for a given choice of \\(\\theta_1\\) on our dataset.\n\n\nCode\ndef mse_single_arg(theta1):\n    \"\"\"Returns the MSE on our data for the given theta1\"\"\"\n    x = df[\"total_bill\"]\n    y_obs = df[\"tip\"]\n    y_hat = theta1 * x\n    return np.mean((y_hat - y_obs) ** 2)\n\nmse_single_arg(0.1437)\n# The minimum loss value that we can achieve is 1.178 dollars on average away from the truth\n\n\n1.1781165940051928\n\n\n\n\n13.3.3 Plotting the MSE Function\nSince we only have 1 parameter, we can simply cross reference our results with a simnple plot. We do not want to always do this since some functions can have thousands of inputs, making them difficult to plot. We can plot the MSE as a function of theta1. It turns out to look pretty smooth, and quite similar to a parabola.\n\n\nCode\ntheta1s = np.linspace(0, 0.2, 200)\nx = df[\"total_bill\"]\ny_obs = df[\"tip\"]\n\nMSEs = [mse_single_arg(theta1) for theta1 in theta1s]\n\nplt.plot(theta1s, MSEs)\nplt.xlabel(r\"Choice for $\\theta_1$\")\nplt.ylabel(r\"MSE\");\n\n\n\n\n\nThe minimum appears to be around \\(\\theta_1 = 0.14\\). We can once again check this naively.\n\n\nCode\nsimple_minimize(mse_single_arg, np.linspace(0, 0.2, 21))\n\n\n0.14\n\n\nAs before, what we’re doing is computing all the starred values below and then returning the \\(\\theta_1\\) that goes with the minimum value.\n\n\nCode\ntheta1s = np.linspace(0, 0.2, 200)\nsparse_theta1s = np.linspace(0, 0.2, 21)\n\nloss = [mse_single_arg(theta1) for theta1 in theta1s]\nsparse_loss = [mse_single_arg(theta1) for theta1 in sparse_theta1s]\n\nplt.plot(theta1s, loss)\nplt.plot(sparse_theta1s, sparse_loss, 'r*')\nplt.xlabel(r\"Choice for $\\theta_1$\")\nplt.ylabel(r\"MSE\");\n\n\n\n\n\n\n13.3.3.1 Using Scipy.Optimize.minimize\n\n\nCode\nimport scipy.optimize\nfrom scipy.optimize import minimize\nminimize(mse_single_arg, x0 = 0)\n\n\n      fun: 1.1781161154513213\n hess_inv: array([[1]])\n      jac: array([4.24683094e-06])\n  message: 'Optimization terminated successfully.'\n     nfev: 6\n      nit: 1\n     njev: 3\n   status: 0\n  success: True\n        x: array([0.14373189])\n\n\n\n\n13.3.3.2 Using Our Gradient Descent Function\nAnother approach is to use our 1D gradient descent algorithm from earlier. This is the exact same function as earlier. We can run it for 100 steps and see where it ultimately ends up.\n\n\nCode\ndef mse_loss_derivative_single_arg(theta_1):\n    \"\"\"Returns the derivative of the MSE on our data for the given theta1\"\"\"\n    x = df[\"total_bill\"]\n    y_obs = df[\"tip\"]\n    y_hat = theta_1 * x\n    \n    return np.mean(2 * (y_hat - y_obs) * x)\n\ngradient_descent(mse_loss_derivative_single_arg, 0.05, 0.0001, 100)[-5:]\n\n\narray([0.14372404, 0.14372478, 0.14372545, 0.14372605, 0.1437266 ])"
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#multidimensional-gradient-descent",
    "href": "gradient_descent/gradient_descent.html#multidimensional-gradient-descent",
    "title": "13  Gradient Descent",
    "section": "13.4 Multidimensional Gradient Descent",
    "text": "13.4 Multidimensional Gradient Descent\nWe’re in good shape now: we’ve developed a technique to find the minimum value of a more complex objective function.\nThe function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, \\(\\theta\\). However, as we’ve seen before, we often need to optimize a cost function with respect to several parameters (for example, when selecting the best model parameters for multiple linear regression). We’ll need to extend our gradient descent rule to multidimensional objective functions.\nNow suppose we improve our model so that we want to predict the tip from the total_bill plus a constant offset, in other words:\n\\[\\textrm{tip} = \\theta_0 + \\theta_1 \\textrm{bill}\\]\nTo put this in more concrete terms what this means, let’s return to the familiar case of simple linear regression with MSE loss. \\[\\text{MSE}(\\theta_0,\\:\\theta_1) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x)^2\\]\nNow, loss is expressed in terms of two parameters, \\(\\theta_0\\) and \\(\\theta_1\\). Rather than a one-dimensional loss function as we had above, we are now dealing with a two-dimensional loss surface.\n\n\nCode\n# This code is for illustration purposes only\n# It contains a lot of syntax you have not seen\nimport plotly.graph_objects as go\n\nmodel = LinearRegression(fit_intercept = False)\ndf = sns.load_dataset(\"tips\")\ndf[\"bias\"] = 1\nmodel.fit(df[[\"bias\",\"total_bill\"]], df[\"tip\"])\nmodel.coef_\n\nuvalues = np.linspace(0, 2, 10)\nvvalues = np.linspace(0, 0.2, 10)\n(u,v) = np.meshgrid(uvalues, vvalues)\nthetas = np.vstack((u.flatten(),v.flatten()))\n\nX = df[[\"bias\",\"total_bill\"]].to_numpy()\nY = df[\"tip\"].to_numpy()\n\ndef mse_loss_single_arg(theta):\n    return mse_loss(theta, X, Y)\n\ndef mse_loss(theta, X, y_obs):\n    y_hat = X @ theta\n    return np.mean((y_hat - Y) ** 2)    \n\nMSE = np.array([mse_loss_single_arg(t) for t in thetas.T])\n\nloss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n\nind = np.argmin(MSE)\noptimal_point = go.Scatter3d(name = \"Optimal Point\",\n    x = [thetas.T[ind,0]], y = [thetas.T[ind,1]], \n    z = [MSE[ind]],\n    marker=dict(size=10, color=\"red\"))\n\nfig = go.Figure(data=[loss_surface, optimal_point])\nfig.update_layout(scene = dict(\n    xaxis_title = \"theta0\",\n    yaxis_title = \"theta1\",\n    zaxis_title = \"MSE\"))\nfig.show()\n\n\n\n                                                \n\n\nThough our objective function looks a little different, we can use the same principles as we did earlier to locate the optimal model parameters. Notice how the minimum value of MSE, marked by the red dot in the plot above, occurs in the “valley” of the loss surface. Like before, we want our guesses for the best pair of \\((\\theta_0,\\:\\theta_1)\\) to move “downhill” towards this minimum point.\nThe difference now is that we need to update guesses for both \\(\\theta_0\\) and \\(\\theta_1\\) that minimize a loss function \\(L(\\theta, \\mathbb{X}, \\mathbb{Y})\\): \\[\\theta_0^{(t+1)} = \\theta_0^{(t)} - \\alpha \\frac{\\partial L}{\\partial \\theta_0}\\Bigm\\vert_{\\theta=\\theta^{(t)}} \\qquad \\qquad \\theta_1^{(t+1)} = \\theta_1^{(t)} - \\alpha \\frac{\\partial L}{\\partial \\theta_1}\\Bigm\\vert_{\\theta=\\theta^{(t)}}\\]\nWe can tidy this statement up by using vector notation: \\[\\begin{bmatrix}\n           \\theta_{0}^{(t+1)} \\\\\n           \\theta_{1}^{(t+1)} \\\\\n         \\end{bmatrix}\n=\n\\begin{bmatrix}\n           \\theta_{0}^{(t)} \\\\\n           \\theta_{1}^{(t)} \\\\\n         \\end{bmatrix}\n- \\alpha\n\\begin{bmatrix}\n           \\frac{\\partial L}{\\partial \\theta_{0}}\\vert_{\\theta=\\theta^{(t)}} \\\\\n           \\frac{\\partial L}{\\partial \\theta_{1}}\\vert_{\\theta=\\theta^{(t)}} \\\\\n         \\end{bmatrix}\n\\]\nTo save ourselves from writing out long column vectors, we’ll introduce some new notation. \\(\\vec{\\theta}^{(t)}\\) is a column vector of guesses for each model parameter \\(\\theta_i\\) at timestep \\(t\\). We call \\(\\nabla_{\\vec{\\theta}} L\\) the gradient vector. In plain English, it means “take the derivative of loss, \\(L(\\theta, \\mathbb{X}, \\mathbb{Y})\\), with respect to each model parameter in \\(\\vec{\\theta}\\), and evaluate it at the given \\(\\theta = \\theta^{(t)}\\).”\n\\[\\vec{\\theta}^{(t+1)}\n= \\vec{\\theta}^{(t)} - \\alpha \\nabla_{\\vec{\\theta}} L(\\theta^{(t)}, \\mathbb{X}, \\mathbb{Y})\n\\]\n\n13.4.1 Gradient Notation\nConsider the 2D function: \\[f(\\theta_0, \\theta_1) = 8\\theta_0^2 + 3\\theta_0\\theta_1\\]\nFor a function of 2 variables, \\(f(\\theta_0, \\theta_1)\\) we define the gradient as \\(\\nabla_\\theta f = \\frac{\\partial f}{\\partial \\theta_0} \\vec{i} + \\frac{\\partial f}{\\partial \\theta_1} \\vec{j}\\), where \\(\\vec{i}\\) and \\(\\vec{j}\\) are the unit vectors in the \\(\\theta_0\\) and \\(\\theta_1\\) directions.\n\\[\\frac{\\partial f}{\\partial \\theta_0} = 16\\theta_0 + 3\\theta_1\\]\n\\[\\frac{\\partial f}{\\partial \\theta_1} = 3\\theta_0\\]\n\\[\\nabla_\\theta f =  (16\\theta_0 + 3\\theta_1)\\vec{i} + (3\\theta_0)\\vec{j}\\]\nWe can also write it in column vector notation.\n\\[\\nabla_\\theta f =  \\begin{bmatrix} \\frac{\\partial f}{\\partial \\theta_0} \\\\ \\frac{\\partial f}{\\partial \\theta_1} \\\\... \\end{bmatrix}\\]\nEX: \\(\\nabla_\\theta f = \\begin{bmatrix}16\\theta_0 + 3\\theta_1 \\\\ 3\\theta_0\\end{bmatrix}\\)\nYou should read these gradients as:\n\n\\(\\frac{\\partial f}{\\partial \\theta_0}\\) : If I nudge the 1st model weight, what happens to loss?\n\\(\\frac{\\partial f}{\\partial \\theta_1}\\) :If I nudge the 2nd model weight, what happens to loss?\netc.\n\n\n\n13.4.2 Visualizing Gradient Descent\nFirst, we need to be able to easily determine the gradient for any pair of values, \\(\\theta_0, \\theta_1\\).\n\n\nCode\ntips_with_bias = df.copy()\ntips_with_bias[\"bias\"] = 1\nX = tips_with_bias[[\"bias\", \"total_bill\"]]\nX.head(5)\n\ndef mse_gradient(theta, X, y_obs):\n    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"    \n    x0 = X.iloc[:, 0]\n    x1 = X.iloc[:, 1]\n    dth0 = np.mean(-2 * (y_obs - theta[0]*x0 - theta[1]*x1) * x0)\n    dth1 = np.mean(-2 * (y_obs - theta[0]*x0 - theta[1]*x1) * x1)\n    return np.array([dth0, dth1])\n\ndef mse_gradient_single_arg(theta):\n    \"\"\"Returns the gradient of the MSE on our data for the given theta\"\"\"\n    X = tips_with_bias[[\"bias\", \"total_bill\"]]\n    y_obs = tips_with_bias[\"tip\"]\n    return mse_gradient(theta, X, y_obs)\n\nX = tips_with_bias[[\"bias\", \"total_bill\"]]\ny_obs = tips_with_bias[\"tip\"]\nex1_mse = mse_gradient(np.array([0, 0]), X, y_obs)\n\nprint(f\"Gradient for values theta0 = 0 and theta1 = 0 : {ex1_mse}\")\n\n\nGradient for values theta0 = 0 and theta1 = 0 : [  -5.99655738 -135.22631803]\n\n\nUsing our previously previously defined gradient_descent function, we can see if our intuition extends to higher dimensions.\n\n\nCode\n#print out the last 10 guesses our algorithm outputs to save space\nguesses = gradient_descent(mse_gradient_single_arg, np.array([0, 0]), 0.001, 10000)[-10:]"
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#mini-batch-gradient-decsent-and-stochastic-gradient-descent",
    "href": "gradient_descent/gradient_descent.html#mini-batch-gradient-decsent-and-stochastic-gradient-descent",
    "title": "13  Gradient Descent",
    "section": "13.5 Mini-Batch Gradient Decsent and Stochastic Gradient Descent",
    "text": "13.5 Mini-Batch Gradient Decsent and Stochastic Gradient Descent\nFormally, the algorithm we derived above is called batch gradient descent. For each iteration of the algorithm, the derivative of loss is computed across the entire batch of available data. While this update rule works well in theory, it is not practical in all circumstances. For large datasets (with perhaps billions of data points), finding the gradient across all the data is incredibly computationally taxing.\nMini-batch gradient descent tries to address this issue. In mini-batch descent, only a subset of the data is used to compute an estimate of the gradient. For example, we might consider only 10% of the total data at each gradient descent update step. At the next iteration, a different 10% of the data is sampled to perform the following update. Once the entire dataset has been used, the process is repeated. Each complete “pass” through the data is known as a training epoch. In practice, we choose the mini-batch size to be \\(32\\).\nIn the extreme case, we might choose a batch size of only 1 data point – that is, a single data point is used to estimate the gradient of loss with each update step. This is known as stochastic gradient descent.\nBatch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, both mini-batch and stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for \\(\\vec{\\theta}\\) at each iteration, there’s a chance the algorithm will not progress towards the true minimum of loss with each update. Over the longer term, these stochastic techniques should still converge towards the optimal solution.\nThe diagrams below represent a “bird’s eye view” of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal \\(\\hat{\\theta}\\). Stochastic gradient descent, in contrast, “hops around” on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step."
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#convexity",
    "href": "gradient_descent/gradient_descent.html#convexity",
    "title": "13  Gradient Descent",
    "section": "13.6 Convexity",
    "text": "13.6 Convexity\nIn our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum just to the left?\nIf we had chosen a different starting guess for \\(\\theta\\), or a different value for the learning rate \\(\\alpha\\), we may have converged on the local minimum, rather than on the true optimum value of loss.\n\nIf the loss function is convex, gradient descent is guaranteed to find the global minimum of the objective function. Formally, a function \\(f\\) is convex if: \\[tf(a) + (1-t)f(b) \\geq f(ta + (1-t)b)\\]\nTo put this into words: if you drew a line between any two points on the curve, all values on the curve must be on or below the line. Importantly, any local minimum of a convex function is also its global minimum.\n\nIn summary, non-convex loss functions can cause problems with optimization. This means that our choice of loss function is an key factor in our modeling process. It turns out that MSE is convex, which is a major reason why it is such a popular choice of loss function."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#feature-engineering",
    "href": "feature_engineering/feature_engineering.html#feature-engineering",
    "title": "14  Feature Engineering",
    "section": "14.1 Feature Engineering",
    "text": "14.1 Feature Engineering\nFeature Engineering is the process of transforming the raw features into more informative features that can be used in modeling or EDA tasks.\nFeature engineering allows you to: Capture domain knowledge (e.g. periodicity or relationships between features). Express non-linear relationships using simple linear models. Encode non-numeric features to be used as inputs to models. Example: Using the country of origin of a car as an input to modeling its efficiency.\nWhy doesn’t sklearn doesn’t have SquareRegression /PolynomialRegression.\n\nWe can translate these into linear models with features that are polynomials of x.\nFeature engineering saves sklearn a lot of redundancy in their library.\nLinear models have really nice properties."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#feature-functions",
    "href": "feature_engineering/feature_engineering.html#feature-functions",
    "title": "14  Feature Engineering",
    "section": "14.2 Feature Functions",
    "text": "14.2 Feature Functions\nA feature function takes our original d dimensional input, \\(\\mathbb{X}\\), and transforms it into a \\(d'\\) dimensional input \\(\\Phi\\).\nFor example, when we add the squared term of an existing column, we are effectively using kind of feature function, taking a \\(n \\times 1\\) matrix, \\([hp]\\), and turning it into an \\(n \\times 2\\) matrix \\([hp,hp^2]\\)\nAs number of features grows, we can capture arbitrarily complex relationships.\nLet’s take a moment to dig further in to remind ourselves where this linearity comes from. Consider the following dataset on vehicles:\n\n\nCode\nimport seaborn as sns\nvehicles = sns.load_dataset(\"mpg\").rename(columns={\"horsepower\":\"hp\"}).dropna()\nvehicles.head(5)\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      hp\n      weight\n      acceleration\n      model_year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      70\n      usa\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      70\n      usa\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      70\n      usa\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      70\n      usa\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      70\n      usa\n      ford torino\n    \n  \n\n\n\n\nSuppose we wish to develop a model to predict a vehicle’s fuel efficiency (\"mpg\") as a function of its horsepower (\"hp\"). Glancing at the plot below, we see that the relationship between \"mpg\" and \"hp\" is non-linear – an SLR fit doesn’t capture the relationship between the two variables.\n\nRecall our standard multiple linear regression model. In its current form, it is linear in terms of both \\(\\theta_i\\) and \\(x\\):\n\\[\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x\\:+\\:...\\]\nJust by eyeballing the vehicle data plotted above, it seems that a quadratic model might be more appropriate. In other words, a model of the form below would likely do a better job of capturing the non-linear relationship between the two variables:\n\\[\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2\\]\nThis looks fairly similar to our original multiple regression framework! Importantly, it is still linear in \\(\\theta_i\\) – the prediction \\(\\hat{y}\\) is a linear combination of the model parameters. This means that we can use the same linear algebra methods as before to derive the optimal model parameters when fitting the model.\nYou may be wondering: how can this be a linear model if there is now a \\(x^2\\) term? Although the model contains non-linear \\(x\\) terms, it is linear with respect to the model parameters, \\(\\theta_i\\). Because our OLS derivation relied on assuming a linear model of \\(\\theta_i\\), the method is still valid to fit this new model.\nIf we refit the model with \"hp\" squared as its own feature, we see that the model follows the data much more closely.\n\\[\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2\\]\n\nLooks much better! What we’ve done here is called feature engineering: the process of transforming the raw features of a dataset into more informative features for modeling. By squaring the \"hp\" feature, we were able to create a new feature that signficantly improved the quality of our model.\nWe perform feature engineering by defining a feature function. A feature function is some function applied to the original variables in the data to generate one or more new features. More formally, a feature function is said to take a \\(d\\) dimensional input and transform it to a \\(p\\) dimensional input. This results in a new, feature-engineered design matrix that we rename \\(\\Phi\\).\n\\[\\mathbb{X} \\in \\mathbb{R}^{n \\times d} \\longrightarrow \\Phi \\in \\mathbb{R}^{n \\times p}\\]\nIn the vehicles example above, we applied a feature function to transform the original input with \\(d=1\\) features into an engineered design matrix with \\(p=2\\) features."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#one-hot-encoding",
    "href": "feature_engineering/feature_engineering.html#one-hot-encoding",
    "title": "14  Feature Engineering",
    "section": "14.3 One Hot Encoding",
    "text": "14.3 One Hot Encoding\nFeature engineering opens up a whole new set of possibilities for designing better performing models. As you will see in lab and homework, feature engineering is one of the most important parts of the entire modeling process.\nA particularly powerful use of feature engineering is to allow us to perform regression on non-numeric features. One hot encoding is a feature engineering technique that generates numeric features from categorical data, allowing us to use our usual methods to fit a regression model on the data.\nTo illustrate how this works, we’ll refer back to the tips data from last lecture. Consider the \"day\" column of the dataset:\n\n\nCode\nimport numpy as np\nnp.random.seed(1337)\ntips_df = sns.load_dataset(\"tips\").sample(100)\ntips_df[[\"day\"]].head(5)\n\n\n\n\n\n\n  \n    \n      \n      day\n    \n  \n  \n    \n      54\n      Sun\n    \n    \n      46\n      Sun\n    \n    \n      86\n      Thur\n    \n    \n      199\n      Thur\n    \n    \n      106\n      Sat\n    \n  \n\n\n\n\nAt first glance, it doesn’t seem possible to fit a regression model to this data – we can’t directly perform any mathematical operations on the entry “Thur”.\nTo resolve this, we instead create a new table with a feature for each unique value in the original \"day\" column. We then iterate through the \"day\" column. For each entry in \"day\" we fill the corresponding feature in the new table with 1. All other features are set to 0.\n\nThis can be implemented in code using sklearn’s OneHotEncoder() to generate the one hot encoding, then calling pd.concat to combine these new features with the original DataFrame.\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Perform the one hot encoding\noh_enc = OneHotEncoder()\noh_enc.fit(tips_df[['day']])\nohe_data = oh_enc.transform(tips_df[['day']]).toarray()\n\n\n# Combine with original features\ndata_w_ohe = (tips_df\n              .join(\n                  pd.DataFrame(ohe_data, columns=oh_enc.get_feature_names(), index=tips_df.index))\n             )\ndata_w_ohe\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      x0_Fri\n      x0_Sat\n      x0_Sun\n      x0_Thur\n    \n  \n  \n    \n      54\n      25.56\n      4.34\n      Male\n      No\n      Sun\n      Dinner\n      4\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      46\n      22.23\n      5.00\n      Male\n      No\n      Sun\n      Dinner\n      2\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      86\n      13.03\n      2.00\n      Male\n      No\n      Thur\n      Lunch\n      2\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      199\n      13.51\n      2.00\n      Male\n      Yes\n      Thur\n      Lunch\n      2\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      106\n      20.49\n      4.06\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      44\n      30.40\n      5.60\n      Male\n      No\n      Sun\n      Dinner\n      4\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      221\n      13.42\n      3.48\n      Female\n      Yes\n      Fri\n      Lunch\n      2\n      1.0\n      0.0\n      0.0\n      0.0\n    \n    \n      59\n      48.27\n      6.73\n      Male\n      No\n      Sat\n      Dinner\n      4\n      0.0\n      1.0\n      0.0\n      0.0\n    \n    \n      100\n      11.35\n      2.50\n      Female\n      Yes\n      Fri\n      Dinner\n      2\n      1.0\n      0.0\n      0.0\n      0.0\n    \n    \n      127\n      14.52\n      2.00\n      Female\n      No\n      Thur\n      Lunch\n      2\n      0.0\n      0.0\n      0.0\n      1.0\n    \n  \n\n100 rows × 11 columns\n\n\n\nNow, the “day” feature (or rather, the four new boolean features that represent day) can be used to fit a model."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#higher-order-polynomial-example",
    "href": "feature_engineering/feature_engineering.html#higher-order-polynomial-example",
    "title": "14  Feature Engineering",
    "section": "14.4 Higher-order Polynomial Example",
    "text": "14.4 Higher-order Polynomial Example\nLet’s return to where we started today: Creating higher-order polynomial features for the mpg dataset.\nWhat happens if we add a feature corresponding to the horsepower, cubed? or to the fourth power? the fifth power?\n\nWill we get better results?\nWhat will the model look like?\n\nLet’s try it out. The below code plots polynomial models fit to the mpg dataset, from order 0 (the constant model) to order 5 (polynomial features through horsepower to the fifth power).\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n\nvehicle_data = sns.load_dataset(\"mpg\")\nvehicle_data = vehicle_data.rename(columns = {\"horsepower\": \"hp\"})\nvehicle_data = vehicle_data.dropna()\n\n\ndef get_MSE_for_degree_k_model(k):\n    pipelined_model = Pipeline([\n        ('poly_transform', PolynomialFeatures(degree = k)),\n        ('regression', LinearRegression(fit_intercept = True))    \n    ])\n    pipelined_model.fit(vehicle_data[[\"hp\"]], vehicle_data[\"mpg\"])\n    return mean_squared_error(pipelined_model.predict(vehicle_data[[\"hp\"]]), vehicle_data[\"mpg\"])\n\nks = np.array(range(0, 7))\nMSEs = [get_MSE_for_degree_k_model(k) for k in ks]\nMSEs_and_k = pd.DataFrame({\"k\": ks, \"MSE\": MSEs})\nMSEs_and_k.set_index(\"k\")\n\ndef plot_degree_k_model(k, MSEs_and_k, axs):\n    pipelined_model = Pipeline([\n        ('poly_transform', PolynomialFeatures(degree = k)),\n        ('regression', LinearRegression(fit_intercept = True))    \n    ])\n    pipelined_model.fit(vehicle_data[[\"hp\"]], vehicle_data[\"mpg\"])\n    \n    row = k // 3\n    col = k % 3\n    ax = axs[row, col]\n    \n    sns.scatterplot(data=vehicle_data, x='hp', y='mpg', ax=ax)\n    \n    x_range = np.linspace(45, 210, 100).reshape(-1, 1)\n    ax.plot(x_range, pipelined_model.predict(pd.DataFrame(x_range, columns=['hp'])), c='orange', linewidth=2)\n    \n    ax.set_ylim((0, 50))\n    mse_str = f\"MSE: {MSEs_and_k.loc[k, 'MSE']:.4}\\norder: {k}\"\n    ax.text(150, 30, mse_str, dict(size=13))\n\nfig = plt.figure(figsize=(10, 4))\naxs = fig.subplots(nrows=2, ncols=3)\n\nfor k in range(6):\n    plot_degree_k_model(k, MSEs_and_k, axs)\nfig.tight_layout()\n\n\n\n\n\nWith constant and linear models, there seems to be a clear pattern in prediction error. With a quadratic model (order 2), the plot seems to match our data much more consistently across different hp values. For higher order polynomials, we observe a small improvement in MSE, but not much beyond 18.98. The MSE will continue to marginally decrease as we add more and more terms to our model. However, there ain’t no free lunch. This decreasing of the MSE is coming at a major cost!"
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#variance-and-training-error",
    "href": "feature_engineering/feature_engineering.html#variance-and-training-error",
    "title": "14  Feature Engineering",
    "section": "14.5 Variance and Training Error",
    "text": "14.5 Variance and Training Error\nWe’ve seen now that feature engineering allows us to build all sorts of features to improve the performance of the model. In particular, we saw that designing a more complex feature (squaring \"hp\" in the vehicles data previously) substantially improved the model’s ability to capture non-linear relationships. To take full advantage of this, we might be inclined to design increasingly complex features. Consider the following three models, each of different order (the maximum exponent power of each model):\n\nModel with order 1: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp})\\)\nModel with order 2: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2\\)\nModel with order 4: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2 + \\theta_3 (\\text{hp})^3 + \\theta_4 (\\text{hp})^4\\)\n\n\n\nWhen we use our model to make predictions on the same data that was used to fit the model, we find that the MSE decreases with increasingly complex models. The training error is the model’s error when generating predictions from the same data that was used for training purposes. We can conclude that the training error goes down as the complexity of the model increases.\n\nThis seems like good news – when working on the training data, we can improve model performance by designing increasingly complex models.\nHowever, high model complexity comes with its own set of issues. When a model has many complicated features, it becomes increasingly sensitive to the data used to fit it. Even a small variation in the data points used to train the model may result in wildly different results for the fitted model. The plots below illustrate this idea. In each case, we’ve fit a model to two very similar sets of data (in fact, they only differ by two data points!). Notice that the model with order 2 appears roughly the same across the two sets of data; in contrast, the model with order 4 changes erratically across the two datasets.\n\nThe sensitivity of the model to the data used to train it is called the model variance. As we saw above, model variance tends to increase with model complexity.\n\nWe will further explore this tradeoff (and more precisely define model variance) in future lectures."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#overfitting",
    "href": "feature_engineering/feature_engineering.html#overfitting",
    "title": "14  Feature Engineering",
    "section": "14.6 Overfitting",
    "text": "14.6 Overfitting\nWe can see that there is a clear “trade-off” that comes from the complexity of our model. As model complexity increases, the model’s error on the training data decreases. At the same time, the model’s variance tends to increase.\nWhy does this matter? To answer this question, let’s take a moment to review our modeling workflow when making predictions on new data.\n\nSample a dataset of training data from the real world\nUse this training data to fit a model\nApply this fitted model to generate predictions on unseen data\n\nThis first step – sampling training data – is important to remember in our analysis. As we saw above, a highly complex model may produce results that vary wildly across different samples of training data. If we happen to sample a set of training data that is a poor representation of the population we are trying to model, our model may perform poorly on any new set of data it has not seen before.\nTo see why, consider a model fit using the training data shown on the left. Because the model is so complex, it achieves zero error on the training set – it perfectly predicts each value in the training data! When we go to use this model to make predictions on a new sample of data, however, things aren’t so good. The model now has enormous error on the unseen data.\n\nThe phenomenon above is called overfitting. The model effectively just memorized the training data it encountered when it was fitted, leaving it unable to handle new situations.\nThe takeaway here: we need to strike a balance in the complexity of our models. A model that is too simple won’t be able to capture the key relationships between our variables of interest; a model that is too complex runs the risk of overfitting.\nThis begs the question: how do we control the complexity of a model? Stay tuned for the next lecture."
  },
  {
    "objectID": "cv_regularization/cv_reg.html#cross-validation",
    "href": "cv_regularization/cv_reg.html#cross-validation",
    "title": "15  Cross Validation and Regularization",
    "section": "15.1 Cross Validation",
    "text": "15.1 Cross Validation\nFrom the last lecture, we learned that increasing model complexity decreased our model’s training error but increased variance. This makes intuitive sense; adding more features causes our model to better fit the given data, but generalize worse to new data. For this reason, a low training error is not representative of our model’s underlying performance – this may be a side effect of overfitting.\nTruly, the only way to know when our model overfits is by evaluating it on unseen data. Unfortunately, that means we need to wait for more data. This may be very expensive and time consuming.\nHow should we proceed? In this section, we will build up a viable solution to this problem.\n\n15.1.1 The Holdout Method\nThe simplest approach to avoid overfitting is to keep some of our data secret from ourselves. This is known as the holdout method. We will train our models on most of the available data points – known as the training data . We’ll then evaluate the models’ performance on the unseen data points (called the validation set) to measure overfitting.\nImagine the following example where we wish to train a model on 35 data points. We choose the training set to be a random sample of 25 of these points, and the validation set to be the remaining 10 points. Using this data, we train 7 models, each with a higher polynomial degree than the last.\nWe get the following mean squared error (MSE) on the training data.\n\n\n\n\n\n\n\n\n\nUsing these same models, we compute the MSE on our 10 validation points and observe the following.\n\n\n\n\n\n\n\n\n\nNotice how the training error monotonically decreases as polynomial degree increases. This is consistent with our knowledge. However, validation error first decreases, then increases (from k = 3). These higher degree models are performing worse on unseen data – this indicates they are overfitting. As such, the best choice of polynomial degree in this example is k = 2.\nMore generally, we can represent this relationship with the following diagram.\n\nOur goal is to train a model with complexity near the red line. Note that this relationship is a simplification of the real-world. But for purposes of Data 100, this is good enough.\n\n15.1.1.1 Hyperparameters\nIn machine learning, a hyperparameter is a value that controls the learning process. In our example, we built seven models, each of which had a hyperparameter k that controlled the polynomial degree of the model.\nTo choose between hyperparameters, we use the validation set. This was evident in our example above; we found that k = 2 had the lowest validation error. However, this holdout method is a bit naive. Imagine our random sample of 10 validation points coincidentally favored a higher degree polynomial. This would have led us to incorrectly favor a more complex model. In other words, our small amount of validation data may be different from real-world data.\nTo minimize this possiblity, we need to evaluate our model on more data. Decreasing the size of the training set is not an option – doing so will worsen our model. How should we proceed?\n\n\n\n15.1.2 K-Fold Cross Validation\nIn the holdout method, we train a model on only the training set, and assess the quality only on the validation set. On the other hand, K-Fold Cross Validation is a technique that determines the quality of a hyperparameter by evaluating a model-hyperparameter combination on k independent “folds” of data, which together make up the entire dataset. This is a more robust alternative to the holdout method. Let’s break down each step.\nNote: The k in k-fold cross validation is different from the k polynomial degree discussed in the earlier example.\nIn the k-fold cross-validation approach, we split our data into k equally sized groups (often called folds). In the example where k = 5:\n\nTo determine the “quality” of a particular hyperparameter:\n\nPick a fold, which we’ll call the validation fold. Train a model on all other k-1 folds. Compute an error on the validation fold.\nRepeat the step above for all k possible choices of validation fold, each time training a new model.\nAverage the k validation fold errors. This will give a single error for the hyperparameter.\n\nFor k = 5, we have the following partitions. At each iteration, we train a model on the blue data and validate on the orange data. This gives us a total of 5 errors which we average to obtain a single representative error for the hyperparameter.\n\nNote that the value of the hyperparameter is fixed during this process. By doing so, we can be confident in the hyperparameter’s performance on the entire dataset. To compare multiple choices of a hyperparameter –say m choices of hyperparameter– we run k-fold cross validation m times. The smallest of the m resulting errors corresponds to the best hyperparameter value.\n\n15.1.2.1 Hyperparameter Selection Example\nK-fold cross validation can aid in choosing the best hyperparameter values in respect to our model and loss function.\nConsider an example where we run k-fold cross validation with k = 3. We are implementing a model that depends on a hyperparameter \\(\\alpha\\), and we are searching for an \\(\\alpha\\) that minimizes our loss function. We have narrowed down our hyperparameters such that \\(\\alpha = [0.01, 0.1, 1, 10]\\).\n\nThe losses of the model are shown per k fold of training data (arrows are pointing to the loss value) for each value of \\(\\alpha\\). The average loss over the k-fold cross validation is displayed to the right of all the k-fold losses.\nTo determine the best \\(\\alpha\\) value, we must compare the average loss over the k-folds of training data. \\(\\alpha = 0.01\\) yields us an average loss of \\(5\\), \\(\\alpha = 0.1\\) yields us an average loss of \\(4.67\\), \\(\\alpha = 1\\) yields us an average loss of \\(7\\), and \\(\\alpha = 10\\) yields us an average loss of \\(10.67\\).\nThus, we would select \\(\\alpha = 0.1\\) as our hyperparameter value as it results in the lowest average loss over the k-folds of training data out of our possible \\(\\alpha\\) values.\n\n\n15.1.2.2 Picking K\nTypical choices of k are 5, 10, and N, where N is the number of data points.\nk = N is known as “leave one out cross validation”, and will typically give you the best results.\n\nIn this approach, each validation set is only one point.\nEvery point gets a chance to get used as the validation set.\n\nHowever, k = N is very expensive and requires you to fit a large number of models.\n\n\n\n15.1.3 Test Sets\nSuppose we’re researchers building a state of the art regression model. We choose a model with the lowest validation error and want to report this out to the world. Unfortunately, our validation error may be biased; that is, not representative of error on real-world data. This is because during our hyperparameter search, we were implicitly “learning” from our validation data by tuning our model to achieve better results. Before reporting our results, we should run our model on a special test set that we’ve never seen or used for any purpose whatsoever.\nA test set can be something that we generate ourselves, or it can be a common dataset whose true values are unknown. In the case of the former, we can split our our available data into 3 partitions: the training set, testing set, and validation set. The exact amount of data allocated to each partition varies, but a common split is 80% train, 10% test, 10% validation.\nBelow is an implementation of extracting a training, testing and validation set using sklearn.train_test_split on a data matrix X and an array of observations y.\nX_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.1)\nX_train, X_val, y_train, y_val = train_test_split(X_train_valid, y_train_valid, test_size=0.11)\nAs a recap:\n\nTraining set used to pick parameters.\nValidation set used to pick hyperparameters (or pick between different models).\nTest set used to provide an unbiased error at the end.\n\nHere is an idealized relationship between training error, test error, and validation error.\n\nNotice how the test error behaves similarily to the validation error. Both come from data that is unseen during the model training process, so both are fairly good estimates of real-world data. Of the two, the test error is more unbiased for the reasons mentioned above.\nAs before, the optimal complexity level exists where validation error is minimized. Logically, we can’t design a model that minimizes test error because we don’t use the test set until final evaluation."
  },
  {
    "objectID": "cv_regularization/cv_reg.html#regularization",
    "href": "cv_regularization/cv_reg.html#regularization",
    "title": "15  Cross Validation and Regularization",
    "section": "15.2 Regularization",
    "text": "15.2 Regularization\nEarlier, we found an optimal model complexity by choosing the hyperparameter that minimized validation error. This was the polynomial degree k= 2. Tweaking the “complexity” was simple; it was only a matter of adjusting the polynomial degree.\nHowever, in most machine learning problems, complexity is defined differently. Today, we’ll explore two different definitions of complexity: the squared and absolute magnitude of \\(\\theta_i\\) coefficients.\n\n15.2.0.1 Constraining Gradient Descent\nBefore we discuss these definitions, let’s first familiarize ourselves with the concept of constrained gradient descent. Imagine we have a two feature model with coeffiecient weights of \\(\\theta_1\\) and \\(\\theta_2\\). Below we’ve plotted a two dimensional contour plot of the OLS loss surface – darker areas indicate regions of lower loss. Gradient descent will find the optimal paramaters during training \\((\\theta_1, \\theta_2) = \\hat\\theta_{No Reg.}\\).\n\nSuppose we arbitrarily decide that gradient descent can never land outside of the green ball.\n\nGradient descent finds a new solution at \\(\\hat\\theta_{Reg.}\\). This is far from the global optimal solution \\(\\hat\\theta_{No Reg.}\\) – however, it is the closest point that lives in the green ball. In other words, \\(\\hat\\theta_{Reg.}\\) is the constrained optimal solution.\nThe size of this ball is completely arbitrary. Increasing its size allows for a constrained solution closer to the global optimum, and vice versa.\n\n\n\n\n\n\n\n\n\nIn fact, the size of this ball is inherently linked to model complexity. A smaller ball constrains (\\(\\theta_1\\), \\(\\theta_2\\)) more than a larger ball. This is synonymous with the behavior of a less complex model, which finds a solution farther from the optimum. A larger ball, on the other hand, is synonymous to a more complex model that can achieve a near optimal solution.\nConsider the extreme case where the radius is infinitely small. The solution to every constrained modeling problem would lie on the origin, at \\((\\theta_1, \\theta_2) = (0, 0)\\). This is equivalent to the constant model we studied – the least complex of all models. In the case where the radius is infinitely large, the optimal constrained solution exists at \\(\\hat\\theta_{No Reg.}\\) itself! This is the solution obtained from OLS with no limitations on complexity.\n\nThe intercept coefficient is typically not constrained; \\(\\theta_0\\) can be any value. This way, if all \\(\\theta_i = 0\\) except \\(\\theta_0\\), the resulting model is a constant model (and \\(\\theta_0\\) is the mean of all observations).\n\n\n15.2.1 L2 Regularization\n\n15.2.1.1 The Constrained Form\nRegularization is the formal term that describes the process of limiting a model’s complexity. This is done by constraining the solution of a cost function, much like how we constrained the set of permissible (\\(\\theta_1\\), \\(\\theta_2\\)) above. L2 Regularization, commonly referred to as Ridge Regression, is the technique of constraining our model’s parameters to lie within a ball around the origin. Formally, it is defined as\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\]\n\nsuch that \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\)\n\n\nThe mathematical definition of complexity in Ridge Regression is \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\). This formulation of complexity limits the total squared magnitude of the coefficients to some constant \\(Q\\). In two dimensional space, this is \\(\\theta_{1}^{2} + \\theta_{2}^{2} \\le Q\\). You’ll recognize this as the equation of a circle with axes \\(\\theta_{1}, \\theta_{2}\\) and radius \\(Q\\). In higher dimensions, this circle becomes a hypersphere and is conventionally referred to as the L2 norm ball. Decreasing \\(Q\\) shrinks the norm ball, and limits the complexity of the model (discussed in the previous section). Likewise, expanding the norm ball increases the allowable model complexity.\nWithout the constraint \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\), the optimal solution is \\(\\hat{\\theta} = \\hat\\theta_{No Reg.}\\). With an appropriate value of \\(Q\\) applied to the constraint, the solution \\(\\hat{\\theta} = \\hat\\theta_{Reg.}\\) is sub-optimal on the training data but generalizes better to new data.\n\n\n15.2.1.2 The Functional Form\nUnfortunately, the function above requires some work. It’s not easy to mathematically optimize over a constraint. Instead, in most machine learning text, you’ll see a different formulation of Ridge Regression.\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\]\nThese two equations are equivalent by Lagrangian Duality (not in scope).\nNotice that we’ve replaced the constraint with a second term in our cost function. We’re now minimizing a function with a regularization term that penalizes large coefficients. The \\(\\alpha\\) factor controls the degree of regularization. In fact, \\(\\alpha \\approx \\frac{1}{Q}\\).  To understand why, let’s consider these 2 extreme examples:\n\nAssume \\(\\alpha \\rightarrow \\infty\\). Then, \\(\\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\) dominates the cost function. To minimize this term, we set \\(\\theta_j = 0\\) for all \\(j \\ge 1\\). This is a very constrained model that is mathematically equivalent to the constant model. Earlier, we explained the constant model also arises when the L2 norm ball radius \\(Q \\rightarrow 0\\).\nAssume \\(\\alpha \\rightarrow 0\\). Then, \\(\\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\) is infinitely small. Minimizing the cost function is equivalent to \\(\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\). This is just OLS, and the optimal solution is the global minimum \\(\\hat{\\theta} = \\hat\\theta_{No Reg.}\\). We showed that the global optimum is achieved when the L2 norm ball radius \\(Q \\rightarrow \\infty\\).\n\n\n\n15.2.1.3 Closed Form Solution\nAn additional benefit to Ridge Regression is that it has a closed form solution.\n\\[\\hat\\theta_{ridge} = (X^TX + n\\alpha I)^{-1}X^TY\\]\nThis solution exists even if there is linear dependence in the columns of the data matrix. We will not derive this result in Data 100, as it involves a fair bit of matrix calculus.\n\n\n15.2.1.4 Implementation of Ridge Regression\nOf course, sklearn has a built-in implementation of Ridge Regression. Simply import the Ridge class of the sklearn.linear_model library.\n\nfrom sklearn.linear_model import Ridge\n\nWe will various Ridge Regression models on the familiar vehicles DataFrame from last lecture. This will help solidfy some of the theoretical concepts discussed earlier.\n\n\nCode\nimport pandas as pd\nvehicles = pd.read_csv(\"data/vehicle_data.csv\", index_col=0)\nvehicles_mpg = pd.read_csv(\"data/vehicle_mpg.csv\", index_col=0)\n\n\n\nvehicles.head(5)\n\n\n\n\n\n  \n    \n      \n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      cylinders^2\n      displacement^2\n      horsepower^2\n      weight^2\n      acceleration^2\n    \n  \n  \n    \n      0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      64\n      94249.0\n      16900.0\n      12278016\n      144.00\n    \n    \n      1\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      64\n      122500.0\n      27225.0\n      13638249\n      132.25\n    \n    \n      2\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      64\n      101124.0\n      22500.0\n      11806096\n      121.00\n    \n    \n      3\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      64\n      92416.0\n      22500.0\n      11785489\n      144.00\n    \n    \n      4\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      64\n      91204.0\n      19600.0\n      11895601\n      110.25\n    \n  \n\n\n\n\nHere, we fit an extremeley regularized model without an intercept. Note the small coefficient values.\n\nridge_model_large_reg = Ridge(alpha = 10000)\nridge_model_large_reg.fit(vehicles, vehicles_mpg)\nridge_model_large_reg.coef_\n\narray([[ 8.56292915e-04, -5.92399474e-02, -9.81013894e-02,\n        -9.66985253e-03, -5.08353226e-03,  1.49576895e-02,\n         1.04959034e-04,  1.14786826e-04,  9.07086742e-07,\n        -4.60397349e-04]])\n\n\nNote how Ridge Regression effectively spreads a small weight across many features.\nWhen we apply very little regularization, our coefficients increase in size. Notice how they are identical to the coefficients retrieved from the LinearRegression model. This indicates the radius \\(Q\\) of the L2 norm ball is massive and encompasses the unregularized optimal solution. Once again, we see that \\(\\alpha\\) and \\(Q\\) are inversely related.\n\nridge_model_small_reg = Ridge(alpha = 10**-5)\nridge_model_small_reg.fit(vehicles, vehicles_mpg)\nridge_model_small_reg.coef_\n\narray([[-8.06754383e-01, -6.32025048e-02, -2.92851012e-01,\n        -3.41032156e-03, -1.43877512e+00,  1.25829303e-01,\n         7.72841216e-05,  6.99398090e-04,  3.11031744e-07,\n         3.16084838e-02]])\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(vehicles, vehicles_mpg)\nlinear_model.coef_\n\narray([[-8.06756280e-01, -6.32024872e-02, -2.92851021e-01,\n        -3.41032211e-03, -1.43877559e+00,  1.25829450e-01,\n         7.72840884e-05,  6.99398114e-04,  3.11031832e-07,\n         3.16084973e-02]])\n\n\n\n\n\n15.2.2 Scaling Data for Regularization\nOne issue with our approach is that our features are on vastly different scales. For example, weight^2 is in the millions, while the number of cylinders are under 10. Intuitively, the coefficient value for weight^2 must be very small to offset the large magnitude of the feature. On the other hand, the coefficient of the cylinders feature is likely quite large in comparison. We see these claims are true in the LinearRegression model above.\nHowever, a problem arises in Ridge Regression. If we constrain our coefficients to a small region around the origin, we are unfairly restricting larger coefficients – like that of the cylinders feature. A smaller coefficient – that of the weight^2 feature – likely lies within this region, so the value changes very little. Compare the coefficients of the regularized and unregularized Ridge models above, and you’ll see this is true.\nTherefore, it’s imperative to standardize your data. We can do so using z-scores.\n\\[z_k = \\frac{x_k - u_k}{\\sigma_k}\\]\nYou’ll do this on lab 8 using the “StandardScaler” transformer. The resulting model coefficients will be all on the same scale.\n\n\n15.2.3 L1 Regularization\nL1 Regularization, commonly referred to as Lasso Regression, is an alternate regularization technique that limits the sum of absolute \\(\\theta_i\\) coefficients.\n\n15.2.3.1 The Constrained Form\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\]\n\nsuch that \\(\\sum_{j=1}^{d} |\\theta_j| \\le Q\\)\n\n\nIn two dimensions, our constraint equation is \\(|\\theta_1| + |\\theta_2| \\le Q\\). This is the graph of a diamond centered on the origin with endpoints \\(Q\\) units away on each axis.\n\n\n\n15.2.3.2 The Functional Form\nA more convenient way to express Lasso Regression is as follows:\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} |\\theta_j|\\]\nAs with Ridge Regression, the hyperparameter \\(\\alpha\\) has the same effect on Lasso Regression. That is, increasing  \\(\\alpha\\) (equivalently, decreasing  \\(Q\\)) increases the amount of regularization, and vice versa.\nUnfortunately, Lasso Regression does not have a closed form solution – the cost function is not differentiable everywhere. Specifically, the sum \\(\\sum_{j=1}^{d} |\\theta_j|\\) is problematic because it is composed of absolute value functions, each of which are non-differentiable at the origin.\nSo why use Lasso Regression? As we’ll see shortly, it is great at implicit feature selection.\n\n\n15.2.3.3 Implementation of Lasso Regression\nLasso Regression is great at reducing complexity by eliminating the least important features in a model. It does so by setting their respective feature weights to \\(0\\). See the following example.\n\nfrom sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha = 1)\n\nstandardized_vehicles=(vehicles-vehicles.mean())/vehicles.std()\nlasso_model.fit(standardized_vehicles, vehicles_mpg)\nlasso_model.coef_\n\narray([-0.14009981, -0.28452369, -1.14351999, -4.11329618,  0.        ,\n       -0.        , -0.        , -0.        , -0.        ,  0.        ])\n\n\nNotice how we standardized our data first. Lasso Regression then set the coefficients of our squared features to \\(0\\) – presumably, these are the least important predictors of mpg.\n\n\n\n15.2.4 Summary of Regularization Methods\nA summary of our regression models is shwon below:\n\nUnderstanding the distinction between Ridge Regression and Lasso Regression is important. We’ve provided a helpful visual that summarizes the key differences.\n\nThis diagram displays the L1 and L2 constrained solution for various orientations of the OLS loss surface. Notice how the L1 (Lasso) solution almost always lies on some axis, or edge of the diamond. Graphically, this makes sense; the edges of the diamond are the farthest from the origin, and usually closest to the global optimum. When this happens, only one feature has a non-zero coefficient; this “feature selection” argument extends quite nicely to multiple features in higher dimensional space.\nThe L2 (Ridge) solution, however, typically has an optimal solution in some quadrant of the graph. Every point on the circumference of the L2 norm ball is equidistant from the origin, and thus similar in distance to the global optimum. As such, this technique of regularization is great at distributing a small weight across both features."
  },
  {
    "objectID": "probability_1/probability_1.html#random-variables-and-distributions",
    "href": "probability_1/probability_1.html#random-variables-and-distributions",
    "title": "16  Probability I",
    "section": "16.1 Random Variables and Distributions",
    "text": "16.1 Random Variables and Distributions\nSuppose we generate a set of random data, like a random sample from some population. A random variable is a numerical function of the randomness in the data. It is random from the randomness of the sample; it is variable because its exact value depends on how this random sample came out. We typically denote random variables with uppercase letters, such as \\(X\\) or \\(Y\\).\nTo give a concrete example: say we draw a random sample \\(s\\) of size 3 from all students enrolled in Data 100. We might then define the random variable \\(X\\) to be the number of Data Science majors in this sample.\n\nThe distribution of a random variable \\(X\\) describes how the total probability of 100% is split over all possible values that \\(X\\) could take. If \\(X\\) is a discrete random variable with a finite number of possible values, define its distribution by stating the probability of \\(X\\) taking on some specific value, \\(x\\), for all possible values of \\(x\\).\n\nThe distribution of a discrete variable can also be represented using a histogram. If a variable is continuous – it can take on infinitely many values – we can illustrate its distribution using a density curve.\n\nOften, we will work with multiple random variables at the same time. In our example above, we could have defined the random variable \\(X\\) as the number of Data Science majors in our sample of students, and the variable \\(Y\\) as the number of Statistics majors in the sample. For any two random variables \\(X\\) and \\(Y\\):\n\n\\(X\\) and \\(Y\\) are equal if \\(X(s) = Y(s)\\) for every sample \\(s\\). Regardless of the exact sample drawn, \\(X\\) is always equal to \\(Y\\).\n\\(X\\) and \\(Y\\) are identically distributed if the distribution of \\(X\\) is equal to the distribution of \\(Y\\). That is, \\(X\\) and \\(Y\\) take on the same set of possible values, and each of these possible values is taken with the same probability. On any specific sample \\(s\\), identically distributed variables do not necessarily share the same value.\n\\(X\\) and \\(Y\\) are independent and identically distributed (IID) if 1) the variables are identically distributed and 2) knowing the outcome of one variable does not influence our belief of the outcome of the other."
  },
  {
    "objectID": "probability_1/probability_1.html#expectation-and-variance",
    "href": "probability_1/probability_1.html#expectation-and-variance",
    "title": "16  Probability I",
    "section": "16.2 Expectation and Variance",
    "text": "16.2 Expectation and Variance\nOften, it is easier to describe a random variable using some numerical summary, rather than fully defining its distribution. These numerical summaries are numbers that characterize some properties of the random variable. Because they give a “summary” of how the variable tends to behave, they are not random – think of them as a static number that describes a certain property of the random variable. In Data 100, we will focus our attention on the expectation and variance of a random variable.\n\n16.2.1 Expectation\nThe expectation of a random variable \\(X\\) is the weighted average of the values of \\(X\\), where the weights are the probabilities of each value occurring. To compute the expectation, we find each value \\(x\\) that the variable could possibly take, weight by the probability of the variable taking on each specific value, and sum across all possible values of \\(x\\).\n\\[\\mathbb{E}[X] = \\sum_{\\text{all possible } x} x P(X=x)\\]\nAn important property in probability is the linearity of expectation. The expectation of the linear transformation \\(aX+b\\), where \\(a\\) and \\(b\\) are constants, is:\n\\[\\mathbb{E}[aX+b] = aE[\\mathbb{X}] + b\\]\nExpectation is also linear in sums of random variables.\n\\[\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y]\\]\n\n\n16.2.2 Variance\nThe variance of a random variable is a measure of its chance error. It is defined as the expected squared deviation from the expectation of \\(X\\). Put more simply, variance asks: how far does \\(X\\) typically vary from its average value? What is the spread of \\(X\\)’s distribution?\n\\[\\text{Var}(X) = \\mathbb{E}[(X-\\mathbb{E}[X])^2]\\]\nIf we expand the square and use properties of expectation, we can re-express this statement as the computational formula for variance. This form is often more convenient to use when computing the variance of a variable by hand.\n\\[\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\]\nHow do we compute the expectation of \\(X^2\\)? Any function of a random variable is also a random variable – that means that by squaring \\(X\\), we’ve created a new random variable. To compute \\(\\mathbb{E}[X^2]\\), we can simply apply our definition of expectation to the random variable \\(X^2\\).\n\\[\\mathbb{E}[X^2] = \\sum_{\\text{all possible } x} x^2 P(X^2 = x^2)\\]\nUnlike expectation, variance is non-linear. The variance of the linear transformation \\(aX+b\\) is:\n\\[\\text{Var}(aX+b) = a^2 \\text{Var}(X)\\]\nThe full proof of this fact can be found using the definition of variance. As general intuition, consider that \\(aX+b\\) scales the variable \\(X\\) by a factor of \\(a\\), then shifts the distribution of \\(X\\) by \\(b\\) units.\n\nShifting the distribution by \\(b\\) does not impact the spread of the distribution. Thus, \\(\\text{Var}(aX+b) = \\text{Var}(aX)\\).\nScaling the distribution by \\(a\\) does impact the spread of the distribution.\n\n\nIf we wish to understand the spread in the distribution of the summed random variables \\(X + Y\\), we can manipulate the definition of variance to find:\n\\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\\]\nThis last term is of special significance. We define the covariance of two random variables as the expected product of deviations from expectation. Put more simply, covariance is a generalization of variance to two random variables: \\(\\text{Cov}(X, X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\text{Var}(X)\\).\n\\[\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]\\]\nWe can treat the covariance as a measure of association. Remember the definition of correlation given when we first established SLR?\n\\[r(X, Y) = \\mathbb{E}\\left[\\left(\\frac{X-\\mathbb{E}[X]}{\\text{SD}(X)}\\right)\\left(\\frac{Y-\\mathbb{E}[Y]}{\\text{SD}(Y)}\\right)\\right] = \\frac{\\text{Cov}(X, Y)}{\\text{SD}(X)\\text{SD}(Y)}\\]\nIt turns out we’ve been quietly using covariance for some time now! If \\(X\\) and \\(Y\\) are independent, then \\(\\text{Cov}(X, Y) =0\\) and \\(r(X, Y) = 0\\). Note, however, that the converse is not always true: \\(X\\) and \\(Y\\) could have \\(\\text{Cov}(X, Y) = r(X, Y) = 0\\) but not be independent. This means that the variance of a sum of independent random variables is the sum of their variances: \\[\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) \\qquad \\text{if } X, Y \\text{ independent}\\]\n\n\n16.2.3 Standard Deviation\nNotice that the units of variance are the square of the units of \\(X\\). For example, if the random variable \\(X\\) was measured in meters, its variance would be measured in meters\\(^2\\). The standard deviation of a random variable converts things back to the correct scale by taking the square root of variance.\n\\[\\text{SD}(X)  = \\sqrt{\\text{Var}(X)}\\]\nTo find the standard deviation of a linear transformation \\(aX+b\\), take the square root of the variance:\n\\[\\text{SD}(aX+b) = \\sqrt{\\text{Var}(aX+b)} = \\sqrt{a^2 \\text{Var}(X)} = |a|\\text{SD}(X)\\]"
  },
  {
    "objectID": "case_study_HCE/case_study_HCE.html#the-problem",
    "href": "case_study_HCE/case_study_HCE.html#the-problem",
    "title": "17  Case Study in Human Contexts and Ethics",
    "section": "17.1 The Problem",
    "text": "17.1 The Problem\nIn a report by the Chicago Tribune uncovered a major scandal. The team showed that the model perpetuated a highly regressive tax system which disproportionately burdened African American and Latinx homeowners in Cook County. How did they know?\n \nIn the field of housing assessment, there are standard metrics that assessors use across the world to estimate the fairness of assessments: coefficient of dispersion and price-related differential. These metrics have been rigorously tested by experts in the field, and are out of scope for our class. Calculating these metrics for the Cook County prices and found that the pricing created by the CCAO did not fall in acceptable ranges (See figure above). This by itself is not the end of the story, but a good indicator that something fishy was going on.\n \nThis then prompted them to investigate if the model itself was producing fair tax rates. Evidently, when accounting for the home owner’s income they found that the model actually produced a regressive tax rate (See figure above).\n\n\n\n\n\n\nDefinition\n\n\n\n\n\nRegressive (tax): A tax rate is regressive if the percentage tax rate is higher for individuals with lower net income.\nProgressive (tax): A tax rate is progressive if the percentage tax rate is higher for individuals with higher net income.\n\n\n\n  \nDigging further shows that not only was the system unfair to people across the axis of income, it was also unfair across the axis of race(See figure above). The likelihood that your home was under or over assessed was highly dependent on your race did not sit will with many home owners.\n\n17.1.1 Spotlight: Appeals\nWhat actually caused this to come about? The whole answer is a lot bigger than just models. At the end of the day, these are real systems that have a lot of moving parts. One of those moving parts was the appeals system. Homeowners are mailed the value their home is assess for by CCAO, and the homeowner can choose to appeal to a board of elected officials to try and change the listed value of their home, and thus how much they are taxed. In theory, this sounds like a very fair system: there is a human that oversees the final pricing of houses rather than just an algorithm. However, it ended up exacerbating the problems.\n“Appeals are a good thing,” Thomas Jaconetty, deputy assessor for valuation and appeals, said in an interview. “The goal here is fairness. We made the numbers. We can change them.”\n  \nHere we can borrow lessons from Critical Race Theory. On the surface, everyone having the legal right to try and appeal is undeniable. However, not everyone has an equal ability to. Those who have the money to hire tax lawyers to appeal for them have a drastically higher chance trying and succeeding (See above figure). This model is part of a deeper institutional pattern rife with potential corruption.\n  \nHomeowners who appealed were generally underassessed relative to other homeowners (See above figure). Those with higher incomes pay less in property tax, tax lawyers are able to grow their business due to their role in appeals, and politicians are commonly socially connected to the aforementioned tax lawyers and wealthy homeowners. All these stakeholders have reasons to advertise the model as a integral part of a fair system. Here lies the value in asking questions. A system that seems fair on the surface may in actuality be unfair upon taking a closer look.\n\n\n17.1.2 Human Impacts\n  \nThe impact of the housing model extends beyond the realm of home ownership and taxation. Discriminatory practices have a long history within the United States, and the model served to perpetuate this fact. To this day, Chicago is one of the most segregated cities in the United States (source). These factors are central to informing us, as data scientists, about what is at stake.\n\n\n17.1.3 Spotlight: Intersection of Real Estate and Race\nHousing has been a persistent source of racial inequality throughout US History, amongst other factors. It is one of the main areas where inequalities are created and reproduced. In the beginning, Jim Crow laws were explicit in forbidding Persons of Color from Schools, public utilities, etc.\n\nToday, while advancements in Civil Rights have been made, the spirit of the laws are alive in many parts of the US. The real estate industry was “professionalized” in the 1920’s and 1930’s by aspiring to become a science guided by strict methods and principles:\n\nRedlining: making it difficult or impossible to get a federally-backed mortgage to buy a house in specific neighborhoods coded as “risky” (red).\n\nWhat made them “risky” according to the makers of these was racial composition.\nSegregation was not only a result of federal policy, but developed by real estate professionals.\n\nThe methods centered on creating objective rating systems (information technologies) for the appraisal of property values which encoded race as a factor of valuation (See figure below)\n\nThis, in turn, influenced federal policy and practice\n\n\n\nSource: Colin Koopman, How We Became Our Data (2019) p. 137"
  },
  {
    "objectID": "case_study_HCE/case_study_HCE.html#the-response-cook-county-open-data-initiative",
    "href": "case_study_HCE/case_study_HCE.html#the-response-cook-county-open-data-initiative",
    "title": "17  Case Study in Human Contexts and Ethics",
    "section": "17.2 The Response: Cook County Open Data Initiative",
    "text": "17.2 The Response: Cook County Open Data Initiative\nThe response started in politics. A new assessor, Fritz Kaegi, was elected and created a new mandate with two goals:\n\nDistributional equity in property taxation = properties of same value treated alike during assessments\nCreates new Office of Data Science\n\n\n\n17.2.1 Question/Problem Formulation\n\nWhat do we want to know?\nWhat problems are we trying to solve?\nWhat are the hypotheses we want to test?\nWhat are our metrics for success?\n\nThe new Office of Data Science started by redefining their goals.\n\nAccurately, uniformly, and impartially assess the value of a home.\n\nFollowing international standards (coefficient of dispersion)\nPredicting value of all homes with as little total error as possible.\n\nCreate a system that assesses house values that is fair to all people, across perceived racial and income differences.\n\nDisrupts the circuit of corruption (Board of Review appeals process)\nEliminates regressivity\nEngenders trust in the system among all stakeholders\n\n\n\n\n\n\n\n\nDefinitions\n\n\n\n\n\nDefinitions: Fairness and Transparency\nFairness: The ability of our pipeline to accurately assess property values, accounting for disparities in geography, information, etc.\nTransparency: The ability of the data science department to share and explain pipeline results and decisions to both internal and external stakeholders\n\n\n\n\n\n17.2.2 Data Acquisition and Cleaning\n\nWhat data do we have and what data do we need?\nHow will we sample more data?\nIs our data representative of the population we want to study?\n\nExample: Sales data\n\nThey also critically examined their original sales data:\n\nHow was this data collected?\nWhen was this data collected?\nWho collected this data?\nFor what purposes was the data collected?\nHow and why were particular categories created?\n\nFor example, attributes can affect likelihood of appearing in the data. For example, housing data in the floodplains geographic region of Chicago were less represented than other regions.\nThe features can even be reported at different rates. Improvements in homes, which tend to increase property value, were unlikely to be reported by the homeowners.\nAdditionally, they found that there was simply more missing data in lower income neighborhoods.\n\n\n17.2.3 Exploratory Data Analysis\n\nHow is our data organized and what does it contain?\nDo we already have relevant data?\nWhat are the biases, anomalies, or other issues with the data?\nHow do we transform the data to enable effective analysis?\n\nBefore the modeling step, they investigated a multitude of crucial questions:\n\nWhich attributes are most predictive of sales price?\nIs the data uniformly distributed?\nDo all neighborhoods have up to date data? Do all neighborhoods have the same granularity?\n\nDo some neighborhoods have missing or outdated data?\n\nFirstly, they found that the impact of certain features, such as bedroom number, were much more impactful in determining house value inside certain neighborhoods more than others. This informed them that different models should be used depending on the neighborhood.\nThey also noticed that low income neighborhoods had disproportionately spottier data. This informed them that they needed to develop new data collection practices–including finding new sources of data.\n\n\n17.2.4 Prediction and Inference\n\nWhat does the data say about the world?\nDoes it answer our questions or accurately solve the problem?\nHow robust are our conclusions and can we trust the predictions?\n\nRather than using a singular model to predict sale prices (“fair market value”) of unsold properties, the CCAO fit machine learning models that discover patterns using known sale prices and characteristics of similar and nearby properties. It uses different model weights for each township.\nCompared to traditional mass appraisal, the CCAO’s new approach is more granular and more sensitive to neighborhood variations.\n\n\n17.2.5 Reports Decisions, and Conclusions\n\nHow successful is the system for each goal?\n\naccuracy/uniformity of the model.\nfairness and transparency that eliminates regressivity and engenders trust.\n\nHow do you know?\n\nThe model is not the end of the road. The new Office still sends homeowners their house evaluations, but now the data that they get sent back from the homeowners is taken into account. More detailed reports are being written by the office itself to democratize the information. Town halls and other public facing outreach helps involves the whole community in the process of housing evaluations, rather than limiting participation to a select few."
  },
  {
    "objectID": "case_study_HCE/case_study_HCE.html#key-takeaways",
    "href": "case_study_HCE/case_study_HCE.html#key-takeaways",
    "title": "17  Case Study in Human Contexts and Ethics",
    "section": "17.3 Key Takeaways",
    "text": "17.3 Key Takeaways\n\nAccuracy is a necessary, but not sufficient, condition of a fair system.\nFairness and transparency are context-dependent and sociotechnical concepts\nLearn to work with contexts, and consider how your data analysis will reshape them\nKeep in mind the power, and limits, of data analysis"
  },
  {
    "objectID": "case_study_HCE/case_study_HCE.html#lessons-for-data-science-practice",
    "href": "case_study_HCE/case_study_HCE.html#lessons-for-data-science-practice",
    "title": "17  Case Study in Human Contexts and Ethics",
    "section": "17.4 Lessons for Data Science Practice",
    "text": "17.4 Lessons for Data Science Practice\n\nQuestion/Problem formulation\n\nWho is responsible for framing the problem?\nWho are the stakeholders? How are they involved in the problem framing?\nWhat do you bring to the table? How does your positionality affect your understanding of the problem?\nWhat are the narratives that you’re tapping into?\n\nData Acquisition and Cleaning\n\nWhere does the data come from?\nWho collected it? For what purpose?\nWhat kinds of collecting and recording systems and techniques were used?\nHow has this data been used in the past?\nWhat restrictions are there on access to the data, and what enables you to have access?\n\nExploratory Data Analysis & Visualization\n\nWhat kind of personal or group identities have become salient in this data?\nWhich variables became salient, and what kinds of relationship obtain between them?\nDo any of the relationships made visible lend themselves to arguments that might be potentially harmful to a particular community?\n\nPrediction and Inference\n\nWhat does the prediction or inference do in the world?\nAre the results useful for the intended purposes?\nAre there benchmarks to compare the results?\nHow are your predictions and inferences dependent upon the larger system in which your model works?\n\nReports, Decisions, and Solutions\n\nHow do we know if we have accomplished our goals?\nHow does your work fit in the broader literature?\nWhere does your work agree or disagree with the status quo?\nDo your conclusions make sense?"
  }
]