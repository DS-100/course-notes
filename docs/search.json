[
  {
    "objectID": "pandas_1/pandas_1.html",
    "href": "pandas_1/pandas_1.html",
    "title": "Pandas I",
    "section": "",
    "text": "Learning Outcomes\nData scientists work with data stored in a variety of formats. The primary focus of this class is in understanding tabular data - one of the most widely used formats in data science. This note introduces DataFrames, which are among the most popular representations of tabular data. We’ll also introduce pandas, the standard Python package for manipulating data in DataFrames."
  },
  {
    "objectID": "pandas_1/pandas_1.html#dataframes",
    "href": "pandas_1/pandas_1.html#dataframes",
    "title": "Pandas I",
    "section": "DataFrames",
    "text": "DataFrames\nIn Data 8, you encountered the Table class of the datascience library. In Data 100, we’ll be using the DataFrame class of the pandas library.\nHere is an example of a DataFrame containing election data.\n\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n      loss\n      1.073699\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n      win\n      51.311515\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n182 rows × 6 columns\n\n\n\nLet’s dissect the code above.\n\nWe first import the pandas library into our Python environment, using the alias pd.   import pandas as pd\nThere are a number of ways to read data into a DataFrame. In Data 100, our data are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.   pd.read_csv(\"elections.csv\")\n\nThis code stores our DataFrame object into the elections variable. Upon inspection, our elections DataFrame has 182 rows and 6 columns. Each row represents a single record - in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nThe API (application programming interface) for the DataFrame class is enormous. In the next section, we’ll discuss several methods of the DataFrame API that allow us to extract subsets of data."
  },
  {
    "objectID": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "href": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "title": "Pandas I",
    "section": "Slicing in DataFrames",
    "text": "Slicing in DataFrames\nThe most fundamental way to manipulate a DataFrame is to extract a subset of rows and columns. This is called slicing. We will do so with three primary methods of the DataFrame class:\n\n.loc\n.iloc\n[]\n\n\nIndexing with .loc\nThe .loc operator selects rows and columns in a DataFrame by their row and column label(s), respectively. The row label (commonly referred to as the index) is the bold text on the far left of a DataFrame, while the column label is the text found at the top of a DataFrame. By default, row labels in pandas are the sequential list of integers beginning from 0. The column labels in our elections DataFrame are the column names themselves: Year, Candidate, Party, Popular Vote, Result, and %.\n.loc lets us grab data by specifying the appropriate row and column label(s) where the data exists. The row labels are the first argument to the .loc function; the column labels are the second. For example, to select the the row labeled 0 and the column labeled Candidate from our elections DataFrame we can write:\n\nelections.loc[0, 'Candidate']\n\n'Andrew Jackson'\n\n\nTo select multiple rows and columns, we can use Python slice notation. We can select the first four rows and first four columns.\n\nelections.loc[0:3, 'Year':'Popular vote']\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nSuppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : is useful for this.\n\nelections.loc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\nThere are a couple of things we should note. Unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.\nEquivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame.\n\nelections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nLastly, we can interchange list and slicing notation.\n\nelections.loc[[0, 1, 2, 3], :]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\nIndexing with .iloc\nSlicing with .iloc works similarily to .loc, although .iloc uses the integer positions of rows and columns rather the labels. The arguments to the .iloc function also behave similarly - single values, lists, indices, and any combination of these are permitted.\nLet’s begin reproducing our results from above. We’ll begin by selecting for the first presedential candidate in our elections DataFrame:\n\n# elections.loc[0, \"Candidate\"] - Previous approach\nelections.iloc[0, 1]\n\n'Andrew Jackson'\n\n\nNotice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th (or first) position of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.\nHowever, when we select for the first four rows and columns using .iloc, we notice something.\n\n# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach\nelections.iloc[0:4, 0:4]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nSlicing is no longer inclusive in .iloc - it’s exclusive. This is one of Pandas syntatical subtleties; you’ll get used to with practice.\nList behavior works just as expected.\n\n#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach\nelections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nThis discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.\n\n\nIndexing with []\nThe [] selection operator is the most baffling of all, yet it is the commonly used. It only takes a single argument, which may be one of the following:\n\nA slice of row numbers\nA list of column labels\nA single column label\n\nThat is, [] is context dependent. Let’s see some examples.\n\nA slice of row numbers\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\nA list of column labels\nSuppose we now want the first four columns.\n\nelections[[\"Year\", \"Candidate\", \"Party\", \"Popular vote\"]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n    \n  \n\n182 rows × 4 columns\n\n\n\n\n\nA single column label\nLastly, if we only want the Candidate column.\n\nelections[\"Candidate\"]\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThe output looks quite different - it’s no longer a DataFrame! This is a Series. We’ll talk about what a Series is in the next section."
  },
  {
    "objectID": "pandas_1/pandas_1.html#dataframes-series-and-indices",
    "href": "pandas_1/pandas_1.html#dataframes-series-and-indices",
    "title": "Pandas I",
    "section": "DataFrames, Series, and Indices",
    "text": "DataFrames, Series, and Indices\nWe saw that selecting a single column from a DataFrame using the [] operator outputted a new data format, called a Series. Let’s verify this claim.\n\ntype(elections)\n\npandas.core.frame.DataFrame\n\n\n\ntype(elections['Candidate'])\n\npandas.core.series.Series\n\n\nA Series is a one dimensional object that represents a single column of data. It has two components - an index and the data. A DataFrame is equivalent to a collection of multiple Series, which all share the same index. Notice how the index of a Series is equivalent to the index (or row labels) of a DataFrame.\n\nHowever, a DataFrame index doesn’t have to be an integer, nor does it have to be unique. For example, we can set our index to be the name of our presedential candidates. Selecting a new Series from this modified DataFrame yields the following:\n\nelections.set_index(\"Candidate\", inplace=True)\n\n\nTo retrieve the indices of a DataFrame, simply use the .index attribute of the DataFrame class.\n\nelections.index\n\nIndex(['Andrew Jackson', 'John Quincy Adams', 'Andrew Jackson',\n       'John Quincy Adams', 'Andrew Jackson', 'Henry Clay', 'William Wirt',\n       'Hugh Lawson White', 'Martin Van Buren', 'William Henry Harrison',\n       ...\n       'Darrell Castle', 'Donald Trump', 'Evan McMullin', 'Gary Johnson',\n       'Hillary Clinton', 'Jill Stein', 'Joseph Biden', 'Donald Trump',\n       'Jo Jorgensen', 'Howard Hawkins'],\n      dtype='object', name='Candidate', length=182)\n\n\n\nelections.reset_index(inplace=True) # This resets the index\n\nEarlier, we mentioned that a Series was just a column of data. What if we wanted a single column as a DataFrame? To obtain this, we can pass in a list containing a single column to the [] selection operator.\n\nelections[[\"Party\"]] # [\"Party\"] is the argument - a list with a single element\n\n\n\n\n\n  \n    \n      \n      Party\n    \n  \n  \n    \n      0\n      Democratic-Republican\n    \n    \n      1\n      Democratic-Republican\n    \n    \n      2\n      Democratic\n    \n    \n      3\n      National Republican\n    \n    \n      4\n      Democratic\n    \n    \n      ...\n      ...\n    \n    \n      177\n      Green\n    \n    \n      178\n      Democratic\n    \n    \n      179\n      Republican\n    \n    \n      180\n      Libertarian\n    \n    \n      181\n      Green\n    \n  \n\n182 rows × 1 columns"
  },
  {
    "objectID": "pandas_1/pandas_1.html#conditional-selection",
    "href": "pandas_1/pandas_1.html#conditional-selection",
    "title": "Pandas I",
    "section": "Conditional Selection",
    "text": "Conditional Selection\nConditional selection allows us to select a subset of rows in a DataFrame if they follow some specified condition.\nTo understand how to use conditional selection, we must look at another input of the .loc and [] methods - a boolean array. This boolean array must have a length equal to the number of rows in the DataFrame. It will return all rows in the position of a corresponding True value in the array.\nHere, we will select all even-indexed rows in the first 10 rows of our DataFrame.\n\n# Why is :9 is the correct slice to select the first 10 rows?\nelections_first_10_rows = elections.loc[:9, :]\n\n# Notice how we have exactly 10 elements in our boolean array argument\nelections_first_10_rows[[True, False, True, False, True, \\\n                         False, True, False, True, False]]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      4\n      Andrew Jackson\n      1832\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      6\n      William Wirt\n      1832\n      Anti-Masonic\n      100715\n      loss\n      7.821583\n    \n    \n      8\n      Martin Van Buren\n      1836\n      Democratic\n      763291\n      win\n      52.272472\n    \n  \n\n\n\n\nUnfortunately, using this method to select multiple rows in a large DataFrame is infeasible. Instead, we can provide a logical condition as an input to .loc or [] that returns a boolean array with said length.\nFor example, to return all candidates affilliated with the Independent party:\n\nlogical_operator = elections['Party'] == \"Independent\"\nelections[logical_operator]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298\n    \n    \n      161\n      Ralph Nader\n      2004\n      Independent\n      465151\n      loss\n      0.380663\n    \n    \n      167\n      Ralph Nader\n      2008\n      Independent\n      739034\n      loss\n      0.563842\n    \n    \n      174\n      Evan McMullin\n      2016\n      Independent\n      732273\n      loss\n      0.539546\n    \n  \n\n\n\n\nHere, logical_operator evaluates to a Series of boolean values with length 182.\n\nlogical_operator\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n177    False\n178    False\n179    False\n180    False\n181    False\nName: Party, Length: 182, dtype: bool\n\n\nRows 121, 130, 143, 161, 167, and 174 evaluate to True and are thus returned in the DataFrame.\n\n\nCode\nprint(logical_operator.loc[[121, 130, 143, 161, 167, 174]])\n\n\n121    True\n130    True\n143    True\n161    True\n167    True\n174    True\nName: Party, dtype: bool\n\n\nPassing a Series as an argument to elections[] has the same affect as using a boolean array. In fact, the [] selection operator can take a boolean Series, array, and list as arguments. These three are used interchangeably thoughout the course.\nSimilarly, we can use .loc to achieve similar results.\n\nelections.loc[elections['Party'] == \"Independent\"]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298\n    \n    \n      161\n      Ralph Nader\n      2004\n      Independent\n      465151\n      loss\n      0.380663\n    \n    \n      167\n      Ralph Nader\n      2008\n      Independent\n      739034\n      loss\n      0.563842\n    \n    \n      174\n      Evan McMullin\n      2016\n      Independent\n      732273\n      loss\n      0.539546\n    \n  \n\n\n\n\nBoolean conditions can be combined using various operators that allow us to filter results by multiple conditions. Some examples include the & (and) operator and | (or) operator.\nNote: When combining multiple conditions with logical operators, be sure to surround each condition with a set of paranthesis (). If you forget, your code will throw an error.\nFor example, if we want to return data on all presidential candidates affiliated with the Independent Party before the 21st century, we can do so:\n\nelections[(elections['Party'] == \"Independent\") \\\n          & (elections['Year'] < 2000)]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      121\n      Eugene McCarthy\n      1976\n      Independent\n      740460\n      loss\n      0.911649\n    \n    \n      130\n      John B. Anderson\n      1980\n      Independent\n      5719850\n      loss\n      6.631143\n    \n    \n      143\n      Ross Perot\n      1992\n      Independent\n      19743821\n      loss\n      18.956298"
  },
  {
    "objectID": "pandas_1/pandas_1.html#handy-utility-functions",
    "href": "pandas_1/pandas_1.html#handy-utility-functions",
    "title": "Pandas I",
    "section": "Handy Utility Functions",
    "text": "Handy Utility Functions\nThere are a large number of operations supported by pandas that allow us to efficiently manipulate data . In this section, we’ll cover a few.\n\n.head and .tail\n.shape and .size\n.describe\n.sample\n.value_counts\n.unique\n.sort_values\n\n\n.head / .tail\n.head(n) and .tail(n) display the first n and last n rows of a DataFrame, respectively.\n\nelections.head(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n  \n\n\n\n\n\nelections.tail(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      179\n      Donald Trump\n      2020\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      Jo Jorgensen\n      2020\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      Howard Hawkins\n      2020\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n\n\n\n\n\n.shape / .size\n.shape returns a tuple with the number of rows and columns in a DataFrame.  .size returns the total number of data entries. This is the product of the number of rows and columns.\n\nelections.shape\n\n(182, 6)\n\n\n\nnum_rows, num_cols = elections.shape\nassert(elections.size == num_rows * num_cols)\nelections.size\n\n1092\n\n\n\n\n.describe\n.describe() returns a DataFrame of useful summary statistics for each numerical column.\n\nelections.describe()\n\n\n\n\n\n  \n    \n      \n      Year\n      Popular vote\n      %\n    \n  \n  \n    \n      count\n      182.000000\n      1.820000e+02\n      182.000000\n    \n    \n      mean\n      1934.087912\n      1.235364e+07\n      27.470350\n    \n    \n      std\n      57.048908\n      1.907715e+07\n      22.968034\n    \n    \n      min\n      1824.000000\n      1.007150e+05\n      0.098088\n    \n    \n      25%\n      1889.000000\n      3.876395e+05\n      1.219996\n    \n    \n      50%\n      1936.000000\n      1.709375e+06\n      37.677893\n    \n    \n      75%\n      1988.000000\n      1.897775e+07\n      48.354977\n    \n    \n      max\n      2020.000000\n      8.126892e+07\n      61.344703\n    \n  \n\n\n\n\n\n\n.sample\n.sample(n) returns a random sample of n rows from the given DataFrame.\n\nelections.sample(3)\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      31\n      Horace Greeley\n      1872\n      Liberal Republican\n      2834761\n      loss\n      44.071406\n    \n    \n      65\n      William Taft\n      1908\n      Republican\n      7678335\n      win\n      52.013300\n    \n    \n      36\n      James Garfield\n      1880\n      Republican\n      4453337\n      win\n      48.369234\n    \n  \n\n\n\n\n\n\n.value_counts\n.value_counts() is called on a column and returns a Series containing the total count of each unique value.\n\nelections['Candidate'].value_counts()\n\nNorman Thomas             5\nEugene V. Debs            4\nFranklin Roosevelt        4\nRalph Nader               4\nWilliam Jennings Bryan    3\n                         ..\nThomas J. Anderson        1\nWinfield Scott            1\nThomas E. Watson          1\nJohn G. Schmitz           1\nBenjamin Butler           1\nName: Candidate, Length: 132, dtype: int64\nThis code tells us how many times each candidate ran for president of the United States.\n\n\n\n\n.unique\n.unique() is called on a Series and returns an array with its unique values.\n\n# For brevity, we have limited the results to 5 candidates \nelections['Candidate'].unique()[:5]\n\narray(['Andrew Jackson', 'John Quincy Adams', 'Henry Clay',\n       'William Wirt', 'Hugh Lawson White'], dtype=object)\n\n\n\n\n.sort_values\n.sort_values() returns a sorted version of the Series it was called on. Numerical values are in sorted magnitude, while text is sorted in alphabetical order. You may specify optional arguments to sort in ascending or descending order.\n\nelections['Candidate'].sort_values()\n\n75           Aaron S. Watkins\n27            Abraham Lincoln\n23            Abraham Lincoln\n108           Adlai Stevenson\n105           Adlai Stevenson\n                ...          \n19             Winfield Scott\n37     Winfield Scott Hancock\n74             Woodrow Wilson\n70             Woodrow Wilson\n16             Zachary Taylor\nName: Candidate, Length: 182, dtype: object\n\n\n\n\nParting Note\nThe pandas library is enormous and contains many useful functions. Here is a link to documentation.\nThis lecture and the next will cover important methods you should be fluent in. However, we want you to get familiar with the real world programming practice of …Googling! Answers to your questions can be found in documentation, Stack Overflow, etc.\nWith that, let’s move on to Pandas II."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Cleaning and EDA",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nRecognize common file formats\nCategorize data by its variable type\nBuild awareness of issues with data faithfulness and develop targeted solutions\nIn the past few lectures, we’ve learned that pandas is a toolkit to restructure, modify, and explore a dataset. What we haven’t yet touched on is how to make these data transformation decisions. When we receive a new set of data from the “real world,” how do we know what processing we should do to convert this data into a usable form?\nData cleaning, also called data wrangling, is the process of transforming raw data to facilitate subsequent analysis. It is often used to address issues like:\nExploratory Data Analysis (EDA) is the process of understanding a new dataset. It is an open-ended, informal analysis that involves familiarizing ourselves with the variables present in the data, discovering potential hypotheses, and identifying potential issues with the data. This last point can often motivate further data cleaning to address any problems with the dataset’s format; because of this, EDA and data cleaning are often thought of as an “infinite loop,” with each process driving the other.\nIn this lecture, we will consider the key properties of data to consider when performing data cleaning and EDA. In doing so, we’ll develop a “checklist” of sorts for you to consider when approaching a new dataset. Throughout this process, we’ll build a deeper understanding of this early (but very important!) stage of the data science lifecycle."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "Data Cleaning and EDA",
    "section": "Structure",
    "text": "Structure\n\nFile Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for Comma-Separated Values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the first few rows of the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (Tab-Separated Values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\n﻿Year   Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\nVariable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nClassification of variable types\n\n\n\n\nPrimary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request #\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "Data Cleaning and EDA",
    "section": "Granularity, Scope, and Temporality",
    "text": "Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "Data Cleaning and EDA",
    "section": "Faithfulness",
    "text": "Faithfulness\nAt this stage in our data cleaning and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "visualization_1/visualization_1.html",
    "href": "visualization_1/visualization_1.html",
    "title": "Visualization I",
    "section": "",
    "text": "In our journey of the data science lifecycle, we have begun to explore the vast world of exploratory data analysis. More recently, we learned how to pre-process data using various data manipulation techniques. As we work towards understanding our data, there is one key component missing in our arsenal - the ability to visualize and discern relationships in existing data.\nThese next two lectures will introduce you to various examples of data visualizations and their underlying theory. In doing so, we’ll motivate their importace in real-world examples with the use of plotting libraries.\n\nVisualizations in Data 8 and Data 100 (so far)\nYou’ve likely encountered several forms of data visualizations in your studies. You may remember two such examples from Data 8: line charts and histograms. Each of these served a unique purpose. For example, line charts displayed how numerical quantities changed over time, while histograms were useful in understanding a variable’s distribution.\n\n\nLine Chart\n\n\n\n\nHistogram\n\n\n\n\n\nGoals of Visualization\nVisualizations are useful for a number of reasons. In Data 100, we consider two areas in particular:\n\nTo broaden your understanding of the data\n\nKey part in exploratory data analysis\nUseful in investigating relationships between variables\n\nTo communicate your results to others\n\nVisualization theory is especially important here\n\n\nOne of the most common applications of visualizations - and the one that will be covered today - is in understanding a distribution of data.\n\n\nAn Overview of Distributions\nA distribution describes the frequency of unique values in a variable. Distributions must satisfy two properties:\n\nEach data point must belong to only one category.\nThe total frequency of all categories must sum to 100%. In other words, their total count should equal the number of values in consideration.\n\nLet’s look at a couple of examples.\n\n\nNot a Valid Distribution\n\n\n\n\nValid Distribution\n\n\n\nLeft Diagram: This is not a valid distribution. Individuals can belong to more than one category and the total frequency of all categories does not sum up to 100%.\nRight Diagram: This example satisfies the two properties of distributions, so it is a valid distribution.\n\n\nBar Plots\nAs we saw above, bar plots are one of the most common ways of displaying the distribution of a qualitative (categorical) variable. The length of a bar plot encodes the frequency of a category; the width encodes no useful information.\nLet’s contextualize this in an example. We will use the familiar births dataset from Data 8 in our analysis.\n\n\nCode\nimport pandas as pd\n\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nWe can visualize the distribution of the Maternal Smoker column using a bar plot. There are a few ways to do this.\n\nPlotting in Pandas\n\nbirths['Maternal Smoker'].value_counts().plot(kind = 'bar');\n\n\n\n\nRecall that .value_counts() returns a Series with the total count of each unique value. We call .plot(kind = 'bar') on this result to visualize these counts as a bar plot.\nPlotting methods in pandas are the least preferred and not supported in Data 100, as their functionality is limited. Instead, future examples will focus on other libaries built specifically for visualizing data. The most well-known library here is matplotlib.\n\n\nPlotting in Matplotlib\n\nimport matplotlib.pyplot as plt\n\nms = births['Maternal Smoker'].value_counts()\nplt.bar(ms.index, ms)\nplt.xlabel(\"Maternal Smoker\")\nplt.ylabel(\"Count\");\n\n\n\n\nWhile more code is required to achieve the same result, matplotlib is often used over pandas for its ability to plot more complex visualizations, some of which are discussed shortly.\nHowever, notice how the x-axis is a range of integers rather than the two categories, True and False. This is because matplotlib coerces True to a value of 1 and False to 0. Also, note how we needed to label the axes with plt.xlabel and plt.ylabel - matplotlib does not support automatic axis labeling. To get around these inconveniences, we can use a more effecient plotting library, seaborn.\n\n\nPlotting in Seaborn\n\nimport seaborn as sns\nsns.countplot(data = births, x = \"Maternal Smoker\");\n\n\n\n\nseaborn.countplot both counts and visualizes the number of unique values in a given column. This column is specified by the x argument to sns.countplot, while the DataFrame is specified by the data argument.\nFor the vast majority of visualizations, seaborn is far more concise and aesthetically pleasing than matplotlib. However, the color scheme of this particular bar plot is abritrary - it encodes no additional information about the categories themselves. This is not always true; color may signify meaningful detail in other visualizations. We’ll explore this more in-depth during the next lecture.\n\n\nPlotting in Plotly\n\nplotly is one of the most versatile plottling libraries and widely used in industry. However, plotly has various dependencies that make it difficult to support in Data 100. Therfore, we have intentionally excluded the code to generate the plot above.\nBy now, you’ll have noticed that each of these plotting libraries have a very different syntax. As with pandas, we’ll teach you the important methods in matplotlib and seaborn, but you’ll learn more through documentation.\n\nMatplotlib Documentation\nSeaborn Documentation\n\n\n\n\nHistograms\nHistograms are a natural extension to bar plots; they visualize the distribution of quantitative (numerical) data.\nRevisiting our example with the births DataFrame, let’s plot the distribution of the Maternal Pregnancy Weight column.\n\n\nCode\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nHow should we define our categories for this variable? In the previous example, these were the unique values of the Maternal Smoker column: True and False. If we use similar logic here, our categories are the different numerical weights contained in the Maternal Pregnancy Weight column.\nUnder this assumption, let’s plot this distribution using the seaborn.countplot function.\n\nsns.countplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis histogram clearly suffers from overplotting. This is somewhat expected for Maternal Pregnancy Weight - it is a quantitative variable that takes on a wide range of values.\nTo combat this problem, statisticians use bins to categorize numerical data. Luckily, seaborn provides a helpful plotting function that automatically bins our data.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\");\n\n\n\n\nThis diagram is known as a histogram. While it looks more reasonable, notice how we lose fine-grain information on the distribution of data contained within each bin. We can introduce rug plots to minimize this information loss. An overlaid “rug plot” displays the within-bin distribution of our data, as denoted by the thickness of the colored line on the x-axis.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\");\nsns.rugplot(data = births, x = \"Maternal Pregnancy Weight\", color = 'red');\n\n\n\n\nYou may have seen histograms drawn differently - perhaps with an overlaid density curve and normalized y-axis. We can display both with a few tweaks to our code.\nTo visualize a density curve, we can set the the kde = True argument of the sns.histplot. Setting the argument stat = \"density\" normalizes our histogram and displays densities, instead of counts, on the y-axis. You’ll notice that the area under the density curve is 1.\n\nsns.histplot(data = births, x = \"Maternal Pregnancy Weight\", kde = True, \n             stat = \"density\")\nsns.rugplot(data = births, x = \"Maternal Pregnancy Weight\", color = 'red');\n\n\n\n\n\n\nEvaluating Histograms\nHistograms allow us to assess a distribution by their shape. There are a few properties of histograms we can analyze:\n\nSkewness and Tails\n\nSkewed left vs skewed right\nLeft tail vs right tail\n\nOutliers\n\nDefined arbitrarily for now\n\nModes\n\nMost commonly occuring data\n\n\n\nSkewness and Tails\nIf a distribution has a long right tail (such as Maternal Pregancy Weight), it is skewed right. In a right-skewed distribution, the few large outliers “pull” the mean to the right of the median.\nIf a distribution has a long left tail, it is skewed left. In a left-skewed distribution, the few small outliers “pull” the mean to the left of the median.\nIn the case where a distribution has equal-sized right and left tails, it is symmetric. The mean is approximately equal to the median.\n\n\nOutliers\nLoosely speaking, an outlier is defined as a data point that lies an abnormally large distance away from other values. We’ll define the statistical measure for this shortly.\nOutliers disproportionately influce the mean because their magnitude is directly involved in computing the average. However, the median is largely unaffected - the magnitude of an outlier is irrelevant; we only care that it is some non-zero distance away from the midpoint of the data.\n\n\nModes\nA mode of a distribution is a local or global maximum. A distribution with a single clear maximum is unimodal, distributions with two modes are bimodal, and those with 3 or more are multimodal.\nFor example, the distribution of birth weights for maternal smokers is (weakly) multimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight');\n\n\n\n\nOn the other hand, the distribution of birth weights for maternal non-smokers is unimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight');\n\n\n\n\n\n\n\nBox Plots and Violin Plots\n\nBoxplots\nBoxplots are an alternative to histograms that visualize numerical distributions. They are especially useful in graphicaly summarizing several characteristics of a distribution. These include:\n\nLower Quartile (\\(1\\)st Quartile)\nMedian (\\(2\\)nd Quartile)\nUpper Quartile (\\(3\\)rd Quartile)\nInterquartile Range (IQR)\nWhiskers\nOutliers\n\nThe lower quartile, median, and uper quartile are the \\(25\\)th, \\(50\\)th, and \\(75\\)th percentiles of data, respectively. The interquartile range measures the spread of the middle \\(50\\)% of the distribution, calculated as the (\\(3\\)rd Quartile \\(-\\) \\(1\\)st Quartile).\nThe whiskers of a box-plot are the two points that lie at the \\(1\\)st Quartile \\(-\\) (\\(1.5\\) * IQR), and the \\(3\\)rd Quartile \\(+\\) (\\(1.5\\) * IQR). They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than (\\(1.5\\) \\(*\\) IQR) from the extreme quartiles.\nLet’s visualize a box-plot of the Birth Weight column.\n\n\nCode\nimport numpy as np\n\nsns.boxplot(data = births, y = \"Birth Weight\");\n\nbweights = births['Birth Weight']\nq1 = np.percentile(bweights, 25)\nq2 = np.percentile(bweights, 50)\nq3 = np.percentile(bweights, 75)\niqr = q3 - q1\nwhisk1 = q1 - (1.5 * iqr)\nwhisk2 = q3 + (1.5 * iqr)\n\nprint(\"The first quartile is {}\".format(q1))\nprint(\"The second quartile is {}\".format(q2))\nprint(\"The third quartile is {}\".format(q3))\nprint(\"The interquartile range is {}\".format(iqr))\nprint(\"The whiskers are {} and {}\".format(whisk1, whisk2))\n\n\nThe first quartile is 108.0\nThe second quartile is 120.0\nThe third quartile is 131.0\nThe interquartile range is 23.0\nThe whiskers are 73.5 and 165.5\n\n\n\n\n\nHere is a helpful visual that summarizes our discussion above.\n\n\n\nViolin Plots\nAnother diagram that is useful in visualizing a variable’s distribution is the violin plot. A violin plot supplements a box-plot with a smoothed density curve on either side of the plot. These density curves highlight the relative frequency of variable’s possible values. If you look closely, you’ll be able to discern the quartiles, whiskers, and other hallmark features of the box-plot.\n\nsns.violinplot(data = births, y = 'Birth Weight');\n\n\n\n\n\n\n\nComparing Quantitative Distributions\nEarlier in our discussion of the mode, we visualized two histograms that described the distribution of birth weights for maternal smokers and non-smokers. However, comparing these histograms was difficult because they were displayed on seperate plots. Can we overlay the two to tell a more compelling story?\nIn seaborn, multiple calls to a plotting library in the same code cell will overlay the plots. For example:\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == True]\n\nsns.histplot(data = births_maternal_smoker, x= 'Birth Weight',\n             color = 'orange', label = 'smoker')\nsns.histplot(data = births_non_maternal_smoker, x= 'Birth Weight',\n             color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nHowever, notice how this diagram suffers from overplotting. We can fix this with a call to sns.kdeplot. This will remove the bins and overlay the histogram with a density curve that better summarizes the distribution.\n\nsns.kdeplot(data = births_maternal_smoker, x= 'Birth Weight', color = 'orange', label = 'smoker')\nsns.kdeplot(data = births_non_maternal_smoker, x= 'Birth Weight', color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nUnfortunately, we lose critical information in our distribution by removing small details. Therefore, we typically prefer to use box-plots and violin plots when comparing distributions. These are more concise and allow us to compare summary statistics across many distributions.\n\nsns.violinplot(data=births, x='Maternal Smoker', y='Birth Weight');\n# The following line of code plots a box-plot\n#sns.boxplot(data=births, x='Maternal Smoker', y='Birth Weight');\n\n\n\n\n\n\nRelationships Between Quantitative Variables\nUp until now, we’ve discussed how to visualize single-variable distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n\nScatter Plots\nScatter plots are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\nFor example, let’s plot a scatter plot comparing the Maternal Pregnancy Weight and Birth Weight colums, using both matplotlib and seaborn.\n\n# Matplotlib Example\nplt.scatter(births['Maternal Pregnancy Weight'], births['Birth Weight'])\n# For brevity, we have excluded code to label the axes\n\n<matplotlib.collections.PathCollection at 0x7fbeeabec430>\n\n\n\n\n\n\n# Seaborn Example\nsns.scatterplot(data = births, x = \"Maternal Pregnancy Weight\", y = \"Birth Weight\",\n                hue = \"Maternal Smoker\")\n\n<AxesSubplot: xlabel='Maternal Pregnancy Weight', ylabel='Birth Weight'>\n\n\n\n\n\nThis is an example where color is used to add a third dimension to our plot. This is possible with the hue paramater in seaborn, which adds a categorical column encoding to an existing visualization. This way, we can look for relationships in Maternal Pregnancy Weight and Birth Weight in both maternal smokers and non-smokers. If we wish to see the relationship’s strength more clearly, we can use sns.lmplot.\n\nsns.lmplot(data = births, x = \"Maternal Pregnancy Weight\", y = \"Birth Weight\", \n           hue=\"Maternal Smoker\", ci=False);\n\n\n\n\nWe can make out a weak, positive relationship in pregnancy weight and birth weight for both maternal smokers and non-smokers (slightly more positive in maternal smokers).\n\n\nHex Plots and Contour Plots\nUnfortunately, our scatter plots above suffered from overplotting, which made them hard to interpret. And with a large number of points, jittering is unlikely to resolve the issue. Instead, we can look to hex plots and contour plots.\nHex Plots can be thought of as a two dimensional histogram that shows the joint distribution between two variables. This is particularly useful working with very dense data.\n\nsns.jointplot(data = births, x = \"Maternal Pregnancy Weight\", \n              y = \"Birth Weight\", kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x7fbeeac58700>\n\n\n\n\n\nThe axes are evidently binned into hexagons, which makes the linear relationship easier to decipher. Darker regions generally indicate a higher density of points.\nOn the other hand, contour plots are two dimensional versions of density curves with marginal distributions of each variable on the axes. We’ve used very similar code here to generate our contour plots, with the addition of the kind = 'kde' and fill = True arguments.\n\nsns.jointplot(data = births, x = \"Maternal Pregnancy Weight\", \n              y = \"Birth Weight\", kind = 'kde', fill = True)\n\n<seaborn.axisgrid.JointGrid at 0x7fbec89c0f70>"
  },
  {
    "objectID": "cv_regularization/cv_reg.html",
    "href": "cv_regularization/cv_reg.html",
    "title": "Cross Validation and Regularization",
    "section": "",
    "text": "We ended last lecture with a question: how do we control the complexity of our model? To answer this, we must know precisely when our model begins to overfit. The key to this lies in evaluating the model on unseen data using a process called Cross-Validation. A second point this note will address is how to combat overfitting – namely, through a technique known as regularization."
  },
  {
    "objectID": "cv_regularization/cv_reg.html#cross-validation",
    "href": "cv_regularization/cv_reg.html#cross-validation",
    "title": "Cross Validation and Regularization",
    "section": "Cross Validation",
    "text": "Cross Validation\nFrom the last lecture, we learned that increasing model complexity decreased our model’s training error but increased variance. This makes intuitive sense; adding more features causes our model to better fit the given data, but generalize worse to new data. For this reason, a low training error is not representative of our model’s underlying performance – this may be a side effect of overfitting.\nTruly, the only way to know when our model overfits is by evaluating it on unseen data. Unfortunately, that means we need to wait for more data. This may be very expensive and time consuming.\nHow should we proceed? In this section, we will build up a viable solution to this problem.\n\nThe Holdout Method\nThe simplest approach to avoid overfitting is to keep some of our data secret from ourselves. This is known as the holdout method. We will train our models on most of the available data points – known as the training data . We’ll then evaluate the models’ performance on the unseen data points (called the validation set) to measure overfitting.\nImagine the following example where we wish to train a model on 35 data points. We choose the training set to be a random sample of 25 of these points, and the validation set to be the remaining 10 points. Using this data, we train 7 models, each with a higher polynomial degree than the last.\nWe get the following mean squared error (MSE) on the training data.\n\n\n\n\n\n\n\n\n\nUsing these same models, we compute the MSE on our 10 validation points and observe the following.\n\n\n\n\n\n\n\n\n\nNotice how the training error monotonically decreases as polynomial degree increases. This is consistent with our knowledge. However, validation error first decreases, then increases (from k = 3). These higher degree models are performing worse on unseen data – this indicates they are overfitting. As such, the best choice of polynomial degree in this example is k = 2.\nMore generally, we can represent this relationship with the following diagram.\n\nOur goal is to train a model with complexity near the red line. Note that this relationship is a simplification of the real-world. But for purposes of Data 100, this is good enough.\n\nHyperparameters\nIn machine learning, a hyperparameter is a value that controls the learning process. In our example, we built seven models, each of which had a hyperparameter k that controlled the polynomial degree of the model.\nTo choose between hyperparameters, we use the validation set. This was evident in our example above; we found that k = 2 had the lowest validation error. However, this holdout method is a bit naive. Imagine our random sample of 10 validation points coincidentally favored a higher degree polynomial. This would have led us to incorrectly favor a more complex model. In other words, our small amount of validation data may be different from real-world data.\nTo minimize this possiblity, we need to evaluate our model on more data. Decreasing the size of the training set is not an option – doing so will worsen our model. How should we proceed?\n\n\n\nK-Fold Cross Validation\nIn the holdout method, we train a model on only the training set, and assess the quality only on the validation set. On the other hand, K-Fold Cross Validation is a technique that determines the quality of a hyperparameter by evaluating a model-hyperparameter combination on k independent “folds” of data, which together make up the entire dataset. This is a more robust alternative to the holdout method. Let’s break down each step.\nNote: The k in k-fold cross validation is different from the k polynomial degree discussed in the earlier example.\nIn the k-fold cross-validation approach, we split our data into k equally sized groups (often called folds). In the example where k = 5:\n\nTo determine the “quality” of a particular hyperparameter:\n\nPick a fold, which we’ll call the validation fold. Train a model on all other k-1 folds. Compute an error on the validation fold.\nRepeat the step above for all k possible choices of validation fold, each time training a new model.\nAverage the k validation fold errors. This will give a single error for the hyperparameter.\n\nFor k = 5, we have the following partitions. At each iteration, we train a model on the blue data and validate on the orange data. This gives us a total of 5 errors which we average to obtain a single representative error for the hyperparameter.\n\nNote that the value of the hyperparameter is fixed during this process. By doing so, we can be confident in the hyperparameter’s performance on the entire dataset. To compare multiple choices of a hyperparameter, say m, we run k-fold cross validation m times. The smallest of the m resulting errors corresponds to the best hyperparameter value.\n\nPicking K\nTypical choices of k are 5, 10, and N, where N is the number of data points.\nk = N is known as “leave one out cross validation”, and will typically give you the best results.\n\nIn this approach, each validation set is only one point.\nEvery point gets a chance to get used as the validation set.\n\nHowever, k = N is very expensive and requires you to fit a large number of models.\n\n\n\nTest Sets\nSuppose we’re researchers building a state of the art regression model. We choose a model with the lowest validation error and want to report this out to the world. Unfortunately, our validation error may be biased; that is, not representative of error on real-world data. This is because during our hyperparameter search, we were implicitly “learning” from our validation data by tuning our model to achieve better results. Before reporting our results, we should run our model on a special test set that we’ve never seen or used for any purpose whatsoever.\nA test set can be something that we generate ourselves, or it can be a common dataset whose true values are unknown. In the case of the former, we can split our our available data into 3 partitions: the training set, testing set, and validation set. The exact amount of data allocated to each partition varies, but a common split is 80% train, 10% test, 10% validation.\nBelow is an implementation of extracting a training, testing and validation set using sklearn.train_test_split on a data matrix X and an array of observations y.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11)\nAs a recap:\n\nTraining set used to pick parameters.\nValidation set used to pick hyperparameters (or pick between different models).\nTest set used to provide an unbiased error at the end.\n\nHere is an idealized relationship between training error, test error, and validation error.\n\nNotice how the test error behaves similarily to the validation error. Both come from data that is unseen during the model training process, so both are fairly good estimates of real-world data. Of the two, the test error is more unbiased for the reasons mentioned above.\nAs before, the optimal complexity level exists where validation error is minimized. Logically, we can’t design a model that minimizes test error because we don’t use the test set until final evaluation."
  },
  {
    "objectID": "cv_regularization/cv_reg.html#regularization",
    "href": "cv_regularization/cv_reg.html#regularization",
    "title": "Cross Validation and Regularization",
    "section": "Regularization",
    "text": "Regularization\nEarlier, we found an optimal model complexity by choosing the hyperparameter that minimized validation error. This was the polynomial degree k= 2. Tweaking the “complexity” was simple; it was only a matter of adjusting the polynomial degree.\nHowever, in most machine learning problems, complexity is defined differently. Today, we’ll explore two different definitions of complexity: the squared and absolute magnitude of \\(\\theta_i\\) coefficients.\n\nConstraining Gradient Descent\nBefore we discuss these definitions, let’s first familiarize ourselves with the concept of constrained gradient descent. Imagine we have a two feature model with coeffiecient weights of \\(\\theta_1\\) and \\(\\theta_2\\). Below we’ve plotted a two dimensional contour plot of the OLS loss surface – darker areas indicate regions of lower loss. Gradient descent will find the optimal paramaters during training \\((\\theta_1, \\theta_2) = \\hat\\theta_{No Reg.}\\).\n\nSuppose we arbitrarily decide that gradient descent can never land outside of the green ball.\n\nGradient descent finds a new solution at \\(\\hat\\theta_{Reg.}\\). This is far from the global optimal solution \\(\\hat\\theta_{No Reg.}\\) – however, it is the closest point that lives in the green ball. In other words, \\(\\hat\\theta_{Reg.}\\) is the constrained optimal solution.\nThe size of this ball is completely arbitrary. Increasing its size allows for a constrained solution closer to the global optimum, and vice versa.\n\n\n\n\n\n\n\n\n\nIn fact, the size of this ball is inherently linked to model complexity. A smaller ball constrains (\\(\\theta_1\\), \\(\\theta_2\\)) more than a larger ball. This is synonymous with the behavior of a less complex model, which finds a solution farther from the optimum. A larger ball, on the other hand, is synonymous to a more complex model that can achieve a near optimal solution.\nConsider the extreme case where the radius is infinitely small. The solution to every constrained modeling problem would lie on the origin, at \\((\\theta_1, \\theta_2) = (0, 0)\\). This is equivalent to the constant model we studied – the least complex of all models. In the case where the radius is infinitely large, the optimal constrained solution exists at \\(\\hat\\theta_{No Reg.}\\) itself! This is the solution obtained from OLS with no limitations on complexity.\n\nThe intercept coefficient is typically not constrained; \\(\\theta_0\\) can be any value. This way, if all \\(\\theta_i = 0\\) except \\(\\theta_0\\), the resulting model is a constant model (and \\(\\theta_0\\) is the mean of all observations).\n\n\nL2 Regularization\n\nThe Constrained Form\nRegularization is the formal term that describes the process of limiting a model’s complexity. This is done by constraining the solution of a cost function, much like how we constrained the set of permissible (\\(\\theta_1\\), \\(\\theta_2\\)) above. L2 Regularization, commonly referred to as Ridge Regression, is the technique of constraining our model’s parameters to lie within a ball around the origin. Formally, it is defined as\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\]\n\nsuch that \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\)\n\n\nThe mathematical definition of complexity in Ridge Regression is \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\). This formulation of complexity limits the total squared magnitude of the coefficients to some constant \\(Q\\). In two dimensional space, this is \\(\\theta_{1}^{2} + \\theta_{2}^{2} \\le Q\\). You’ll recognize this as the equation of a circle with axes \\(\\theta_{1}, \\theta_{2}\\) and radius \\(Q\\). In higher dimensions, this circle becomes a hypersphere and is conventionally referred to as the L2 norm ball. Decreasing \\(Q\\) shrinks the norm ball, and limits the complexity of the model (discussed in the previous section). Likewise, expanding the norm ball increases the allowable model complexity.\nWithout the constraint \\(\\sum_{j=1}^{d} \\theta_j^{2} \\le Q\\), the optimal solution is \\(\\hat{\\theta} = \\hat\\theta_{No Reg.}\\). With an appropriate value of \\(Q\\) applied to the constraint, the solution \\(\\hat{\\theta} = \\hat\\theta_{Reg.}\\) is sub-optimal on the training data but generalizes better to new data.\n\n\nThe Functional Form\nUnfortunately, the function above requires some work. It’s not easy to mathematically optimize over a constraint. Instead, in most machine learning text, you’ll see a different formulation of Ridge Regression.\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\]\nThese two equations are equivalent by Lagrangian Duality (not in scope).\nNotice that we’ve replaced the constraint with a second term in our cost function. We’re now minimizing a function with a regularization term that penalizes large coefficients. The \\(\\alpha\\) factor controls the degree of regularization. In fact, \\(\\alpha \\approx \\frac{1}{Q}\\).  To understand why, let’s consider these 2 extreme examples:\n\nAssume \\(\\alpha \\rightarrow \\infty\\). Then, \\(\\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\) dominates the cost function. To minimize this term, we set \\(\\theta_j = 0\\) for all \\(j \\ge 1\\). This is a very constrained model that is mathematically equivalent to the constant model. Earlier, we explained the constant model also arises when the L2 norm ball radius \\(Q \\rightarrow 0\\).\nAssume \\(\\alpha \\rightarrow 0\\). Then, \\(\\alpha \\sum_{j=1}^{d} \\theta_j^{2}\\) is infinitely small. Minimizing the cost function is equivalent to \\(\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\). This is just OLS, and the optimal solution is the global minimum \\(\\hat{\\theta} = \\hat\\theta_{No Reg.}\\). We showed that the global optimum is achieved when the L2 norm ball radius \\(Q \\rightarrow \\infty\\).\n\n\n\nClosed Form Solution\nAn additional benefit to Ridge Regression is that it has a closed form solution.\n\\[\\hat\\theta_{ridge} = (X^TX + n\\alpha I)^{-1}X^TY\\]\nThis solution exists even if there is linear dependence in the columns of the data matrix. We will not derive this result in Data 100, as it involves a fair bit of matrix calculus.\n\n\nImplementation of Ridge Regression\nOf course, sklearn has a built-in implementation of Ridge Regression. Simply import the Ridge class of the sklearn.linear_model library.\n\nfrom sklearn.linear_model import Ridge\n\nWe will various Ridge Regression models on the familiar vehicles DataFrame from last lecture. This will help solidfy some of the theoretical concepts discussed earlier.\n\n\nCode\nimport pandas as pd\nvehicles = pd.read_csv(\"data/vehicle_data.csv\", index_col=0)\nvehicles_mpg = pd.read_csv(\"data/vehicle_mpg.csv\", index_col=0)\n\n\n\nvehicles.head(5)\n\n\n\n\n\n  \n    \n      \n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      cylinders^2\n      displacement^2\n      horsepower^2\n      weight^2\n      acceleration^2\n    \n  \n  \n    \n      0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      64\n      94249.0\n      16900.0\n      12278016\n      144.00\n    \n    \n      1\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      64\n      122500.0\n      27225.0\n      13638249\n      132.25\n    \n    \n      2\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      64\n      101124.0\n      22500.0\n      11806096\n      121.00\n    \n    \n      3\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      64\n      92416.0\n      22500.0\n      11785489\n      144.00\n    \n    \n      4\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      64\n      91204.0\n      19600.0\n      11895601\n      110.25\n    \n  \n\n\n\n\nHere, we fit an extremeley regularized model without an intercept. Note the small coefficient values.\n\nridge_model_large_reg = Ridge(alpha = 10000)\nridge_model_large_reg.fit(vehicles, vehicles_mpg)\nridge_model_large_reg.coef_\n\narray([[ 8.56292915e-04, -5.92399474e-02, -9.81013894e-02,\n        -9.66985253e-03, -5.08353226e-03,  1.49576895e-02,\n         1.04959034e-04,  1.14786826e-04,  9.07086742e-07,\n        -4.60397349e-04]])\n\n\nNote how Ridge Regression effectively spreads a small weight across many features.\nWhen we apply very little regularization, our coefficients increase in size. Notice how they are identical to the coefficients retrieved from the LinearRegression model. This indicates the radius \\(Q\\) of the L2 norm ball is massive and encompasses the unregularized optimal solution. Once again, we see that \\(\\alpha\\) and \\(Q\\) are inversely related.\n\nridge_model_small_reg = Ridge(alpha = 10**-5)\nridge_model_small_reg.fit(vehicles, vehicles_mpg)\nridge_model_small_reg.coef_\n\narray([[-8.06754383e-01, -6.32025048e-02, -2.92851012e-01,\n        -3.41032156e-03, -1.43877512e+00,  1.25829303e-01,\n         7.72841216e-05,  6.99398090e-04,  3.11031744e-07,\n         3.16084838e-02]])\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(vehicles, vehicles_mpg)\nlinear_model.coef_\n\narray([[-8.06756280e-01, -6.32024872e-02, -2.92851021e-01,\n        -3.41032211e-03, -1.43877559e+00,  1.25829450e-01,\n         7.72840884e-05,  6.99398114e-04,  3.11031832e-07,\n         3.16084973e-02]])\n\n\n\n\n\nScaling Data for Regularization\nOne issue with our approach is that our features are on vastly different scales. For example, weight^2 is in the millions, while the number of cylinders are under 10. Intuitively, the coefficient value for weight^2 must be very small to offset the large magnitude of the feature. On the other hand, the coefficient of the cylinders feature is likely quite large in comparison. We see these claims are true in the LinearRegression model above.\nHowever, a problem arises in Ridge Regression. If we constrain our coefficients to a small region around the origin, we are unfairly restricting larger coefficients – like that of the cylinders feature. A smaller coefficient – that of the weight^2 feature – likely lies within this region, so the value changes very little. Compare the coefficients of the regularized and unregularized Ridge models above, and you’ll see this is true.\nTherefore, it’s imperative to standardize your data. We can do so using z-scores.\n\\[z_k = \\frac{x_k - u_k}{\\sigma_k}\\]\nYou’ll do this on lab 8 using the “StandardScaler” transformer. The resulting model coefficients will be all on the same scale.\n\n\nL1 Regularization\nL1 Regularization, commonly referred to as Lasso Regression, is an alternate regularization technique that limits the the absolute sum of \\(\\theta_i\\) coefficients.\n\nThe Constrained Form\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||\\]\n\nsuch that \\(\\sum_{j=1}^{d} |\\theta_j| \\le Q\\)\n\n\nIn two dimensions, our constraint equation is \\(|\\theta_1| + |\\theta_2| \\le Q\\). This is the graph of a diamond centered on the origin with endpoints \\(Q\\) units away on each axis.\n\n\n\nThe Functional Form\nA more convenient way to express Lasso Regression is as follows:\n\\[\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} |\\theta_j|\\]\nAs with Ridge Regression, the hyperparameter \\(\\alpha\\) has the same effect on Lasso Regression. That is, increasing  \\(\\alpha\\) (equivalently, decreasing  \\(Q\\)) increases the amount of regularization, and vice versa.\nUnfortunately, Lasso Regression does not have a closed form solution – the cost function is not differentiable everywhere. Specifically, the sum \\(\\sum_{j=1}^{d} |\\theta_j|\\) is problematic because it is composed of absolute value functions, each of which are non-differentiable at the origin.\nSo why use Lasso Regression? As we’ll see shortly, it is great at implicit feature selection.\n\n\nImplementation of Lasso Regression\nLasso Regression is great at reducing complexity by eliminating the least important features in a model. It does so by setting their respective feature weights to \\(0\\). See the following example.\n\nfrom sklearn.linear_model import Lasso\nlasso_model = Lasso(alpha = 1)\n\nstandardized_vehicles=(vehicles-vehicles.mean())/vehicles.std()\nlasso_model.fit(standardized_vehicles, vehicles_mpg)\nlasso_model.coef_\n\narray([-0.14009981, -0.28452369, -1.14351999, -4.11329618,  0.        ,\n       -0.        , -0.        , -0.        , -0.        ,  0.        ])\n\n\nNotice how we standardized our data first. Lasso Regression then set the coefficients of our squared features to \\(0\\) – presumably, these are the least important predictors of mpg.\n\n\n\nSummary of Regularization Methods\nUnderstanding the distinction between Ridge Regression and Lasso Regression is important. We’ve provided a helpful visual that summarizes the key differences.\n\nThis diagram displays the L1 and L2 constrained solution for various orientations of the OLS loss surface. Notice how the L1 (Lasso) solution almost always lies on some axis, or edge of the diamond. Graphically, this makes sense; the edges of the diamond are the farthest from the origin, and usually closest to the global optimum. For the most part, only one feature has a non-zero coefficient; this argument extends quite nicely to multiple features in higher dimensional space.\nThe L2 (Ridge) solution, however, typically has an optimal solution in some quadrant of the graph. Every point on the circumference of the L2 norm ball is equidistant from the origin, and thus similar in distance to the global optimum. As such, this technique of regularization is great at distributing a small weight across both features."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data100-course-notes",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "regex/regex.html",
    "href": "regex/regex.html",
    "title": "Regular Expressions",
    "section": "",
    "text": "Learning Outcomes"
  },
  {
    "objectID": "regex/regex.html#why-work-with-text",
    "href": "regex/regex.html#why-work-with-text",
    "title": "Regular Expressions",
    "section": "Why Work with Text?",
    "text": "Why Work with Text?\nLast lecture, we learned of the difference between quantitative and qualitative variable types. The latter includes string data - the primary focus of today’s lecture. In this note, we’ll discuss the necessary tools to manipulate text: Python string manipulation and regular expressions.\nThere are two main reasons for working with text.\n\nCanonicalization: Convert data that has multiple formats into a standard form.\n\nBy manipulating text, we can join tables with mismatched string labels\n\nExtract information into a new feature.\n\nWe can extract data and time features from raw log files"
  },
  {
    "objectID": "regex/regex.html#python-string-methods",
    "href": "regex/regex.html#python-string-methods",
    "title": "Regular Expressions",
    "section": "Python String Methods",
    "text": "Python String Methods\nFirst, we’ll introduce a few methods useful for string manipulation. The following table includes a number of string operations supported by Python and pandas. The Python functions operate on a single string, while their equivalent in pandas are vectorized - they operate on a Series of string data.\n\n\n\n\n\n\n\n\nOperation\nPython\nPandas (Series)\n\n\n\n\nTransformation\n\ns.lower(_)\ns.upper(_)\n\n\nser.str.lower(_)\nser.str.upper(_)\n\n\n\nReplacement + Deletion\n\ns.replace(_)\n\n\nser.str.replace(_)\n\n\n\nSplit\n\ns.split(_)\n\n\nser.str.split(_)\n\n\n\nSubstring\n\ns[1:4]\n\n\nser.str[1:4]\n\n\n\nMembership\n\n'_' in s\n\n\nser.str.contains(_)\n\n\n\nLength\n\nlen(s)\n\n\nser.str.len()\n\n\n\n\nWe’ll discuss the differences between Python string functions and pandas Series methods in the following section on canonicalization.\n\nCanonicalization\nAssume we want to merge the given tables.\n\n\nCode\nimport pandas as pd\n\nwith open('data/county_and_state.csv') as f:\n    county_and_state = pd.read_csv(f)\n    \nwith open('data/county_and_population.csv') as f:\n    county_and_pop = pd.read_csv(f)\n\n\n\ndisplay(county_and_state), display(county_and_pop);\n\n\n\n\n\n  \n    \n      \n      County\n      State\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n    \n    \n      1\n      Lac qui Parle County\n      MN\n    \n    \n      2\n      Lewis and Clark County\n      MT\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n    \n    \n      1\n      Lac Qui Parle\n      8067\n    \n    \n      2\n      Lewis & Clark\n      55716\n    \n    \n      3\n      St. John the Baptist\n      43044\n    \n  \n\n\n\n\nLast time, we used a primary key and foreign key to join two tables. While neither of these keys exist in our DataFrames, the County columns look similar enough. Can we convert these columns into one standard, canonical form?\nThe following function uses Python string manipulation to convert a single county name into canonical form. It does so by eliminating whitespace, punctuation, and unnecessary text.\n\ndef canonicalize_county(county_name):\n    return (\n        county_name\n            .lower()\n            .replace(' ', '')\n            .replace('&', 'and')\n            .replace('.', '')\n            .replace('county', '')\n            .replace('parish', '')\n    )\n\ncanonicalize_county(\"St. John the Baptist\")\n\n'stjohnthebaptist'\n\n\nWe will use the pandas map function to apply the canonicalize_county function to every row in both DataFrames. Then, we can create new columns called clean_county_python containing the canonical form.\n\ncounty_and_pop['clean_county_python'] = county_and_pop['County'].map(canonicalize_county)\ncounty_and_state['clean_county_python'] = county_and_state['County'].map(canonicalize_county)\n\n\ndisplay(county_and_state), display(county_and_pop);\n\n\n\n\n\n  \n    \n      \n      County\n      State\n      clean_county_python\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n      dewitt\n    \n    \n      1\n      Lac qui Parle County\n      MN\n      lacquiparle\n    \n    \n      2\n      Lewis and Clark County\n      MT\n      lewisandclark\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n      clean_county_python\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n      dewitt\n    \n    \n      1\n      Lac Qui Parle\n      8067\n      lacquiparle\n    \n    \n      2\n      Lewis & Clark\n      55716\n      lewisandclark\n    \n    \n      3\n      St. John the Baptist\n      43044\n      stjohnthebaptist\n    \n  \n\n\n\n\nAlternatively, we can use pandas Series methods to create this standardized column. To do so, we must call the .str attribute of our Series prior to calling any methods, like .lower and .replace. Notice how these method names match their equivalent built-in Python string function.\nChaining multiple Series methods in this manner eliminates the need to use the map function.\n\ndef canonicalize_county_series(county_series):\n    return (\n        county_series\n            .str.lower()\n            .str.replace(' ', '')\n            .str.replace('&', 'and')\n            .str.replace('.', '')\n            .str.replace('county', '')\n            .str.replace('parish', '')\n    )\n\ncounty_and_pop['clean_county_pandas'] = canonicalize_county_series(county_and_pop['County'])\ncounty_and_state['clean_county_pandas'] = canonicalize_county_series(county_and_state['County'])\n\n\ndisplay(county_and_pop), display(county_and_state);\n\n\n\n\n\n  \n    \n      \n      County\n      Population\n      clean_county_python\n      clean_county_pandas\n    \n  \n  \n    \n      0\n      DeWitt\n      16798\n      dewitt\n      dewitt\n    \n    \n      1\n      Lac Qui Parle\n      8067\n      lacquiparle\n      lacquiparle\n    \n    \n      2\n      Lewis & Clark\n      55716\n      lewisandclark\n      lewisandclark\n    \n    \n      3\n      St. John the Baptist\n      43044\n      stjohnthebaptist\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      County\n      State\n      clean_county_python\n      clean_county_pandas\n    \n  \n  \n    \n      0\n      De Witt County\n      IL\n      dewitt\n      dewitt\n    \n    \n      1\n      Lac qui Parle County\n      MN\n      lacquiparle\n      lacquiparle\n    \n    \n      2\n      Lewis and Clark County\n      MT\n      lewisandclark\n      lewisandclark\n    \n    \n      3\n      St John the Baptist Parish\n      LS\n      stjohnthebaptist\n      stjohnthebaptist\n    \n  \n\n\n\n\n\n\nExtraction\nExtraction explores the idea of obtaining useful information from text data. This will be particularily important in model building, which we’ll study in a few weeks.\nSay we want to read some data from a .txt file.\n\nwith open('data/log.txt', 'r') as f:\n    log_lines = f.readlines()\n\n\nlog_lines\n\n['169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n',\n '193.205.203.3 - - [2/Feb/2005:17:23:6 -0800] \"GET /stat141/Notes/dim.html HTTP/1.0\" 404 302 \"http://eeyore.ucdavis.edu/stat141/Notes/session.html\"\\n',\n '169.237.46.240 - \"\" [3/Feb/2006:10:18:37 -0800] \"GET /stat141/homework/Solutions/hw1Sol.pdf HTTP/1.1\"\\n']\n\n\nSuppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Unfortunately, these items are not in a fixed position from the beginning of the string. Slicing by some fixed offset won’t work.\nInstead, we can use some clever thinking. Notice how the relevant information is contained within a set of brackets, seperated by / and :. We can hone in on this region of text, and split the data on these characters. Python’s built-in .split function makes this easy.\n\nfirst = log_lines[0] # Only considering the first row of data\n\npertinent = first.split(\"[\")[1].split(']')[0]\nday, month, rest = pertinent.split('/')\nyear, hour, minute, rest = rest.split(':')\nseconds, time_zone = rest.split(' ')\nday, month, year, hour, minute, seconds, time_zone\n\n('26', 'Jan', '2014', '10', '47', '58', '-0800')\n\n\nThere are two problems with this code:\n\nPython’s built-in functions limit us to extract data one row at a time\n\nThis can be resolved using a map function or Pandas Series methods.\n\nThe code is quite verbose\n\nThis is a larger issue that is trickier to solve\n\n\nIn the next section, we’ll introduce regular expressions - a tool that solves problem 2)."
  },
  {
    "objectID": "regex/regex.html#regex-basics",
    "href": "regex/regex.html#regex-basics",
    "title": "Regular Expressions",
    "section": "Regex Basics",
    "text": "Regex Basics\nA regular expression (“regex”) is a sequence of characters that specifies a search pattern. They are written to extract specific information from text.\nThe language of regular expressions - regular language - aims to replicate formal language, which is a way to describe conditions on a set of strings. Regular expressions are essentially part of a smaller programming language embedded in Python, made available through the re module. As such, they have a stand-alone syntax and methods for various capabilities.\nRegular expressions are useful in many applications beyond data science. For example, Social Security Numbers (SSNs) are often validated with regular expresions.\n\nr\"[0-9]{3}-[0-9]{2}-[0-9]{4}\" # Regular Expression Syntax\n\n# 3 of any digit, then a dash,\n# then 2 of any digit, then a dash,\n# then 4 of any digit\n\n'[0-9]{3}-[0-9]{2}-[0-9]{4}'\n\n\n\n\nThere are a ton of resources to learn and experiment with regular expressions. A few are provided below:\n\nOfficial Regex Guide\nData 100 Reference Sheet\nRegex101.com\n\nBe sure to check Python under the category on the left.\n\n\n\nBasics Regex Syntax\nThere are four basic operations with regular expressions.\n\n\n\n\n\n\n\n\n\n\nOperation\nOrder\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nConcatenation\n3\nAABAAB\nAABAAB\nevery other string\n\n\nOr: |\n4\nAA|BAAB\nAA BAAB\nevery other string\n\n\nClosure: *  (zero or more)\n2\nAB*A\nAA ABBBBBBA\nAB  ABABA\n\n\nGroup: ()  (parenthesis)\n1\nA(A|B)AAB    (AB)*A\nAAAAB ABAAB   A ABABABABA\nevery other string    AA  ABBA\n\n\n\nNotice how these metacharacter operations are ordered. Rather than being literal characters, these metacharacters manipulate adjacent characters. () takes precedence, followed by *, and finally |. This allows us to differentiate between very different regex commands like AB* and (AB)*. The former reads “A then zero or more copies of B”, while the latter specifies “zero or more copies of AB”.\n\nExamples\nQuestion 1: Give a regular expression that matches moon, moooon, etc. Your expression should match any even number of os except zero (i.e. don’t match mn).\nAnswer 1: moo(oo)*n\n\nHardcoding oo before the capture group ensures that mn is not matched.\nA capture group of (oo)* ensures the number of o’s is even.\n\nQuestion 2: Using only basic operations, formulate a regex that matches muun, muuuun, moon, moooon, etc. Your expression should match any even number of us or os except zero (i.e. don’t match mn).\nAnswer 2: m(uu(uu)*|oo(oo)*)n\n\nThe leading m and trailing n ensures that only strings beginning with m and ending with n are matched.\nNotice how the outer capture group surrounds the |. This ensures.\n\nConsider the regex m(uu(uu)*)|(oo(oo)*)n. This incorrectly matches muu and oooon.\n\nEach OR clause is everything to the left and right of |. The incorrect solution matches only half of the string, and ignores either m or n\nA set of paranthesis must surround |. That way, each OR clause is everything to the left and right of | within the group."
  },
  {
    "objectID": "regex/regex.html#regex-expanded",
    "href": "regex/regex.html#regex-expanded",
    "title": "Regular Expressions",
    "section": "Regex Expanded",
    "text": "Regex Expanded\nProvided below are more complex regular expression functions.\n\n\n\n\n\n\n\n\n\nOperation\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nAny Character: .  (except newline)\n.U.U.U.\nCUMULUS  JUGULUM\nSUCCUBUS TUMULTUOUS\n\n\nCharacter Class: []\n[A-Za-z][a-z]*\nword  Capitalized\ncamelCase 4illegal\n\n\nRepeated \"a\" Times: {a}\nj[aeiou]{3}hn\njaoehn  jooohn\njhn  jaeiouhn\n\n\nRepeated \"from a to b\" Times: {a, b}\nj[0u]{1,2}hn\njohn  juohn\njhn  jooohn\n\n\nAt Least One: +\njo+hn\njohn  joooooohn\njhn  jjohn\n\n\nZero or One: ?\njoh?n\njon  john\nany other string\n\n\n\n\nExamples\nLet’s analyze a few examples of complex regular expressions.\n\n\n\n\n\n\n\nMatches\nDoes Not Match\n\n\n\n\n\n.*SPB.*\n\n\n\n\nRASPBERRY  SPBOO\nSUBSPACE  SUBSPECIES\n\n\n\n[0-9]{3}-[0-9]{2}-[0-9]{4}\n\n\n\n\n231-41-5121  573-57-1821\n231415121  57-3571821\n\n\n\n[a-z]+@([a-z]+\\.)+(edu|com)\n\n\n\n\nhorse@pizza.com  horse@pizza.food.com\nfrank_99@yahoo.com  hug@cs\n\n\n\nExplanations\n\n.*SPB.* only matches strings that contain the substring SPB.\n\nThe .* metacharacter matches any amount of non-negative characters. Newlines do not count.\n\n\nThis regular expression matches 3 of any digit, then a dash, then 2 of any digit, then a dash, then 4 of any digit\n\nYou’ll recognize this as the familiar Social Security Number regular expression\n\nMatches any email with a com or edu domain, where all characters of the email are letters.\n\nAt least one . must preceed the domain name. Including a backslash \\ before any metacharacter (in this case, the .) tells regex to match that character exactly."
  },
  {
    "objectID": "regex/regex.html#convenient-regex",
    "href": "regex/regex.html#convenient-regex",
    "title": "Regular Expressions",
    "section": "Convenient Regex",
    "text": "Convenient Regex\nHere are a few more convenient regular expressions.\n\n\n\n\n\n\n\n\n\nOperation\nSyntax Example\nMatches\nDoesn’t Match\n\n\n\n\nbuilt in character class\n\\w+  \\d+ \\s+ \nFawef_03  231123  whitespace\nthis person 423 people non-whitespace\n\n\ncharacter class negation: [^] (everything except the given characters)\n[^a-z]+.\nPEPPERS3982 17211!↑å\nporch  CLAMS\n\n\nescape character: \\  (match the literal next character)\ncow.com\njaoehn  jooohn\njhn  jaeiouhn\n\n\nbeginning of line: ^\n^ark\nark two ark o ark\ndark\n\n\nend of line: $\nark$\ndark  ark o ark\nark two\n\n\nlazy version of zero or more : *?\n5.*?5\n5005  55\n5005005\n\n\n\n\nExamples\nLet’s revist our earlier problem of extracting date/time data from the given .txt files. Here is how the data looked.\n\nlog_lines[0]\n\n'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n'\n\n\nQuestion: Give a regular expression that matches everything contained within and including the brackets - the day, month, year, hour, minutes, seconds, and timezone.\nAnswer: \\[.*\\]\n\nNotice how matching the literal [ and ] is necessary. Therefore, an escape character \\ is required before both [ and ] - otherwise these metacharacters will match character classes.\nWe need to match a particular format between [ and ]. For this example, .* will suffice.\n\nAlternative Solution: \\[\\w+/\\w+/\\w+:\\w+:\\w+:\\w+\\s-\\w+\\]\n\nThis solution is much safer.\n\nImagine the data between [ and ] was garbage - .* will still match that.\nThe alternate solution will only match data that follows the correct format."
  },
  {
    "objectID": "regex/regex.html#regex-in-python-and-pandas-regex-groups",
    "href": "regex/regex.html#regex-in-python-and-pandas-regex-groups",
    "title": "Regular Expressions",
    "section": "Regex in Python and Pandas (Regex Groups)",
    "text": "Regex in Python and Pandas (Regex Groups)\n\nCanonicalization\n\nCanonicalization with Regex\nEarlier in this note, we examined the process of canonicalization using Python string manipulation and pandas Series methods. However, this approach had a major flaw: our code was unnecessarily verbose. Equipped with our knowledge of regular expressions, let’s fix this.\nTo do so, we need to understand a few functions in the re module. The first of these is the substitute function: re.sub(pattern, rep1, text). It behaves similarily to Python’s built-in .replace function, and returns text with all instances of pattern replaced by rep1.\nThe regular expression here removes text surrounded by <> (also known as HTML tags).\n\nimport re\n\ntext = \"<div><td valign='top'>Moo</td></div>\"\npattern = r\"<[^>]+>\"\nre.sub(pattern, '', text) \n\n'Moo'\n\n\nNotice the r preceeding the regular expression pattern; this specifies the regular expression is a raw string. Raw strings do not recognize escape sequences (ie the Python newline metacharacter \\n). This makes them useful for regular expressions, which often contain literal \\ characters.\nIn other words, don’t forget to tag your regex with a r.\n\n\nCanonicalization with Pandas\nWe can also use regular expressions with Pandas Series methods. This gives us the benefit of operating on an entire column of data as opposed to a single value. The code is simple:  ser.str.replace(pattern, repl, regex=True).\nConsider the following DataFrame html_data with a single column.\n\n\nCode\ndata = {\"HTML\": [\"<div><td valign='top'>Moo</td></div>\", \\\n                 \"<a href='http://ds100.org'>Link</a>\", \\\n                 \"<b>Bold text</b>\"]}\nhtml_data = pd.DataFrame(data)\n\n\n\nhtml_data\n\n\n\n\n\n  \n    \n      \n      HTML\n    \n  \n  \n    \n      0\n      <div><td valign='top'>Moo</td></div>\n    \n    \n      1\n      <a href='http://ds100.org'>Link</a>\n    \n    \n      2\n      <b>Bold text</b>\n    \n  \n\n\n\n\n\npattern = r\"<[^>]+>\"\nhtml_data['HTML'].str.replace(pattern, '', regex=True)\n\n0          Moo\n1         Link\n2    Bold text\nName: HTML, dtype: object\n\n\n\n\n\nExtraction\n\nExtraction with Regex\nJust like with canonicalization, the re module provides capability to extract relevant text from a string:  re.findall(pattern, text). This function returns a list of all matches to pattern.\nUsing the familiar regular expression for Social Security Numbers:\n\ntext = \"My social security number is 123-45-6789 bro, or maybe it’s 321-45-6789.\"\npattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\nre.findall(pattern, text)  \n\n['123-45-6789', '321-45-6789']\n\n\n\n\nExtraction with Pandas\nPandas similarily provides extraction functionality on a Series of data: ser.str.findall(pattern)\nConsider the following DataFrame ssn_data.\n\n\nCode\ndata = {\"SSN\": [\"987-65-4321\", \"forty\", \\\n                \"123-45-6789 bro or 321-45-6789\",\n               \"999-99-9999\"]}\nssn_data = pd.DataFrame(data)\n\n\n\nssn_data\n\n\n\n\n\n  \n    \n      \n      SSN\n    \n  \n  \n    \n      0\n      987-65-4321\n    \n    \n      1\n      forty\n    \n    \n      2\n      123-45-6789 bro or 321-45-6789\n    \n    \n      3\n      999-99-9999\n    \n  \n\n\n\n\n\nssn_data[\"SSN\"].str.findall(pattern)\n\n0                 [987-65-4321]\n1                            []\n2    [123-45-6789, 321-45-6789]\n3                 [999-99-9999]\nName: SSN, dtype: object\n\n\nThis function returns a list for every row containing the pattern matches in a given string.\n\n\n\nRegular Expression Capture Groups\nEarlier we used parentheses ( ) to specify the highest order of operation in regular expressions. However, they have another meaning; paranthesis are often used to represent capture groups. Capture groups are essentially, a set of smaller regular expressions that match multiple substrings in text data.\nLet’s take a look at an example.\n\nExample 1\n\ntext = \"Observations: 03:04:53 - Horse awakens. \\\n        03:05:14 - Horse goes back to sleep.\"\n\nSay we want to capture all occurences of time data (hour, minute, and second) as seperate entities.\n\npattern_1 = r\"(\\d\\d):(\\d\\d):(\\d\\d)\"\nre.findall(pattern_1, text)\n\n[('03', '04', '53'), ('03', '05', '14')]\n\n\nNotice how the given pattern has 3 capture groups, each specified by the regular expression (\\d\\d). We then use re.findall to return these capture groups, each as tuples containing 3 matches.\nThese regular expression capture groups can be different. We can use the (\\d{2}) shorthand to extract the same data.\n\npattern_2 = r\"(\\d\\d):(\\d\\d):(\\d{2})\"\nre.findall(pattern_2, text)\n\n[('03', '04', '53'), ('03', '05', '14')]\n\n\n\n\nExample 2\nWith the notion of capture groups, convince yourself how the following regular expression works.\n\nfirst = log_lines[0]\nfirst\n\n'169.237.46.168 - - [26/Jan/2014:10:47:58 -0800] \"GET /stat141/Winter04/ HTTP/1.1\" 200 2585 \"http://anson.ucdavis.edu/courses/\"\\n'\n\n\n\npattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\nday, month, year, hour, minute, second, time_zone = re.findall(pattern, first)[0]\nprint(day, month, year, hour, minute, second, time_zone)\n\n26 Jan 2014 10 47 58 -0800"
  },
  {
    "objectID": "regex/regex.html#limitations-of-regular-expressions",
    "href": "regex/regex.html#limitations-of-regular-expressions",
    "title": "Regular Expressions",
    "section": "Limitations of Regular Expressions",
    "text": "Limitations of Regular Expressions\nToday, we explored the capabilities of regular expressions in data wrangling with text data. However, there are a few things to be wary of.\nWriting regular expressions is like writing a program.\n\nNeed to know the syntax well.\nCan be easier to write than to read.\nCan be difficult to debug.\n\nRegular expressions are terrible at certain types of problems:\n\nFor parsing a hierarchical structure, such as JSON, use the json.load() parser, not regex!\nComplex features (e.g. valid email address).\nCounting (same number of instances of a and b). (impossible)\nComplex properties (palindromes, balanced parentheses). (impossible)"
  },
  {
    "objectID": "pandas_2/pandas_2.html",
    "href": "pandas_2/pandas_2.html",
    "title": "Pandas II",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nBuild familiarity with advanced pandas syntax\nRecognize situations where aggregation is useful and identify the correct technique for performing an aggregation\nUnderstand the real-world need for joining and perform a simple merge\nLast time, we introduced the pandas library as a toolkit for processing data. We learned the DataFrame and Series data structures, familiarized ourselves with the basic syntax for manipulating tabular data, and began writing our first lines of pandas code.\nIn this lecture, we’ll start to dive into some advanced pandas syntax. You may find it helpful to follow along with a notebook of your own as we walk through these new pieces of code.\nWe’ll start by loading the babynames dataset."
  },
  {
    "objectID": "pandas_2/pandas_2.html#sorting-with-a-custom-key",
    "href": "pandas_2/pandas_2.html#sorting-with-a-custom-key",
    "title": "Pandas II",
    "section": "Sorting With a Custom Key",
    "text": "Sorting With a Custom Key\nIn the last lecture, we learned how to sort a DataFrame by the values in one or more of its columns using .sort_values. pandas automatically sorted values in order according to numeric value (for number data) or alphabetical order (for string data).\n\n# Sort names by reverse-alphabetical order\n# Recall that `.head(5)` displays the first five rows in the DataFrame\nbabynames.sort_values(\"Name\", ascending=False).head(5) \n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      400761\n      CA\n      M\n      2021\n      Zyrus\n      5\n    \n    \n      197519\n      CA\n      F\n      2011\n      Zyrah\n      5\n    \n    \n      232144\n      CA\n      F\n      2020\n      Zyrah\n      5\n    \n    \n      217415\n      CA\n      F\n      2016\n      Zyrah\n      5\n    \n    \n      220674\n      CA\n      F\n      2017\n      Zyrah\n      6\n    \n  \n\n\n\n\nThis offers us a lot of functionality, but what if we need to sort by some other metric? For example, what if we wanted to find the longest names in the DataFrame?\nWe can do this by specifying the key parameter of .sort_values. The key parameter is assigned to a function of our choice. This function is then applied to each value in the specified column. pandas will, finally, sort the DataFrame by the values outputted by the function.\n\n# Here, a lambda function is applied to find the length of each value, `x`, in the \"Name\" column\nbabynames.sort_values(\"Name\", key = lambda x: x.str.len(), ascending=False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      313143\n      CA\n      M\n      1989\n      Franciscojavier\n      6\n    \n    \n      333732\n      CA\n      M\n      1997\n      Ryanchristopher\n      5\n    \n    \n      330421\n      CA\n      M\n      1996\n      Franciscojavier\n      8\n    \n    \n      323615\n      CA\n      M\n      1993\n      Johnchristopher\n      5\n    \n    \n      310235\n      CA\n      M\n      1988\n      Franciscojavier\n      10"
  },
  {
    "objectID": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "href": "pandas_2/pandas_2.html#adding-and-removing-columns",
    "title": "Pandas II",
    "section": "Adding and Removing Columns",
    "text": "Adding and Removing Columns\nTo add a new column to a DataFrame, we use a syntax similar to that used when accessing an existing column. Specify the name of the new column by writing dataframe[\"new_column\"], then assign this to a Series or Array containing the values that will populate this column.\n\n# Add a column named \"Length\" that includes the length of each name\nbabynames[\"Length\"] = babynames[\"Name\"].str.len()\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      Length\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n      4\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n      5\n    \n    \n      2\n      CA\n      F\n      1910\n      Dorothy\n      220\n      7\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n      8\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n      7\n    \n  \n\n\n\n\nIn the example above, we made use of an in-built function given to us by the str accessor. What if we had wanted to generate the values in our new column using a function of our own making?\nWe can do this using the Series .map method. .map takes in a function as input, and will apply this function to each value of a Series.\nFor example, say we wanted to find the number of occurrences of the sequence “dr” or “ea” in each name.\n\n# First, define a function to count the number of times \"dr\" or \"ea\" appear in each name\ndef dr_ea_count(string):\n    return string.count(\"dr\") + string.count(\"ea\")\n\n# Then, use `map` to apply `dr_ea_count` to each name in the \"Name\" column\nbabynames[\"dr_ea_count\"] = babynames[\"Name\"].map(dr_ea_count)\n\n# Sort the DataFrame by the new \"dr_ea_count\" column so we can see our handiwork\nbabynames.sort_values(by = \"dr_ea_count\", ascending = False).head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n      Length\n      dr_ea_count\n    \n  \n  \n    \n      101969\n      CA\n      F\n      1986\n      Deandrea\n      6\n      8\n      3\n    \n    \n      115950\n      CA\n      F\n      1990\n      Deandrea\n      5\n      8\n      3\n    \n    \n      131022\n      CA\n      F\n      1994\n      Leandrea\n      5\n      8\n      3\n    \n    \n      304390\n      CA\n      M\n      1985\n      Deandrea\n      6\n      8\n      3\n    \n    \n      108723\n      CA\n      F\n      1988\n      Deandrea\n      5\n      8\n      3\n    \n  \n\n\n\n\nIf we want to remove a column or row of a DataFrame, we can call the .drop method. Use the axis parameter to specify whether a column or row should be dropped. Unless otherwise specified, pandas will assume that we are dropping a row by default.\n\n# Drop the row of the DataFrame with label 2\nbabynames = babynames.drop(2, axis=\"rows\")\n\n# Drop our \"dr_ea_count\" and \"length\" columns from the DataFrame\nbabynames = babynames.drop([\"dr_ea_count\", \"Length\"], axis=\"columns\")\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      5\n      CA\n      F\n      1910\n      Ruth\n      128\n    \n  \n\n\n\n\nNotice that we reassigned babynames to the result of babynames.drop(...). This is a subtle, but important point: pandas table operations do not occur in-place. Calling dataframe.drop(...) will output a copy of dataframe with the row/column of interest removed, without modifying the original dataframe table.\nIn other words, if we simply call:\n\n# This creates a copy of `babynames` and removes the row with label 3...\nbabynames.drop(3, axis=\"rows\")\n\n# ...but the original `babynames` is unchanged! \n# Notice that the row with label 3 is still present\nbabynames.head(5)\n\n\n\n\n\n  \n    \n      \n      State\n      Sex\n      Year\n      Name\n      Count\n    \n  \n  \n    \n      0\n      CA\n      F\n      1910\n      Mary\n      295\n    \n    \n      1\n      CA\n      F\n      1910\n      Helen\n      239\n    \n    \n      3\n      CA\n      F\n      1910\n      Margaret\n      163\n    \n    \n      4\n      CA\n      F\n      1910\n      Frances\n      134\n    \n    \n      5\n      CA\n      F\n      1910\n      Ruth\n      128\n    \n  \n\n\n\n\nOur original babynames DataFrame will remain unmodified."
  },
  {
    "objectID": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "href": "pandas_2/pandas_2.html#aggregating-data-with-groupby",
    "title": "Pandas II",
    "section": "Aggregating Data with GroupBy",
    "text": "Aggregating Data with GroupBy\nUp until this point, we have been working with individual rows of DataFrames. As data scientists, we often wish to investigate trends across a larger subset of our data. For example, we may want to compute some summary statistic (the mean, median, sum, etc.) for a group of rows in our DataFrame. To do this, we’ll use pandas GroupBy objects.\nLet’s say we wanted to aggregate all rows in babynames for a given year.\n\nbabynames.groupby(\"Year\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fe4db6bbdf0>\n\n\nWhat does this strange output mean? Calling .groupby has generated a GroupBy object. You can imagine this as a set of “mini” sub-DataFrames, where each subframe contains all of the rows from babynames that correspond to a particular year.\nThe diagram below shows a simplified view of babynames to help illustrate this idea.\n\n\n\nCreating a GroupBy object\n\n\nWe can’t work with a GroupBy object directly – that is why you saw that strange output earlier, rather than a standard view of a DataFrame. To actually manipulate values within these “mini” DataFrames, we’ll need to call an aggregation method. This is a method that tells pandas how to aggregate the values within the GroupBy object. Once the aggregation is applied, pandas will return a normal (now grouped) DataFrame.\nThe first aggregation method we’ll consider is .agg. The .agg method takes in a function as its argument; this function is then applied to each column of a “mini” grouped DataFrame. We end up with a new DataFrame with one aggregated row per subframe. Let’s see this in action by finding the sum of all counts for each year in babynames – this is equivalent to finding the number of babies born in each year.\n\nbabynames.groupby(\"Year\").agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      8943\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\nWe can relate this back to the diagram we used above. Remember that the diagram uses a simplified version of babynames, which is why we see smaller values for the summed counts.\n\n\n\nPerforming an aggregation\n\n\nCalling .agg has condensed each subframe back into a single row. This gives us our final output: a DataFrame that is now indexed by \"Year\", with a single row for each unique year in the original babynames DataFrame.\nYou may be wondering: where did the \"State\", \"Sex\", and \"Name\" columns go? Logically, it doesn’t make sense to sum the string data in these columns (how would we add “Mary” + “Ann”?). Because of this, pandas will simply omit these columns when it performs the aggregation on the DataFrame. Since this happens implicitly, without the user specifying that these columns should be ignored, it’s easy to run into troubling situations where columns are removed without the programmer noticing. It is better coding practice to select only the columns we care about before performing the aggregation.\n\n# Same result, but now we explicitly tell Pandas to only consider the \"Count\" column when summing\nbabynames.groupby(\"Year\")[[\"Count\"]].agg(sum).head(5)\n\n\n\n\n\n  \n    \n      \n      Count\n    \n    \n      Year\n      \n    \n  \n  \n    \n      1910\n      8943\n    \n    \n      1911\n      9983\n    \n    \n      1912\n      17946\n    \n    \n      1913\n      22094\n    \n    \n      1914\n      26926\n    \n  \n\n\n\n\nThere is a whole host of aggregation methods we can use other than .agg. Some useful options are:\n\n.max: creates a new DataFrame with the maximum value of each group\n.mean: creates a new DataFrame with the mean value of each group\n.size: creates a new Series with the number of entries in each group\n.filter: creates a copy of the original DataFrame, keeping only the rows from subframes that obey a provided condition"
  },
  {
    "objectID": "pandas_2/pandas_2.html#aggregating-data-with-pivot-tables",
    "href": "pandas_2/pandas_2.html#aggregating-data-with-pivot-tables",
    "title": "Pandas II",
    "section": "Aggregating Data with Pivot Tables",
    "text": "Aggregating Data with Pivot Tables\nWe know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of columns names to .groupby.\nLet’s find the total number of baby names associated with each sex for each year in babynames. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\n# Find the total number of baby names associated with each sex for each year in the data\nbabynames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].agg(sum).head(6)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1910\n      F\n      5730\n    \n    \n      M\n      3213\n    \n    \n      1911\n      F\n      6602\n    \n    \n      M\n      3381\n    \n    \n      1912\n      F\n      9804\n    \n    \n      M\n      8142\n    \n  \n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multindex where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multindexes have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. You saw these back in Data 8. One set of values is used to create the index of the table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nbabynames.pivot_table(index = \"Year\", columns = \"Sex\", values = \"Count\", aggfunc = sum).head(5)\n\n\n\n\n\n  \n    \n      Sex\n      F\n      M\n    \n    \n      Year\n      \n      \n    \n  \n  \n    \n      1910\n      5730\n      3213\n    \n    \n      1911\n      6602\n      3381\n    \n    \n      1912\n      9804\n      8142\n    \n    \n      1913\n      11860\n      10234\n    \n    \n      1914\n      13815\n      13111\n    \n  \n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\""
  },
  {
    "objectID": "pandas_2/pandas_2.html#joining-tables",
    "href": "pandas_2/pandas_2.html#joining-tables",
    "title": "Pandas II",
    "section": "Joining Tables",
    "text": "Joining Tables\nWhen working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can join two or more tables into a single DataFrame.\nTo put this into practice, we’ll revisit the elections dataset from last lecture.\n\n\nCode\nelections = pd.read_csv(\"data/elections.csv\")\nelections.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nSay we want to understand the 2023 popularity of the names of each presidential candidate. To do this, we’ll need the combined data of babynames and elections.\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in babynames.\n\n# This `str` operation splits each candidate's full name at each \n# blank space, then takes just the candidiate's first name\nelections[\"First Name\"] = elections[\"Candidate\"].str.split().str[0]\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n      John\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n      John\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n    \n  \n\n\n\n\nNow, we’re ready to join the two tables. pd.merge is the pandas method used to join DataFrames together. The left and right parameters are used to specify the DataFrames to be joined. The left_on and right_on parameters are assigned to the string names of the columns to be used when performing the join. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.\n\npd.merge(left = elections, right = babynames, left_on = \"First Name\", right_on = \"Name\")\n\n\n\n\n\n  \n    \n      \n      Year_x\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n      State\n      Sex\n      Year_y\n      Name\n      Count\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1963\n      Andrew\n      5\n    \n    \n      1\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1968\n      Andrew\n      7\n    \n    \n      2\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1970\n      Andrew\n      5\n    \n    \n      3\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1971\n      Andrew\n      13\n    \n    \n      4\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      CA\n      F\n      1974\n      Andrew\n      7\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      21780\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      1996\n      Jo\n      8\n    \n    \n      21781\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      1999\n      Jo\n      6\n    \n    \n      21782\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      F\n      2016\n      Jo\n      5\n    \n    \n      21783\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      M\n      1934\n      Jo\n      5\n    \n    \n      21784\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n      Jo\n      CA\n      M\n      1985\n      Jo\n      5\n    \n  \n\n21785 rows × 12 columns"
  },
  {
    "objectID": "quarto-documentation/Quarto_Guide.html",
    "href": "quarto-documentation/Quarto_Guide.html",
    "title": "Name of the Lecture",
    "section": "",
    "text": "Begin by installing Quarto.\nTexts can be authored in Quarto using JupyterLab or classic Jupyter Notebook. To start a new document, open Jupyter and create a new notebook file.\nTo set up the document, create a new Raw NBConvert cell. This will be used to set document-level YAML options. The Data 100 lecture notes are generated using the following YAML settings:\nNow, the notebook is ready for writing content. Quarto supports all the functionality of a standard ipynb file – code cells, markdown, and LaTeX. To begin writing lecture notes, it’s a good idea to first set out the main headings of the document. These typically correspond to the title slides of each lecture (example) and are written with the Markdown second headng level (##). Quarto will auto-populate the table of contents as these headings are created.\nTo view the Quarto file, open a terminal window (either within Jupyter or through your machine’s terminal) and navigate to the notebook’s directory. Running the command quarto preview notebook.ipynb will render the document and open it in a new web browser tab.\nWith the preview activated, the rendered view will update every time a change is saved in the notebook. When editing the document, it’s helpful to have side-by-side views of the notebook and preview so you can watch changes in real-time.\n\n\n\nA pdf view of how this notebook renders in Quarto can be found here.\n\n\nThe code-fold: true option in the YAML set-up will automatically collapse all code cells in the rendered document. If a particular code cell should be uncollapsed by default (e.g. to explicitly show a pandas example), a cell-specific YAML option can be specified:\n\nprint(\"this code is now visible\")\n\nthis code is now visible\n\n\n\n\n\nInserting images in a Quarto document is similar to the standard Markdown syntax. The difference is that Quarto will insert figure captions automatically. The syntax below will insert an image with an accompanying description.\nTODO (as of 12/23/22): Convert HTML Markdown images to Quarto-supported images\n\n\nCode\n#![The best class at Berkeley](data.png)\n\n\n\n\n\nThe best class at Berkeley\n\n\n\n\n\nEach lecture note should start with a brief list of intended student learning outcomes. These are formatted as collapsable call-out cells, which can be created in a Markdown cell using the syntax below.\n\n\n\n\n\n\nLearning Outcomes\n\n\n\n\n\n\nGain familiarity with Quarto\nCreate your first Quarto document\nWrite A+ Data 100 lecture notes\n\n\n\n\n\n\n\n\nTo generate the final notebook as an HTML, run the terminal command quarto render notebook.ipynb. The HTML will be outputted in the same directory as the notebook.\n\n\n\nQuick Start Guide\nComprehensive Guide\nMarkdown in Quarto\n\n\n\nFollow the local installation set-up above. Then, clone the course-notes repository: https://github.com/DS-100/course-notes.git.\nTo begin working on a new lecture note, create a folder with the lecture title. Within this folder, you can create new sub-folders titlted images and data, where you’ll store these\nquarto convert lecture_title.ipynb converts the notebook to a .qmd file. The live set of lecture notes are generated from the .qmd files that lie in _quarto.yaml. If you’d like to change the current lecture notes, make sure you re-generate the .qmd files rather than just editing the notebook"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html",
    "href": "constant_model_loss_transformations/loss_transformations.html",
    "title": "Constant Model, Loss, and Transformations",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nDerive the optimal model parameters for the constant model under MSE and MAE cost functions\nEvaluate the differences between MSE and MAE risk\nUnderstand the need for linearization of variables and apply the Tukey-Mosteller bulge diagram for transformations\nLast time, we introduced the modeling process. We set up a framework to predict target variables as functions of our features, following a set workflow:\nTo illustrate this process, we derived the optimal model parameters under simple linear regression with mean squared error as the cost function. In this lecture, we’ll continue familiarizing ourselves with the modeling process by finding the best model parameters under a new model. We’ll also test out two different loss functions to understand how our choice of loss influences model design. Later on, we’ll consider what happens when a linear model isn’t the best choice to capture trends in our data – and what solutions there are to create better models."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Constant Model + MSE",
    "text": "Constant Model + MSE\nIn today’s lecture, our focus will be on the constant model. The constant model is slightly different from the simple linear regression model we’ve explored previously. Rather than generate predictions from an inputted feature variable, the constant model predicts the same constant number every time. We call this constant \\(\\theta\\).\n\\[\\hat{y}_i = \\theta\\]\n\\(\\theta\\) is the parameter of the constant model, just as \\(\\theta_0\\) and \\(\\theta_1\\) were the parameters in SLR. Our task now is to determine what value of \\(\\theta\\) represents the optimal model – in other words, what number should we guess each time to have the lowest possible average loss on our data?\nConsider the case where L2 (squared) loss is used as the loss function and mean squared error is used as the cost function. At this stage, we’re well into the modeling process:\n\nChoose a model: constant model\nChoose a loss function: L2 loss\nFit the model\nEvaluate model performance\n\nIn Homework 5, you will fit the constant model under MSE cost to find that the best choice of \\(\\theta\\) is the mean of the observed \\(y\\) values. In other words, \\(\\hat{\\theta} = \\bar{y}\\).\nLet’s take a moment to interpret this result. Our optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:\n\\[R(\\hat{\\theta}) = \\min_{\\theta} R(\\theta)\\]\nTo restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, \\(\\hat{\\theta}\\), is the value of \\(\\theta\\) that minimizes the cost \\(R\\).\nFor modeling purposes, we care less about the minimum value of cost, \\(R(\\hat{\\theta})\\), and more about the value of \\(\\theta\\) that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n\\[\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)\\]\nThat is, we want to find the argument \\(\\theta\\) that minimizes the cost function."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Constant Model + MAE",
    "text": "Constant Model + MAE\nWe see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?\nThis time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the mean absolute error.\n\nChoose a model: constant model\nChoose a loss function: L1 loss\nFit the model\nEvaluate model performance\n\nTo fit the model and find the optimal parameter value \\(\\hat{\\theta}\\), follow the usual process of differentiating the cost function with respect to \\(\\theta\\), setting the derivative equal to zero, and solving for \\(\\theta\\). Writing this out in longhand:\n\\[\\begin{align}\nR(\\theta) &= \\frac{1}{n}\\sum^{n}_{i=1} |y_i - \\theta| \\\\\n\\frac{d}{d\\theta} R(\\theta) &= \\frac{d}{d\\theta} \\left(\\frac{1}{n} \\sum^{n}_{i=1} |y_i - \\theta| \\right) \\\\\n\\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta|\n\\end{align}\\]\nHere, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when \\(y_i = \\theta\\)). For now, we’ll ignore this issue. It turns out that disregarding this case doesn’t influence our final result.\nTo perform the derivative, consider two cases. When \\(\\theta\\) is less than \\(y_i\\), the term \\(y_i - \\theta\\) will be positive and the absolute value has no impact. When \\(\\theta\\) is greater than \\(y_i\\), the term \\(y_i - \\theta\\) will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying \\(-(y_i - \\theta) = \\theta - y_i\\).\n\\[|y_i - \\theta| = \\begin{cases} y_i - \\theta && \\text{if}\\:\\theta \\lt y_i \\\\ \\theta - y_i && \\text{if}\\:\\theta \\gt y_i \\end{cases}\\]\nTaking derivatives:\n\\[\\frac{d}{d\\theta} |y_i - \\theta| = \\begin{cases} \\frac{d}{d\\theta} (y_i - \\theta) = -1 && \\text{if}\\:\\theta \\lt y_i \\\\ \\frac{d}{d\\theta} (\\theta - y_i) = 1 && \\text{if}\\:\\theta \\gt y_i \\end{cases}\\]\nThis means that we obtain a different value for the derivative for datapoints where \\(\\theta < y_i\\) and where \\(\\theta > y_i\\). We can summarize this by saying:\n\\[\\begin{align} \\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta| \\\\\n&= \\frac{1}{n} \\left[\\sum_{\\theta < y_i} (-1) + \\sum_{\\theta > y_i} (+1) \\right]\n\\end{align}\\]\nTo finish finding the best value of \\(\\theta\\), set this derivative equal to zero and solve for \\(\\theta\\). You’ll do this in Homework 5 to show that \\(\\hat{\\theta} = \\text{median}(y)\\)."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "href": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Comparing Loss Functions",
    "text": "Comparing Loss Functions\nNow, we’ve tried our hand at fitting a model under both MSE and MAE cost functions. How do the two results compare?\nLet’s consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We’ll fit a constant model to predict the number of drinks that will be sold tomorrow.\n\nimport numpy as np\ndrinks = np.array([20, 21, 22, 29, 33])\ndrinks\n\narray([20, 21, 22, 29, 33])\n\n\nFrom our derivations above, we know that the optimal model parameter under MSE cost is the mean of the dataset. Under MAE cost, the optimal parameter is the median of the dataset.\n\nnp.mean(drinks), np.median(drinks)\n\n(25.0, 22.0)\n\n\nIf we plot each empirical risk function across several possible values of \\(\\theta\\), we find that each \\(\\hat{\\theta}\\) does indeed correspond to the lowest value of error:\n\nNotice that the MSE above is a smooth function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE, in contrast, is not differentiable at each of its “kinks.” We’ll explore how the smoothness of the cost function can impact our ability to apply numerical optimization in a few weeks.\nHow do outliers affect each cost function? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected.\n\ndrinks_with_outlier = np.append(drinks, 1000)\ndisplay(drinks_with_outlier)\nnp.mean(drinks_with_outlier), np.median(drinks_with_outlier)\n\narray([  20,   21,   22,   29,   33, 1000])\n\n\n(187.5, 25.5)\n\n\nThis means that under the MSE, the optimal model parameter \\(\\hat{\\theta}\\) is strongly affected by the presence of outliers. Under the MAE, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is sensitive to outliers, while the MAE is robust to outliers.\nLet’s try another experiment. This time, we’ll add an additional, non-outlying datapoint to the data.\n\ndrinks_with_additional_observation = np.append(drinks, 35)\ndrinks_with_additional_observation\n\narray([20, 21, 22, 29, 33, 35])\n\n\nWhen we again visualize the cost functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are infinitely many optimal values for the model parameter: any value \\(\\hat{\\theta} \\in [22, 29]\\) will minimize the MAE. In contrast, the MSE still has a single best value for \\(\\hat{\\theta}\\). In other words, the MSE has a unique solution for \\(\\hat{\\theta}\\); the MAE is not guaranteed to have a single unique solution."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "href": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "title": "Constant Model, Loss, and Transformations",
    "section": "Linear Transformations",
    "text": "Linear Transformations\nAt this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.\nA key word above is linear. When we computed parameter estimates earlier, we assumed that \\(x_i\\) and \\(y_i\\) shared roughly a linear relationship.\nData in the real world isn’t always so straightforward. Consider the dataset below, which contains information about the ages and lengths of dugongs.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndugong = pd.read_csv(\"data/dugongs.txt\", delimiter=\"\\t\").sort_values(\"Length\")\nx, y = dugong[\"Length\"], dugong[\"Age\"]\n\n# `corrcoef` computes the correlation coefficient between two variables\n# `std` finds the standard deviation\nr = np.corrcoef(x, y)[0, 1]\ntheta_1 = r*np.std(y)/np.std(x)\ntheta_0 = np.mean(y) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, y)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(\"Age\")\n\nax[1].scatter(x, y)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(\"Age\");\n\n\n\n\n\nLooking at the plot on the left, we see that there is a slight curvature to the data points. Plotting the SLR curve on the right results in a poor fit.\nFor SLR to perform well, we’d like there to be a rough linear trend relating \"Age\" and \"Length\". What is making the raw data deviate from a linear relationship? Notice that the data points with \"Length\" greater than 2.6 have disproportionately high values of \"Age\" relative to the rest of the data. If we could manipulate these data points to have lower \"Age\" values, we’d “shift” these points downwards and reduce the curvature in the data. Applying a logarithmic transformation to \\(y_i\\) (that is, taking \\(\\log(\\) \"Age\" \\()\\) ) would achieve just that.\nAn important word on \\(\\log\\): in Data 100 (and most upper-division STEM courses), \\(\\log\\) denotes the natural logarithm with base \\(e\\). The base-10 logarithm, where relevant, is indicated by \\(\\log_{10}\\).\n\n\nCode\nz = np.log(y)\n\nr = np.corrcoef(x, z)[0, 1]\ntheta_1 = r*np.std(z)/np.std(x)\ntheta_0 = np.mean(z) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, z)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(r\"$\\log{(Age)}$\")\n\nax[1].scatter(x, z)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(r\"$\\log{(Age)}$\")\n\nplt.subplots_adjust(wspace=0.3);\n\n\n\n\n\nOur SLR fit looks a lot better! We now have a new target variable: the SLR model is now trying to predict the log of \"Age\", rather than the untransformed \"Age\". In other words, we are applying the transformation \\(z_i = \\log{(y_i)}\\). The SLR model becomes:\n\\[\\hat{\\log{(y_i)}} = \\theta_0 + \\theta_1 x_i\\] \\[\\hat{z}_i = \\theta_0 + \\theta_1 x_i\\]\nIt turns out that this linearized relationship can help us understand the underlying relationship between \\(x_i\\) and \\(y_i\\). If we rearrange the relationship above, we find: \\[\\begin{align}\n\\log{(y_i)} &= \\theta_0 + \\theta_1 x_i \\\\\ny_i &= e^{\\theta_0 + \\theta_1 x_i} \\\\\ny_i &= (e^{\\theta_0})e^{\\theta_1 x_i} \\\\\ny_i &= C e^{k x_i}\n\\end{align}\\]\nFor some constants \\(C\\) and \\(k\\).\n\\(y_i\\) is an exponential function of \\(x_i\\). Applying an exponential fit to the untransformed variables corroborates this finding.\n\n\nCode\nplt.figure(dpi=120, figsize=(4, 3))\n\nplt.scatter(x, y)\nplt.plot(x, np.exp(theta_0)*np.exp(theta_1*x), \"tab:red\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Age\");\n\n\n\n\n\nYou may wonder: why did we choose to apply a log transformation specifically? Why not some other function to linearize the data?\nPractically, many other mathematical operations that modify the relative scales of \"Age\" and \"Length\" could have worked here. The Tukey-Mosteller Bulge Diagram is a useful tool for summarizing what transformations can linearize the relationship between two variables. To determine what transformations might be appropriate, trace the shape of the “bulge” made by your data. Find the quadrant of the diagram that matches this bulge. The transformations shown on the vertical and horizontal axes of this quadrant can help improve the fit between the variables."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html",
    "href": "intro_to_modeling/intro_to_modeling.html",
    "title": "Introduction to Modeling",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nUnderstand and carry out the four-step modeling process\nDefine the concept of loss and gain familiarity with L1 and L2 loss\nFit a model using minimization techniques\nUp until this point in the semester, we’ve focused on analyzing datasets. We’ve looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.\nThis lecture marks a shift in focus. We will move away from examining datasets to actually using our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some prediction about the world around us. In this lecture, we’ll introduce the conceptual framework for setting up a modeling task. In the next few lectures, we’ll put this framework into practice by implementing several kinds of models."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "href": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "title": "Introduction to Modeling",
    "section": "The Modeling Process",
    "text": "The Modeling Process\nAt a high level, a model is some way of representing a system. In Data 100, we’ll treat a model as some mathematical rule we use to describe the relationship between variables.\nWhat variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset \\(\\mathbb{D}\\):\n\\[\\mathbb{D} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\]\nEach pair of values \\((x_i, y_i)\\) represents a datapoint. In a modeling setting, we call these observations. \\(y_i\\) is the dependent variable we are trying to model, also called an output or response. \\(x_i\\) is the independent variable inputted into the model to make predictions, also known as a feature.\nOur goal in modeling is to use the observed data \\(\\mathbb{D}\\) to predict the output variable \\(y_i\\). We denote each prediction as \\(\\hat{y}_i\\) (read: “y hat sub i”).\nHow do we generate these predictions? Some examples of models we’ll encounter in the next few lectures are given below:\n\n\\(\\hat{y}_i = \\theta\\)\n\\(\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\)\n\nThe examples above are known as parametric models. They relate the collected data, \\(x_i\\), to the prediction we make, \\(\\hat{y}_i\\). A few parameters (\\(\\theta\\), \\(\\theta_0\\), \\(\\theta_1\\)) are used to describe the relationship between \\(x_i\\) and \\(\\hat{y}_i\\).\nNotice that we don’t immediately know the values of these parameters. While the features, \\(x_i\\), are taken from our observed data, we need to decide what values to give \\(\\theta\\), \\(\\theta_0\\), and \\(\\theta_1\\) ourselves. This is the heart of parametric modeling: what parameter values should we choose so our model makes the best possible predictions?\nTo choose our model parameters, we’ll work through the modeling process.\n\nChoose a model: how should we represent the world?\nChoose a loss function: how do we quantify prediction error?\nFit the model: how do we choose the best parameters of our model given our data?\nEvaluate model performance: how do we evaluate whether this process gave rise to a good model?"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "title": "Introduction to Modeling",
    "section": "Choosing a Model",
    "text": "Choosing a Model\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, \\(x_i\\), and predictions \\(\\hat{y}_i\\).\nIn Data 8, you learned about the Simple Linear Regression (SLR) model. You learned that the model takes the form: \\[\\hat{y}_i = a + bx_i\\]\nIn Data 100, we’ll use slightly different notation: we will replace \\(a\\) with \\(\\theta_0\\) and \\(b\\) with \\(\\theta_1\\). This will allow us to use the same notation when we explore more complex models later on in the course.\n\\[\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\]\nThe parameters of the SLR model are \\(\\theta_0\\), also called the intercept term, and \\(\\theta_1\\), also called the slope term. To create an effective model, we want to choose values for \\(\\theta_0\\) and \\(\\theta_1\\) that most accurately predict the output variable. The “best” fitting model parameters are given the special names \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\) – they are the specific parameter values that allow our model to generate the best possible predictions.\nIn Data 8, you learned that the best SLR model parameters are: \\[\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} \\qquad \\qquad \\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nA quick word on notation:\n\n\\(\\bar{y}\\) and \\(\\bar{x}\\) indicate the mean value of \\(y\\) and \\(x\\), respectively\n\\(\\sigma_y\\) and \\(\\sigma_x\\) indicate the standard deviations of \\(y\\) and \\(x\\)\n\\(r\\) is the correlation coefficient, defined as the average of the product of \\(x\\) and \\(y\\) measured in standard units: \\(\\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y})\\)\n\nIn Data 100, we want to understand how to derive these best model coefficients. To do so, we’ll introduce the concept of a loss function."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "title": "Introduction to Modeling",
    "section": "Choosing a Loss Function",
    "text": "Choosing a Loss Function\nWe’ve talked about the idea of creating the “best” possible predictions. This begs the question: how do we decide how “good” or “bad” our model’s predictions are?\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, \\(L(y, \\hat{y})\\), quantifies how “far off” a single prediction by our model is from a true, observed value in our collected data.\nThe choice of loss function for a particular model depends on the modeling task at hand. Regardless of the specific function used, a loss function should follow two basic principles:\n\nIf the prediction \\(\\hat{y}_i\\) is close to the actual value \\(y_i\\), loss should be low\nIf the prediction \\(\\hat{y}_i\\) is far from the actual value \\(y_i\\), loss should be high\n\nTwo common choices of loss function are squared loss and absolute loss.\nSquared loss, also known as L2 loss, computes loss as the square of the difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\\]\nAbsolute loss, also known as L1 loss, computes loss as the absolute difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = |y_i - \\hat{y}_i|\\]\nL1 and L2 loss give us a tool for quantifying our model’s performance on a single datapoint. This is a good start, but ideally we want to understand how our model performs across our entire dataset. A natural way to do this is to compute the average loss across all datapoints in the dataset. This is known as the cost function, \\(\\hat{R}(\\theta)\\): \\[\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)\\]\nThe cost function has many names in statistics literature. You may also encounter the terms:\n\nEmpirical risk (this is why we give the cost function the name \\(R\\))\nError function\nAverage loss\n\nWe can substitute our L1 and L2 loss into the cost function definition. The Mean Squared Error (MSE) is the average squared loss across a dataset: \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nThe Mean Absolute Error (MAE) is the average absolute loss across a dataset: \\[\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\\]"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "href": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "title": "Introduction to Modeling",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nNow that we’ve established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model’s cost on our dataset. This process is called fitting the model.\nWe know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the objective function (our objective is to find its minimum).\nTo find the optimal model parameter, we:\n\nTake the derivative of the cost function with respect to that parameter\nSet the derivative equal to 0\nSolve for the parameter\n\nWe repeat this process for each parameter present in the model. For now, we’ll disregard the second derivative condition.\nTo help us make sense of this process, let’s put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!\nStep 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, \\(\\hat{y}_i = \\theta_0+\\theta_1 x_i\\), into the definition of MSE above and differentiate with respect to \\(\\theta_0\\) and \\(\\theta_1\\). \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)^2\\]\n\\[\\frac{\\partial}{\\partial \\theta_0} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i\\]\n\\[\\frac{\\partial}{\\partial \\theta_1} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i\\]\nStep 2: set the derivatives equal to 0. After simplifying terms, this produces two estimating equations. The best set of model parameters \\((\\theta_0, \\theta_1)\\) must satisfy these two optimality conditions. \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{y}_i = 0\\] \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_i = 0\\]\nStep 3: solve the estimating equations to compute estimates for \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\).\nTaking the first equation gives the estimate of \\(\\hat{\\theta}_0\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n y_i - \\hat{\\theta}_0 - \\hat{\\theta}_1 x_i &= 0 \\\\\n\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{\\theta}_0 - \\hat{\\theta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) &= 0 \\\\\n\\hat{\\theta}_0 &= \\bar{y} - \\hat{\\theta}_1 \\bar{x}\n\\end{align}\\]\nWith a bit more maneuvering, the second equation gives the estimate of \\(\\hat{\\theta}_1\\). Start by multiplying the first estimating equation by \\(\\bar{x}\\), then subtracting the result from the second estimating equation. \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\bar{x} &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)(x_i - \\bar{x}) &= 0\n\\end{align}\\]\nNext, plug in \\(\\hat{y}_i = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_i = \\bar{y} + \\hat{\\theta}_1(x_i - \\bar{x})\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{\\theta}_1(x - \\bar{x}))(x_i - \\bar{x}) &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) = \\hat{\\theta}_1 \\times \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{align}\\]\nBy using the definition of correlation \\(\\left(r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)\\) and standard deviation \\(\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)\\), we can conclude: \\[r \\sigma_x \\sigma_y = \\hat{\\theta}_1 \\times \\sigma_x^2\\] \\[\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nJust as was given in Data 8!\nRemember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can always follow these three steps to fit the model."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "href": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "title": "Introduction to Modeling",
    "section": "Evaluating Performance",
    "text": "Evaluating Performance\nAt this point, we’ve:\n\nDefined our model\nDefined our loss function\nFit the model to identify the best model parameters\n\nThis leaves us with one final question – how “good” are the predictions made by this “best” fitted model?\nOne way we might want to evaluate our model’s performance is by computing summary statistics. If the mean and standard deviation of our predictions are close to those of the original observed \\(y_i\\)s, we might be inclined to say that our model has done well. A large magnitude for the correlation coefficient between the feature and response variables might also support this conclusion. However, we should be cautious with this approach. To see why, we’ll consider a classic dataset called Anscombe’s quartet.\n\nIt turns out that the four sets of points shown here all have identical means, standard deviations, and correlation coefficients. However, it only makes sense to model the first of these four sets of data using SLR! It is important to visualize your data before starting to model to confirm that your choice of model makes sense for the data.\nAnother way of evaluating model performance is by using performance metrics. A common choice of metric is the Root Mean Squared Error, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of \\(y_i\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset. \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nWe may also wish to visualize the model’s residuals, defined as the difference between the observed and predicted \\(y_i\\) value (\\(e_i = y_i - \\hat{y}_i\\)). This gives a high-level view of how “off” each prediction is from the true observed value. Recall that you explored this concept in Data 8: a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe’s quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points."
  },
  {
    "objectID": "visualization_2/visualization_2.html",
    "href": "visualization_2/visualization_2.html",
    "title": "Visualization II",
    "section": "",
    "text": "In the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we’ll learn how to construct density curves using Kernel Density Estimation.\n\n\nKernel Density Estimation involves a technique called smoothing - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\nMany of the visualizations we learned during the last lecture are examples of smoothing. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the larger properties of our distribution.\n\n\n\nKernel Density Estimation is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\nPlace a kernel at each data point\nNormalize kernels to have total area of 1\nSum kernels together\n\nSuppose we have 5 data points: \\([2.2, 2.8, 3.7, 5.3, 5.7]\\). We wish to recreate the following Kernel Density Estimate:\n\n\nCode\nimport seaborn as sns\n\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n\n\n\n\n\nLet’s walk through each step to construct this density curve.\n\n\nTo begin generating a density curve, we need to choose a kernel and bandwidth value. What are these exactly? A kernel is a density curve itself, and the bandwidth is a measure of the kernel’s width. Recall that a valid density has an area of 1.\nAt each of our 5 points (depicted in the rug plot on the left), we’ve placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We’ll explore what these are in the next section.\n\n\nRugplot of Data\n\n\n\n\nKernelized Data\n\n\n\n\n\n\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by \\(\\frac{1}{5}\\).\n\n\nKernelized Data\n\n\n\n\nNormalized Kernels\n\n\n\n\n\n\nOur kernel density estimate (KDE) is the vertical sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n\nNormalized Kernels\n\n\n\n\nKernel Density Estimate\n\n\n\n\n\n\n\n\n\n\nA kernel (for our purposes) is a valid density function. This means it:\n\nMust be non-negative for all inputs.\nMust integrate to 1.\n\n\n\nThe most common kernel is the Gaussian kernel. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value \\(x_i\\) with a standard deviation of \\(\\alpha\\) (this is known as the bandwidth parameter).\n\\(K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2a^{2}}}\\)$\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Gaussian kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nIf you’ve taken a probability class, you’ll recognize that the mean of this Gaussian kernel is \\(x_i\\) and the standard deviation is \\(\\alpha\\). Increasing \\(\\alpha\\) - equivalently, the bandwidth - smoothens the density curve. Larger values of \\(\\alpha\\) are typically easier to understand; however, we begin to lose important distributional information.\nHere is how adjusting \\(\\alpha\\) affects a distribution in some variable from an arbitrary dataset.\n\n\nGaussian Kernel, Alpha = 0.1\n\n\n\n\nGaussian Kernel, Alpha = 1\n\n\n\n\n\nGaussian Kernel, Alpha = 2\n\n\n\n\nGaussian Kernel, Alpha = 10\n\n\n\n\n\n\nAnother example of a kernel is the Boxcar kernel. The boxcar kernel assigns a uniform density to points within a “window” of the observation, and a density of 0 elsewhere.\n\\(K_a(x, x_i) = \\begin{cases}  \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\  0, & \\text{else }  \\end{cases}\\)\n\n\nCode\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Boxcar kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth \\(\\alpha\\) = 1."
  },
  {
    "objectID": "visualization_2/visualization_2.html#visualization-theory",
    "href": "visualization_2/visualization_2.html#visualization-theory",
    "title": "Visualization II",
    "section": "Visualization Theory",
    "text": "Visualization Theory\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We’ll discuss the abstract nature of visualizations and analyze how they convey information.\nRemember, we had two goals for visualizing data. This section is particularly important in:\n\nHelping us understand the data and results\nCommunicating our results and conclusions with others\n\n\nInformation Channels\nVisualizations are able to convey information through various encodings. In the remainder of this lecture, we’ll look at the use of color, scale, and depth, to name a few.\nTODO Delete: Additionally, some visuals convey more detail than others. We learned how rugplots, KDEs, and histograms are all different representations of a distribution. One such example is in the rugplot.\n\nEncodings in Rugplots\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n\n\n\nMulti-Dimensional Encodings\nEncodings are also useful for representing multi-dimensional data. Notice how the following visual highlights four distinct “dimensions” of data:\n\nX-axis\nY-axis\nArea\nColor\n\n\nThe human visual perception sytem is only capable of visualizing data in a three-dimensional plane, but as you’ve seen, we can encode many more channels of information.\n\n\n\nHarnessing the Axes\n\nConsider Scale of the Data\nHowever, we should be careful to not misrepresent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n\nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not be intersecting.\nWhen there is a large difference in the magnitude of the data, it’s advised to analyze percentages instead of counts. The following diagrams correctly display the trends in cancer screening and abortion rates.\n\n\n\n\n\n\n\n\n\n\n\nReveal the Data\nGreat visualizations not only consider the scale of the data, but also utilize the axes in a way that best conveys information. For example, data scientists commonly set certain axes limits to highlight parts of the visualization they are most interested in.\n\n\n\n\n\n\n\n\n\nThe visualization on the right captures the trend in coronavirus cases during the month March in 2020. From only looking at the visualization on the left, a viewer may incorrectly believe that coronavirus began to skyrocket on March 4th, 2020. However, the second illustration tells a different story - cases rose closer to March 21th, 2020.\n\n\n\nHarnessing Color\nColor is another important feature in visualizations that does more than what meets the eye.\nLast lecture, we used color to encode a categorical variable in our scatter plot. In this section, we will discuss uses of color in novel visualizations like colormaps and heatmaps.\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors which work together is evidently a challenging task!\n\nColormaps\nColormaps are mappings from pixel data to color values, and they’re often used to highlight distinct parts of an image. Let’s investigate a few properties of colormaps.\n\n\nJet Colormap \n\n\n\nViridis Colormap \n\n\nThe jet colormap is infamous for being misleading. While it seems more vibrant than viridis, the aggressive colors poorly encode numerical data. To understand why, let’s analyze the following images.\n\n\n\n\n\n\n\n\n\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low intensity. These include the jet colormap (row a) and grayscale (row b). Notice how the grayscale images do the best job in smoothly transitioning between pixel data. The jet colormap is the worst at this - the four images in row (a) look like a conglomeration of individual colors.\nThe difference is also evident in the images labeled (a) and (b) on the left side. The grayscale image is better at preserving finer detail in the vertical line strokes. Additionally, grayscale is preferred in x-ray scans for being more neutral. The intensity of dark red color in the jet colormap is frightening and indicates something is wrong.\nWhy is the jet colormap so much worse? The answer lies in how its color composition is percieved to the human eye.\n\n\nJet Colormap Perception \n\n\n\nViridis Colormap Perception \n\n\nThe jet colormap is largely misleading because it is not perceptually uniform. Perceptually uniform colormaps have the property that if the pixel data goes from 0.1 to 0.2, the perceptual change is the same as when the data goes from 0.8 to 0.9.\nNotice how the said uniformity is present within the linear trend displayed in the viridis colormap. On the other hand, the jet colormap is largely non-linear - this is precisely why it’s considered a worse colormap.\n\n\n\nHarnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these “markings”.\n\nThere are a few key takeaways from this diagram\n\nLengths are easy to discern. Don’t use plots with jiggled baselines - keep everything axis-aligned.\nAvoid pie charts! Angle judgements are inaccurate.\nAreas and volumes are hard to distinguish (area charts, word clouds, etc)\n\n\n\nHarnessing Conditioning\nConditioning is the process of comparing data that belong to seperate groups. We’ve seen this before in overlayed distributions, side-by-side box-plots, and scatter plots with categorical encodings. Here, we’ll introduce terminology that formalizes these examples.\nConsider an example where we want to analyze income earnings for male and females with varying levels of education. There are multiple ways to compare this data.\n\n\n\n\n\n\n\n\n\nThe barplot is an example of juxtaposition: placing multiple plots side by side, with the same scale. The scatter plot is an example of superposition: placing multiple density curves, scatter plots on top of each other.\nWhich is better depends on the problem at hand. Here, superposition makes the precise wage difference very clear from a quick glance. But many sophisticated plots convey information that favors the use of juxtaposition. Below is one example.\n\n\n\nHarnessing Context\nThe last component to a great visualization is perhaps the most critical - the use of context. Adding informative titles, axis labels, and descriptive captions are all best practices that we’ve heard repeatedly in Data 8.\nA publication-ready plot (and every Data 100 plot) needs:\n\nInformative title (takeaway, not description)\nAxis labels\nReference lines, markers, etc\nLegends, if appropriate\nCaptions that describe data\n\nCaptions should be:\n\nComprehensive and self-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graphs"
  },
  {
    "objectID": "visualization_2/visualization_2.html#transformations",
    "href": "visualization_2/visualization_2.html#transformations",
    "title": "Visualization II",
    "section": "Transformations",
    "text": "Transformations\nThese last two lectures have covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\nMuch of this was done to uncover insights in data, which will prove necessary for the modeling process. A strong graphical correlation between two variables hinted an underlying relationship that has reason for further study. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\nTransformations are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that “transform” their range of possible values and highlight some previously hidden associations between data.\n\nTransforming a Distribution\nWhen a distribution has a large dynamic range, it can be useful to take the logarithm of the data. For example, computing the logarithm of the ticket prices on the Titanic reduces skeweness and yields a distribution that is more “spread” across the x-axis. While it makes individual observations harder to interpret, the distribution is more favorable for subsequent analysis.\n\n\n\n\n\n\n\n\n\n\n\nLinearizing a Relationship\nTransformations are perhaps most useful to linearize a relationship between variables. If we find a transformation to make a scatter plot of two variables linear, we can “backtrack” to find the exact relationship between the variables. Linear relationships are particularly simple to interpret, and we’ll be doing a lot of linear modeling in Data 100 - starting next lecture!\nSay we want to understand the relationship between healthcare and life expectancy. Intuitively there should be a positive correlation, but upon plotting values from a dataset, we find a non-linear relationship that is somewhat hard to understand. However, applying a logarithmic transformation to both variables - healthcare and life expectancy - results in a scatter plot with a linear trend that we can interpret.\n\n\n\n\n\n\n\n\n\nHow can we find the relationship between the original variables? We know that taking a log of both axes gave us a linear relationship, so we can say (roughly) that\n\\[\\log y= a*\\log x + b\\]\nSolving for \\(y\\) implies a power relationship in the original plot.\n\\[y= e^{a*\\log x + b}\\] \\[y= Ce^{a*\\log x}\\] \\[y= Cx^{a}\\]\nHow did we know that taking the logarithm of both sides would result in a linear relationship? The Tukey-Mosteller Bulge Diagram is helpful here. We can use the direction of the buldge in our original data to find the appropriate transformations that will linearize the relationship. These transformations are found on axes that are nearest to the buldge. The buldge in our earlier example lay in Quadrant 2, so the transformations \\(\\log x\\), \\(\\sqrt x\\), \\(y^{2}\\), or \\(y^{3}\\) are possible contenders. It’s important to note that this diagram is not perfect, and some transformations will work better than others. In our case, \\(\\log x\\) and \\(\\log y\\) (found in Quadrant 3) were the best."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html",
    "href": "feature_engineering/feature_engineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nRecognize the value of feature engineering as a tool to improve model performance\nImplement polynominal feature generation and one hot encoding\nUnderstand the interactions between model complexity, model variance, and training error\nAt this point in the course, we’ve equipped ourselves with some powerful techniques to build and optimize models. We’ve explored how to develop models of multiple variables, as well as how to fit these models to maximize their performance.\nAll of this was done with one major caveat: the regression models we’ve worked with so far are all linear. We’ve assumed that our predictions should be some linear combination of the features. While this works well in some cases, the real world isn’t always so straightforward. In today’s lecture, we’ll learn an important method to address this issue – and consider some new problems that can arise when we do so."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#feature-functions",
    "href": "feature_engineering/feature_engineering.html#feature-functions",
    "title": "Feature Engineering",
    "section": "Feature Functions",
    "text": "Feature Functions\nLet’s take a moment to remind ourselves where this linearity comes from. Consider the following dataset on vehicles:\n\n\nCode\nimport seaborn as sns\nvehicles = sns.load_dataset(\"mpg\").rename(columns={\"horsepower\":\"hp\"}).dropna()\nvehicles.head(5)\n\n\n\n\n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      hp\n      weight\n      acceleration\n      model_year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504\n      12.0\n      70\n      usa\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693\n      11.5\n      70\n      usa\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436\n      11.0\n      70\n      usa\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433\n      12.0\n      70\n      usa\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449\n      10.5\n      70\n      usa\n      ford torino\n    \n  \n\n\n\n\nSuppose we wish to develop a model to predict a vehicle’s fuel efficiency (\"mpg\") as a function of its horsepower (\"hp\"). Glancing at the plot below, we see that the relationship between \"mpg\" and \"hp\" is non-linear – an SLR fit doesn’t capture the relationship between the two variables.\n\nRecall our standard multiple linear regression model. In its current form, it is linear in terms of both \\(\\theta_i\\) and \\(x\\):\n\\[\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x\\:+\\:...\\]\nJust by eyeballing the vehicle data plotted above, it seems that a quadratic model might be more appropriate. In other words, a model of the form below would likely do a better job of capturing the non-linear relationship between the two variables:\n\\[\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2\\]\nThis looks fairly similar to our original multiple regression framework! Importantly, it is still linear in \\(\\theta_i\\) – the prediction \\(\\hat{y}\\) is a linear combination of the model parameters. This means that we can use the same linear algebra methods as before to derive the optimal model parameters when fitting the model.\nYou may be wondering: how can this be a linear model if there is now a \\(x^2\\) term? Although the model contains non-linear \\(x\\) terms, it is linear with respect to the model parameters, \\(\\theta_i\\). Because our OLS derivation relied on assuming a linear model of \\(\\theta_i\\), the method is still valid to fit this new model.\nIf we refit the model with \"hp\" squared as its own feature, we see that the model follows the data much more closely.\n\\[\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2\\]\n\nLooks much better! What we’ve done here is called feature engineering: the process of transforming the raw features of a dataset into more informative features for modeling. By squaring the \"hp\" feature, we were able to create a new feature that signficantly improved the quality of our model.\nWe perform feature engineering by defining a feature function. A feature function is some function applied to the original variables in the data to generate one or more new features. More formally, a feature function is said to take a \\(d\\) dimensional input and transform it to a \\(p\\) dimensional input. This results in a new, feature-engineered design matrix that we rename \\(\\Phi\\).\n\\[X \\in \\mathbb{R}^{n \\times d} \\longrightarrow \\Phi \\in \\mathbb{R}^{n \\times p}\\]\nIn the vehicles example above, we applied a feature function to transform the original input with \\(d=1\\) features into an engineered design matrix with \\(p=2\\) features."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#one-hot-encoding",
    "href": "feature_engineering/feature_engineering.html#one-hot-encoding",
    "title": "Feature Engineering",
    "section": "One Hot Encoding",
    "text": "One Hot Encoding\nFeature engineering opens up a whole new set of possibilities for designing better performing models. As you will see in lab and homework, feature engineering is one of the most important parts of the entire modeling process.\nA particularly powerful use of feature engineering is to allow us to perform regression on non-numeric features. One hot encoding is a feature engineering technique that generates numeric features from categorical data, allowing us to use our usual methods to fit a regression model on the data.\nTo illustrate how this works, we’ll refer back to the tips data from last lecture. Consider the \"day\" column of the dataset:\n\n\nCode\nimport numpy as np\nnp.random.seed(1337)\ntips = sns.load_dataset(\"tips\").sample(100)\ntips[[\"day\"]].head(5)\n\n\n\n\n\n\n  \n    \n      \n      day\n    \n  \n  \n    \n      54\n      Sun\n    \n    \n      46\n      Sun\n    \n    \n      86\n      Thur\n    \n    \n      199\n      Thur\n    \n    \n      106\n      Sat\n    \n  \n\n\n\n\nAt first glance, it doesn’t seem possible to fit a regression model to this data – we can’t directly perform any mathematical operations on the entry “Thur”.\nTo resolve this, we instead create a new table with a feature for each unique value in the original \"day\" column. We then iterate through the \"day\" column. For each entry in \"day\" we fill the corresponding feature in the new table with 1. All other features are set to 0.\n\nThis can be implemented in code using pd.get_dummies to generate the one hot encoding, then calling pd.concat to combine these new features with the original DataFrame.\n\nimport pandas as pd\n\n# Perform the one hot encoding\nohe = pd.get_dummies(tips[\"day\"])\n\n# Combine with original features\ntips = pd.concat([tips, ohe], axis=1)\ntips.head(5)\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n      Thur\n      Fri\n      Sat\n      Sun\n    \n  \n  \n    \n      54\n      25.56\n      4.34\n      Male\n      No\n      Sun\n      Dinner\n      4\n      0\n      0\n      0\n      1\n    \n    \n      46\n      22.23\n      5.00\n      Male\n      No\n      Sun\n      Dinner\n      2\n      0\n      0\n      0\n      1\n    \n    \n      86\n      13.03\n      2.00\n      Male\n      No\n      Thur\n      Lunch\n      2\n      1\n      0\n      0\n      0\n    \n    \n      199\n      13.51\n      2.00\n      Male\n      Yes\n      Thur\n      Lunch\n      2\n      1\n      0\n      0\n      0\n    \n    \n      106\n      20.49\n      4.06\n      Male\n      Yes\n      Sat\n      Dinner\n      2\n      0\n      0\n      1\n      0\n    \n  \n\n\n\n\nNow, the data is ready to fit a model."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#variance-and-training-error",
    "href": "feature_engineering/feature_engineering.html#variance-and-training-error",
    "title": "Feature Engineering",
    "section": "Variance and Training Error",
    "text": "Variance and Training Error\nWe’ve seen now that feature engineering allows us to build all sorts of features to improve the performance of the model. In particular, we saw that designing a more complex feature (squaring \"hp\" in the vehicles data previously) substantially improved the model’s ability to capture non-linear relationships. To take full advantage of this, we might be inclined to design increasingly complex features. Consider the following three models, each of different order (the maximum exponent power of each model):\n\nModel with order 1: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp})\\)\nModel with order 2: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2\\)\nModel with order 4: \\(\\hat{\\text{mpg}} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp})^2 + \\theta_3 (\\text{hp})^3 + \\theta_4 (\\text{hp})^4\\)\n\n\n\nWhen we use our model to make predictions on the same data that was used to fit the model, we find that the MSE decreases with increasingly complex models. The training error is the model’s error when generating predictions from the same data that was used for training purposes. We can conclude that the training error goes down as the complexity of the model increases.\n\nThis seems like good news – when working on the training data, we can improve model performance by designing increasingly complex models.\nHowever, high model complexity comes with its own set of issues. When a model has many complicated features, it becomes increasingly sensitive to the data used to fit it. Even a small variation in the data points used to train the model may result in wildly different results for the fitted model. The plots below illustrate this idea. In each case, we’ve fit a model to two very similar sets of data (in fact, they only differ by two data points!). Notice that the model with order 2 appears roughly the same across the two sets of data; in contrast, the model with order 4 changes erratically across the two datasets.\n\nThe sensitivity of the model to the data used to train it is called the model variance. As we saw above, model variance tends to increase with model complexity."
  },
  {
    "objectID": "feature_engineering/feature_engineering.html#overfitting",
    "href": "feature_engineering/feature_engineering.html#overfitting",
    "title": "Feature Engineering",
    "section": "Overfitting",
    "text": "Overfitting\nWe can see that there is a clear “trade-off” that comes from the complexity of our model. As model complexity increases, the model’s error on the training data decreases. At the same time, the model’s variance tends to increase.\nWhy does this matter? To answer this question, let’s take a moment to review our modeling workflow when making predictions on new data.\n\nSample a dataset of training data from the real world\nUse this training data to fit a model\nApply this fitted model to generate predictions on unseen data\n\nThis first step – sampling training data – is important to remember in our analysis. As we saw above, a highly complex model may produce results that vary wildly across different samples of training data. If we happen to sample a set of training data that is a poor representation of the population we are trying to model, our model may perform poorly on any new set of data it has not seen before.\nTo see why, consider a model fit using the training data shown on the left. Because the model is so complex, it achieves zero error on the training set – it perfectly predicts each value in the training data! When we go to use this model to make predictions on a new sample of data, however, things aren’t so good. The model now has enormous error on the unseen data.\n\nThe phenomenon above is called overfitting. The model effectively just memorized the training data it encountered when it was fitted, leaving it unable to handle new situations.\nThe takeaway here: we need to strike a balance in the complexity of our models. A model that is too simple won’t be able to capture the key relationships between our variables of interest; a model that is too complex runs the risk of overfitting.\nThis begs the question: how do we control the complexity of a model? Stay tuned for the next lecture."
  },
  {
    "objectID": "gradient_descent/gradient_descent.html",
    "href": "gradient_descent/gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\n\n\nUnderstand the standard workflow for fitting models in sklearn\nDescribe the conceptual basis for gradient descent\nCompute the gradient descent update on a provided dataset\nAt this point, we’ve grown quite familiar with the modeling process. We’ve introduced the concept of loss, used it to fit several types of models, and, most recently, extended our analysis to multiple regression. Along the way, we’ve forged our way through the mathematics of deriving the optimal model parameters in all of its gory detail. It’s time to make our lives a little easier – let’s implement the modeling process in code!\nIn this lecture, we’ll explore three techniques for model fitting:"
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#implementing-derived-formulas-in-code",
    "href": "gradient_descent/gradient_descent.html#implementing-derived-formulas-in-code",
    "title": "Gradient Descent",
    "section": "Implementing Derived Formulas in Code",
    "text": "Implementing Derived Formulas in Code\nThroughout this lecture, we’ll refer to the tips dataset.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntips = sns.load_dataset(\"tips\")\ntips.head(5)\n\n\n\n\n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n\n\nOur goal will be to predict the value of the \"tip\" for a particular meal given its \"total_bill\" and \"size\". We’ll also add a bias column of all ones to represent the intercept term of our models.\n\n# Add a bias column of all ones to `tips`\ntips[\"bias\"] = np.ones(len(tips), dtype=int) \n\n# Define the design matrix, X...\nX = tips[[\"bias\", \"total_bill\", \"size\"]]\n\n# ...as well as the target variable, y\nY = tips[[\"tip\"]]\n\nIn the previous lecture, we expressed multiple linear regression using matrix notation.\n\\[\\hat{\\mathbb{Y}} = \\mathbb{X}\\theta\\]\nWe used a geometric approach to derive the following expression for the optimal model parameters under MSE error:\n\\[\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1}\\mathbb{X}^T \\mathbb{Y}\\]\nThat’s a whole lot of matrix manipulation. How do we implement it in Python?\nThere are three operations we need to perform here: multiplying matrices, taking transposes, and finding inverses.\n\nTo perform matrix multiplication, use the @ operator\nTo take a transpose, call the .T attribute of an array or DataFrame\nTo compute an inverse, use numpy’s in-built method np.linalg.inv\n\nBecause the methods above were designed for use with numpy arrays, we’ll first convert X and Y to their numpy representations. Then, we can compute the optimal model parameters.\n\nX, Y = X.to_numpy(), Y.to_numpy()\n\ntheta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\ntheta_hat\n\narray([[0.66894474],\n       [0.09271334],\n       [0.19259779]])\n\n\nTo make predictions using our newly-fitted model coefficients, matrix-multiply X and theta_hat.\n\ny_hat = X @ theta_hat\n\n# Show just the first 5 predictions to save space on the page\ny_hat[:5, :]\n\narray([[2.62933992],\n       [2.20539403],\n       [3.19464533],\n       [3.24959215],\n       [3.71915687]])"
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#sklearn",
    "href": "gradient_descent/gradient_descent.html#sklearn",
    "title": "Gradient Descent",
    "section": "sklearn",
    "text": "sklearn\nWe’ve already saved a lot of time (and avoided tedious calculations) by translating our derived formulas into code. However, we still had to go through the process of writing out the linear algebra ourselves.\nTo make life even easier, we can turn to the sklearn Python library. sklearn is a robust library of machine learning tools used extensively in research and industry. It gives us a wide variety of in-built modeling frameworks and methods, so we’ll keep returning to sklearn techniques as we progress through Data 100.\nRegardless of the specific type of model being implemented, sklearn follows a standard set of steps for creating a model.\n\nCreate a model object. This generates a new instance of the model class. You can think of it as making a new copy of a standard “template” for a model. In pseudocode, this looks like: my_model = ModelName()\nFit the model to the X design matrix and Y target vector. This calculates the optimal model parameters “behind the scenes” without us explicitly working through the calculations ourselves. The fitted parameters are then stored within the model for use in future predictions: my_model.fit(X, Y)\nUse the fitted model to make predictions on the X input data. The model will output an array of new Y predictions: my_model.predict(X)\n\nLet’s put this into action with our multiple regression task. First, initialize an instance of the LinearRegression class.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n\nNext, fit the model instance to the design matrix X and target vector Y by calling .fit.\n\nmodel.fit(X, Y)\n\nLinearRegression()\n\n\nAnd, lastly, generate predictions for \\(\\hat{Y}\\) using the .predict method.\n\n# Like before, show just the first 5 predictions\nmodel.predict(X)[:5, :]\n\narray([[2.62933992],\n       [2.20539403],\n       [3.19464533],\n       [3.24959215],\n       [3.71915687]])\n\n\nWe’ve generated the exact same predictions as before, but without any need for manipulating matrices ourselves!"
  },
  {
    "objectID": "gradient_descent/gradient_descent.html#gradient-descent",
    "href": "gradient_descent/gradient_descent.html#gradient-descent",
    "title": "Gradient Descent",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nAt this point, we’re fairly comfortable with fitting a regression model under MSE risk (indeed, we’ve done it three times now!). It’s important to remember, however, that the results we’ve found previously apply to one very specific case: the equations we used above are only relevant to a linear regression model using MSE as the cost function. In reality, we’ll be working with a wide range of model types and objective functions, not all of which are as straightforward as the scenario we’ve discussed previously. This means that we need some more generalizable way of fitting a model to minimize loss.\nTo do this, we’ll introduce the technique of gradient descent.\n\nMinimizing a 1D Function\nLet’s shift our focus away from MSE to consider some new, arbitrary cost function. You can think of this function as outputting the empirical risk associated with some parameter theta.\n\nLooking at the function across this domain, it is clear that the function’s minimum value occurs around \\(\\theta = 5.3\\). Let’s pretend for a moment that we couldn’t see the full view of the cost function. How would we guess the value of \\(\\theta\\) that minimizes the function?\nIt turns out that the first derivative of the function can give us a clue. In the plot below, the dotted line indicates the value of the derivative of each value of \\(\\theta\\). The derivative is negative where it is red and positive where it is green.\n\nSay we make a guess for the minimizing value of \\(\\theta\\). If this guess “undershoots” the true minimizing value – our guess for \\(\\theta\\) is lower than the \\(\\hat{\\theta}\\) that truly minimizes the function – the derivative will be negative in value. If this guess “overshoots” the true minimizing value, the derivative will be positive in value.\n\nWe can use this pattern to help formulate our next guess for the optimal \\(\\hat{\\theta}\\). Consider the case where we’ve undershot \\(\\theta\\) by guessing too low of a value. We’ll want our next guess to be greater in value than the previous guess – that is, we want to shift our guess to the right. You can think of this as following the slope “downhill” to the function’s minimum value.\n\nIf we’ve overshot \\(\\hat{\\theta}\\) by guessing too high of a value, we’ll want our next guess to be lower in value – we want to shift our guess for \\(\\hat{\\theta}\\) to the left.\n\n\n\nFormalizing Gradient Descent\nThese observations lead us to the gradient descent update rule: \\[\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{dL}{d\\theta}\\]\nBegin with our guess for \\(\\hat{\\theta}\\) at timestep \\(t\\). To find our guess for \\(\\hat{\\theta}\\) at the next timestep, \\(t+1\\), subtract a multiple of the objective function’s derivative, \\(\\frac{dL}{d\\theta}\\). We’ve replaced the generic function \\(f\\) with \\(L\\) to indicate that we are minimizing loss.\n\nIf our guess \\(\\theta^{(t)}\\) was too low (undershooting \\(\\hat{\\theta}\\)), \\(\\frac{dL}{d\\theta}\\) will be negative. Subtracting a negative number from \\(\\theta^{(t)}\\) will increase the value of the next guess, \\(\\theta^{(t+1)}\\). The guess will shift to the right.\nIf our guess \\(\\theta^{(t)}\\) was too high (overshooting \\(\\hat{\\theta}\\)), \\(\\frac{dL}{d\\theta}\\) will be positive. Subtracting a positive number from \\(\\theta^{(t)}\\) will decrease the value of the next guess, \\(\\theta^{(t+1)}\\). The guess will shift to the left.\n\nPut together, this captures the same behavior we reasoned through above. We repeatedly update our guess for the optimal \\(\\theta\\) until we’ve completed a set number of updates, or until each additional update iteration does not change the value of \\(\\theta\\). In this second case, we say that gradient descent has converged on a solution.\nThe \\(\\alpha\\) term in the update rule is known as the learning rate. It represents the size of each gradient descent update step – in other words, how “far” should we step to the left or right with each updated guess? A high value of \\(\\alpha\\) will lead to large differences in value between consecutive guesses for \\(\\hat{\\theta}\\); a low value of \\(\\alpha\\) will result in smaller differences in value between consecutive guesses.\n\n\nA Word on Covexity\nIn our analysis above, we focused our attention on the global minimum of the loss function. You may be wondering: what about the local minimum just to the left?\nIf we had chosen a different starting guess for \\(\\theta\\), or a different value for the learning rate \\(\\alpha\\), we may have converged on the local minimum, rather than on the true optimum value of loss.\n\nIf the loss function is convex, gradient descent is guaranteed to find the global minimum of the objective function. Formally, a function \\(f\\) is convex if: \\[tf(a) + (1-t)f(b) \\geq f(ta + (1-t)b)\\]\nTo put this into words: if you drew a line between any two points on the curve, all values on the curve must be on or below the line. Importantly, any local minimum of a convex function is also its global minimum.\n\nIn summary, non-convex loss functions can cause problems with optimization. This means that our choice of loss function is an key factor in our modeling process. It turns out that MSE is convex, which is a major reason why it is such a popular choice of loss function.\n\n\nMultidimensional Gradient Descent\nWe’re in good shape now: we’ve developed a technique to find the minimum value of a more complex objective function.\nThe function we worked with above was one-dimensional – we were only minimizing the function with respect to a single parameter, \\(\\theta\\). However, as we’ve seen before, we often need to optimize a cost function with respect to several parameters (for example, when selecting the best model parameters for multiple linear regression). We’ll need to extend our gradient descent rule to multidimensional objective functions.\nTo put this in more concrete terms, let’s return to the familiar case of simple linear regression with MSE loss. \\[\\text{MSE}(\\theta_0,\\:\\theta_1) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x)^2\\]\nNow, loss is expressed in terms of two parameters, \\(\\theta_0\\) and \\(\\theta_1\\). Rather than a one-dimensional loss function as we had above, we are now dealing with a two-dimensional loss surface.\n\n\nCode\n# This code is for illustration purposes only\n# It contains a lot of syntax you have not seen\nimport plotly.graph_objects as go\n\nuvalues = np.linspace(0, 2, 10)\nvvalues = np.linspace(0, 0.2, 10)\n(u,v) = np.meshgrid(uvalues, vvalues)\nthetas = np.vstack((u.flatten(),v.flatten()))\n\nX = tips[[\"bias\", \"total_bill\"]].to_numpy()\nY = tips[\"tip\"].to_numpy()\n\ndef mse_loss_single_arg(theta):\n    return mse_loss(theta, X, Y)\n\ndef mse_loss(theta, X, y_obs):\n    y_hat = X @ theta\n    return np.mean((y_hat - Y) ** 2)    \n\nMSE = np.array([mse_loss_single_arg(t) for t in thetas.T])\n\nloss_surface = go.Surface(x=u, y=v, z=np.reshape(MSE, u.shape))\n\nind = np.argmin(MSE)\noptimal_point = go.Scatter3d(name = \"Optimal Point\",\n    x = [thetas.T[ind,0]], y = [thetas.T[ind,1]], \n    z = [MSE[ind]],\n    marker=dict(size=10, color=\"red\"))\n\nfig = go.Figure(data=[loss_surface, optimal_point])\nfig.update_layout(scene = dict(\n    xaxis_title = \"theta0\",\n    yaxis_title = \"theta1\",\n    zaxis_title = \"MSE\"))\nfig.show()\n\n\n\n                                                \n\n\nThough our objective function looks a little different, we can use the same principles as we did earlier to locate the optimal model parameters. Notice how the minimum value of MSE, marked by the red dot in the plot above, occurs in the “valley” of the loss surface. Like before, we want our guesses for the best pair of \\((\\theta_0,\\:\\theta_1)\\) to move “downhill” towards this minimum point.\nThe difference now is that we need to update guesses for both \\(\\theta_0\\) and \\(\\theta_1\\): \\[\\theta_0^{(t+1)} = \\theta_0^{(t)} - \\alpha \\frac{dL}{d\\theta_0} \\qquad \\qquad \\theta_1^{(t+1)} = \\theta_1^{(t)} - \\alpha \\frac{dL}{d\\theta_1}\\]\nWe can tidy this statement up by using vector notation: \\[\\begin{bmatrix}\n           \\theta_{0}^{(t+1)} \\\\\n           \\theta_{1}^{(t+1)} \\\\\n         \\end{bmatrix}\n=\n\\begin{bmatrix}\n           \\theta_{0}^{(t)} \\\\\n           \\theta_{1}^{(t)} \\\\\n         \\end{bmatrix}\n- \\alpha\n\\begin{bmatrix}\n           \\frac{dL}{d\\theta_{0}} \\\\\n           \\frac{dL}{d\\theta_{1}} \\\\\n         \\end{bmatrix}\n\\]\nTo save ourselves from writing out long column vectors, we’ll introduce some new notation. \\(\\vec{\\theta}^{(t)}\\) is a column vector of guesses for each model parameter \\(\\theta_i\\) at timestep \\(t\\). We call \\(\\nabla_{\\vec{\\theta}} L\\) the gradient vector. In plain English, it means “take the derivative of loss, \\(L\\), with respect to each model parameter in \\(\\vec{\\theta}\\).”\n\\[\\vec{\\theta}^{(t+1)}\n= \\vec{\\theta}^{(t)} - \\alpha \\nabla_{\\vec{\\theta}} L\n\\]\n\n\nBatch, Mini-Batch, and Stochastic Descent\nFormally, the algorithm we derived above is called batch gradient descent. For each iteration of the algorithm, the derivative of loss is computed across the entire batch of available data. While this update rule works well in theory, it is not practical in all circumstances. For large datasets (with perhaps billions of data points), finding the gradient across all the data is incredibly computationally taxing.\nMini-batch gradient descent tries to address this issue. In mini-batch descent, only a subset of the data is used to compute an estimate of the gradient. For example, we might consider only 10% of the total data at each gradient descent update step. At the next iteration, a different 10% of the data is sampled to perform the following update. Once the entire dataset has been used, the process is repeated. Each complete “pass” through the data is known as a training epoch.\nIn the extreme case, we might choose a batch size of only one data point – that is, a single data point is used to estimate the gradient of loss with each update step. This is known as stochastic gradient descent.\nBatch gradient descent is a deterministic technique – because the entire dataset is used at each update iteration, the algorithm will always advance towards the minimum of the loss surface. In contrast, both mini-batch and stochastic gradient descent involve an element of randomness. Since only a subset of the full data is used to update the guess for \\(\\vec{\\theta}\\) at each iteration, there’s a chance the algorithm will not progress towards the true minimum of loss with each update. Over the longer term, these stochastic techniques should still converge towards the optimal solution.\nThe diagrams below represent a “bird’s eye view” of a loss surface from above. Notice that batch gradient descent takes a direct path towards the optimal \\(\\hat{\\theta}\\). Stochastic gradient descent, in contrast, “hops around” on its path to the minimum point on the loss surface. This reflects the randomness of the sampling process at each update step."
  }
]