[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "visualization_2/visualization_2.html#kernel-density-functions",
    "href": "visualization_2/visualization_2.html#kernel-density-functions",
    "title": "8  Visualization II",
    "section": "8.1 Kernel Density Functions",
    "text": "8.1 Kernel Density Functions\nIn the last lecture, we learned that density curves are smooth, continuous functions that represent a distribution of values. In this section, we’ll learn how to construct density curves using Kernel Density Estimation (KDE).\n\n8.1.1 KDE Mechanics\n\n8.1.1.1 Smoothing\nKernel Density Estimation involves a technique called smoothing - a process applied to a distribution of values that allows us to analyze the more general structure of the dataset.\nMany of the visualizations we learned during the last lecture are examples of smoothing. Histograms are smoothed versions of one-dimensional rug plots, and hex plots are smoother alternatives to two-dimensional scatter plots. They remove the detail from individual observations so we can visualize the larger properties of our distribution.\n\n\nCode\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\nsns.rugplot(titanic['age'],height = 0.5);\n\n\n\n\n\n\nsns.histplot(titanic['age']);\n\n\n\n\n\n\n8.1.1.2 Kernel Density Estimation\nKernel Density Estimation is a smoothing technique that allows us to estimate a density curve (also known as a probability density function) from a set of observations. There are a few steps in this process:\n\nPlace a kernel at each data point\nNormalize kernels to have total area of 1 (across all kernels)\nSum kernels together\n\nSuppose we have 5 data points: \\([2.2, 2.8, 3.7, 5.3, 5.7]\\). We wish to recreate the following Kernel Density Estimate:\n\n\nCode\ndata = [2.2, 2.8, 3.7, 5.3, 5.7]\nsns.kdeplot(data);\n\n\n\n\n\nLet’s walk through each step to construct this density curve.\n\n8.1.1.2.1 Step 1 - Place a Kernel at Each Data Point\nTo begin generating a density curve, we need to choose a kernel and bandwidth value (\\(\\alpha\\)). What are these exactly? A kernel is a density curve itself, and the bandwidth (\\(\\alpha\\)) is a measure of the kernel’s width. Recall that a valid density has an area of 1.\nAt each of our 5 points (depicted in the rug plot on the left), we’ve placed a Gaussian kernel with a bandwidth parameter of alpha = 1. We’ll explore what these are in the next section.\n\n\nRugplot of Data\n\n\n\n\nKernelized Data\n\n\n\n\n\n8.1.1.2.2 Step 2 - Normalize Kernels to Have Total Area of 1\nNotice how these 5 kernels are density curves - meaning they each have an area of 1. In Step 3, we will be summing each these kernels, and we want the result to be a valid density that has an area of 1. Therefore, it makes sense to normalize our current set of kernels by multiplying each by \\(\\frac{1}{5}\\).\n\n\nKernelized Data\n\n\n\n\nNormalized Kernels\n\n\n\n\n\n8.1.1.2.3 Step 3 - Sum Kernels Together\nOur kernel density estimate (KDE) is the sum of the normalized kernels along the x-axis. It is depicted below on the right.\n\n\nNormalized Kernels\n\n\n\n\nKernel Density Estimate\n\n\n\n\n\n\n\n8.1.2 Kernel Functions and Bandwidth\n\n8.1.2.1 Kernels\nA kernel (for our purposes) is a valid density function. This means it:\n\nMust be non-negative for all inputs.\nMust integrate to 1.\n\n\nA general “KDE formula” function is given above.\n\n\\(K_{\\alpha}(x, xi)\\) is the kernel centered on the observation i.\n\nEach kernel individually has area 1.\nx represents any number on the number line. It is the input to our function.\n\n\\(n\\) is the number of observed data points that we have.\n\nWe multiply by \\(\\frac{1}{n}\\) so that the total area of the KDE is still 1.\n\nEach \\(x_i \\in \\{x_1, x_2, \\dots, x_n\\}\\) represents an observed data point.\n\nThese are what we use to create our KDE by summing multiple shifted kernels centered at these points.\n\n\n*\\(\\alpha\\) (alpha) is the bandwidth or smoothing parameter.\n\n8.1.2.1.1 Gaussian Kernel\nThe most common kernel is the Gaussian kernel. The Gaussian kernel is equivalent to the Gaussian probability density function (the Normal distribution), centered at the observed value \\(x_i\\) with a standard deviation of \\(\\alpha\\) (this is known as the bandwidth parameter).\n\\(K_a(x, x_i) = \\frac{1}{\\sqrt{2\\pi\\alpha^{2}}}e^{-\\frac{(x-x_i)^{2}}{2\\alpha^{2}}}\\)\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt \n\ndef gaussian_kernel(alpha, x, z):\n    return 1.0/np.sqrt(2. * np.pi * alpha**2) * np.exp(-(x - z) ** 2 / (2.0 * alpha**2))\n\nxs = np.linspace(-5, 5, 200)\nalpha = 1\nkde_curve = [gaussian_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Gaussian kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nIf you’ve taken a probability class, you’ll recognize that the mean of this Gaussian kernel is \\(x_i\\) and the standard deviation is \\(\\alpha\\). Increasing \\(\\alpha\\) - equivalently, the bandwidth - smoothens the density curve. Larger values of \\(\\alpha\\) are typically easier to understand; however, we begin to lose important distributional information.\nHere is how adjusting \\(\\alpha\\) affects a distribution in some variable from an arbitrary dataset.\n\n\nGaussian Kernel, \\(\\alpha\\) = 0.1\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 1\n\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 2\n\n\n\n\nGaussian Kernel, \\(\\alpha\\) = 10\n\n\n\n\n\n8.1.2.1.2 Boxcar Kernel\nAnother example of a kernel is the Boxcar kernel. The boxcar kernel assigns a uniform density to points within a “window” of the observation, and a density of 0 elsewhere. The equation below is a Boxcar kernel with the center at \\(x_i\\) and the bandwidth of \\(\\alpha\\).\n\\(K_a(x, x_i) = \\begin{cases}  \\frac{1}{\\alpha}, & |x - x_i| \\le \\frac{\\alpha}{2}\\\\  0, & \\text{else }  \\end{cases}\\)\n\n\nCode\ndef boxcar_kernel(alpha, x, z):\n    return (((x-z)>=-alpha/2)&((x-z)<=alpha/2))/alpha\n\nxs = np.linspace(-5, 5, 200)\nalpha=1\nkde_curve = [boxcar_kernel(alpha, x, 0) for x in xs]\nplt.plot(xs, kde_curve);\n\n\n\n\n\nThe Boxcar kernel centered at 0 with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\nThe diagram on the right is how the density curve for our 5 point dataset would have looked had we used the Boxcar kernel with bandwidth \\(\\alpha\\) = 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Relationships Between Quantitative Variables\nUp until now, we’ve discussed how to visualize single-variable distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n\n8.1.3.1 Scatter Plots\nScatter plots are one of the most useful tools in representing the relationship between two quantitative variables. They are particularly important in gauging the strength, or correlation between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\nFor example, let’s plot a scatter plot comparing the Maternal Pregnancy Weight and Birth Weight colums, using both matplotlib and seaborn.\n\nimport pandas as pd\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n# Matplotlib Example\nplt.scatter(births['Maternal Pregnancy Weight'], births['Birth Weight'])\nplt.xlabel('Maternal Pregnancy Weight')\nplt.ylabel('Birth Weight');\n\n\n\n\n\n# Seaborn Example\nsns.scatterplot(data = births, x = 'Maternal Pregnancy Weight', y = 'Birth Weight',\n                hue = 'Maternal Smoker')\n\n<AxesSubplot: xlabel='Maternal Pregnancy Weight', ylabel='Birth Weight'>\n\n\n\n\n\nThis is an example where color is used to add a third dimension to our plot. This is possible with the hue paramater in seaborn, which adds a categorical column encoding to an existing visualization. This way, we can look for relationships in Maternal Pregnancy Weight and Birth Weight in both maternal smokers and non-smokers. If we wish to see the relationship’s strength more clearly, we can use sns.lmplot.\n\nsns.lmplot(data = births, x = 'Maternal Pregnancy Weight', y = 'Birth Weight', \n           hue = 'Maternal Smoker', ci = False);\n\n\n\n\nWe can make out a weak, positive relationship in pregnancy weight and birth weight for both maternal smokers and non-smokers (slightly more positive in maternal smokers).\n\n\n8.1.3.2 Hex Plots and Contour Plots\nUnfortunately, our scatter plots above suffered from overplotting, which made them hard to interpret. And with a large number of points, jittering is unlikely to resolve the issue. Instead, we can look to hex plots and contour plots.\nHex Plots can be thought of as a two dimensional histogram that shows the joint distribution between two variables. This is particularly useful working with very dense data.\n\nsns.jointplot(data = births, x = 'Maternal Pregnancy Weight', \n              y = 'Birth Weight', kind = 'hex')\n\n<seaborn.axisgrid.JointGrid at 0x7f8cb4401550>\n\n\n\n\n\nThe axes are evidently binned into hexagons, which makes the linear relationship easier to decipher. Darker regions generally indicate a higher density of points.\nOn the other hand, contour plots are two dimensional versions of density curves with marginal distributions of each variable on the axes. We’ve used very similar code here to generate our contour plots, with the addition of the kind = 'kde' and fill = True arguments.\n\nsns.jointplot(data = births, x = 'Maternal Height', y = 'Birth Weight',\\\n              kind = 'kde', fill = True)\n\n<seaborn.axisgrid.JointGrid at 0x7f8ca0461d60>"
  },
  {
    "objectID": "visualization_2/visualization_2.html#transformations",
    "href": "visualization_2/visualization_2.html#transformations",
    "title": "8  Visualization II",
    "section": "8.2 Transformations",
    "text": "8.2 Transformations\nThese last two lectures have covered visualizations in great depth. We looked at various forms of visualizations, plotting libraries, and high-level theory.\nMuch of this was done to uncover insights in data, which will prove necessary for the modeling process. A strong graphical correlation between two variables hinted an underlying relationship that has reason for further study. However, relying on visual relationships alone is limiting - not all plots show association. The presence of outliers and other statistical anomalies make it hard to interpret data.\nTransformations are the process of manipulating data to find significant relationships between variables. These are often found by applying mathematical functions to variables that “transform” their range of possible values and highlight some previously hidden associations between data.\n\n8.2.0.1 Transforming a Distribution\nWhen a distribution has a large dynamic range, it can be useful to take the logarithm of the data. For example, computing the logarithm of the ticket prices on the Titanic reduces skeweness and yields a distribution that is more “spread” across the x-axis. While it makes individual observations harder to interpret, the distribution is more favorable for subsequent analysis.\n\n\n\n\n\n\n\n\n\n\n\n8.2.0.2 Linearizing a Relationship\nTransformations are perhaps most useful to linearize a relationship between variables. If we find a transformation to make a scatter plot of two variables linear, we can “backtrack” to find the exact relationship between the variables. Linear relationships are particularly simple to interpret, and we’ll be doing a lot of linear modeling in Data 100 - starting next week!\nSay we want to understand the relationship between healthcare and life expectancy. Intuitively there should be a positive correlation, but upon plotting values from a dataset, we find a non-linear relationship that is somewhat hard to understand. However, applying a logarithmic transformation to both variables - healthcare and life expectancy - results in a scatter plot with a linear trend that we can interpret.\n\n\n\n\n\n\n\n\n\nHow can we find the relationship between the original variables? We know that taking a log of both axes gave us a linear relationship, so we can say (roughly) that\n\\[\\log y= a\\times\\log x + b\\]\nSolving for \\(y\\) implies a power relationship in the original plot.\n\\[y= e^{a\\times\\log x + b}\\] \\[y= Ce^{a\\times\\log x}\\] \\[y= Cx^{a}\\]\nHow did we know that taking the logarithm of both sides would result in a linear relationship? The Tukey-Mosteller Bulge Diagram is helpful here. We can use the direction of the buldge in our original data to find the appropriate transformations that will linearize the relationship. These transformations are found on axes that are nearest to the buldge. The buldge in our earlier example lay in Quadrant 2, so the transformations \\(\\log x\\), \\(\\sqrt x\\), \\(y^{2}\\), or \\(y^{3}\\) are possible contenders. It’s important to note that this diagram is not perfect, and some transformations will work better than others. In our case, \\(\\log x\\) and \\(\\log y\\) (found in Quadrant 3) were the best.\n\n\n\n8.2.0.3 Additional Remarks\nVisualization requires a lot of thought! - There are many tools for visualizing distributions. - Distribution of a single variable: 1. rug plot 2. histogram 3. density plot 4. box plot 5. violin plot - Joint distribution of two quantitative variables: 1. scatter plot 2. hex plot 3. contour plot.\nThis class primarily uses seaborn and matplotlib, but Pandas also has basic built-in plotting methods. Many other visualization libraries exist, and plotly is one of them. - plotly creates very easily creates interactive plots. - plotly will occasionally appear in lecture code, labs, and assignments!\nNext, we’ll go deeper into the theory behind visualization."
  },
  {
    "objectID": "visualization_2/visualization_2.html#visualization-theory",
    "href": "visualization_2/visualization_2.html#visualization-theory",
    "title": "8  Visualization II",
    "section": "8.3 Visualization Theory",
    "text": "8.3 Visualization Theory\nThis section marks a pivot to the second major topic of this lecture - visualization theory. We’ll discuss the abstract nature of visualizations and analyze how they convey information.\nRemember, we had two goals for visualizing data. This section is particularly important in:\n\nHelping us understand the data and results\nCommunicating our results and conclusions with others\n\n\n8.3.1 Information Channels\nVisualizations are able to convey information through various encodings. In the remainder of this lecture, we’ll look at the use of color, scale, and depth, to name a few.\n\n8.3.1.1 Encodings in Rugplots\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n\n\n\n8.3.1.2 Multi-Dimensional Encodings\nEncodings are also useful for representing multi-dimensional data. Notice how the following visual highlights four distinct “dimensions” of data:\n\nX-axis\nY-axis\nArea\nColor\n\n\nThe human visual perception sytem is only capable of visualizing data in a three-dimensional plane, but as you’ve seen, we can encode many more channels of information.\n\n\n\n8.3.2 Harnessing the Axes\n\n8.3.2.1 Consider Scale of the Data\nHowever, we should be careful to not misrepresent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n\nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not be intersecting.\nWhen there is a large difference in the magnitude of the data, it’s advised to analyze percentages instead of counts. The following diagrams correctly display the trends in cancer screening and abortion rates.\n\n\n\n\n\n\n\n\n\n\n\n8.3.2.2 Reveal the Data\nGreat visualizations not only consider the scale of the data, but also utilize the axes in a way that best conveys information. For example, data scientists commonly set certain axes limits to highlight parts of the visualization they are most interested in.\n\n\n\n\n\n\n\n\n\nThe visualization on the right captures the trend in coronavirus cases during the month March in 2020. From only looking at the visualization on the left, a viewer may incorrectly believe that coronavirus began to skyrocket on March 4th, 2020. However, the second illustration tells a different story - cases rose closer to March 21th, 2020.\n\n\n\n8.3.3 Harnessing Color\nColor is another important feature in visualizations that does more than what meets the eye.\nLast lecture, we used color to encode a categorical variable in our scatter plot. In this section, we will discuss uses of color in novel visualizations like colormaps and heatmaps.\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors which work together is evidently a challenging task!\n\n8.3.3.1 Colormaps\nColormaps are mappings from pixel data to color values, and they’re often used to highlight distinct parts of an image. Let’s investigate a few properties of colormaps.\n\n\nJet Colormap \n\n\n\nViridis Colormap \n\n\nThe jet colormap is infamous for being misleading. While it seems more vibrant than viridis, the aggressive colors poorly encode numerical data. To understand why, let’s analyze the following images.\n\n\n\n\n\n\n\n\n\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low intensity. These include the jet colormap (row a) and grayscale (row b). Notice how the grayscale images do the best job in smoothly transitioning between pixel data. The jet colormap is the worst at this - the four images in row (a) look like a conglomeration of individual colors.\nThe difference is also evident in the images labeled (a) and (b) on the left side. The grayscale image is better at preserving finer detail in the vertical line strokes. Additionally, grayscale is preferred in x-ray scans for being more neutral. The intensity of dark red color in the jet colormap is frightening and indicates something is wrong.\nWhy is the jet colormap so much worse? The answer lies in how its color composition is percieved to the human eye.\n\n\nJet Colormap Perception \n\n\n\nViridis Colormap Perception \n\n\nThe jet colormap is largely misleading because it is not perceptually uniform. Perceptually uniform colormaps have the property that if the pixel data goes from 0.1 to 0.2, the perceptual change is the same as when the data goes from 0.8 to 0.9.\nNotice how the said uniformity is present within the linear trend displayed in the viridis colormap. On the other hand, the jet colormap is largely non-linear - this is precisely why it’s considered a worse colormap.\n\n\n\n8.3.4 Harnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these “markings”.\n\nThere are a few key takeaways from this diagram\n\nLengths are easy to discern. Don’t use plots with jiggled baselines - keep everything axis-aligned.\nAvoid pie charts! Angle judgements are inaccurate.\nAreas and volumes are hard to distinguish (area charts, word clouds, etc)\n\n\n\n8.3.5 Harnessing Conditioning\nConditioning is the process of comparing data that belong to seperate groups. We’ve seen this before in overlayed distributions, side-by-side box-plots, and scatter plots with categorical encodings. Here, we’ll introduce terminology that formalizes these examples.\nConsider an example where we want to analyze income earnings for male and females with varying levels of education. There are multiple ways to compare this data.\n\n\n\n\n\n\n\n\n\nThe barplot is an example of juxtaposition: placing multiple plots side by side, with the same scale. The scatter plot is an example of superposition: placing multiple density curves, scatter plots on top of each other.\nWhich is better depends on the problem at hand. Here, superposition makes the precise wage difference very clear from a quick glance. But many sophisticated plots convey information that favors the use of juxtaposition. Below is one example.\n\n\n\n8.3.6 Harnessing Context\nThe last component to a great visualization is perhaps the most critical - the use of context. Adding informative titles, axis labels, and descriptive captions are all best practices that we’ve heard repeatedly in Data 8.\nA publication-ready plot (and every Data 100 plot) needs:\n\nInformative title (takeaway, not description)\nAxis labels\nReference lines, markers, etc\nLegends, if appropriate\nCaptions that describe data\n\nCaptions should be:\n\nComprehensive and self-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graphs"
  },
  {
    "objectID": "visualization_1/visualization_1.html#visualizations-in-data-8-and-data-100-so-far",
    "href": "visualization_1/visualization_1.html#visualizations-in-data-8-and-data-100-so-far",
    "title": "7  Visualization I",
    "section": "7.1 Visualizations in Data 8 and Data 100 (so far)",
    "text": "7.1 Visualizations in Data 8 and Data 100 (so far)\nYou’ve likely encountered several forms of data visualizations in your studies. You may remember two such examples from Data 8: line charts and histograms. Each of these served a unique purpose. For example, line charts displayed how numerical quantities changed over time, while histograms were useful in understanding a variable’s distribution.\n\n\nLine Chart\n\n\n\n\nHistogram"
  },
  {
    "objectID": "visualization_1/visualization_1.html#goals-of-visualization",
    "href": "visualization_1/visualization_1.html#goals-of-visualization",
    "title": "7  Visualization I",
    "section": "7.2 Goals of Visualization",
    "text": "7.2 Goals of Visualization\nVisualizations are useful for a number of reasons. In Data 100, we consider two areas in particular:\n\nTo broaden your understanding of the data\n\nKey part in exploratory data analysis.\nUseful in investigating relationships between variables.\n\nTo communicate results/conclusions to others\n\nVisualization theory is especially important here.\n\n\nOne of the most common applications of visualizations is in understanding a distribution of data."
  },
  {
    "objectID": "visualization_1/visualization_1.html#an-overview-of-distributions",
    "href": "visualization_1/visualization_1.html#an-overview-of-distributions",
    "title": "7  Visualization I",
    "section": "7.3 An Overview of Distributions",
    "text": "7.3 An Overview of Distributions\nA distribution describes the frequency of unique values in a variable. Distributions must satisfy two properties:\n\nEach data point must belong to only one category.\nThe total frequency of all categories must sum to 100%. In other words, their total count should equal the number of values in consideration.\n\n\n\nNot a Valid Distribution\n\n\n\n\nValid Distribution\n\n\n\nLeft Diagram: This is not a valid distribution since individuals can be associated to more than one category and the bar values demonstrate values in minutes and not probability\nRight Diagram: This example satisfies the two properties of distributions, so it is a valid distribution."
  },
  {
    "objectID": "visualization_1/visualization_1.html#bar-plots",
    "href": "visualization_1/visualization_1.html#bar-plots",
    "title": "7  Visualization I",
    "section": "7.4 Bar Plots",
    "text": "7.4 Bar Plots\nAs we saw above, a bar plot is one of the most common ways of displaying the distribution of a qualitative (categorical) variable. The length of a bar plot encodes the frequency of a category; the width encodes no useful information.\nLet’s contextualize this in an example. We will use the familiar births dataset from Data 8 in our analysis.\n\n\nCode\nimport pandas as pd\n\nbirths = pd.read_csv(\"data/baby.csv\")\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nWe can visualize the distribution of the Maternal Smoker column using a bar plot. There are a few ways to do this.\n\n7.4.1 Plotting in Pandas\n\nbirths['Maternal Smoker'].value_counts().plot(kind = 'bar');\n\n\n\n\nRecall that .value_counts() returns a Series with the total count of each unique value. We call .plot(kind = 'bar') on this result to visualize these counts as a bar plot.\nPlotting methods in pandas are the least preferred and not supported in Data 100, as their functionality is limited. Instead, future examples will focus on other libaries built specifically for visualizing data. The most well-known library here is matplotlib.\n\n\n7.4.2 Plotting in Matplotlib\n\nimport matplotlib.pyplot as plt\n\nms = births['Maternal Smoker'].value_counts()\nplt.bar(ms.index.astype('string'), ms)\nplt.xlabel('Maternal Smoker')\nplt.ylabel('Count');\n\n\n\n\nWhile more code is required to achieve the same result, matplotlib is often used over pandas for its ability to plot more complex visualizations, some of which are discussed shortly.\nHowever, notice how we need to explicitly specify the type of the value for the x-axis to string. In absence of conversion, the x-axis will be a range of integers rather than the two categories, True and False. This is because matplotlib coerces True to a value of 1 and False to 0. Also, note how we needed to label the axes with plt.xlabel and plt.ylabel - matplotlib does not support automatic axis labeling. To get around these inconveniences, we can use a more effecient plotting library, seaborn.\n\n\n7.4.3 Plotting in Seaborn\n\nimport seaborn as sns\nsns.countplot(data = births, x = 'Maternal Smoker');\n\n\n\n\nseaborn.countplot both counts and visualizes the number of unique values in a given column. This column is specified by the x argument to sns.countplot, while the DataFrame is specified by the data argument.\nFor the vast majority of visualizations, seaborn is far more concise and aesthetically pleasing than matplotlib. However, the color scheme of this particular bar plot is abritrary - it encodes no additional information about the categories themselves. This is not always true; color may signify meaningful detail in other visualizations. We’ll explore this more in-depth during the next lecture.\n\n\n7.4.4 Plotting in Plotly\n\nplotly is one of the most versatile plottling libraries and widely used in industry. However, plotly has various dependencies that make it difficult to support in Data 100. Therfore, we have intentionally excluded the code to generate the plot above.\nBy now, you’ll have noticed that each of these plotting libraries have a very different syntax. As with pandas, we’ll teach you the important methods in matplotlib and seaborn, but you’ll learn more through documentation.\n\nMatplotlib Documentation\nSeaborn Documentation\n\nExample Questions: - What colors should we use? - How wide should the bars be? - Should the legend exist? - Should the bars and axes have dark borders?\nTo accomplish goal 2, here are some ways we can improve plot: - Introducing different colors for each bar - Including a legend - Including a title - Labeling the y-axis - Using color-blind friendly palettes - Re-orienting the labels - Increase the font size"
  },
  {
    "objectID": "visualization_1/visualization_1.html#histograms",
    "href": "visualization_1/visualization_1.html#histograms",
    "title": "7  Visualization I",
    "section": "7.5 Histograms",
    "text": "7.5 Histograms\nHistograms are a natural extension to bar plots; they visualize the distribution of quantitative (numerical) data.\nRevisiting our example with the births DataFrame, let’s plot the distribution of the Maternal Pregnancy Weight column.\n\n\nCode\nbirths.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Birth Weight\n      Gestational Days\n      Maternal Age\n      Maternal Height\n      Maternal Pregnancy Weight\n      Maternal Smoker\n    \n  \n  \n    \n      0\n      120\n      284\n      27\n      62\n      100\n      False\n    \n    \n      1\n      113\n      282\n      33\n      64\n      135\n      False\n    \n    \n      2\n      128\n      279\n      28\n      64\n      115\n      True\n    \n    \n      3\n      108\n      282\n      23\n      67\n      125\n      True\n    \n    \n      4\n      136\n      286\n      25\n      62\n      93\n      False\n    \n  \n\n\n\n\nHow should we define our categories for this variable? In the previous example, these were the unique values of the Maternal Smoker column: True and False. If we use similar logic here, our categories are the different numerical weights contained in the Maternal Pregnancy Weight column.\nUnder this assumption, let’s plot this distribution using the seaborn.countplot function.\n\nsns.countplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis histogram clearly suffers from overplotting. This is somewhat expected for Maternal Pregnancy Weight - it is a quantitative variable that takes on a wide range of values.\nTo combat this problem, statisticians use bins to categorize numerical data. Luckily, seaborn provides a helpful plotting function that automatically bins our data.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\n\n\n\n\nThis diagram is known as a histogram. While it looks more reasonable, notice how we lose fine-grain information on the distribution of data contained within each bin. We can introduce rug plots to minimize this information loss. An overlaid “rug plot” displays the within-bin distribution of our data, as denoted by the thickness of the colored line on the x-axis.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\nsns.rugplot(data = births, x = 'Maternal Pregnancy Weight', color = 'red');\n\n\n\n\nYou may have seen histograms drawn differently - perhaps with an overlaid density curve and normalized y-axis. We can display both with a few tweaks to our code.\nTo visualize a density curve, we can set the the kde = True argument of the sns.histplot. Setting the argument stat = 'density' normalizes our histogram and displays densities, instead of counts, on the y-axis. You’ll notice that the area under the density curve is 1.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight', kde = True, \n             stat = 'density')\nsns.rugplot(data = births, x = 'Maternal Pregnancy Weight', color = 'red');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#evaluating-histograms",
    "href": "visualization_1/visualization_1.html#evaluating-histograms",
    "title": "7  Visualization I",
    "section": "7.6 Evaluating Histograms",
    "text": "7.6 Evaluating Histograms\nHistograms allow us to assess a distribution by their shape. There are a few properties of histograms we can analyze:\n\nSkewness and Tails\n\nSkewed left vs skewed right\nLeft tail vs right tail\n\nOutliers\n\nDefined arbitrarily for now\n\nModes\n\nMost commonly occuring data\n\n\n\n7.6.1 Skewness and Tails\nIf a distribution has a long right tail (such as Maternal Pregancy Weight), it is skewed right. In a right-skewed distribution, the few large outliers “pull” the mean to the right of the median.\nIf a distribution has a long left tail, it is skewed left. In a left-skewed distribution, the few small outliers “pull” the mean to the left of the median.\nIn the case where a distribution has equal-sized right and left tails, it is symmetric. The mean is approximately equal to the median. Think of mean as the balancing point of the distribution\n\nimport numpy as np\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\ndf_mean = np.mean(births['Maternal Pregnancy Weight'])\ndf_median = np.median(births['Maternal Pregnancy Weight'])\nprint(\"The mean is: {} and the median is {}\".format(df_mean,df_median))\n\nThe mean is: 128.4787052810903 and the median is 125.0\n\n\n\n\n\n\n\n7.6.2 Outliers\nLoosely speaking, an outlier is defined as a data point that lies an abnormally large distance away from other values. We’ll define the statistical measure for this shortly.\nOutliers disproportionately influce the mean because their magnitude is directly involved in computing the average. However, the median is largely unaffected - the magnitude of an outlier is irrelevant; we only care that it is some non-zero distance away from the midpoint of the data.\n\nsns.histplot(data = births, x = 'Maternal Pregnancy Weight');\n## Where do we draw the line of outlier? \nplt.axvline(df_mean*1.75, color = 'red');\n\n\n\n\n\n\n7.6.3 Modes\nA mode of a distribution is a local or global maximum. A distribution with a single clear maximum is unimodal, distributions with two modes are bimodal, and those with 3 or more are multimodal. You need to distinguish between modes and random noise.\nFor example, the distribution of birth weights for maternal smokers is (weakly) multimodal.\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\nsns.histplot(data = births_maternal_smoker, x = 'Maternal Pregnancy Weight')\\\n            .set(title = 'Maternal Smoker histogram');\n\n\n\n\nOn the other hand, the distribution of birth weights for maternal non-smokers is weakly bi-modal.\n\nbirths_maternal_non_smoker = births[births['Maternal Smoker'] == False]\nsns.histplot(data = births_maternal_non_smoker, x = 'Maternal Pregnancy Weight')\\\n            .set(title = 'Maternal Non-Smoker histogram');\n\n\n\n\nHowever, changing the bins reveals that the data is not bi-modal.\n\nsns.histplot(data = births_maternal_non_smoker, x = 'Maternal Pregnancy Weight',\\\n             bins = 20);"
  },
  {
    "objectID": "visualization_1/visualization_1.html#density-curves",
    "href": "visualization_1/visualization_1.html#density-curves",
    "title": "7  Visualization I",
    "section": "7.7 Density Curves",
    "text": "7.7 Density Curves\nInstead of a discrete histogram, we can visualize what a continuous distribution corresponding to that same data could look like using a curve. - The smooth curve drawn on top of the histogram here is called a density curve.\nIn lecture 8, we will study how exactly to compute these density curves (using a technique is called Kernel Density Estimation).\nIf we plot birth weights of babies of smoking mothers, we get a histogram that appears bimodal. - Density curve reinforces belief in this bimodality.\nHowever, if we plot birth weights of babies of non-smoking mothers, we get a histogram that appears unimodal.\nFrom a goal 1 perspective, this is EDA which tells us there may be something interesting here worth pursuing. - Deeper analysis necessary! - If we found something truly interesting, we’d have to cautiously write up an argument and create goal 2 level visualizations.\n\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_maternal_smoker = births[births['Maternal Smoker'] == True]\n \nsns.histplot(data = births_maternal_smoker , x = 'Birth Weight',\\\n             kde = True);\n\n\n\n\n\nsns.histplot(data = births_non_maternal_smoker , x = 'Birth Weight',\\\n             kde = True);\n\n\n\n\n\n7.7.1 Histograms and Density\nRather than labeling by counts, we can instead plot the density, as shown below. Density gives us a measure that is invariant to the total number of observed units. The numerical values on the Y-axis for a sample of 100 units would be the same for when we observe a sample of 10000 units instead. We can still calculate the absolute number of observed units using density.\nExample: There are 1174 observations total. - Total area of this bin should be: 120/1174 = ~10% - Density of this bin is therefore: 10% / (115 - 110) = 0.02"
  },
  {
    "objectID": "visualization_1/visualization_1.html#box-plots-and-violin-plots",
    "href": "visualization_1/visualization_1.html#box-plots-and-violin-plots",
    "title": "7  Visualization I",
    "section": "7.8 Box Plots and Violin Plots",
    "text": "7.8 Box Plots and Violin Plots\n\n7.8.1 Boxplots\nBoxplots are an alternative to histograms that visualize numerical distributions. They are especially useful in graphicaly summarizing several characteristics of a distribution. These include:\n\nLower Quartile (\\(1^{st}\\) Quartile)\nMedian (\\(2^{nd}\\) Quartile)\nUpper Quartile (\\(3^{rd}\\) Quartile)\nInterquartile Range (IQR)\nWhiskers\nOutliers\n\nThe lower quartile, median, and uper quartile are the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentiles of data, respectively. The interquartile range measures the spread of the middle \\(50\\)% of the distribution, calculated as the (\\(3^{rd}\\) Quartile \\(-\\) \\(1^{st}\\) Quartile).\nThe whiskers of a box-plot are the two points that lie at the [\\(1^{st}\\) Quartile \\(-\\) (\\(1.5\\times\\) IQR)], and the [\\(3^{rd}\\) Quartile \\(+\\) (\\(1.5\\times\\) IQR)]. They are the lower and upper ranges of “normal” data (the points excluding outliers). Subsequently, the outliers are the data points that fall beyond the whiskers, or further than (\\(1.5 \\times\\) IQR) from the extreme quartiles.\nLet’s visualize a box-plot of the Birth Weight column.\n\n\nCode\nsns.boxplot(data = births, y = 'Birth Weight');\n\nbweights = births['Birth Weight']\nq1 = np.percentile(bweights, 25)\nq2 = np.percentile(bweights, 50)\nq3 = np.percentile(bweights, 75)\niqr = q3 - q1\nwhisk1 = q1 - (1.5 * iqr)\nwhisk2 = q3 + (1.5 * iqr)\n\nprint(\"The first quartile is {}\".format(q1))\nprint(\"The second quartile is {}\".format(q2))\nprint(\"The third quartile is {}\".format(q3))\nprint(\"The interquartile range is {}\".format(iqr))\nprint(\"The whiskers are {} and {}\".format(whisk1, whisk2))\n\n\nThe first quartile is 108.0\nThe second quartile is 120.0\nThe third quartile is 131.0\nThe interquartile range is 23.0\nThe whiskers are 73.5 and 165.5\n\n\n\n\n\nHere is a helpful visual that summarizes our discussion above.\n\n\n\n7.8.2 Violin Plots\nAnother diagram that is useful in visualizing a variable’s distribution is the violin plot. A violin plot supplements a box-plot with a smoothed density curve on either side of the plot. These density curves highlight the relative frequency of variable’s possible values. If you look closely, you’ll be able to discern the quartiles, whiskers, and other hallmark features of the box-plot.\n\nsns.violinplot(data = births, y = 'Birth Weight');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#comparing-quantitative-distributions",
    "href": "visualization_1/visualization_1.html#comparing-quantitative-distributions",
    "title": "7  Visualization I",
    "section": "7.9 Comparing Quantitative Distributions",
    "text": "7.9 Comparing Quantitative Distributions\nEarlier in our discussion of the mode, we visualized two histograms that described the distribution of birth weights for maternal smokers and non-smokers. However, comparing these histograms was difficult because they were displayed on seperate plots. Can we overlay the two to tell a more compelling story?\nIn seaborn, multiple calls to a plotting library in the same code cell will overlay the plots. For example:\n\nbirths_maternal_smoker = births[births['Maternal Smoker'] == False]\nbirths_non_maternal_smoker = births[births['Maternal Smoker'] == True]\n\nsns.histplot(data = births_maternal_smoker, x = 'Birth Weight',\n             color = 'orange', label = 'smoker')\nsns.histplot(data = births_non_maternal_smoker, x = 'Birth Weight',\n             color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nHowever, notice how this diagram suffers from overplotting. We can fix this with a call to sns.kdeplot. This will remove the bins and overlay the histogram with a density curve that better summarizes the distribution.\n\nsns.kdeplot(data = births_maternal_smoker, x = 'Birth Weight', color = 'orange', label = 'smoker')\nsns.kdeplot(data = births_non_maternal_smoker, x = 'Birth Weight', color = 'blue', label = 'nonsmoker')\nplt.legend();\n\n\n\n\nUnfortunately, we lose critical information in our distribution by removing small details. Therefore, we typically prefer to use box-plots and violin plots when comparing distributions. These are more concise and allow us to compare summary statistics across many distributions.\n\nsns.violinplot(data = births, x = 'Maternal Smoker', y = 'Birth Weight');\n\n\n\n\n\nsns.boxplot(data=births, x = 'Maternal Smoker', y = 'Birth Weight');"
  },
  {
    "objectID": "visualization_1/visualization_1.html#ridge-plots",
    "href": "visualization_1/visualization_1.html#ridge-plots",
    "title": "7  Visualization I",
    "section": "7.10 Ridge Plots",
    "text": "7.10 Ridge Plots\nRidge plots show many density curves offset from one another with minimal overlap. They are useful when the specific shape of each curve is important."
  },
  {
    "objectID": "pandas_3/pandas_3.html#more-on-agg-function",
    "href": "pandas_3/pandas_3.html#more-on-agg-function",
    "title": "4  Pandas III",
    "section": "4.1 More on agg() Function",
    "text": "4.1 More on agg() Function\nLast time, we introduced the concept of aggregating data – we familiarized ourselves with GroupBy objects and used them as tools to consolidate and summarize a DataFrame. In this lecture, we will explore some advanced .groupby methods to show just how powerful of a resource they can be for understanding our data. We will also introduce other techniques for data aggregation to provide flexibility in how we manipulate our tables."
  },
  {
    "objectID": "pandas_3/pandas_3.html#groupby-continued",
    "href": "pandas_3/pandas_3.html#groupby-continued",
    "title": "4  Pandas III",
    "section": "4.2 GroupBy(), Continued",
    "text": "4.2 GroupBy(), Continued\nAs we learned last lecture, a groupby operation involves some combination of splitting a DataFrame into grouped subframes, applying a function, and combining the results.\nFor some arbitrary DataFrame df below, the code df.groupby(\"year\").agg(sum) does the following:\n\nOrganizes all rows with the same year into a subframe for that year.\nCreates a new DataFrame with one row representing each subframe year.\nCombines all integer rows in each subframe using the sum function.\n\n\n\n4.2.1 Aggregation with lambda Functions\nThroughout this note, we’ll work with the elections DataFrame.\n\n\nCode\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nWhat if we wish to aggregate our DataFrame using a non-standard function – for example, a function of our own design? We can do so by combining .agg with lambda expressions.\nLet’s first consider a puzzle to jog our memory. We will attempt to find the Candidate from each Party with the highest % of votes.\nA naive approach may be to group by the Party column and aggregate by the maximum.\n\nelections.groupby(\"Party\").agg(max).head(10)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Popular vote\n      Result\n      %\n    \n    \n      Party\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      American\n      1976\n      Thomas J. Anderson\n      873053\n      loss\n      21.554001\n    \n    \n      American Independent\n      1976\n      Lester Maddox\n      9901118\n      loss\n      13.571218\n    \n    \n      Anti-Masonic\n      1832\n      William Wirt\n      100715\n      loss\n      7.821583\n    \n    \n      Anti-Monopoly\n      1884\n      Benjamin Butler\n      134294\n      loss\n      1.335838\n    \n    \n      Citizens\n      1980\n      Barry Commoner\n      233052\n      loss\n      0.270182\n    \n    \n      Communist\n      1932\n      William Z. Foster\n      103307\n      loss\n      0.261069\n    \n    \n      Constitution\n      2016\n      Michael Peroutka\n      203091\n      loss\n      0.152398\n    \n    \n      Constitutional Union\n      1860\n      John Bell\n      590901\n      loss\n      12.639283\n    \n    \n      Democratic\n      2020\n      Woodrow Wilson\n      81268924\n      win\n      61.344703\n    \n    \n      Democratic-Republican\n      1824\n      John Quincy Adams\n      151271\n      win\n      57.210122\n    \n  \n\n\n\n\nThis approach is clearly wrong – the DataFrame claims that Woodrow Wilson won the presidency in 2020.\nWhy is this happening? Here, the max aggregation function is taken over every column independently. Among Democrats, max is computing:\n\nThe most recent Year a Democratic candidate ran for president (2020)\nThe Candidate with the alphabetically “largest” name (“Woodrow Wilson”)\nThe Result with the alphabetically “largest” outcome (“win”)\n\nInstead, let’s try a different approach. We will:\n\nSort the DataFrame so that rows are in descending order of %\nGroup by Party and select the first row of each groupby object\n\nWhile it may seem unintuitive, sorting elections by descending order of % is extremely helpful. If we then group by Party, the first row of each groupby object will contain information about the Candidate with the highest voter %.\n\nelections_sorted_by_percent = elections.sort_values(\"%\", ascending=False)\nelections_sorted_by_percent.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      114\n      1964\n      Lyndon Johnson\n      Democratic\n      43127041\n      win\n      61.344703\n    \n    \n      91\n      1936\n      Franklin Roosevelt\n      Democratic\n      27752648\n      win\n      60.978107\n    \n    \n      120\n      1972\n      Richard Nixon\n      Republican\n      47168710\n      win\n      60.907806\n    \n    \n      79\n      1920\n      Warren Harding\n      Republican\n      16144093\n      win\n      60.574501\n    \n    \n      133\n      1984\n      Ronald Reagan\n      Republican\n      54455472\n      win\n      59.023326\n    \n  \n\n\n\n\n\nelections_sorted_by_percent.groupby(\"Party\").agg(lambda x : x.iloc[0]).head(10)\n\n# Equivalent to the below code\n# elections_sorted_by_percent.groupby(\"Party\").agg('first').head(10)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Popular vote\n      Result\n      %\n    \n    \n      Party\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      American\n      1856\n      Millard Fillmore\n      873053\n      loss\n      21.554001\n    \n    \n      American Independent\n      1968\n      George Wallace\n      9901118\n      loss\n      13.571218\n    \n    \n      Anti-Masonic\n      1832\n      William Wirt\n      100715\n      loss\n      7.821583\n    \n    \n      Anti-Monopoly\n      1884\n      Benjamin Butler\n      134294\n      loss\n      1.335838\n    \n    \n      Citizens\n      1980\n      Barry Commoner\n      233052\n      loss\n      0.270182\n    \n    \n      Communist\n      1932\n      William Z. Foster\n      103307\n      loss\n      0.261069\n    \n    \n      Constitution\n      2008\n      Chuck Baldwin\n      199750\n      loss\n      0.152398\n    \n    \n      Constitutional Union\n      1860\n      John Bell\n      590901\n      loss\n      12.639283\n    \n    \n      Democratic\n      1964\n      Lyndon Johnson\n      43127041\n      win\n      61.344703\n    \n    \n      Democratic-Republican\n      1824\n      Andrew Jackson\n      151271\n      loss\n      57.210122\n    \n  \n\n\n\n\nNotice how our code correctly determines that Lyndon Johnson from the Democratic Party has the highest voter %.\nMore generally, lambda functions are used to design custom aggregation functions that aren’t pre-defined by Python. The input parameter x to the lambda function is a GroupBy object. Therefore, it should make sense why lambda x : x.iloc[0] selects the first row in each groupby object.\nIn fact, there’s a few different ways to approach this problem. Each approach has different tradeoffs in terms of readability, performance, memory consumption, complexity, etc. We’ve given a few examples below.\nNote: Understanding these alternative solutions is not required. They are given to demonstrate the vast number of problem-solving approaches in pandas.\n\n# Using the idxmax function\nbest_per_party = elections.loc[elections.groupby('Party')['%'].idxmax()]\nbest_per_party.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      22\n      1856\n      Millard Fillmore\n      American\n      873053\n      loss\n      21.554001\n    \n    \n      115\n      1968\n      George Wallace\n      American Independent\n      9901118\n      loss\n      13.571218\n    \n    \n      6\n      1832\n      William Wirt\n      Anti-Masonic\n      100715\n      loss\n      7.821583\n    \n    \n      38\n      1884\n      Benjamin Butler\n      Anti-Monopoly\n      134294\n      loss\n      1.335838\n    \n    \n      127\n      1980\n      Barry Commoner\n      Citizens\n      233052\n      loss\n      0.270182\n    \n  \n\n\n\n\n\n# Using the .drop_duplicates function\nbest_per_party2 = elections.sort_values('%').drop_duplicates(['Party'], keep='last')\nbest_per_party2.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      148\n      1996\n      John Hagelin\n      Natural Law\n      113670\n      loss\n      0.118219\n    \n    \n      164\n      2008\n      Chuck Baldwin\n      Constitution\n      199750\n      loss\n      0.152398\n    \n    \n      110\n      1956\n      T. Coleman Andrews\n      States' Rights\n      107929\n      loss\n      0.174883\n    \n    \n      147\n      1996\n      Howard Phillips\n      Taxpayers\n      184656\n      loss\n      0.192045\n    \n    \n      136\n      1988\n      Lenora Fulani\n      New Alliance\n      217221\n      loss\n      0.237804\n    \n  \n\n\n\n\n\n\n4.2.2 Other GroupBy Features\nThere are many aggregation methods we can use with .agg. Some useful options are:\n\n.max: creates a new DataFrame with the maximum value of each group\n.mean: creates a new DataFrame with the mean value of each group\n.size: creates a new Series with the number of entries in each group\n\nIn fact, these (and other) aggregation functions are so common that pandas allows for writing shorthand. Instead of explicitly stating the use of .agg, we can call the function directly on the GroupBy object.\nFor example, the following are equivalent:\n\nelections.groupby(\"Candidate\").agg(mean)\nelections.groupby(\"Candidate\").mean()\n\n\n\n4.2.3 The groupby.filter() function\nAnother common use for GroupBy objects is to filter data by group.\ngroupby.filter takes an argument \\(\\text{f}\\), where \\(\\text{f}\\) is a function that:\n\nTakes a GroupBy object as input\nReturns a single True or False for the entire subframe\n\nGroupBy objects that correspond to True are returned in the final result, whereas those with a False value are not. Importantly, groupby.filter is different from groupby.agg in that the entire subframe is returned in the final DataFrame, not just a single row.\nTo illustrate how this happens, consider the following .filter function applied on some arbitrary data. Say we want to identify “tight” election years – that is, we want to find all rows that correspond to elections years where all candidates in that year won a similar portion of the total vote. Specifically, let’s find all rows corresponding to a year where no candidate won more than 45% of the total vote.\nAn equivalent way of framing this goal is to say:\n\nFind the years where the maximum % in that year is less than 45%\nReturn all DataFrame rows that correspond to these years\n\nFor each year, we need to find the maximum % among all rows for that year. If this maximum % is lower than 45%, we will tell pandas to keep all rows corresponding to that year.\n\nelections.groupby(\"Year\").filter(lambda sf: sf[\"%\"].max() < 45).head(9)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      23\n      1860\n      Abraham Lincoln\n      Republican\n      1855993\n      win\n      39.699408\n    \n    \n      24\n      1860\n      John Bell\n      Constitutional Union\n      590901\n      loss\n      12.639283\n    \n    \n      25\n      1860\n      John C. Breckinridge\n      Southern Democratic\n      848019\n      loss\n      18.138998\n    \n    \n      26\n      1860\n      Stephen A. Douglas\n      Northern Democratic\n      1380202\n      loss\n      29.522311\n    \n    \n      66\n      1912\n      Eugene V. Debs\n      Socialist\n      901551\n      loss\n      6.004354\n    \n    \n      67\n      1912\n      Eugene W. Chafin\n      Prohibition\n      208156\n      loss\n      1.386325\n    \n    \n      68\n      1912\n      Theodore Roosevelt\n      Progressive\n      4122721\n      loss\n      27.457433\n    \n    \n      69\n      1912\n      William Taft\n      Republican\n      3486242\n      loss\n      23.218466\n    \n    \n      70\n      1912\n      Woodrow Wilson\n      Democratic\n      6296284\n      win\n      41.933422\n    \n  \n\n\n\n\nWhat’s going on here? In this example, we’ve defined our filtering function, \\(\\text{f}\\), to be lambda sf: sf[\"%\"].max() < 45. This filtering function will find the maximum \"%\" value among all entries in the grouped subframe, which we call sf. If the maximum value is less than 45, then the filter function will return True and all rows in that grouped subframe will appear in the final output DataFrame.\nExamine the DataFrame above. Notice how, in this preview of the first 9 rows, all entries from the years 1860 and 1912 appear. This means that in 1860 and 1912, no candidate in that year won more than 45% of the total vote.\nYou may ask: how is the groupby.filter procedure different to the boolean filtering we’ve seen previously? Boolean filtering considers individual rows when applying a boolean condition. For example, the code elections[elections[\"%\"] < 45] will check the \"%\" value of every single row in elections; if it is less than 45, then that row will be kept in the output. groupby.filter, in contrast, applies a boolean condition across all rows in a group. If not all rows in that group satisfy the condition specified by the filter, the entire group will be discarded in the output."
  },
  {
    "objectID": "pandas_3/pandas_3.html#aggregating-data-with-pivot-tables",
    "href": "pandas_3/pandas_3.html#aggregating-data-with-pivot-tables",
    "title": "4  Pandas III",
    "section": "4.3 Aggregating Data with Pivot Tables",
    "text": "4.3 Aggregating Data with Pivot Tables\nWe know now that .groupby gives us the ability to group and aggregate data across our DataFrame. The examples above formed groups using just one column in the DataFrame. It’s possible to group by multiple columns at once by passing in a list of columns names to .groupby.\nLet’s consider the babynames dataset from last lecture. In this problem, we will find the total number of baby names associated with each sex for each year. To do this, we’ll group by both the \"Year\" and \"Sex\" columns.\n\n\nCode\nimport urllib.request\nimport os.path\n\n# Download data from the web directly\ndata_url = \"https://www.ssa.gov/oact/babynames/names.zip\"\nlocal_filename = \"data/babynames.zip\"\nif not os.path.exists(local_filename): # if the data exists don't download again\n    with urllib.request.urlopen(data_url) as resp, open(local_filename, 'wb') as f:\n        f.write(resp.read())\n\n        \n# Load data without unzipping the file\nimport zipfile\nbabynames = [] \nwith zipfile.ZipFile(local_filename, \"r\") as zf:\n    data_files = [f for f in zf.filelist if f.filename[-3:] == \"txt\"]\n    def extract_year_from_filename(fn):\n        return int(fn[3:7])\n    for f in data_files:\n        year = extract_year_from_filename(f.filename)\n        with zf.open(f) as fp:\n            df = pd.read_csv(fp, names=[\"Name\", \"Sex\", \"Count\"])\n            df[\"Year\"] = year\n            babynames.append(df)\nbabynames = pd.concat(babynames)\n\n\n\nbabynames.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Mary\n      F\n      7065\n      1880\n    \n    \n      1\n      Anna\n      F\n      2604\n      1880\n    \n    \n      2\n      Emma\n      F\n      2003\n      1880\n    \n    \n      3\n      Elizabeth\n      F\n      1939\n      1880\n    \n    \n      4\n      Minnie\n      F\n      1746\n      1880\n    \n  \n\n\n\n\n\n# Find the total number of baby names associated with each sex for each year in the data\nbabynames.groupby([\"Year\", \"Sex\"])[[\"Count\"]].agg(sum).head(6)\n\n\n\n\n\n  \n    \n      \n      \n      Count\n    \n    \n      Year\n      Sex\n      \n    \n  \n  \n    \n      1880\n      F\n      90994\n    \n    \n      M\n      110490\n    \n    \n      1881\n      F\n      91953\n    \n    \n      M\n      100737\n    \n    \n      1882\n      F\n      107847\n    \n    \n      M\n      113686\n    \n  \n\n\n\n\nNotice that both \"Year\" and \"Sex\" serve as the index of the DataFrame (they are both rendered in bold). We’ve created a multindex where two different index values, the year and sex, are used to uniquely identify each row.\nThis isn’t the most intuitive way of representing this data – and, because multindexes have multiple dimensions in their index, they can often be difficult to use.\nAnother strategy to aggregate across two columns is to create a pivot table. You saw these back in Data 8. One set of values is used to create the index of the table; another set is used to define the column names. The values contained in each cell of the table correspond to the aggregated data for each index-column pair.\nThe best way to understand pivot tables is to see one in action. Let’s return to our original goal of summing the total number of names associated with each combination of year and sex. We’ll call the pandas .pivot_table method to create a new table.\n\n# The `pivot_table` method is used to generate a Pandas pivot table\nimport numpy as np\nbabynames.pivot_table(index = \"Year\", columns = \"Sex\", values = \"Count\", aggfunc = np.sum).head(5)\n\n\n\n\n\n  \n    \n      Sex\n      F\n      M\n    \n    \n      Year\n      \n      \n    \n  \n  \n    \n      1880\n      90994\n      110490\n    \n    \n      1881\n      91953\n      100737\n    \n    \n      1882\n      107847\n      113686\n    \n    \n      1883\n      112319\n      104625\n    \n    \n      1884\n      129019\n      114442\n    \n  \n\n\n\n\nLooks a lot better! Now, our DataFrame is structured with clear index-column combinations. Each entry in the pivot table represents the summed count of names for a given combination of \"Year\" and \"Sex\".\nLet’s take a closer look at the code implemented above.\n\nindex = \"Year\" specifies the column name in the original DataFrame that should be used as the index of the pivot table\ncolumns = \"Sex\" specifies the column name in the original DataFrame that should be used to generate the columns of the pivot table\nvalues = \"Count\" indicates what values from the original DataFrame should be used to populate the entry for each index-column combination\naggfunc = np.sum tells pandas what function to use when aggregating the data specified by values. Here, we are summing the name counts for each pair of \"Year\" and \"Sex\"\n\nWe can even include multiple values in the index or columns of our pivot tables.\n\nbabynames_pivot = babynames.pivot_table(\n    index=\"Year\",     # the rows (turned into index)\n    columns=\"Sex\",    # the column values\n    values=[\"Count\", \"Name\"], \n    aggfunc=max,   # group operation\n)\nbabynames_pivot.head(6)\n\n\n\n\n\n  \n    \n      \n      Count\n      Name\n    \n    \n      Sex\n      F\n      M\n      F\n      M\n    \n    \n      Year\n      \n      \n      \n      \n    \n  \n  \n    \n      1880\n      7065\n      9655\n      Zula\n      Zeke\n    \n    \n      1881\n      6919\n      8769\n      Zula\n      Zeb\n    \n    \n      1882\n      8148\n      9557\n      Zula\n      Zed\n    \n    \n      1883\n      8012\n      8894\n      Zula\n      Zeno\n    \n    \n      1884\n      9217\n      9388\n      Zula\n      Zollie\n    \n    \n      1885\n      9128\n      8756\n      Zula\n      Zollie"
  },
  {
    "objectID": "pandas_3/pandas_3.html#joining-tables",
    "href": "pandas_3/pandas_3.html#joining-tables",
    "title": "4  Pandas III",
    "section": "4.4 Joining Tables",
    "text": "4.4 Joining Tables\nWhen working on data science projects, we’re unlikely to have absolutely all the data we want contained in a single DataFrame – a real-world data scientist needs to grapple with data coming from multiple sources. If we have access to multiple datasets with related information, we can join two or more tables into a single DataFrame.\nTo put this into practice, we’ll revisit the elections dataset.\n\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nSay we want to understand the 2020 popularity of the names of each presidential candidate. To do this, we’ll need the combined data of babynames and elections.\nWe’ll start by creating a new column containing the first name of each presidential candidate. This will help us join each name in elections to the corresponding name data in babynames.\n\n# This `str` operation splits each candidate's full name at each \n# blank space, then takes just the candidiate's first name\nelections[\"First Name\"] = elections[\"Candidate\"].str.split().str[0]\nelections.head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n      John\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n      John\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n    \n  \n\n\n\n\n\n# Here, we'll only consider `babynames` data from 2020\nbabynames_2020 = babynames[babynames[\"Year\"]==2020]\nbabynames_2020.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Count\n      Year\n    \n  \n  \n    \n      0\n      Olivia\n      F\n      17641\n      2020\n    \n    \n      1\n      Emma\n      F\n      15656\n      2020\n    \n    \n      2\n      Ava\n      F\n      13160\n      2020\n    \n    \n      3\n      Charlotte\n      F\n      13065\n      2020\n    \n    \n      4\n      Sophia\n      F\n      13036\n      2020\n    \n  \n\n\n\n\nNow, we’re ready to join the two tables. pd.merge is the pandas method used to join DataFrames together. The left and right parameters are used to specify the DataFrames to be joined. The left_on and right_on parameters are assigned to the string names of the columns to be used when performing the join. These two on parameters tell pandas what values should act as pairing keys to determine which rows to merge across the DataFrames. We’ll talk more about this idea of a pairing key next lecture.\n\nmerged = pd.merge(left = elections, right = babynames_2020, \\\n                  left_on = \"First Name\", right_on = \"Name\")\nmerged.head()\n# Notice that pandas automatically specifies `Year_x` and `Year_y` \n# when both merged DataFrames have the same column name to avoid confusion\n\n\n\n\n\n  \n    \n      \n      Year_x\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n      First Name\n      Name\n      Sex\n      Count\n      Year_y\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      Andrew\n      F\n      12\n      2020\n    \n    \n      1\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n      Andrew\n      Andrew\n      M\n      6036\n      2020\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n      Andrew\n      F\n      12\n      2020\n    \n    \n      3\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n      Andrew\n      Andrew\n      M\n      6036\n      2020\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n      Andrew\n      Andrew\n      F\n      12\n      2020"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "5  Data Cleaning and EDA",
    "section": "",
    "text": "6 EDA Demo: Tuberculosis in the United States\nNow, let’s follow this data-cleaning and EDA workflow to see what can we say about the presence of Tuberculosis in the United States!\nWe will examine the data included in the original CDC article published in 2021."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "5  Data Cleaning and EDA",
    "section": "5.1 Structure",
    "text": "5.1 Structure\n\n5.1.1 File Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for Comma-Separated Values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the first few rows of the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (Tab-Separated Values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\n﻿Year   Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\n5.1.2 Variable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nClassification of variable types\n\n\n\n\n5.1.3 Primary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "5  Data Cleaning and EDA",
    "section": "5.2 Granularity, Scope, and Temporality",
    "text": "5.2 Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "5  Data Cleaning and EDA",
    "section": "5.3 Faithfulness",
    "text": "5.3 Faithfulness\nAt this stage in our data cleaning and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "eda/eda.html#csvs-and-field-names",
    "href": "eda/eda.html#csvs-and-field-names",
    "title": "5  Data Cleaning and EDA",
    "section": "6.1 CSVs and Field Names",
    "text": "6.1 CSVs and Field Names\nSuppose Table 1 was saved as a CSV file located in data/cdc_tuberculosis.csv.\nWe can then explore the CSV (which is a text file, and does not contain binary-encoded data) in many ways: 1. Using a text editor like emacs, vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only), Excel, Google Sheets, etc. 3. The Python file object 4. pandas, using pd.read_csv()\n1, 2. Let’s start with the first two so we really solidify the idea of a CSV as rectangular data (i.e., tabular data) stored as comma-separated values.\n\nNext, let’s try using the Python file object. Let’s check out the first three lines:\n\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(row)\n        i += 1\n        if i > 3:\n            break\n\n\n,No. of TB cases,,,TB incidence,,\n\nU.S. jurisdiction,2019,2020,2021,2019,2020,2021\n\nTotal,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\n\nAlabama,87,72,92,1.77,1.43,1.83\n\n\n\nWhoa, why are there blank lines interspaced between the lines of the CSV?\nYou may recall that all line breaks in text files are encoded as the special newline character \\n. Python’s print() prints each string (including the newline), and an additional newline on top of that.\nIf you’re curious, we can use the repr() function to return the raw string with all special characters:\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(repr(row)) # print raw strings\n        i += 1\n        if i > 3:\n            break\n\n\n',No. of TB cases,,,TB incidence,,\\n'\n'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\\n'\n'Total,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\\n'\n'Alabama,87,72,92,1.77,1.43,1.83\\n'\n\n\n\nFinally, let’s see the tried-and-true Data 100 approach: pandas.\n\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\")\ntb_df\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      No. of TB cases\n      Unnamed: 2\n      Unnamed: 3\n      TB incidence\n      Unnamed: 5\n      Unnamed: 6\n    \n  \n  \n    \n      0\n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.00\n      2020.00\n      2021.00\n    \n    \n      1\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      2\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      3\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      4\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      5\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n    \n      6\n      California\n      2,111\n      1,706\n      1,750\n      5.35\n      4.32\n      4.46\n    \n    \n      7\n      Colorado\n      66\n      52\n      58\n      1.15\n      0.90\n      1.00\n    \n    \n      8\n      Connecticut\n      67\n      54\n      54\n      1.88\n      1.50\n      1.50\n    \n    \n      9\n      Delaware\n      18\n      17\n      43\n      1.84\n      1.71\n      4.29\n    \n    \n      10\n      District of Columbia\n      24\n      19\n      19\n      3.39\n      2.75\n      2.84\n    \n    \n      11\n      Florida\n      558\n      412\n      499\n      2.60\n      1.91\n      2.29\n    \n    \n      12\n      Georgia\n      302\n      221\n      228\n      2.84\n      2.06\n      2.11\n    \n    \n      13\n      Hawaii\n      99\n      92\n      106\n      6.99\n      6.34\n      7.35\n    \n    \n      14\n      Idaho\n      7\n      8\n      4\n      0.39\n      0.43\n      0.21\n    \n    \n      15\n      Illinois\n      326\n      216\n      255\n      2.57\n      1.69\n      2.01\n    \n    \n      16\n      Indiana\n      108\n      92\n      127\n      1.60\n      1.36\n      1.87\n    \n    \n      17\n      Iowa\n      52\n      39\n      49\n      1.65\n      1.22\n      1.53\n    \n    \n      18\n      Kansas\n      37\n      37\n      43\n      1.27\n      1.26\n      1.47\n    \n    \n      19\n      Kentucky\n      66\n      67\n      57\n      1.48\n      1.49\n      1.26\n    \n    \n      20\n      Louisiana\n      88\n      99\n      86\n      1.89\n      2.13\n      1.86\n    \n    \n      21\n      Maine\n      18\n      17\n      14\n      1.34\n      1.25\n      1.02\n    \n    \n      22\n      Maryland\n      209\n      148\n      192\n      3.45\n      2.40\n      3.11\n    \n    \n      23\n      Massachusetts\n      178\n      142\n      151\n      2.58\n      2.02\n      2.16\n    \n    \n      24\n      Michigan\n      131\n      101\n      136\n      1.31\n      1.00\n      1.35\n    \n    \n      25\n      Minnesota\n      148\n      117\n      134\n      2.62\n      2.05\n      2.35\n    \n    \n      26\n      Mississippi\n      58\n      41\n      45\n      1.95\n      1.39\n      1.53\n    \n    \n      27\n      Missouri\n      70\n      79\n      77\n      1.14\n      1.28\n      1.25\n    \n    \n      28\n      Montana\n      2\n      4\n      3\n      0.19\n      0.37\n      0.27\n    \n    \n      29\n      Nebraska\n      17\n      33\n      22\n      0.88\n      1.68\n      1.12\n    \n    \n      30\n      Nevada\n      53\n      57\n      61\n      1.71\n      1.83\n      1.94\n    \n    \n      31\n      New Hampshire\n      6\n      12\n      12\n      0.44\n      0.87\n      0.86\n    \n    \n      32\n      New Jersey\n      310\n      245\n      279\n      3.49\n      2.64\n      3.01\n    \n    \n      33\n      New Mexico\n      41\n      29\n      24\n      1.95\n      1.37\n      1.13\n    \n    \n      34\n      New York\n      746\n      605\n      681\n      3.83\n      3.00\n      3.43\n    \n    \n      35\n      North Carolina\n      185\n      159\n      178\n      1.76\n      1.52\n      1.69\n    \n    \n      36\n      North Dakota\n      18\n      10\n      15\n      2.36\n      1.28\n      1.94\n    \n    \n      37\n      Ohio\n      150\n      130\n      149\n      1.28\n      1.10\n      1.26\n    \n    \n      38\n      Oklahoma\n      73\n      67\n      69\n      1.84\n      1.69\n      1.73\n    \n    \n      39\n      Oregon\n      70\n      67\n      78\n      1.66\n      1.58\n      1.84\n    \n    \n      40\n      Pennsylvania\n      199\n      156\n      166\n      1.55\n      1.20\n      1.28\n    \n    \n      41\n      Rhode Island\n      14\n      7\n      17\n      1.32\n      0.64\n      1.55\n    \n    \n      42\n      South Carolina\n      80\n      67\n      88\n      1.55\n      1.31\n      1.70\n    \n    \n      43\n      South Dakota\n      16\n      16\n      12\n      1.80\n      1.80\n      1.34\n    \n    \n      44\n      Tennessee\n      129\n      113\n      85\n      1.89\n      1.63\n      1.22\n    \n    \n      45\n      Texas\n      1,154\n      883\n      991\n      3.98\n      3.02\n      3.36\n    \n    \n      46\n      Utah\n      27\n      29\n      17\n      0.84\n      0.88\n      0.51\n    \n    \n      47\n      Vermont\n      4\n      3\n      2\n      0.64\n      0.47\n      0.31\n    \n    \n      48\n      Virginia\n      191\n      169\n      161\n      2.23\n      1.96\n      1.86\n    \n    \n      49\n      Washington\n      221\n      163\n      199\n      2.90\n      2.11\n      2.57\n    \n    \n      50\n      West Virginia\n      9\n      13\n      7\n      0.50\n      0.73\n      0.39\n    \n    \n      51\n      Wisconsin\n      51\n      35\n      66\n      0.88\n      0.59\n      1.12\n    \n    \n      52\n      Wyoming\n      1\n      0\n      3\n      0.17\n      0.00\n      0.52\n    \n  \n\n\n\n\nWait, what’s up with the “Unnamed” column names? And the first row, for that matter?\nCongratulations – you’re ready to wrangle your data. Because of how things are stored, we’ll need to clean the data a bit to name our columns better.\nA reasonable first step is to identify the row with the right header. The pd.read_csv() function (documentation) has the convenient header parameter:\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1) # row index\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.1\n      2020.1\n      2021.1\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nWait…but now we can’t differentiate betwen the “Number of TB cases” and “TB incidence” year columns. pandas has tried to make our lives easier by automatically adding “.1” to the latter columns, but this doesn’t help us as humans understand the data.\nWe can do this manually with df.rename() (documentation):\n\n\nCode\nrename_dict = {'2019': 'TB cases 2019',\n               '2020': 'TB cases 2020',\n               '2021': 'TB cases 2021',\n               '2019.1': 'TB incidence 2019',\n               '2020.1': 'TB incidence 2020',\n               '2021.1': 'TB incidence 2021'}\ntb_df = tb_df.rename(columns=rename_dict)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28"
  },
  {
    "objectID": "eda/eda.html#record-granularity",
    "href": "eda/eda.html#record-granularity",
    "title": "5  Data Cleaning and EDA",
    "section": "6.2 Record Granularity",
    "text": "6.2 Record Granularity\nYou might already be wondering: What’s up with that first record?\nRow 0 is what we call a rollup record, or summary record. It’s often useful when displaying tables to humans. The granularity of record 0 (Totals) vs the rest of the records (States) is different.\nOkay, EDA step two. How was the rollup record aggregated?\nLet’s check if Total TB cases is the sum of all state TB cases. If we sum over all rows, we should get 2x the total cases in each of our TB cases by year (why?).\n\n\nCode\ntb_df.sum(axis=0)\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019        8,9008758183642,111666718245583029973261085237...\nTB cases 2020        7,1737258136591,706525417194122219282169239376...\nTB cases 2021        7,8609258129691,750585443194992281064255127494...\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nWhoa, what’s going on? Check out the column types:\n\n\nCode\ntb_df.dtypes\n\n\nU.S. jurisdiction     object\nTB cases 2019         object\nTB cases 2020         object\nTB cases 2021         object\nTB incidence 2019    float64\nTB incidence 2020    float64\nTB incidence 2021    float64\ndtype: object\n\n\nLooks like those commas are causing all TB cases to be read as the object datatype, or storage type (close to the Python string datatype), so pandas is concatenating strings instead of adding integers.\nFortunately read_csv also has a thousands parameter (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):\n\n\nCode\n# improve readability: chaining method calls with outer parentheses/line breaks\ntb_df = (\n    pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1, thousands=',')\n    .rename(columns=rename_dict)\n)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ntb_df.sum()\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019                                                    17800\nTB cases 2020                                                    14346\nTB cases 2021                                                    15720\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nThe Total TB cases look right. Phew!\nLet’s just look at the records with state-level granularity:\n\n\nCode\nstate_tb_df = tb_df[1:]\nstate_tb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n    \n      5\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46"
  },
  {
    "objectID": "eda/eda.html#gather-more-data-census",
    "href": "eda/eda.html#gather-more-data-census",
    "title": "5  Data Cleaning and EDA",
    "section": "6.3 Gather More Data: Census",
    "text": "6.3 Gather More Data: Census\nU.S. Census population estimates source (2019), source (2020-2021).\nRunning the below cells cleans the data. There are a few new methods here: * df.convert_dtypes() (documentation) conveniently converts all float dtypes into ints and is out of scope for the class. * df.drop_na() (documentation) will be explained in more detail next time.\n\n\nCode\n# 2010s census data\ncensus_2010s_df = pd.read_csv(\"data/nst-est2019-01.csv\", header=3, thousands=\",\")\ncensus_2010s_df = (\n    census_2010s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Census\", \"Estimates Base\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2010s_df['Geographic Area'] = census_2010s_df['Geographic Area'].str.strip('.')\n\n# with pd.option_context('display.min_rows', 30): # shows more rows\n#     display(census_2010s_df)\n    \ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      1\n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      2\n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      3\n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      4\n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nOccasionally, you will want to modify code that you have imported. To reimport those modifications you can either use the python importlib library:\nfrom importlib import reload\nreload(utils)\nor use iPython magic which will intelligently import code when files change:\n%load_ext autoreload\n%autoreload 2\n\n\nCode\n# census 2020s data\ncensus_2020s_df = pd.read_csv(\"data/NST-EST2022-POP.csv\", header=3, thousands=\",\")\ncensus_2020s_df = (\n    census_2020s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Unnamed: 1\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2020s_df['Geographic Area'] = census_2020s_df['Geographic Area'].str.strip('.')\n\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      1\n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      2\n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      3\n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      4\n      West\n      78650958\n      78589763\n      78743364"
  },
  {
    "objectID": "eda/eda.html#joining-data-on-primary-keys",
    "href": "eda/eda.html#joining-data-on-primary-keys",
    "title": "5  Data Cleaning and EDA",
    "section": "6.4 Joining Data on Primary Keys",
    "text": "6.4 Joining Data on Primary Keys\nTime to merge! Here we use the DataFrame method df1.merge(right=df2, ...) on DataFrame df1 (documentation). Contrast this with the function pd.merge(left=df1, right=df2, ...) (documentation). Feel free to use either.\n\n\nCode\n# merge TB dataframe with two US census dataframes\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .merge(right=census_2020s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      Geographic Area_x\n      2010\n      2011\n      ...\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      Geographic Area_y\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      Alabama\n      4785437\n      4799069\n      ...\n      4841799\n      4852347\n      4863525\n      4874486\n      4887681\n      4903185\n      Alabama\n      5031362\n      5049846\n      5074296\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      Alaska\n      713910\n      722128\n      ...\n      736283\n      737498\n      741456\n      739700\n      735139\n      731545\n      Alaska\n      732923\n      734182\n      733583\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      Arizona\n      6407172\n      6472643\n      ...\n      6730413\n      6829676\n      6941072\n      7044008\n      7158024\n      7278717\n      Arizona\n      7179943\n      7264877\n      7359197\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      Arkansas\n      2921964\n      2940667\n      ...\n      2967392\n      2978048\n      2989918\n      3001345\n      3009733\n      3017804\n      Arkansas\n      3014195\n      3028122\n      3045637\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      California\n      37319502\n      37638369\n      ...\n      38596972\n      38918045\n      39167117\n      39358497\n      39461588\n      39512223\n      California\n      39501653\n      39142991\n      39029342\n    \n  \n\n5 rows × 22 columns\n\n\n\nThis is a little unwieldy. We could either drop the unneeded columns now, or just merge on smaller census DataFrames. Let’s do the latter.\n\n\nCode\n# try merging again, but cleaner this time\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"Geographic Area\", \"2019\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n    .merge(right=census_2020s_df[[\"Geographic Area\", \"2020\", \"2021\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991"
  },
  {
    "objectID": "eda/eda.html#reproducing-data-compute-incidence",
    "href": "eda/eda.html#reproducing-data-compute-incidence",
    "title": "5  Data Cleaning and EDA",
    "section": "6.5 Reproducing Data: Compute Incidence",
    "text": "6.5 Reproducing Data: Compute Incidence\nLet’s recompute incidence to make sure we know where the original CDC numbers came from.\nFrom the CDC report: TB incidence is computed as “Cases per 100,000 persons using mid-year population estimates from the U.S. Census Bureau.”\nIf we define a group as 100,000 people, then we can compute the TB incidence for a given state population as\n\\[\\text{TB incidence} = \\frac{\\text{TB cases in population}}{\\text{groups in population}} = \\frac{\\text{TB cases in population}}{\\text{population}/100000} \\]\n\\[= \\frac{\\text{TB cases in population}}{\\text{population}} \\times 100000\\]\nLet’s try this for 2019:\n\n\nCode\ntb_census_df[\"recompute incidence 2019\"] = tb_census_df[\"TB cases 2019\"]/tb_census_df[\"2019\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n    \n  \n\n\n\n\nAwesome!!!\nLet’s use a for-loop and Python format strings to compute TB incidence for all years. Python f-strings are just used for the purposes of this demo, but they’re handy to know when you explore data beyond this course (Python documentation).\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n      4.318807\n      4.470788\n    \n  \n\n\n\n\nThese numbers look pretty close!!! There are a few errors in the hundredths place, particularly in 2021. It may be useful to further explore reasons behind this discrepancy.\n\n\nCode\ntb_census_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      count\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      5.100000e+01\n      5.100000e+01\n      5.100000e+01\n      51.000000\n      51.000000\n      51.000000\n    \n    \n      mean\n      174.509804\n      140.647059\n      154.117647\n      2.102549\n      1.782941\n      1.971961\n      6.436069e+06\n      6.500226e+06\n      6.510423e+06\n      2.104969\n      1.784655\n      1.969928\n    \n    \n      std\n      341.738752\n      271.055775\n      286.781007\n      1.498745\n      1.337414\n      1.478468\n      7.360660e+06\n      7.408168e+06\n      7.394300e+06\n      1.500236\n      1.338263\n      1.474929\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.170000\n      0.000000\n      0.210000\n      5.787590e+05\n      5.776050e+05\n      5.794830e+05\n      0.172783\n      0.000000\n      0.210049\n    \n    \n      25%\n      25.500000\n      29.000000\n      23.000000\n      1.295000\n      1.210000\n      1.235000\n      1.789606e+06\n      1.820311e+06\n      1.844920e+06\n      1.297485\n      1.211433\n      1.233905\n    \n    \n      50%\n      70.000000\n      67.000000\n      69.000000\n      1.800000\n      1.520000\n      1.700000\n      4.467673e+06\n      4.507445e+06\n      4.506589e+06\n      1.808606\n      1.521612\n      1.694502\n    \n    \n      75%\n      180.500000\n      139.000000\n      150.000000\n      2.575000\n      1.990000\n      2.220000\n      7.446805e+06\n      7.451987e+06\n      7.502811e+06\n      2.577577\n      1.993607\n      2.219482\n    \n    \n      max\n      2111.000000\n      1706.000000\n      1750.000000\n      7.910000\n      7.920000\n      7.920000\n      3.951222e+07\n      3.950165e+07\n      3.914299e+07\n      7.928425\n      7.913519\n      7.899949"
  },
  {
    "objectID": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "href": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "title": "5  Data Cleaning and EDA",
    "section": "6.6 Bonus EDA: Reproducing the reported statistic",
    "text": "6.6 Bonus EDA: Reproducing the reported statistic\nHow do we reproduce that reported statistic in the original CDC report?\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nThis is TB incidence computed across the entire U.S. population! How do we reproduce this * We need to reproduce the “Total” TB incidences in our rolled record. * But our current tb_census_df only has 51 entries (50 states plus Washington, D.C.). There is no rolled record. * What happened…?\nLet’s get exploring!\nBefore we keep exploring, we’ll set all indexes to more meaningful values, instead of just numbers that pertained to some row at some point. This will make our cleaning slightly easier.\n\n\nCode\ntb_df = tb_df.set_index(\"U.S. jurisdiction\")\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ncensus_2010s_df = census_2010s_df.set_index(\"Geographic Area\")\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\ncensus_2020s_df = census_2020s_df.set_index(\"Geographic Area\")\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\nIt turns out that our merge above only kept state records, even though our original tb_df had the “Total” rolled record:\n\n\nCode\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nRecall that merge by default does an inner merge by default, meaning that it only preserves keys that are present in both DataFrames.\nThe rolled records in our census dataframes have different Geographic Area fields, which was the key we merged on:\n\n\nCode\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nThe Census DataFrame has several rolled records. The aggregate record we are looking for actually has the Geographic Area named “United States”.\nOne straightforward way to get the right merge is to rename the value itself. Because we now have the Geographic Area index, we’ll use df.rename() (documentation):\n\n\nCode\n# rename rolled record for 2010s\ncensus_2010s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\n# same, but for 2020s rename rolled record\ncensus_2020s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      Total\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\n\nNext let’s rerun our merge. Note the different chaining, because we are now merging on indexes (df.merge() documentation).\n\n\nCode\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"2019\"]],\n           left_index=True, right_index=True)\n    .merge(right=census_2020s_df[[\"2020\", \"2021\"]],\n           left_index=True, right_index=True)\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n  \n\n\n\n\n\nFinally, let’s recompute our incidences:\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n      2.711435\n      2.163726\n      2.367245\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n  \n\n\n\n\nWe reproduced the total U.S. incidences correctly!\nWe’re almost there. Let’s revisit the quote:\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nRecall that percent change from \\(A\\) to \\(B\\) is computed as \\(\\text{percent change} = \\frac{B - A}{A} \\times 100\\).\n\n\nCode\nincidence_2020 = tb_census_df.loc['Total', 'recompute incidence 2020']\nincidence_2020\n\n\n2.1637257652759883\n\n\n\n\nCode\nincidence_2021 = tb_census_df.loc['Total', 'recompute incidence 2021']\nincidence_2021\n\n\n2.3672448914298068\n\n\n\n\nCode\ndifference = (incidence_2021 - incidence_2020)/incidence_2020 * 100\ndifference\n\n\n9.405957511804143"
  }
]