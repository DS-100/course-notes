[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "pandas_1/pandas_1.html#introduction-to-exploratory-data-analysis",
    "href": "pandas_1/pandas_1.html#introduction-to-exploratory-data-analysis",
    "title": "2  Pandas I",
    "section": "2.1 Introduction to Exploratory Data Analysis",
    "text": "2.1 Introduction to Exploratory Data Analysis\nImagine you collected, or have been given a box of data. What do you do next?\n\nThe first step is to clean your data. Data cleaning often corrects issues in the structure and formatting of data, including missing values and unit conversions.\nData scientists have coined the term exploratory data analysis (EDA) to describe the process of transforming raw data to insightful observations. EDA is an open-ended analysis of transforming, visualizing, and summarizing patterns in data. In order to conduct EDA, we first need to familiarize ourselves with pandas – an important programming tool."
  },
  {
    "objectID": "pandas_1/pandas_1.html#introduction-to-pandas",
    "href": "pandas_1/pandas_1.html#introduction-to-pandas",
    "title": "2  Pandas I",
    "section": "2.2 Introduction to Pandas",
    "text": "2.2 Introduction to Pandas\npandas is a data analysis library to make data cleaning and analysis fast and convenient in Python.\nThe pandas library adopts many coding idioms from NumPy. The biggest difference is that pandas is designed for working with tabular data, one of the most common data formats (and the focus of Data 100).\nBefore writing any code, we must import pandas into our Python environment.\n\n# `pd` is the conventional alias for Pandas, as `np` is for NumPy\nimport pandas as pd"
  },
  {
    "objectID": "pandas_1/pandas_1.html#series-dataframes-and-indices",
    "href": "pandas_1/pandas_1.html#series-dataframes-and-indices",
    "title": "2  Pandas I",
    "section": "2.3 Series, DataFrames, and Indices",
    "text": "2.3 Series, DataFrames, and Indices\nThere are three fundamental data structures in pandas:\n\nSeries: 1D labeled array data; best thought of as columnar data\nDataFrame: 2D tabular data with rows and columns\nIndex: A sequence of row/column labels\n\nDataFrames, Series, and Indices can be represented visually in the following diagram.\n\nNotice how the DataFrame is a two dimensional object – it contains both rows and columns. The Series above is a singular column of this DataFrame, namely the Candidate column. Both contain an Index, or a shared list of row labels (the integers from 0 to 5, inclusive).\n\n2.3.1 Series\nA Series represents a column of a DataFrame; more generally, it can be any 1-dimensional array-like object containing values of the same type with associated data labels, called its index.\n\nimport pandas as pd\n\ns = pd.Series([-1, 10, 2])\nprint(s)\n\n0    -1\n1    10\n2     2\ndtype: int64\n\n\n\ns.array # Data contained within the Series\n\n<PandasArray>\n[-1, 10, 2]\nLength: 3, dtype: int64\n\n\n\ns.index # The Index of the Series\n\nRangeIndex(start=0, stop=3, step=1)\n\n\nBy default, row indices in pandas are a sequential list of integers beginning from 0. Optionally, a list of desired indices can be passed to the index argument.\n\ns = pd.Series([-1, 10, 2], index = [\"a\", \"b\", \"c\"])\nprint(s)\n\na    -1\nb    10\nc     2\ndtype: int64\n\n\nIndices can also be changed after initialization.\n\ns.index = [\"first\", \"second\", \"third\"]\nprint(s)\n\nfirst     -1\nsecond    10\nthird      2\ndtype: int64\n\n\n\n2.3.1.1 Selection in Series\nSimilar to an array, we can select a single value or a set of values from a Series. There are 3 primary methods of selecting data.\n\nA single index label\nA list of index labels\nA filtering condition\n\nLet’s define the following Series ser.\n\nser = pd.Series([4, -2, 0, 6], index = [\"a\", \"b\", \"c\", \"d\"])\nprint(ser)\n\na    4\nb   -2\nc    0\nd    6\ndtype: int64\n\n\n\n2.3.1.1.1 A Single Index Label\n\nprint(ser[\"a\"]) # Notice how the return value is a single array element\n\n4\n\n\n\n\n2.3.1.1.2 A List of Index Labels\n\nser[[\"a\", \"c\"]] # Notice how the return value is another Series\n\na    4\nc    0\ndtype: int64\n\n\n\n\n2.3.1.1.3 A Filtering Condition\nPerhaps the most interesting (and useful) method of selecting data from a Series is with a filtering condition.\nWe first must apply a vectorized boolean operation to our Series that encodes the filter conditon.\n\nser > 0 # Filter condition: select all elements greater than 0\n\na     True\nb    False\nc    False\nd     True\ndtype: bool\n\n\nUpon “indexing” in our Series with this condition, pandas selects only the rows with True values.\n\nser[ser > 0] \n\na    4\nd    6\ndtype: int64\n\n\n\n\n\n\n2.3.2 DataFrames\nIn Data 8, you encountered the Table class of the datascience library, which represented tabular data. In Data 100, we’ll be using the DataFrame class of the pandas library.\nHere is an example of a DataFrame that contains election data.\n\nimport pandas as pd\n\nelections = pd.read_csv(\"data/elections.csv\")\nelections\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n      loss\n      1.073699\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n      win\n      51.311515\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n      loss\n      46.858542\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n      loss\n      1.177979\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n      loss\n      0.255731\n    \n  \n\n182 rows × 6 columns\n\n\n\nLet’s dissect the code above.\n\nWe first import the pandas library into our Python environment, using the alias pd.   import pandas as pd\nThere are a number of ways to read data into a DataFrame. In Data 100, our data are typically stored in a CSV (comma-seperated values) file format. We can import a CSV file into a DataFrame by passing the data path as an argument to the following pandas function.   pd.read_csv(\"elections.csv\")\n\nThis code stores our DataFrame object in the elections variable. Upon inspection, our elections DataFrame has 182 rows and 6 columns (Year, Candidate, Party, Popular Vote, Result, %). Each row represents a single record – in our example, a presedential candidate from some particular year. Each column represents a single attribute, or feature of the record.\nIn the example above, we constructed a DataFrame object using data from a CSV file. As we’ll explore in the next section, we can create a DataFrame with data of our own.\n\n2.3.2.1 Creating a DataFrame\nThere are many ways to create a DataFrame. Here, we will cover the most popular approaches.\n\nUsing a list and column names\nFrom a dictionary\nFrom a Series\n\n\n2.3.2.1.1 Using a List and Column Names\nConsider the following examples.\n\ndf_list = pd.DataFrame([1, 2, 3], columns=[\"Numbers\"])\ndf_list\n\n\n\n\n\n  \n    \n      \n      Numbers\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      2\n    \n    \n      2\n      3\n    \n  \n\n\n\n\nThe first code cell creates a DataFrame with a single column Numbers, while the second creates a DataFrame with an additional column Description. Notice how a 2D list of values is required to initialize the second DataFrame – each nested list represents a single row of data.\n\ndf_list = pd.DataFrame([[1, \"one\"], [2, \"two\"]], columns = [\"Number\", \"Description\"])\ndf_list\n\n\n\n\n\n  \n    \n      \n      Number\n      Description\n    \n  \n  \n    \n      0\n      1\n      one\n    \n    \n      1\n      2\n      two\n    \n  \n\n\n\n\n\n\n2.3.2.1.2 From a Dictionary\nA second (and more common) way to create a DataFrame is with a dictionary. The dictionary keys represent the column names, and the dictionary values represent the column values.\n\ndf_dict = pd.DataFrame({\"Fruit\": [\"Strawberry\", \"Orange\"], \"Price\": [5.49, 3.99]})\ndf_dict\n\n\n\n\n\n  \n    \n      \n      Fruit\n      Price\n    \n  \n  \n    \n      0\n      Strawberry\n      5.49\n    \n    \n      1\n      Orange\n      3.99\n    \n  \n\n\n\n\n\n\n2.3.2.1.3 From a Series\nEarlier, we explained how a Series was synonymous to a column in a DataFrame. It follows then, that a DataFrame is equivalent to a collection of Series, which all share the same index.\nIn fact, we can initialize a DataFrame by merging two or more Series.\n\n# Notice how our indices, or row labels, are the same\n\ns_a = pd.Series([\"a1\", \"a2\", \"a3\"], index = [\"r1\", \"r2\", \"r3\"])\ns_b = pd.Series([\"b1\", \"b2\", \"b3\"], index = [\"r1\", \"r2\", \"r3\"])\n\npd.DataFrame({\"A-column\": s_a, \"B-column\": s_b})\n\n\n\n\n\n  \n    \n      \n      A-column\n      B-column\n    \n  \n  \n    \n      r1\n      a1\n      b1\n    \n    \n      r2\n      a2\n      b2\n    \n    \n      r3\n      a3\n      b3\n    \n  \n\n\n\n\n\n\n\n\n2.3.3 Indices\nThe major takeaway: we can think of a DataFrame as a collection of Series that all share the same Index.\nOn a more technical note, an Index doesn’t have to be an integer, nor does it have to be unique. For example, we can set the index of the elections Dataframe to be the name of presedential candidates. Selecting a new Series from this modified DataFrame yields the following.\n\n# This sets the index to the \"Candidate\" column\nelections.set_index(\"Candidate\", inplace=True)\n\n\nTo retrieve the indices of a DataFrame, simply use the .index attribute of the DataFrame class.\n\nelections.index\n\nIndex(['Andrew Jackson', 'John Quincy Adams', 'Andrew Jackson',\n       'John Quincy Adams', 'Andrew Jackson', 'Henry Clay', 'William Wirt',\n       'Hugh Lawson White', 'Martin Van Buren', 'William Henry Harrison',\n       ...\n       'Darrell Castle', 'Donald Trump', 'Evan McMullin', 'Gary Johnson',\n       'Hillary Clinton', 'Jill Stein', 'Joseph Biden', 'Donald Trump',\n       'Jo Jorgensen', 'Howard Hawkins'],\n      dtype='object', name='Candidate', length=182)\n\n\n\n# This resets the index to be the default list of integers\nelections.reset_index(inplace=True)"
  },
  {
    "objectID": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "href": "pandas_1/pandas_1.html#slicing-in-dataframes",
    "title": "2  Pandas I",
    "section": "2.4 Slicing in DataFrames",
    "text": "2.4 Slicing in DataFrames\nNow that we’ve learned how to create DataFrames, let’s dive deeper into their capabilities.\nThe API (application programming interface) for the DataFrame class is enormous. In this section, we’ll discuss several methods of the DataFrame API that allow us to extract subsets of data.\nThe simplest way to manipulate a DataFrame is to extract a subset of rows and columns, known as slicing. We will do so with three primary methods of the DataFrame class:\n\n.loc\n.iloc\n[]\n\n\n2.4.1 Indexing with .loc\nThe .loc operator selects rows and columns in a DataFrame by their row and column label(s), respectively. The row labels (commonly referred to as the indices) are the bold text on the far left of a DataFrame, while the column labels are the column names found at the top of a DataFrame.\nTo grab data with .loc, we must specify the row and column label(s) where the data exists. The row labels are the first argument to the .loc function; the column labels are the second. For example, we can select the the row labeled 0 and the column labeled Candidate from the elections DataFrame.\n\nelections.loc[0, 'Candidate']\n\n'Andrew Jackson'\n\n\nTo select multiple rows and columns, we can use Python slice notation. Here, we select both the first four rows and columns.\n\nelections.loc[0:3, 'Year':'Popular vote']\n\n\n\n\n\n  \n    \n      \n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nSuppose that instead, we wanted every column value for the first four rows in the elections DataFrame. The shorthand : is useful for this.\n\nelections.loc[0:3, :]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\nThere are a couple of things we should note. Unlike conventional Python, Pandas allows us to slice string values (in our example, the column labels). Secondly, slicing with .loc is inclusive. Notice how our resulting DataFrame includes every row and column between and including the slice labels we specified.\nEquivalently, we can use a list to obtain multiple rows and columns in our elections DataFrame.\n\nelections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n  \n\n\n\n\nLastly, we can interchange list and slicing notation.\n\nelections.loc[[0, 1, 2, 3], :]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\n2.4.2 Indexing with .iloc\nSlicing with .iloc works similarily to .loc, although .iloc uses the integer positions of rows and columns rather the labels. The arguments to the .iloc function also behave similarly - single values, lists, indices, and any combination of these are permitted.\nLet’s begin reproducing our results from above. We’ll begin by selecting for the first presedential candidate in our elections DataFrame:\n\n# elections.loc[0, \"Candidate\"] - Previous approach\nelections.iloc[0, 1]\n\n1824\n\n\nNotice how the first argument to both .loc and .iloc are the same. This is because the row with a label of 0 is conveniently in the 0th (or first) position of the elections DataFrame. Generally, this is true of any DataFrame where the row labels are incremented in ascending order from 0.\nHowever, when we select the first four rows and columns using .iloc, we notice something.\n\n# elections.loc[0:3, 'Year':'Popular vote'] - Previous approach\nelections.iloc[0:4, 0:4]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nSlicing is no longer inclusive in .iloc - it’s exclusive. This is one of Pandas syntatical subtleties; you’ll get used to with practice.\nList behavior works just as expected.\n\n#elections.loc[[0, 1, 2, 3], ['Year', 'Candidate', 'Party', 'Popular vote']] - Previous Approach\nelections.iloc[[0, 1, 2, 3], [0, 1, 2, 3]]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n    \n  \n\n\n\n\nThis discussion begs the question: when should we use .loc vs .iloc? In most cases, .loc is generally safer to use. You can imagine .iloc may return incorrect values when applied to a dataset where the ordering of data can change.\n\n\n2.4.3 Indexing with []\nThe [] selection operator is the most baffling of all, yet the commonly used. It only takes a single argument, which may be one of the following:\n\nA slice of row numbers\nA list of column labels\nA single column label\n\nThat is, [] is context dependent. Let’s see some examples.\n\n2.4.3.1 A slice of row numbers\nSay we wanted the first four rows of our elections DataFrame.\n\nelections[0:4]\n\n\n\n\n\n  \n    \n      \n      Candidate\n      Year\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      Andrew Jackson\n      1824\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      John Quincy Adams\n      1824\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      Andrew Jackson\n      1828\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      John Quincy Adams\n      1828\n      National Republican\n      500897\n      loss\n      43.796073\n    \n  \n\n\n\n\n\n\n2.4.3.2 A list of column labels\nSuppose we now want the first four columns.\n\nelections[[\"Year\", \"Candidate\", \"Party\", \"Popular vote\"]]\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      177\n      2016\n      Jill Stein\n      Green\n      1457226\n    \n    \n      178\n      2020\n      Joseph Biden\n      Democratic\n      81268924\n    \n    \n      179\n      2020\n      Donald Trump\n      Republican\n      74216154\n    \n    \n      180\n      2020\n      Jo Jorgensen\n      Libertarian\n      1865724\n    \n    \n      181\n      2020\n      Howard Hawkins\n      Green\n      405035\n    \n  \n\n182 rows × 4 columns\n\n\n\n\n\n2.4.3.3 A single column label\nLastly, if we only want the Candidate column.\n\nelections[\"Candidate\"]\n\n0         Andrew Jackson\n1      John Quincy Adams\n2         Andrew Jackson\n3      John Quincy Adams\n4         Andrew Jackson\n             ...        \n177           Jill Stein\n178         Joseph Biden\n179         Donald Trump\n180         Jo Jorgensen\n181       Howard Hawkins\nName: Candidate, Length: 182, dtype: object\n\n\nThe output looks like a Series! In this course, we’ll become very comfortable with [], especially for selecting columns. In practice, [] is much more common than .loc."
  },
  {
    "objectID": "pandas_1/pandas_1.html#parting-note",
    "href": "pandas_1/pandas_1.html#parting-note",
    "title": "2  Pandas I",
    "section": "2.5 Parting Note",
    "text": "2.5 Parting Note\nThe pandas library is enormous and contains many useful functions. Here is a link to documentation.\nThe introductory pandas lectures will cover important data structures and methods you should be fluent in. However, we want you to get familiar with the real world programming practice of …Googling! Answers to your questions can be found in documentation, Stack Overflow, etc.\nWith that, let’s move on to Pandas II."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "5  Data Cleaning and EDA",
    "section": "",
    "text": "6 EDA Demo: Tuberculosis in the United States\nNow, let’s follow this data-cleaning and EDA workflow to see what can we say about the presence of Tuberculosis in the United States!\nWe will examine the data included in the original CDC article published in 2021."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "5  Data Cleaning and EDA",
    "section": "5.1 Structure",
    "text": "5.1 Structure\n\n5.1.1 File Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for Comma-Separated Values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the first few rows of the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (Tab-Separated Values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\n﻿Year   Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\n5.1.2 Variable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nClassification of variable types\n\n\n\n\n5.1.3 Primary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "5  Data Cleaning and EDA",
    "section": "5.2 Granularity, Scope, and Temporality",
    "text": "5.2 Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "5  Data Cleaning and EDA",
    "section": "5.3 Faithfulness",
    "text": "5.3 Faithfulness\nAt this stage in our data cleaning and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "eda/eda.html#csvs-and-field-names",
    "href": "eda/eda.html#csvs-and-field-names",
    "title": "5  Data Cleaning and EDA",
    "section": "6.1 CSVs and Field Names",
    "text": "6.1 CSVs and Field Names\nSuppose Table 1 was saved as a CSV file located in data/cdc_tuberculosis.csv.\nWe can then explore the CSV (which is a text file, and does not contain binary-encoded data) in many ways: 1. Using a text editor like emacs, vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only), Excel, Google Sheets, etc. 3. The Python file object 4. pandas, using pd.read_csv()\n1, 2. Let’s start with the first two so we really solidify the idea of a CSV as rectangular data (i.e., tabular data) stored as comma-separated values.\n\nNext, let’s try using the Python file object. Let’s check out the first three lines:\n\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(row)\n        i += 1\n        if i > 3:\n            break\n\n\n,No. of TB cases,,,TB incidence,,\n\nU.S. jurisdiction,2019,2020,2021,2019,2020,2021\n\nTotal,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\n\nAlabama,87,72,92,1.77,1.43,1.83\n\n\n\nWhoa, why are there blank lines interspaced between the lines of the CSV?\nYou may recall that all line breaks in text files are encoded as the special newline character \\n. Python’s print() prints each string (including the newline), and an additional newline on top of that.\nIf you’re curious, we can use the repr() function to return the raw string with all special characters:\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(repr(row)) # print raw strings\n        i += 1\n        if i > 3:\n            break\n\n\n',No. of TB cases,,,TB incidence,,\\n'\n'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\\n'\n'Total,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\\n'\n'Alabama,87,72,92,1.77,1.43,1.83\\n'\n\n\n\nFinally, let’s see the tried-and-true Data 100 approach: pandas.\n\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\")\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      No. of TB cases\n      Unnamed: 2\n      Unnamed: 3\n      TB incidence\n      Unnamed: 5\n      Unnamed: 6\n    \n  \n  \n    \n      0\n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.00\n      2020.00\n      2021.00\n    \n    \n      1\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      2\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      3\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      4\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n  \n\n\n\n\nWait, what’s up with the “Unnamed” column names? And the first row, for that matter?\nCongratulations – you’re ready to wrangle your data. Because of how things are stored, we’ll need to clean the data a bit to name our columns better.\nA reasonable first step is to identify the row with the right header. The pd.read_csv() function (documentation) has the convenient header parameter:\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1) # row index\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.1\n      2020.1\n      2021.1\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nWait…but now we can’t differentiate betwen the “Number of TB cases” and “TB incidence” year columns. pandas has tried to make our lives easier by automatically adding “.1” to the latter columns, but this doesn’t help us as humans understand the data.\nWe can do this manually with df.rename() (documentation):\n\n\nCode\nrename_dict = {'2019': 'TB cases 2019',\n               '2020': 'TB cases 2020',\n               '2021': 'TB cases 2021',\n               '2019.1': 'TB incidence 2019',\n               '2020.1': 'TB incidence 2020',\n               '2021.1': 'TB incidence 2021'}\ntb_df = tb_df.rename(columns=rename_dict)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28"
  },
  {
    "objectID": "eda/eda.html#record-granularity",
    "href": "eda/eda.html#record-granularity",
    "title": "5  Data Cleaning and EDA",
    "section": "6.2 Record Granularity",
    "text": "6.2 Record Granularity\nYou might already be wondering: What’s up with that first record?\nRow 0 is what we call a rollup record, or summary record. It’s often useful when displaying tables to humans. The granularity of record 0 (Totals) vs the rest of the records (States) is different.\nOkay, EDA step two. How was the rollup record aggregated?\nLet’s check if Total TB cases is the sum of all state TB cases. If we sum over all rows, we should get 2x the total cases in each of our TB cases by year (why?).\n\n\nCode\ntb_df.sum(axis=0)\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019        8,9008758183642,111666718245583029973261085237...\nTB cases 2020        7,1737258136591,706525417194122219282169239376...\nTB cases 2021        7,8609258129691,750585443194992281064255127494...\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nWhoa, what’s going on? Check out the column types:\n\n\nCode\ntb_df.dtypes\n\n\nU.S. jurisdiction     object\nTB cases 2019         object\nTB cases 2020         object\nTB cases 2021         object\nTB incidence 2019    float64\nTB incidence 2020    float64\nTB incidence 2021    float64\ndtype: object\n\n\nLooks like those commas are causing all TB cases to be read as the object datatype, or storage type (close to the Python string datatype), so pandas is concatenating strings instead of adding integers.\nFortunately read_csv also has a thousands parameter (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):\n\n\nCode\n# improve readability: chaining method calls with outer parentheses/line breaks\ntb_df = (\n    pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1, thousands=',')\n    .rename(columns=rename_dict)\n)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ntb_df.sum()\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019                                                    17800\nTB cases 2020                                                    14346\nTB cases 2021                                                    15720\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nThe Total TB cases look right. Phew!\nLet’s just look at the records with state-level granularity:\n\n\nCode\nstate_tb_df = tb_df[1:]\nstate_tb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n    \n      5\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46"
  },
  {
    "objectID": "eda/eda.html#gather-more-data-census",
    "href": "eda/eda.html#gather-more-data-census",
    "title": "5  Data Cleaning and EDA",
    "section": "6.3 Gather More Data: Census",
    "text": "6.3 Gather More Data: Census\nU.S. Census population estimates source (2019), source (2020-2021).\nRunning the below cells cleans the data. There are a few new methods here: * df.convert_dtypes() (documentation) conveniently converts all float dtypes into ints and is out of scope for the class. * df.drop_na() (documentation) will be explained in more detail next time.\n\n\nCode\n# 2010s census data\ncensus_2010s_df = pd.read_csv(\"data/nst-est2019-01.csv\", header=3, thousands=\",\")\ncensus_2010s_df = (\n    census_2010s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Census\", \"Estimates Base\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2010s_df['Geographic Area'] = census_2010s_df['Geographic Area'].str.strip('.')\n\n# with pd.option_context('display.min_rows', 30): # shows more rows\n#     display(census_2010s_df)\n    \ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      1\n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      2\n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      3\n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      4\n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nOccasionally, you will want to modify code that you have imported. To reimport those modifications you can either use the python importlib library:\nfrom importlib import reload\nreload(utils)\nor use iPython magic which will intelligently import code when files change:\n%load_ext autoreload\n%autoreload 2\n\n\nCode\n# census 2020s data\ncensus_2020s_df = pd.read_csv(\"data/NST-EST2022-POP.csv\", header=3, thousands=\",\")\ncensus_2020s_df = (\n    census_2020s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Unnamed: 1\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2020s_df['Geographic Area'] = census_2020s_df['Geographic Area'].str.strip('.')\n\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      1\n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      2\n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      3\n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      4\n      West\n      78650958\n      78589763\n      78743364"
  },
  {
    "objectID": "eda/eda.html#joining-data-on-primary-keys",
    "href": "eda/eda.html#joining-data-on-primary-keys",
    "title": "5  Data Cleaning and EDA",
    "section": "6.4 Joining Data on Primary Keys",
    "text": "6.4 Joining Data on Primary Keys\nTime to merge! Here we use the DataFrame method df1.merge(right=df2, ...) on DataFrame df1 (documentation). Contrast this with the function pd.merge(left=df1, right=df2, ...) (documentation). Feel free to use either.\n\n\nCode\n# merge TB dataframe with two US census dataframes\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .merge(right=census_2020s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      Geographic Area_x\n      2010\n      2011\n      ...\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      Geographic Area_y\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      Alabama\n      4785437\n      4799069\n      ...\n      4841799\n      4852347\n      4863525\n      4874486\n      4887681\n      4903185\n      Alabama\n      5031362\n      5049846\n      5074296\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      Alaska\n      713910\n      722128\n      ...\n      736283\n      737498\n      741456\n      739700\n      735139\n      731545\n      Alaska\n      732923\n      734182\n      733583\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      Arizona\n      6407172\n      6472643\n      ...\n      6730413\n      6829676\n      6941072\n      7044008\n      7158024\n      7278717\n      Arizona\n      7179943\n      7264877\n      7359197\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      Arkansas\n      2921964\n      2940667\n      ...\n      2967392\n      2978048\n      2989918\n      3001345\n      3009733\n      3017804\n      Arkansas\n      3014195\n      3028122\n      3045637\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      California\n      37319502\n      37638369\n      ...\n      38596972\n      38918045\n      39167117\n      39358497\n      39461588\n      39512223\n      California\n      39501653\n      39142991\n      39029342\n    \n  \n\n5 rows × 22 columns\n\n\n\nThis is a little unwieldy. We could either drop the unneeded columns now, or just merge on smaller census DataFrames. Let’s do the latter.\n\n\nCode\n# try merging again, but cleaner this time\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"Geographic Area\", \"2019\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n    .merge(right=census_2020s_df[[\"Geographic Area\", \"2020\", \"2021\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991"
  },
  {
    "objectID": "eda/eda.html#reproducing-data-compute-incidence",
    "href": "eda/eda.html#reproducing-data-compute-incidence",
    "title": "5  Data Cleaning and EDA",
    "section": "6.5 Reproducing Data: Compute Incidence",
    "text": "6.5 Reproducing Data: Compute Incidence\nLet’s recompute incidence to make sure we know where the original CDC numbers came from.\nFrom the CDC report: TB incidence is computed as “Cases per 100,000 persons using mid-year population estimates from the U.S. Census Bureau.”\nIf we define a group as 100,000 people, then we can compute the TB incidence for a given state population as\n\\[\\text{TB incidence} = \\frac{\\text{TB cases in population}}{\\text{groups in population}} = \\frac{\\text{TB cases in population}}{\\text{population}/100000} \\]\n\\[= \\frac{\\text{TB cases in population}}{\\text{population}} \\times 100000\\]\nLet’s try this for 2019:\n\n\nCode\ntb_census_df[\"recompute incidence 2019\"] = tb_census_df[\"TB cases 2019\"]/tb_census_df[\"2019\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n    \n  \n\n\n\n\nAwesome!!!\nLet’s use a for-loop and Python format strings to compute TB incidence for all years. Python f-strings are just used for the purposes of this demo, but they’re handy to know when you explore data beyond this course (Python documentation).\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n      4.318807\n      4.470788\n    \n  \n\n\n\n\nThese numbers look pretty close!!! There are a few errors in the hundredths place, particularly in 2021. It may be useful to further explore reasons behind this discrepancy.\n\n\nCode\ntb_census_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      count\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      5.100000e+01\n      5.100000e+01\n      5.100000e+01\n      51.000000\n      51.000000\n      51.000000\n    \n    \n      mean\n      174.509804\n      140.647059\n      154.117647\n      2.102549\n      1.782941\n      1.971961\n      6.436069e+06\n      6.500226e+06\n      6.510423e+06\n      2.104969\n      1.784655\n      1.969928\n    \n    \n      std\n      341.738752\n      271.055775\n      286.781007\n      1.498745\n      1.337414\n      1.478468\n      7.360660e+06\n      7.408168e+06\n      7.394300e+06\n      1.500236\n      1.338263\n      1.474929\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.170000\n      0.000000\n      0.210000\n      5.787590e+05\n      5.776050e+05\n      5.794830e+05\n      0.172783\n      0.000000\n      0.210049\n    \n    \n      25%\n      25.500000\n      29.000000\n      23.000000\n      1.295000\n      1.210000\n      1.235000\n      1.789606e+06\n      1.820311e+06\n      1.844920e+06\n      1.297485\n      1.211433\n      1.233905\n    \n    \n      50%\n      70.000000\n      67.000000\n      69.000000\n      1.800000\n      1.520000\n      1.700000\n      4.467673e+06\n      4.507445e+06\n      4.506589e+06\n      1.808606\n      1.521612\n      1.694502\n    \n    \n      75%\n      180.500000\n      139.000000\n      150.000000\n      2.575000\n      1.990000\n      2.220000\n      7.446805e+06\n      7.451987e+06\n      7.502811e+06\n      2.577577\n      1.993607\n      2.219482\n    \n    \n      max\n      2111.000000\n      1706.000000\n      1750.000000\n      7.910000\n      7.920000\n      7.920000\n      3.951222e+07\n      3.950165e+07\n      3.914299e+07\n      7.928425\n      7.913519\n      7.899949"
  },
  {
    "objectID": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "href": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "title": "5  Data Cleaning and EDA",
    "section": "6.6 Bonus EDA: Reproducing the reported statistic",
    "text": "6.6 Bonus EDA: Reproducing the reported statistic\nHow do we reproduce that reported statistic in the original CDC report?\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nThis is TB incidence computed across the entire U.S. population! How do we reproduce this * We need to reproduce the “Total” TB incidences in our rolled record. * But our current tb_census_df only has 51 entries (50 states plus Washington, D.C.). There is no rolled record. * What happened…?\nLet’s get exploring!\nBefore we keep exploring, we’ll set all indexes to more meaningful values, instead of just numbers that pertained to some row at some point. This will make our cleaning slightly easier.\n\n\nCode\ntb_df = tb_df.set_index(\"U.S. jurisdiction\")\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ncensus_2010s_df = census_2010s_df.set_index(\"Geographic Area\")\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\ncensus_2020s_df = census_2020s_df.set_index(\"Geographic Area\")\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\nIt turns out that our merge above only kept state records, even though our original tb_df had the “Total” rolled record:\n\n\nCode\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nRecall that merge by default does an inner merge by default, meaning that it only preserves keys that are present in both DataFrames.\nThe rolled records in our census dataframes have different Geographic Area fields, which was the key we merged on:\n\n\nCode\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nThe Census DataFrame has several rolled records. The aggregate record we are looking for actually has the Geographic Area named “United States”.\nOne straightforward way to get the right merge is to rename the value itself. Because we now have the Geographic Area index, we’ll use df.rename() (documentation):\n\n\nCode\n# rename rolled record for 2010s\ncensus_2010s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\n# same, but for 2020s rename rolled record\ncensus_2020s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      Total\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\n\nNext let’s rerun our merge. Note the different chaining, because we are now merging on indexes (df.merge() documentation).\n\n\nCode\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"2019\"]],\n           left_index=True, right_index=True)\n    .merge(right=census_2020s_df[[\"2020\", \"2021\"]],\n           left_index=True, right_index=True)\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n  \n\n\n\n\n\nFinally, let’s recompute our incidences:\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n      2.711435\n      2.163726\n      2.367245\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n  \n\n\n\n\nWe reproduced the total U.S. incidences correctly!\nWe’re almost there. Let’s revisit the quote:\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nRecall that percent change from \\(A\\) to \\(B\\) is computed as \\(\\text{percent change} = \\frac{B - A}{A} \\times 100\\).\n\n\nCode\nincidence_2020 = tb_census_df.loc['Total', 'recompute incidence 2020']\nincidence_2020\n\n\n2.1637257652759883\n\n\n\n\nCode\nincidence_2021 = tb_census_df.loc['Total', 'recompute incidence 2021']\nincidence_2021\n\n\n2.3672448914298068\n\n\n\n\nCode\ndifference = (incidence_2021 - incidence_2020)/incidence_2020 * 100\ndifference\n\n\n9.405957511804143"
  },
  {
    "objectID": "sampling/sampling.html#censuses-and-surveys",
    "href": "sampling/sampling.html#censuses-and-surveys",
    "title": "9  Sampling",
    "section": "9.1 Censuses and Surveys",
    "text": "9.1 Censuses and Surveys\nIn general: a census is “an official count or survey of a population, typically recording various details of individuals.”\n\nExample: The U.S. Decennial Census was held in April 2020, and it counts every person living in all 50 states, DC, and US territories. (Not just citizens.) Participation is required by law (it is mandated by the U.S. Constitution). Important uses include the allocation of Federal funds, congressional representation, and drawing congressional and state legislative districts. The census is composed of a survey mailed to different housing addresses in the United States.\nIndividuals in a population are not always people. Other populations include: bacteria in your gut (sampled using DNA sequencing); trees of a certain species; small businesses receiving a microloan; or published results in an academic journal / field.\n\nA survey is a set of questions. An example is workers sampling individuals and households. What is asked, and how it is asked, can affect how the respondent answers, or even whether the respondent answers in the first place.\nWhile censuses are great, it is often difficult and expensive to survey everyone in a population. Thus, we usually survey a subset of the population instead.\nA sample is often used to make inferences about the population. That being said, how the sample is drawn will affect the reliability of such inferences. Two common source of error in sampling are chance error, where random samples can vary from what is expected, in any direction; and bias, which is a a systematic error in one direction.\nBecause of how surveys and samples are drawn, it turns out that samples are usually—but not always—a subset of the population: * Population: The group that you want to learn something about. * Sampling Frame: The list from which the sample is drawn. For example, if sampling people, then the sampling frame is the set of all people that could possibly end up in your sample. * Sample: Who you actually end up sampling. The sample is therefore a subset of your sampling frame.\nWhile ideally these three sets would be exactly the same, in practice they usually aren’t. For example, there may be individuals in your sampling frame (and hence, your sample) that are not in your population. And generally, sample sizes are much smaller than population sizes.\n\n\n\nSampling_Frames"
  },
  {
    "objectID": "sampling/sampling.html#bias-a-case-study",
    "href": "sampling/sampling.html#bias-a-case-study",
    "title": "9  Sampling",
    "section": "9.2 Bias: A Case Study",
    "text": "9.2 Bias: A Case Study\nThe following case study is adapted from Statistics by Freedman, Pisani, and Purves, W.W. Norton NY, 1978.\nIn 1936, President Franklin D. Roosevelt (D) went up for re-election against Alf Landon (R) . As is usual, polls were conducted in the months leading up to the election to try and predict the outcome. The Literary Digest was a magazine that had successfully predicted the outcome of 5 general elections coming into 1936. In their polling for the 1936 election, they sent out their survey to 10 million individuals, who they found from phone books, lists of magazine subscribers, and lists of country club members. Of the roughly 2.4 million people who filled out the survey, only 43% reported they would vote for Roosevelt; thus the Digest predicted that Landon would win.\nOn election day, Roosevelt won in a landslide, winning 61% of the popular vote of about 45 million voters. How could the Digest have been so wrong with their polling?\nIt turns out that the Literary Digest sample was not representative of the population. Their sampling frame inherently skewed towards more affluent voters, who tended to vote Republican, and they completely overlooked the lion’s share of voters who were still suffering through the Great Depression. Furthermore, they had a dismal response rate (about 24%); who knows how the other non-respondents would have polled? The Digest folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. His estimate (56% Roosevelt) was much closer despite having a smaller sample size of “only” 50,000 (still more than necessary; more when we cover the Central Limit Theorem). Gallup also predicted the Digest’s prediction within 1%, with a sample size of only 3000 people. He did so by anticipating the Digest’s affluent sampling frame and subsampled those individuals. The Gallup Poll today is one of the leading polls for election results.\nSo what’s the moral of the story? Samples, while convenient, are subject to chance error and bias. Election polling, in particular, can involve many sources of bias. To name a few: * Selection bias systematically excludes (or favors) particular groups. * Response bias occurs because people don’t always respond truthfully. Survey designers pay special detail to the nature and wording of questions to avoid this type of bias. * Non-response bias occurs because people don’t always respond to survey requests, which can skew responses. For example, the Gallup poll is conducted through landline phone calls, but many different populations in the U.S. do not pay for a landline, and still more do not always answer the phone. Surveyers address this bias by staying persistent and keeping surveys short."
  },
  {
    "objectID": "sampling/sampling.html#probability-samples",
    "href": "sampling/sampling.html#probability-samples",
    "title": "9  Sampling",
    "section": "9.3 Probability Samples",
    "text": "9.3 Probability Samples\nWhen sampling, it is essential to focus on the quality of the sample rather than the quantity of the sample. A huge sample size does not fix a bad sampling method. Our main goal is to gather a sample that is representative of the population it came from. The most common way to accomplish this is by randomly sampling from the population.\n\nA convenience sample is whatever you can get ahold of. Note that haphazard sampling is not necessarily random sampling; there are many potential sources of bias.\nIn a probability sample, we know the chance any given set of individuals will be in the sample.\n\nProbability samples allow us to estimate the bias and chance error, which helps us quantify uncertainty (more in a future lecture).\nNote that this does not imply that all individuals in the population need have the same chance of being selected (see: stratified random samples).\nFurther note that the real world is usually more complicated. For example, we do not generally know the probability that a given bacterium is in a microbiome sample, or whether people will answer when Gallup calls landlines. That being said, we try to model probability sampling where possible if the sampling or measurement process is not fully under our control.\n\n\nA few common random sampling schemes: * A random sample with replacement is a sample drawn uniformly at random with replacement. * Random doesn’t always mean “uniformly at random,” but in this specific context, it does. * Some individuals in the population might get picked more than once\n\nA simple random sample (SRS) is a sample drawn uniformly at random without replacement.\n\nEvery individual (and subset of individuals) has the same chance of being selected.\nEvery pair has the same chance as every other pair.\nEvery triple has the same chance as every other triple.\nAnd so on.\n\nA stratified random sample, where random sampling is performed on strata (specific groups), and the groups together compose a sample.\n\n\n9.3.1 Example: Stratified random sample\nSuppose that we are trying to run a poll to predict the mayoral election in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose we try a stratified random sample to select 100 voters as follows: 1. First, we take a simple random sample and obtain 50 voters that are above the median city income (“above-median-income”), i.e., in the upper 50-th percentile of income in the city. 2. We then take a simple random sample of the other 50 from voters that are below the median city income.\nThis is a probability sample: For any group of 100 people, if there are not exactly 50 “above-median-income” voters, then that group has zero probability of being chosen. For any other group (which has exactly 50 “above-median-income” voters), then the chance of it being chosen is 1/ # of such groups.\nNote that even if we replace the group counts with 80/20 (80 “above-median-income” voters, 20 others), then it is still a probability sample, because we can compute the precise probability of each group being chosen. However, the sampling scheme (and thus the modeling of voter preferences) becomes biased towards voters with income above the median."
  },
  {
    "objectID": "sampling/sampling.html#approximating-simple-random-sampling",
    "href": "sampling/sampling.html#approximating-simple-random-sampling",
    "title": "9  Sampling",
    "section": "9.4 Approximating Simple Random Sampling",
    "text": "9.4 Approximating Simple Random Sampling\nThe following is a very common situation in data science: - We have an enormous population. - We can only afford to sample a relatively small number of individuals. If the population is huge compared to the sample, then random sampling with and without replacement are pretty much the same.\nExample : Suppose there are 10,000 people in a population. Exactly 7,500 of them like Snack 1; the other 2,500 like Snack 2. What is the probability that in a random sample of 20, all people like Snack 1?\n\nMethod 1: SRS (Random Sample Without Replacement): \\(\\prod\\limits_{k=0}^{19} \\dfrac{7500 - k}{10000 - k} \\approx 0.003151\\)\nMethod 2: Random Sample with Replacement: \\((0.75)^{20} \\approx 0.003171\\)\n\nAs seen here, when the population size is large, probabilities of sampling with replacement are much easier to compute and lead to a reasonable approximation.\n\n9.4.1 Multinomial Probabilities\nThe approximation discussed above suggests the convenience of multinomial probabilities, which arise from sampling a categorical distribution at random **with replacement*.\nSuppose that we have a bag of marbles with the following distribution: 60% are blue, 30% are green, and 10% are red. If we then proceed to draw 100 marbles from this bag, at random with replacement, then the resulting 100-size sample is modeled as a multinomial distribution using np.random.multinomial:\n\nimport numpy as np\nnp.random.multinomial(100, [0.60, 0.30, 0.10])\n\narray([64, 30,  6])\n\n\nThis method allows us to generate, say, 10 samples of size 100 using the size parameter:\n\nnp.random.multinomial(100, [0.60, 0.30, 0.10], size=10)\n\narray([[62, 28, 10],\n       [56, 28, 16],\n       [52, 39,  9],\n       [62, 28, 10],\n       [61, 31,  8],\n       [57, 34,  9],\n       [59, 28, 13],\n       [47, 40, 13],\n       [70, 22,  8],\n       [61, 27, 12]])"
  },
  {
    "objectID": "sampling/sampling.html#comparing-convenience-sample-and-srs",
    "href": "sampling/sampling.html#comparing-convenience-sample-and-srs",
    "title": "9  Sampling",
    "section": "9.5 Comparing Convenience Sample and SRS",
    "text": "9.5 Comparing Convenience Sample and SRS\nSuppose that we are trying to run a poll to predict the mayoral election in Bearkeley City (an imaginary city that neighbors Berkeley). Suppose we took a sample to predict the election outcome by polling all retirees. Even if they answer truthfully, we have a convenience sample. How biased would this sample be in predicting the results? While we will not numerically quantify the bias, in this demo we’ll visually show that because of the voter population distribution, any error in our prediction from a retiree sample cannot be simply due to chance:\nFirst, let’s grab a data set that has every single voter in the Bearkeley (again, this is a fake dataset) and how they actually voted in the election. For the purposes of this example, assume: * “high income” indicates a voter is above the median household income, which is $97,834 (actual Berkeley number). * There are only two mayoral candidates: one Democrat and one Republican. * Every registered voter votes in the election for the candidate under their registered party (Dem or Rep).\n\nimport pandas as pd\nimport numpy as np\nbearkeley = pd.read_csv(\"data/bearkeley.csv\")\n\n# create a 1/0 int that indicates democratic vote\nbearkeley['vote.dem'] = (bearkeley['vote'] == 'Dem').astype(int)\nbearkeley.head()\n\n\n\n\n\n  \n    \n      \n      age\n      high_income\n      vote\n      vote.dem\n    \n  \n  \n    \n      0\n      35\n      False\n      Dem\n      1\n    \n    \n      1\n      42\n      True\n      Rep\n      0\n    \n    \n      2\n      55\n      False\n      Dem\n      1\n    \n    \n      3\n      77\n      True\n      Rep\n      0\n    \n    \n      4\n      31\n      False\n      Dem\n      1\n    \n  \n\n\n\n\n\nbearkeley.shape\n\n(1300000, 4)\n\n\n\nactual_vote = np.mean(bearkeley[\"vote.dem\"])\nactual_vote\n\n0.5302792307692308\n\n\nThis is the actual outcome of the election. Based on this result, the Democratic candidate would win. However, if we were to only consider retiree voters (a retired person is anyone age 65 and up):\n\nconvenience_sample = bearkeley[bearkeley['age'] >= 65]\nnp.mean(convenience_sample[\"vote.dem\"])\n\n0.3744755089093924\n\n\nBased on this result, we would have predicted that the Republican candidate would win! This error is not due to the sample being too small to yield accurate predictions, because there are 359,396 retirees (about 27% of the 1.3 million Bearkeley voters). Instead, there seems to be something larger happening. Let’s visualize the voter preferences of the entire population to see how retirees trend:\nLet us aggregate all voters by age and visualize the fraction of Democratic voters, split by income.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nsns.set_theme(style='darkgrid', font_scale = 1.5,\n              rc={'figure.figsize':(7,5)})\n\n# aggregate all voters by age\nvotes_by_demo = bearkeley.groupby([\"age\",\"high_income\"]).agg(\"mean\").reset_index()\n\nfig = plt.figure();\nred_blue = [\"#bf1518\", \"#397eb7\"]\nwith sns.color_palette(sns.color_palette(red_blue)):\n    ax = sns.pointplot(data=votes_by_demo, x = \"age\", y = \"vote.dem\", hue = \"high_income\")\n\nax.set_title(\"Voting preferences by demographics\")\nfig.canvas.draw()\nnew_ticks = [i.get_text() for i in ax.get_xticklabels()];\nplt.xticks(range(0, len(new_ticks), 10), new_ticks[::10]);\n\n\n\n\nFrom the plot above, we see that retirees in the imaginary city of Bearkeley tend to vote less Democrat, which skewed our predictions from our sample. We also note that high-income voters tend to vote less Democrat (and more Republican).\nLet’s compare our biased convenience sample to a simple random sample. Supposing we took an SRS the same size as our retiree sample, we see that we get a result very close to the actual vote:\n\n## By default, replace = False\nn = len(convenience_sample)\nrandom_sample = bearkeley.sample(n, replace = False)\n\nnp.mean(random_sample[\"vote.dem\"])\n\n0.5315000723435988\n\n\nThis is very close to the actual vote!\nWe could even get pretty close with a much smaller sample size, say 800:\nIt turns out that we are pretty close, much smaller sample size, say, 800 (we’ll learn how to choose this number when we introduce the Central Limit Theorem):\n\nn = 800\nrandom_sample = bearkeley.sample(n, replace = False)\nnp.mean(random_sample[\"vote.dem\"])\n\n0.54625\n\n\nTo visualize the chance error in an SRS, let’s simulate 1000 samples of the 800-size Simple Random Sample:\n\npoll_result = []\nnrep = 1000   # number of simulations\nn = 800       # size of our sample\nfor i in range(0,nrep):\n    random_sample = bearkeley.sample(n, replace = False)\n    poll_result.append(np.mean(random_sample[\"vote.dem\"]))\nsns.histplot(poll_result, stat='density', kde=True)\n\n# What fraction of these simulated samples would have predicted Democrat?\npoll_result = pd.Series(poll_result)\nnp.sum(poll_result >= 0.5)/1000\n\n0.959\n\n\n\n\n\nA few observations: First, the KDE looks roughly Gaussian. Second, supposing that we predicted a Democratic winner if 50% of our sample voted Democrat, then just about 4% of our simulated samples would have predicted the election result incorrectly. This visualization further justifies why our convenience sample had error that was not entirely just due to chance. We’ll revisit this notion later in the course."
  },
  {
    "objectID": "sampling/sampling.html#summary",
    "href": "sampling/sampling.html#summary",
    "title": "9  Sampling",
    "section": "9.6 Summary",
    "text": "9.6 Summary\nUnderstanding the sampling process is what lets us go from describing the data to understanding the world. Without knowing / assuming something about how the data were collected, there is no connection between the sample and the population. Ultimately, the dataset doesn’t tell us about the world behind the data."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#what-is-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#what-is-a-model",
    "title": "10  Introduction to Modeling",
    "section": "10.1 What is a Model?",
    "text": "10.1 What is a Model?\nA model is an idealized representation of a system. A system is a set of principles or procedures according to which something functions. We live in a world full of systems: the procedure of turning on a light happens according to a specific set of rules dictating the flow of electricity. The truth behind how any event occurs are usually complex, and many times the specifics are unknown. The workings of the world can be viewed is its own giant procedure. Models seek to simplify the world and distill them it into workable pieces.\nExample: We model the fall of an object on Earth as subject to a constant acceleration of \\(9.81 \\frac{m}{s^2}\\) due to gravity.\n\nWhile this describes the behavior of our system, it is merely an approximation.\nIt doesn’t account for the effects of air resistance, local variations in gravity, etc.\nIn practice, it’s accurate enough to be useful!\n\n\n10.1.1 Reasons for building models\nOften times, (1) we care about creating models that are simple and interpretable, allowing us to understand what the relationships between our variables are. Other times, (2) we care more about making extremely accurate predictions, at the cost of having an uninterpretable model. These are sometimes called black-box models, and are common in fields like deep learning.\n\nTo understand complex phenomena occurring in the world we live in.\n\nWhat factors play a role in the growth of COVID-19?\nHow do an object’s velocity and acceleration impact how far it travels? (Physics: \\(d = d_0 + vt + \\frac{1}{2}at^2\\))\n\nTo make accurate predictions about unseen data.\n\nCan we predict if an email is spam or not?\nCan we generate a one-sentence summary of this 10-page long article?\n\n\n\n\n10.1.2 Common Types of Models\nIn general, models can be split into two categories:\nNote: These specific models are not in the scope of Data 100 and exist to serve as motivation.\n\nDeterministic physical (mechanistic) models: Laws that govern how the world works.\n\nKepler’s Third Law of Planetary Motion (1619): The ratio of the square of an object’s orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.\n\n\\(T^2 \\propto R^3\\)\n\nNewton’s Laws: motion and gravitation (1687): Newton’s second law of motion models the relationship between the mass of an object and the force required to accelerate it.\n\n\\(F = ma\\)\n\\(F_g = G \\frac{m_1 m_2}{r^2}\\)\n\n\nProbabilistic models: models that attempt to understand how random processes evolve. These are more general and can be used describe many phenomena in the real world. These models commonly make simplifying assumption about the nature of the world.\n\nPoisson Process models: Used to model random events that can happen with some probability at any point in time and are strictly increasing in count, such as the arrival of customers at a store."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#simple-linear-regression",
    "href": "intro_to_modeling/intro_to_modeling.html#simple-linear-regression",
    "title": "10  Introduction to Modeling",
    "section": "10.2 Simple Linear Regression",
    "text": "10.2 Simple Linear Regression\nThe regression line is the unique straight line that minimizes the mean squared error of estimation among all straight lines. As with any straight line, it can be defined by a slope and a y-intercept:\n\nslope: \\(r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}\\)\ny-intercept: \\(\\text{average of y} - \\text{slope}\\cdot\\text{average of x}\\)\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Set random seed for consistency \nnp.random.seed(43)\nplt.style.use('default') \n\n#Generate random noise for plotting\nx = np.linspace(-3, 3, 100)\ny = x * 0.5 - 1 + np.random.randn(100) * 0.3\n\n#plot regression line\nsns.regplot(x=x,y=y);\n\n\n\n\n\n\n10.2.1 Definitions\nFor a random variable x:\n\nMean: \\(\\bar{x}\\)\nStandard Deviation: \\(\\sigma_x\\)\nPredicted value: \\(\\hat{x}\\)\n\n\n10.2.1.1 Standard Units\nA random variable is represented in standard units if the following are true:\n\n0 in standard units is the mean (\\(\\bar{x}\\)) in the original variable’s units.\nAn increase of 1 standard unit is an increase of 1 standard deviation(\\(\\sigma_x\\)) in the original variable’s units\n\n\n\n10.2.1.2 Correlation\nThe correlation (\\(r\\)) is the average of the product of \\(x\\) and \\(y\\), both measured in standard units. Correlation measures the strength of a linear association between two variables.\n\n\\(r = \\frac{1}{n} \\sum_1^n (\\frac{x_i - \\bar{x}}{\\sigma_x})(\\frac{y_i - \\bar{y}}{\\sigma_y})\\)\nCorrelations are between -1 and 1: \\(|r| < 1\\)\n\n\n\nCode\ndef plot_and_get_corr(ax, x, y, title):\n    ax.set_xlim(-3, 3)\n    ax.set_ylim(-3, 3)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.scatter(x, y, alpha = 0.73)\n    r = np.corrcoef(x, y)[0, 1]\n    ax.set_title(title + \" (corr: {})\".format(r.round(2)))\n    return r\n\nfig, axs = plt.subplots(2, 2, figsize = (10, 10))\n\n# Just noise\nx1, y1 = np.random.randn(2, 100)\ncorr1 = plot_and_get_corr(axs[0, 0], x1, y1, title = \"noise\")\n\n# Strong linear\nx2 = np.linspace(-3, 3, 100)\ny2 = x2 * 0.5 - 1 + np.random.randn(100) * 0.3\ncorr2 = plot_and_get_corr(axs[0, 1], x2, y2, title = \"strong linear\")\n\n# Unequal spread\nx3 = np.linspace(-3, 3, 100)\ny3 = - x3/3 + np.random.randn(100)*(x3)/2.5\ncorr3 = plot_and_get_corr(axs[1, 0], x3, y3, title = \"strong linear\")\nextent = axs[1, 0].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n\n# Strong non-linear\nx4 = np.linspace(-3, 3, 100)\ny4 = 2*np.sin(x3 - 1.5) + np.random.randn(100) * 0.3\ncorr4 = plot_and_get_corr(axs[1, 1], x4, y4, title = \"strong non-linear\")\n\nplt.show()\n\n\n\n\n\n\n\n\n10.2.2 Alternate Form\nWhen the variables \\(y\\) and \\(x\\) are measured in standard units, the regression line for predicting \\(y\\) based on \\(x\\) has slope \\(r\\) and passes through the origin.\n\n${su} = r x{su} $\nBoth measured in standard units\n\n\nIn the original units, this becomes\n\n\\(\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}\\)\n\n\n\n\n10.2.3 Derivation\nStarting from the top, we have our claimed form of the regression line and we want to show that its equivalent to the optimal linear regression line: \\(\\hat{y} = \\hat{a} + \\hat{b}x\\)\nRecall:\n\n\\(\\hat{b}\\): \\(r \\cdot \\frac{\\text{Standard Deviation of y}}{\\text{Standard Deviation of x}}\\)\n\\(\\hat{a}\\): \\(\\text{average of y} - \\text{slope}\\cdot\\text{average of x}\\)\n\nProof:\n\\[\\frac{\\hat{y} - \\bar{y}}{\\sigma_y} = r \\cdot \\frac{x - \\bar{x}}{\\sigma_x}\\]\nMultiply by \\(\\sigma_y\\) and add \\(\\bar{y}\\) on both sides.\n\\[\\hat{y} = \\sigma_y \\cdot r \\cdot \\frac{x - \\bar{x}}{\\sigma_x} + \\bar{y}\\]\nDistribute coefficient \\(\\sigma_{y}\\cdot r\\) to the \\(\\frac{x - \\bar{x}}{\\sigma_x}\\) term\n\\[\\hat{y} = (\\frac{r\\sigma_y}{\\sigma_x} ) \\cdot x + (\\bar{y} - (\\frac{r\\sigma_y}{\\sigma_x} ) \\bar{x})\\]\nWe now see that we have a line that matches our claim:\n\nslope: \\(r\\cdot\\frac{\\text{SD of x}}{\\text{SD of y}} = r\\cdot\\frac{\\sigma_x}{\\sigma_y}\\)\nintercept: \\(\\bar{y} - \\text{slope}\\cdot x\\)"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "href": "intro_to_modeling/intro_to_modeling.html#the-modeling-process",
    "title": "10  Introduction to Modeling",
    "section": "10.3 The Modeling Process",
    "text": "10.3 The Modeling Process\nAt a high level, a model is some way of representing a system. In Data 100, we’ll treat a model as some mathematical rule we use to describe the relationship between variables.\nWhat variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset \\(\\mathbb{D}\\):\n\\[\\mathbb{D} = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}\\]\nEach pair of values \\((x_i, y_i)\\) represents a datapoint. In a modeling setting, we call these observations. \\(y_i\\) is the dependent variable we are trying to model, also called an output or response. \\(x_i\\) is the independent variable inputted into the model to make predictions, also known as a feature.\nOur goal in modeling is to use the observed data \\(\\mathbb{D}\\) to predict the output variable \\(y_i\\). We denote each prediction as \\(\\hat{y}_i\\) (read: “y hat sub i”).\nHow do we generate these predictions? Some examples of models we’ll encounter in the next few lectures are given below:\n\n\\(\\hat{y}_i = \\theta\\)\n\\(\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\)\n\nThe examples above are known as parametric models. They relate the collected data, \\(x_i\\), to the prediction we make, \\(\\hat{y}_i\\). A few parameters (\\(\\theta\\), \\(\\theta_0\\), \\(\\theta_1\\)) are used to describe the relationship between \\(x_i\\) and \\(\\hat{y}_i\\).\nNotice that we don’t immediately know the values of these parameters. While the features, \\(x_i\\), are taken from our observed data, we need to decide what values to give \\(\\theta\\), \\(\\theta_0\\), and \\(\\theta_1\\) ourselves. This is the heart of parametric modeling: what parameter values should we choose so our model makes the best possible predictions?\nTo choose our model parameters, we’ll work through the modeling process.\n\nChoose a model: how should we represent the world?\nChoose a loss function: how do we quantify prediction error?\nFit the model: how do we choose the best parameters of our model given our data?\nEvaluate model performance: how do we evaluate whether this process gave rise to a good model?"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-model",
    "title": "10  Introduction to Modeling",
    "section": "10.4 Choosing a Model",
    "text": "10.4 Choosing a Model\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, \\(x_i\\), and predictions \\(\\hat{y}_i\\).\nIn Data 8, you learned about the Simple Linear Regression (SLR) model. You learned that the model takes the form: \\[\\hat{y}_i = a + bx_i\\]\nIn Data 100, we’ll use slightly different notation: we will replace \\(a\\) with \\(\\theta_0\\) and \\(b\\) with \\(\\theta_1\\). This will allow us to use the same notation when we explore more complex models later on in the course.\n\\[\\hat{y}_i = \\theta_0 + \\theta_1 x_i\\]\nThe parameters of the SLR model are \\(\\theta_0\\), also called the intercept term, and \\(\\theta_1\\), also called the slope term. To create an effective model, we want to choose values for \\(\\theta_0\\) and \\(\\theta_1\\) that most accurately predict the output variable. The “best” fitting model parameters are given the special names \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\) – they are the specific parameter values that allow our model to generate the best possible predictions.\nIn Data 8, you learned that the best SLR model parameters are: \\[\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x} \\qquad \\qquad \\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nA quick reminder on notation:\n\n\\(\\bar{y}\\) and \\(\\bar{x}\\) indicate the mean value of \\(y\\) and \\(x\\), respectively\n\\(\\sigma_y\\) and \\(\\sigma_x\\) indicate the standard deviations of \\(y\\) and \\(x\\)\n\\(r\\) is the correlation coefficient, defined as the average of the product of \\(x\\) and \\(y\\) measured in standard units: \\(\\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y})\\)\n\nIn Data 100, we want to understand how to derive these best model coefficients. To do so, we’ll introduce the concept of a loss function."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "href": "intro_to_modeling/intro_to_modeling.html#choosing-a-loss-function",
    "title": "10  Introduction to Modeling",
    "section": "10.5 Choosing a Loss Function",
    "text": "10.5 Choosing a Loss Function\nWe’ve talked about the idea of creating the “best” possible predictions. This begs the question: how do we decide how “good” or “bad” our model’s predictions are?\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, \\(L(y, \\hat{y})\\), quantifies how “far off” a single prediction by our model is from a true, observed value in our collected data.\nThe choice of loss function for a particular model depends on the modeling task at hand. Regardless of the specific function used, a loss function should follow two basic principles:\n\nIf the prediction \\(\\hat{y}_i\\) is close to the actual value \\(y_i\\), loss should be low\nIf the prediction \\(\\hat{y}_i\\) is far from the actual value \\(y_i\\), loss should be high\n\nTwo common choices of loss function are squared loss and absolute loss.\nSquared loss, also known as L2 loss, computes loss as the square of the difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2\\]\nAbsolute loss, also known as L1 loss, computes loss as the absolute difference between the observed \\(y_i\\) and predicted \\(\\hat{y}_i\\): \\[L(y_i, \\hat{y}_i) = |y_i - \\hat{y}_i|\\]\nL1 and L2 loss give us a tool for quantifying our model’s performance on a single datapoint. This is a good start, but ideally we want to understand how our model performs across our entire dataset. A natural way to do this is to compute the average loss across all datapoints in the dataset. This is known as the cost function, \\(\\hat{R}(\\theta)\\): \\[\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)\\]\nThe cost function has many names in statistics literature. You may also encounter the terms:\n\nEmpirical risk (this is why we give the cost function the name \\(R\\))\nError function\nAverage loss\n\nWe can substitute our L1 and L2 loss into the cost function definition. The Mean Squared Error (MSE) is the average squared loss across a dataset: \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nThe Mean Absolute Error (MAE) is the average absolute loss across a dataset: \\[\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\\]"
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "href": "intro_to_modeling/intro_to_modeling.html#fitting-the-model",
    "title": "10  Introduction to Modeling",
    "section": "10.6 Fitting the Model",
    "text": "10.6 Fitting the Model\nNow that we’ve established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model’s cost on our dataset. This process is called fitting the model.\nWe know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the objective function (our objective is to find its minimum).\nTo find the optimal model parameter, we:\n\nTake the derivative of the cost function with respect to that parameter\nSet the derivative equal to 0\nSolve for the parameter\n\nWe repeat this process for each parameter present in the model. For now, we’ll disregard the second derivative condition.\nTo help us make sense of this process, let’s put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!\nStep 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, \\(\\hat{y}_i = \\theta_0+\\theta_1 x_i\\), into the definition of MSE above and differentiate with respect to \\(\\theta_0\\) and \\(\\theta_1\\). \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)^2\\]\n\\[\\frac{\\partial}{\\partial \\theta_0} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i\\]\n\\[\\frac{\\partial}{\\partial \\theta_1} \\text{MSE} = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i\\]\nStep 2: set the derivatives equal to 0. After simplifying terms, this produces two estimating equations. The best set of model parameters \\((\\theta_0, \\theta_1)\\) must satisfy these two optimality conditions. \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} y_i - \\theta_0 - \\theta_1 x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} y_i - \\hat{y}_i = 0\\] \\[0 = \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i \\Longleftrightarrow \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)x_i = 0\\]\nStep 3: solve the estimating equations to compute estimates for \\(\\hat{\\theta}_0\\) and \\(\\hat{\\theta}_1\\).\nTaking the first equation gives the estimate of \\(\\hat{\\theta}_0\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n y_i - \\hat{\\theta}_0 - \\hat{\\theta}_1 x_i &= 0 \\\\\n\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{\\theta}_0 - \\hat{\\theta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) &= 0 \\\\\n\\hat{\\theta}_0 &= \\bar{y} - \\hat{\\theta}_1 \\bar{x}\n\\end{align}\\]\nWith a bit more maneuvering, the second equation gives the estimate of \\(\\hat{\\theta}_1\\). Start by multiplying the first estimating equation by \\(\\bar{x}\\), then subtracting the result from the second estimating equation. \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)x_i - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\\bar{x} &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)(x_i - \\bar{x}) &= 0\n\\end{align}\\]\nNext, plug in \\(\\hat{y}_i = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_i = \\bar{y} + \\hat{\\theta}_1(x_i - \\bar{x})\\): \\[\\begin{align}\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y} - \\hat{\\theta}_1(x - \\bar{x}))(x_i - \\bar{x}) &= 0 \\\\\n\\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})(x_i - \\bar{x}) = \\hat{\\theta}_1 \\times \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\end{align}\\]\nBy using the definition of correlation \\(\\left(r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)\\) and standard deviation \\(\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)\\), we can conclude: \\[r \\sigma_x \\sigma_y = \\hat{\\theta}_1 \\times \\sigma_x^2\\] \\[\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}\\]\nJust as was given in Data 8!\nRemember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can always follow these three steps to fit the model."
  },
  {
    "objectID": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "href": "intro_to_modeling/intro_to_modeling.html#evaluating-performance",
    "title": "10  Introduction to Modeling",
    "section": "10.7 Evaluating Performance",
    "text": "10.7 Evaluating Performance\nAt this point, we’ve:\n\nDefined our model\nDefined our loss function\nFit the model to identify the best model parameters\n\nNow, what are some ways to determine if our model was a good fit to our data? We will delve into this more in the next chapter, but there are three main ways for evaluating a model.\n\nStatistics:\n\nPlot original data\nCompute column means\nCompute standard deviations\nIf we want to fit a linear model, compute correlation (r)\n\nPerformance metrics:\n\nRoot Mean Square Error (RMSE). It is the square root of MSE, which is the average loss that we’ve been minimizing to determine optimal model parameters.\nRMSE is in the same units as \\(y\\).\nA lower RMSE indicates more “accurate” predictions (lower “average loss” across data)\n\nVisualization:\n\nLook at a residual plot of \\(e_i = y_i - \\hat{y_i}\\) to visualize the difference between actual and predicted \\(y\\) values."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mse",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.1 Constant Model + MSE",
    "text": "11.1 Constant Model + MSE\nIn today’s lecture, our focus will be on the constant model. The constant model is slightly different from the simple linear regression model we’ve explored previously. Rather than generate predictions from an inputted feature variable, the constant model predicts the same constant number every time. We call this constant \\(\\theta\\).\n\\[\\hat{y}_i = \\theta\\]\n\\(\\theta\\) is the parameter of the constant model, just as \\(\\theta_0\\) and \\(\\theta_1\\) were the parameters in SLR. Our task now is to determine what value of \\(\\theta\\) represents the optimal model – in other words, what number should we guess each time to have the lowest possible average loss on our data?\nConsider the case where L2 (squared) loss is used as the loss function and mean squared error is used as the cost function. At this stage, we’re well into the modeling process:\n\nChoose a model: constant model\nChoose a loss function: L2 loss\nFit the model\nEvaluate model performance\n\nIn Homework 5, you will fit the constant model under MSE cost to find that the best choice of \\(\\theta\\) is the mean of the observed \\(y\\) values. In other words, \\(\\hat{\\theta} = \\bar{y}\\).\nLet’s take a moment to interpret this result. Our optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:\n\\[R(\\hat{\\theta}) = \\min_{\\theta} R(\\theta)\\]\nTo restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, \\(\\hat{\\theta}\\), is the value of \\(\\theta\\) that minimizes the cost \\(R\\).\nFor modeling purposes, we care less about the minimum value of cost, \\(R(\\hat{\\theta})\\), and more about the value of \\(\\theta\\) that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n\\[\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)\\]\nThat is, we want to find the argument \\(\\theta\\) that minimizes the cost function."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "href": "constant_model_loss_transformations/loss_transformations.html#constant-model-mae",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.2 Constant Model + MAE",
    "text": "11.2 Constant Model + MAE\nWe see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?\nThis time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the mean absolute error.\n\nChoose a model: constant model\nChoose a loss function: L1 loss\nFit the model\nEvaluate model performance\n\nTo fit the model and find the optimal parameter value \\(\\hat{\\theta}\\), follow the usual process of differentiating the cost function with respect to \\(\\theta\\), setting the derivative equal to zero, and solving for \\(\\theta\\). Writing this out in longhand:\n\\[\nR(\\theta) = \\frac{1}{n}\\sum^{n}_{i=1} |y_i - \\theta| \\\\\n\\frac{d}{d\\theta} R(\\theta) = \\frac{d}{d\\theta} \\left(\\frac{1}{n} \\sum^{n}_{i=1} |y_i - \\theta| \\right) \\\\\n\\frac{d}{d\\theta} R(\\theta) = \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta|\n\\]\nHere, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when \\(y_i = \\theta\\)). For now, we’ll ignore this issue. It turns out that disregarding this case doesn’t influence our final result.\nTo perform the derivative, consider two cases. When \\(\\theta\\) is less than \\(y_i\\), the term \\(y_i - \\theta\\) will be positive and the absolute value has no impact. When \\(\\theta\\) is greater than \\(y_i\\), the term \\(y_i - \\theta\\) will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying \\(-(y_i - \\theta) = \\theta - y_i\\).\n\\[|y_i - \\theta| = \\begin{cases} y_i - \\theta \\quad \\text{ if: } \\theta < y_i \\\\ \\theta - y_i \\quad \\text{if: }\\theta > y_i \\end{cases}\\]\nTaking derivatives:\n\\[\\frac{d}{d\\theta} |y_i - \\theta| = \\begin{cases} \\frac{d}{d\\theta} (y_i - \\theta) = -1 \\quad \\text{if: }\\theta < y_i \\\\ \\frac{d}{d\\theta} (\\theta - y_i) = 1 \\quad \\text{if: }\\theta > y_i \\end{cases}\\]\nThis means that we obtain a different value for the derivative for datapoints where \\(\\theta < y_i\\) and where \\(\\theta > y_i\\). We can summarize this by saying:\n\\[\\frac{d}{d\\theta} R(\\theta) = \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta| \\\\\n= \\frac{1}{n} \\left[\\sum_{\\theta < y_i} (-1) + \\sum_{\\theta > y_i} (+1) \\right]\n\\]\nTo finish finding the best value of \\(\\theta\\), set this derivative equal to zero and solve for \\(\\theta\\). You’ll do this in Homework 5 to show that \\(\\hat{\\theta} = \\text{median}(y)\\)."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "href": "constant_model_loss_transformations/loss_transformations.html#comparing-loss-functions",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.3 Comparing Loss Functions",
    "text": "11.3 Comparing Loss Functions\nNow, we’ve tried our hand at fitting a model under both MSE and MAE cost functions. How do the two results compare?\nLet’s consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We’ll fit a constant model to predict the number of drinks that will be sold tomorrow.\n\nimport numpy as np\ndrinks = np.array([20, 21, 22, 29, 33])\ndrinks\n\narray([20, 21, 22, 29, 33])\n\n\nFrom our derivations above, we know that the optimal model parameter under MSE cost is the mean of the dataset. Under MAE cost, the optimal parameter is the median of the dataset.\n\nnp.mean(drinks), np.median(drinks)\n\n(25.0, 22.0)\n\n\nIf we plot each empirical risk function across several possible values of \\(\\theta\\), we find that each \\(\\hat{\\theta}\\) does indeed correspond to the lowest value of error:\n\nNotice that the MSE above is a smooth function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE, in contrast, is not differentiable at each of its “kinks.” We’ll explore how the smoothness of the cost function can impact our ability to apply numerical optimization in a few weeks.\nHow do outliers affect each cost function? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected.\n\ndrinks_with_outlier = np.append(drinks, 1000)\ndisplay(drinks_with_outlier)\nnp.mean(drinks_with_outlier), np.median(drinks_with_outlier)\n\narray([  20,   21,   22,   29,   33, 1000])\n\n\n(187.5, 25.5)\n\n\nThis means that under the MSE, the optimal model parameter \\(\\hat{\\theta}\\) is strongly affected by the presence of outliers. Under the MAE, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is sensitive to outliers, while the MAE is robust to outliers.\nLet’s try another experiment. This time, we’ll add an additional, non-outlying datapoint to the data.\n\ndrinks_with_additional_observation = np.append(drinks, 35)\ndrinks_with_additional_observation\n\narray([20, 21, 22, 29, 33, 35])\n\n\nWhen we again visualize the cost functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are infinitely many optimal values for the model parameter: any value \\(\\hat{\\theta} \\in [22, 29]\\) will minimize the MAE. In contrast, the MSE still has a single best value for \\(\\hat{\\theta}\\). In other words, the MSE has a unique solution for \\(\\hat{\\theta}\\); the MAE is not guaranteed to have a single unique solution."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#evaluating-models",
    "href": "constant_model_loss_transformations/loss_transformations.html#evaluating-models",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.4 Evaluating Models",
    "text": "11.4 Evaluating Models\nThis leaves us with one final question – how “good” are the predictions made by this “best” fitted model?\nOne way we might want to evaluate our model’s performance is by computing summary statistics. If the mean and standard deviation of our predictions are close to those of the original observed \\(y_i\\)s, we might be inclined to say that our model has done well. A large magnitude for the correlation coefficient between the feature and response variables might also support this conclusion. However, we should be cautious with this approach. To see why, we’ll consider a classic dataset called Anscombe’s quartet.\n\nIt turns out that the four sets of points shown here all have identical means, standard deviations, and correlation coefficients. However, it only makes sense to model the first of these four sets of data using SLR! It is important to visualize your data before starting to model to confirm that your choice of model makes sense for the data.\nAnother way of evaluating model performance is by using performance metrics. A common choice of metric is the Root Mean Squared Error, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of \\(y_i\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset. \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nWe may also wish to visualize the model’s residuals, defined as the difference between the observed and predicted \\(y_i\\) value (\\(e_i = y_i - \\hat{y}_i\\)). This gives a high-level view of how “off” each prediction is from the true observed value. Recall that you explored this concept in Data 8: a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe’s quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points."
  },
  {
    "objectID": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "href": "constant_model_loss_transformations/loss_transformations.html#linear-transformations",
    "title": "11  Constant Model, Loss, and Transformations",
    "section": "11.5 Linear Transformations",
    "text": "11.5 Linear Transformations\nAt this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.\nA key word above is linear. When we computed parameter estimates earlier, we assumed that \\(x_i\\) and \\(y_i\\) shared roughly a linear relationship.\nData in the real world isn’t always so straightforward. Consider the dataset below, which contains information about the ages and lengths of dugongs.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndugong = pd.read_csv(\"data/dugongs.txt\", delimiter=\"\\t\").sort_values(\"Length\")\nx, y = dugong[\"Length\"], dugong[\"Age\"]\n\n# `corrcoef` computes the correlation coefficient between two variables\n# `std` finds the standard deviation\nr = np.corrcoef(x, y)[0, 1]\ntheta_1 = r*np.std(y)/np.std(x)\ntheta_0 = np.mean(y) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, y)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(\"Age\")\n\nax[1].scatter(x, y)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(\"Age\");\n\n\n\n\n\nLooking at the plot on the left, we see that there is a slight curvature to the data points. Plotting the SLR curve on the right results in a poor fit.\nFor SLR to perform well, we’d like there to be a rough linear trend relating \"Age\" and \"Length\". What is making the raw data deviate from a linear relationship? Notice that the data points with \"Length\" greater than 2.6 have disproportionately high values of \"Age\" relative to the rest of the data. If we could manipulate these data points to have lower \"Age\" values, we’d “shift” these points downwards and reduce the curvature in the data. Applying a logarithmic transformation to \\(y_i\\) (that is, taking \\(\\log(\\) \"Age\" \\()\\) ) would achieve just that.\nAn important word on \\(\\log\\): in Data 100 (and most upper-division STEM courses), \\(\\log\\) denotes the natural logarithm with base \\(e\\). The base-10 logarithm, where relevant, is indicated by \\(\\log_{10}\\).\n\n\nCode\nz = np.log(y)\n\nr = np.corrcoef(x, z)[0, 1]\ntheta_1 = r*np.std(z)/np.std(x)\ntheta_0 = np.mean(z) - theta_1*np.mean(x)\n\nfig, ax = plt.subplots(1, 2, dpi=200, figsize=(8, 3))\nax[0].scatter(x, z)\nax[0].set_xlabel(\"Length\")\nax[0].set_ylabel(r\"$\\log{(Age)}$\")\n\nax[1].scatter(x, z)\nax[1].plot(x, theta_0 + theta_1*x, \"tab:red\")\nax[1].set_xlabel(\"Length\")\nax[1].set_ylabel(r\"$\\log{(Age)}$\")\n\nplt.subplots_adjust(wspace=0.3);\n\n\n\n\n\nOur SLR fit looks a lot better! We now have a new target variable: the SLR model is now trying to predict the log of \"Age\", rather than the untransformed \"Age\". In other words, we are applying the transformation \\(z_i = \\log{(y_i)}\\). The SLR model becomes:\n\\[\\log{\\hat{(y_i)}} = \\theta_0 + \\theta_1 x_i\\] \\[\\hat{z}_i = \\theta_0 + \\theta_1 x_i\\]\nIt turns out that this linearized relationship can help us understand the underlying relationship between \\(x_i\\) and \\(y_i\\). If we rearrange the relationship above, we find: \\[\n\\log{(y_i)} = \\theta_0 + \\theta_1 x_i \\\\\ny_i = e^{\\theta_0 + \\theta_1 x_i} \\\\\ny_i = (e^{\\theta_0})e^{\\theta_1 x_i} \\\\\ny_i = C e^{k x_i}\n\\]\nFor some constants \\(C\\) and \\(k\\).\n\\(y_i\\) is an exponential function of \\(x_i\\). Applying an exponential fit to the untransformed variables corroborates this finding.\n\n\nCode\nplt.figure(dpi=120, figsize=(4, 3))\n\nplt.scatter(x, y)\nplt.plot(x, np.exp(theta_0)*np.exp(theta_1*x), \"tab:red\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Age\");\n\n\n\n\n\nYou may wonder: why did we choose to apply a log transformation specifically? Why not some other function to linearize the data?\nPractically, many other mathematical operations that modify the relative scales of \"Age\" and \"Length\" could have worked here. The Tukey-Mosteller Bulge Diagram is a useful tool for summarizing what transformations can linearize the relationship between two variables. To determine what transformations might be appropriate, trace the shape of the “bulge” made by your data. Find the quadrant of the diagram that matches this bulge. The transformations shown on the vertical and horizontal axes of this quadrant can help improve the fit between the variables."
  },
  {
    "objectID": "ols/ols.html#linearity",
    "href": "ols/ols.html#linearity",
    "title": "12  Ordinary Least Squares",
    "section": "12.1 Linearity",
    "text": "12.1 Linearity\nAn expression is linear in \\(\\theta\\) (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms: a vector of \\(\\theta\\) s, and a matrix/vector not involving \\(\\theta\\).\nExample: \\(\\theta = [\\theta_1, \\theta_2, ... \\theta_p]\\)\n\nLinear in theta: \\(\\hat{y} = \\theta_0 + 2\\theta_1 + 3\\theta_2\\)\n\n\\[\\hat{y} = \\begin{bmatrix} 1 \\space 2 \\space 3 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}\\]\n\nNot linear in theta: \\(\\hat{y} = \\theta_0\\theta_1 + 2\\theta_1^2 + 3log(\\theta_2)\\)"
  },
  {
    "objectID": "ols/ols.html#multiple-linear-regression",
    "href": "ols/ols.html#multiple-linear-regression",
    "title": "12  Ordinary Least Squares",
    "section": "12.2 Multiple Linear Regression",
    "text": "12.2 Multiple Linear Regression\nMultiple Linear regression is an extension of simple linear regression that adds additional features into the model. Say we collect information on several variables when making an observation. For example, we may record the age, height, and weekly hours of sleep for a student in Data 100. This single observation now contains data for multiple features. To accommodate for the fact that we now consider several feature variables, we’ll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of \\(p\\) features.\n\nThe multiple Linear regression model takes the form:\n\\[\\hat{y}_i = \\theta_0\\:+\\:\\theta_1x_{i1}\\:+\\:\\theta_2 x_{i2}\\:+\\:...\\:+\\:\\theta_p x_{ip}\\]\nOur \\(i\\)th prediction, \\(\\hat{y}_i\\), is a linear combination of the parameters, \\(\\theta_i\\). Because we are now dealing with many parameter values, we’ll collect them all into a parameter vector with dimensions \\((p+1) \\times 1\\) to keep things tidy.\n\\[\\theta = \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix}\\]\nWe are now working with two vectors: a row vector representing the observed data, and a column vector containing the model parameters. The multiple Linear regression model given above is equivalent to the dot (scalar) product of the observation vector and parameter vector.\n\\[[1,\\:x_{i1},\\:x_{i2},\\:x_{i3},\\:...,\\:x_{ip}] \\theta = [1,\\:x_{i1},\\:x_{i2},\\:x_{i3},\\:...,\\:x_{ip}] \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix} = \\theta_0\\:+\\:\\theta_1x_{i1}\\:+\\:\\theta_2 x_{i2}\\:+\\:...\\:+\\:\\theta_p x_{ip}\\]\nNotice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with \\(\\theta_0\\) to give the intercept of the regression model. We call this 1 entry the intercept or bias term."
  },
  {
    "objectID": "ols/ols.html#linear-algebra-approach",
    "href": "ols/ols.html#linear-algebra-approach",
    "title": "12  Ordinary Least Squares",
    "section": "12.3 Linear Algebra Approach",
    "text": "12.3 Linear Algebra Approach\nWe now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple Linear regression. We want to use the tools of linear algebra.\nLet’s think carefully about what we did to generate the single prediction above. To make a prediction from the first observation in the data, we took the scalar product of the parameter vector and first observation vector. To make a prediction from the second observation, we would repeat this process to find the scalar product of the parameter vector and the second observation vector. If we wanted to find the model predictions for each observation in the dataset, we’d repeat this process for all \\(n\\) observations in the data.\n\\[\\hat{y}_1 = [1,\\:x_{11},\\:x_{12},\\:x_{13},\\:...,\\:x_{1p}] \\theta\\] \\[\\hat{y}_2 = [1,\\:x_{21},\\:x_{22},\\:x_{23},\\:...,\\:x_{2p}] \\theta\\] \\[\\vdots\\] \\[\\hat{y}_n = [1,\\:x_{n1},\\:x_{n2},\\:x_{n3},\\:...,\\:x_{np}] \\theta\\]\nOur observed data is represented by \\(n\\) row vectors, each with dimension \\((p+1)\\). We can collect them all into a single matrix, which we call \\(\\mathbb{X}\\).\n\nThe matrix \\(\\mathbb{X}\\) is known as the design matrix. It contains all observed data for each of our \\(p\\) features. It often (but not always) contains an additional column of all ones to represent the intercept or bias column.\nTo review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.\n\nThe multiple Linear regression model can then be restated in terms of matrices: \\[\\mathbb{\\hat{Y}} = \\mathbb{X} \\theta\\]\nHere, \\(\\mathbb{\\hat{Y}}\\) is the prediction vector with dimensions \\((n \\times 1)\\). It contains the prediction made by the model for each of \\(n\\) input observations.\nWe now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of cost functions and model fitting.\nRecall our definition of MSE: \\[R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nAt its heart, the MSE is a measure of distance – it gives an indication of how “far away” the predictions are from the true values, on average.\nWhen working with vectors, this idea of “distance” is represented by the norm. More precisely, the distance between two vectors \\(\\vec{a}\\) and \\(\\vec{b}\\) can be expressed as: \\[||\\vec{a} - \\vec{b}||_2 = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\ldots + (a_n - b_n)^2} = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}\\]\nThe double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.\nLooks pretty familiar! We can rewrite the MSE to express it as a squared L2 norm in terms of the prediction vector, \\(\\hat{\\mathbb{Y}}\\), and true target vector, \\(\\mathbb{Y}\\):\n\\[R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} ||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2^2\\]\nHere, the superscript 2 outside of the norm double bars means that we are squaring the norm. If we plug in our linear model \\(\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta\\), we find the MSE cost function in vector notation:\n\\[R(\\theta) = \\frac{1}{n} ||\\mathbb{Y} - \\mathbb{X} \\theta||_2^2\\]\nUnder the linear algebra perspective, our new task is to fit the optimal parameter vector \\(\\theta\\) such that the cost function is minimized. Equivalently, we wish to minimize the norm \\[||\\mathbb{Y} - \\mathbb{X} \\theta||_2 = ||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2\\].\nWe can restate this goal in two ways:\n\nMinimize the distance between the vector of true values, \\(\\mathbb{Y}\\), and the vector of predicted values, \\(\\mathbb{\\hat{Y}}\\)\nMinimize the length of the residual vector, defined as: \\[e = \\mathbb{Y} - \\mathbb{\\hat{Y}} = \\begin{bmatrix}\n         y_1 - \\hat{y}_1 \\\\\n         y_2 - \\hat{y}_2 \\\\\n         \\vdots \\\\\n         y_n - \\hat{y}_n\n       \\end{bmatrix}\\]"
  },
  {
    "objectID": "ols/ols.html#geometric-perspective",
    "href": "ols/ols.html#geometric-perspective",
    "title": "12  Ordinary Least Squares",
    "section": "12.4 Geometric Perspective",
    "text": "12.4 Geometric Perspective\nTo derive the best parameter vector to meet this goal, we can turn to the geometric properties of our modeling set-up.\nUp until now, we’ve mostly thought of our model as a scalar product between horizontally-stacked observations and the parameter vector. We can also think of \\(\\hat{\\mathbb{Y}}\\) as a linear combination of feature vectors, scaled by the parameters. We use the notation \\(\\mathbb{X}_{:, i}\\) to denote the \\(i\\)th column of the design matrix. You can think of this as following the same convention as used when calling .iloc and .loc. “:” means that we are taking all entries in the \\(i\\)th column.\n\n\\[\n\\hat{\\mathbb{Y}} =\n\\theta_0 \\begin{bmatrix}\n           1 \\\\\n           1 \\\\\n           \\vdots \\\\\n           1\n         \\end{bmatrix} + \\theta_1 \\begin{bmatrix}\n           x_{11} \\\\\n           x_{21} \\\\\n           \\vdots \\\\\n           x_{n1}\n         \\end{bmatrix} + \\ldots + \\theta_p \\begin{bmatrix}\n           x_{1p} \\\\\n           x_{2p} \\\\\n           \\vdots \\\\\n           x_{np}\n         \\end{bmatrix}\n         = \\theta_0 \\mathbb{X}_{:,\\:1} + \\theta_1 \\mathbb{X}_{:,\\:2} + \\ldots + \\theta_p \\mathbb{X}_{:,\\:p+1}\\]\nThis new approach is useful because it allows us to take advantage of the properties of linear combinations.\nRecall that the span or column space of a matrix is the set of all possible linear combinations of the matrix’s columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns.\nBecause the prediction vector, \\(\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta\\), is a linear combination of the columns of \\(\\mathbb{X}\\), we know that the predictions are contained in the span of \\(\\mathbb{X}\\). That is, we know that \\(\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})\\).\nThe diagram below is a simplified view of \\(\\text{Span}(\\mathbb{X})\\), assuminh that each column of \\(\\mathbb{X}\\) has length \\(n\\). Notice that the columns of \\(\\mathbb{X}\\) define a subspace of \\(\\mathbb{R}^n\\), where each point in the subspace can be reached by a linear combination of \\(\\mathbb{X}\\)’s columns. The prediction vector \\(\\mathbb{\\hat{Y}}\\) lies somewhere in this subspace.\n\nExamining this diagram, we find a problem. The vector of true values, \\(\\mathbb{Y}\\), could theoretically lie anywhere in \\(\\mathbb{R}^n\\) space – its exact location depends on the data we collect out in the real world. However, our multiple Linear regression model can only make predictions in the subspace of \\(\\mathbb{R}^n\\) spanned by \\(\\mathbb{X}\\). Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, \\(\\mathbb{Y}\\), and the vector of predicted values, \\(\\mathbb{\\hat{Y}}\\), is minimized. This means that we want \\(\\mathbb{\\hat{Y}}\\) to be the vector in \\(\\text{Span}(\\mathbb{X})\\) that is closest to \\(\\mathbb{Y}\\).\nAnother way of rephrasing this goal is to say that we wish to minimize the length of the residual vector \\(e\\), as measured by its \\(L_2\\) norm.\n\nThe vector in \\(\\text{Span}(\\mathbb{X})\\) that is closest to \\(\\mathbb{Y}\\) is always the orthogonal projection of \\(\\mathbb{Y}\\) onto \\(\\text{Span}(\\mathbb{X})\\). Thus, we should choose the parameter vector \\(\\theta\\) that makes the residual vector orthogonal to any vector in \\(\\text{Span}(\\mathbb{X})\\). You can visualize this as the vector created by dropping a perpendicular line from \\(\\mathbb{Y}\\) onto the span of \\(\\mathbb{X}\\).\nHow does this help us identify the optimal parameter vector, \\(\\hat{\\theta}\\)? Recall that two vectors are orthogonal if their dot product is zero. A vector \\(\\vec{v}\\) is orthogonal to the span of a matrix \\(M\\) if \\(\\vec{v}\\) is orthogonal to each column in \\(M\\). Put together, a vector \\(\\vec{v}\\) is orthogonal to \\(\\text{Span}(M)\\) if:\n\\[M^T \\vec{v} = \\vec{0}\\]\nBecause our goal is to find \\(\\hat{\\theta}\\) such that the residual vector \\(e = \\mathbb{Y} - \\mathbb{X} \\theta\\) is orthogonal to \\(\\text{Span}(\\mathbb{X})\\), we can write:\n\\[\\mathbb{X}^T e = \\vec{0}\\] \\[\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\vec{0}\\] \\[\\mathbb{X}^T \\mathbb{Y} - \\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\vec{0}\\] \\[\\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}\\]\nThis last line is known as the normal equation. Any vector \\(\\theta\\) that minimizes MSE on a dataset must satisfy this equation.\nIf \\(\\mathbb{X}^T \\mathbb{X}\\) is invertible, we can conclude: \\[\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^T \\mathbb{Y}\\]\nThis is called the least squares estimate of \\(\\theta\\): it is the value of \\(\\theta\\) that minimizes the squared loss.\nNote that the least squares estimate was derived under the assumption that \\(\\mathbb{X}^T \\mathbb{X}\\) is invertible. This condition holds true when \\(\\mathbb{X}^T \\mathbb{X}\\) is full column rank, which, in turn, happens when \\(\\mathbb{X}\\) is full column rank. We will explore the consequences of this fact in lab and homework."
  },
  {
    "objectID": "ols/ols.html#evaluating-model-performance",
    "href": "ols/ols.html#evaluating-model-performance",
    "title": "12  Ordinary Least Squares",
    "section": "12.5 Evaluating Model Performance",
    "text": "12.5 Evaluating Model Performance\nOur geometric view of multiple Linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features.\nNow, we want to understand how well our fitted model performs. One measure of model performance is the Root Mean Squared Error, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of \\(y_i\\), which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset.\n\\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\\]\nWhen working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple Linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple Linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple Linear model performs well if its residual plot shows no patterns.\n\nFor SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple Linear setting, we will need a performance metric that can account for multiple features at once. Multiple \\(R^2\\), also called the coefficient of determination, is the ratio of the variance of the predicted values \\(\\hat{y}_i\\) to the variance of the true values \\(y_i\\). It can be interpreted as the proportion of variance in the observations that is explained by the model.\n\\[R^2 = \\frac{\\text{variance of } \\hat{y}_i}{\\text{variance of } y_i} = \\frac{\\sigma^2_{\\hat{y}}}{\\sigma^2_y}\\]\nAs we add more features, our fitted values tend to become closer and closer to our actual values. Thus, \\(\\mathbb{R}^2\\) increases."
  },
  {
    "objectID": "ols/ols.html#ols-properties",
    "href": "ols/ols.html#ols-properties",
    "title": "12  Ordinary Least Squares",
    "section": "12.6 OLS Properties",
    "text": "12.6 OLS Properties\n\nWhen using the optimal parameter vector, our residuals \\(e = \\mathbb{Y} - \\hat{\\mathbb{Y}}\\) are orthogonal to \\(span(\\mathbb{X})\\)\n\n\\[\\mathbb{X}^Te = 0 \\]\n\n\n\n\n\n\nProof:\nThe optimal parameter vector, \\(\\hat{\\theta}\\), solves the normal equations \\(\\implies \\hat{\\theta} = \\mathbb{X}^T\\mathbb{X}^{-1}\\mathbb{X}^T\\mathbb{Y}\\)\n\\[\\mathbb{X}^Te = \\mathbb{X}^T (\\mathbb{Y} - \\mathbb{\\hat{Y}}) \\]\n\\[\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{X}\\hat{\\theta}\\]\nAny matrix multiplied with its own inverse is the identity matrix \\(\\mathbb{I}\\)\n\\[\\mathbb{X}^T\\mathbb{Y} - (\\mathbb{X}^T\\mathbb{X})(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y} = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{Y} = 0\\]\n\n\n\n\nFor all linear models with an intercept term, the sum of residuals is zero.\n\n\\[\\sum_i^n e_i = 0\\]\n\n\n\n\n\n\nProof:\nFor all linear models with an intercept term, the average of the predicted \\(y\\) values is equal to the average of the true \\(y\\) values.\n\\[\\bar{y} = \\bar{\\hat{y}}\\]\nRewriting the sum of residuals as two separate sums,\n\\[\\sum_i^n e_i = \\sum_i^n y_i - \\sum_i^n\\hat{y}_i\\]\nEach respective sum is a multiple of the average of the sum.\n\\[\\sum_i^n e_i = n\\bar{y} - n\\bar{y} = n(\\bar{y} - \\bar{y}) = 0\\]\n\n\n\n\nThe Least Squares estimate \\(\\hat{\\theta}\\) is unique if and only if \\(\\mathbb{X}\\) is full column rank.\n\n\n\n\n\n\n\nProof:\nWe know the solution to the normal equation \\(\\mathbb{X}^T\\mathbb{X}\\hat{\\theta} = \\mathbb{Y}\\) is the least square estimate that fulfills the prior equality.\n\\(\\hat{\\theta}\\) has a unique solution \\(\\iff\\) the square matrix \\(\\mathbb{X}^T\\mathbb{X}\\) is invertible.\nThe rank of a square matrix is the maximum number of of linearly independent columns it contains. \\(\\mathbb{X}^T\\mathbb{X}\\) has shape \\((p + 1) \\times (p + 1)\\), and therefore has max rank p + 1.\n\\(rank(\\mathbb{X}^T\\mathbb{X})\\) = \\(rank(\\mathbb{X})\\) (proof out of scope).\nTherefore \\(\\mathbb{X}^T\\mathbb{X}\\) has rank p + 1 \\(\\iff\\) \\(\\mathbb{X}\\) has rank p + 1 \\(\\iff \\mathbb{X}\\) is full column rank."
  }
]