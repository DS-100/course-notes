[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text was developed for the Spring 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nAs this project is in development during the Spring 2023 semester, the course notes may be in flux. We appreciate your understanding. If you spot any errors or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "5  Data Cleaning and EDA",
    "section": "",
    "text": "6 EDA Demo: Tuberculosis in the United States\nNow, let’s follow this data-cleaning and EDA workflow to see what can we say about the presence of Tuberculosis in the United States!\nWe will examine the data included in the original CDC article published in 2021."
  },
  {
    "objectID": "eda/eda.html#structure",
    "href": "eda/eda.html#structure",
    "title": "5  Data Cleaning and EDA",
    "section": "5.1 Structure",
    "text": "5.1 Structure\n\n5.1.1 File Format\nIn the past two pandas lectures, we briefly touched on the idea of file format: the way data is encoded in a file for storage. Specifically, our elections and babynames datasets were stored and loaded as CSVs:\n\nimport pandas as pd\npd.read_csv(\"data/elections.csv\").head(5)\n\n\n\n\n\n  \n    \n      \n      Year\n      Candidate\n      Party\n      Popular vote\n      Result\n      %\n    \n  \n  \n    \n      0\n      1824\n      Andrew Jackson\n      Democratic-Republican\n      151271\n      loss\n      57.210122\n    \n    \n      1\n      1824\n      John Quincy Adams\n      Democratic-Republican\n      113142\n      win\n      42.789878\n    \n    \n      2\n      1828\n      Andrew Jackson\n      Democratic\n      642806\n      win\n      56.203927\n    \n    \n      3\n      1828\n      John Quincy Adams\n      National Republican\n      500897\n      loss\n      43.796073\n    \n    \n      4\n      1832\n      Andrew Jackson\n      Democratic\n      702735\n      win\n      54.574789\n    \n  \n\n\n\n\nCSVs, which stand for Comma-Separated Values, are a common tabular data format. To better understand the properties of a CSV, let’s take a look at the first few rows of the raw data file to see what it looks like before being loaded into a DataFrame.\n\n\nYear,Candidate,Party,Popular vote,Result,%\n\n1824,Andrew Jackson,Democratic-Republican,151271,loss,57.21012204\n\n1824,John Quincy Adams,Democratic-Republican,113142,win,42.78987796\n\n1828,Andrew Jackson,Democratic,642806,win,56.20392707\n\n\n\nEach row, or record, in the data is delimited by a newline. Each column, or field, in the data is delimited by a comma (hence, comma-separated!).\nAnother common file type is the TSV (Tab-Separated Values). In a TSV, records are still delimited by a newline, while fields are delimited by \\t tab character. A TSV can be loaded into pandas using pd.read_csv() with the delimiter parameter: pd.read_csv(\"file_name.tsv\", delimiter=\"\\t\"). A raw TSV file is shown below.\n\n\n﻿Year   Candidate   Party   Popular vote    Result  %\n\n1824    Andrew Jackson  Democratic-Republican   151271  loss    57.21012204\n\n1824    John Quincy Adams   Democratic-Republican   113142  win 42.78987796\n\n1828    Andrew Jackson  Democratic  642806  win 56.20392707\n\n\n\nJSON (JavaScript Object Notation) files behave similarly to Python dictionaries. They can be loaded into pandas using pd.read_json. A raw JSON is shown below.\n\n\n[\n\n {\n\n   \"Year\": 1824,\n\n   \"Candidate\": \"Andrew Jackson\",\n\n   \"Party\": \"Democratic-Republican\",\n\n   \"Popular vote\": 151271,\n\n   \"Result\": \"loss\",\n\n   \"%\": 57.21012204\n\n },\n\n\n\n\n\n5.1.2 Variable Types\nAfter loading data into a file, it’s a good idea to take the time to understand what pieces of information are encoded in the dataset. In particular, we want to identify what variable types are present in our data. Broadly speaking, we can categorize variables into one of two overarching types.\nQuantitative variables describe some numeric quantity or amount. We can sub-divide quantitative data into:\n\nContinuous quantitative variables: numeric data that can be measured on a continuous scale to arbitrary precision. Continuous variables do not have a strict set of possible values – they can be recorded to any number of decimal places. For example, weights, GPA, or CO2 concentrations\nDiscrete quantitative variables: numeric data that can only take on a finite set of possible values. For example, someone’s age or number of siblings.\n\nQualitative variables, also known as categorical variables, describe data that isn’t measuring some quantity or amount. The sub-categories of categorical data are:\n\nOrdinal qualitative variables: categories with ordered levels. Specifically, ordinal variables are those where the difference between levels has no consistent, quantifiable meaning. For example, a Yelp rating or set of income brackets.\nNominal qualitative variables: categories with no specific order. For example, someone’s political affiliation or Cal ID number.\n\n\n\n\nClassification of variable types\n\n\n\n\n5.1.3 Primary and Foreign Keys\nLast time, we introduced .merge as the pandas method for joining multiple DataFrames together. In our discussion of joins, we touched on the idea of using a “key” to determine what rows should be merged from each table. Let’s take a moment to examine this idea more closely.\nThe primary key is the column or set of columns in a table that determine the values of the remaining columns. It can be thought of as the unique identifier for each individual row in the table. For example, a table of Data 100 students might use each student’s Cal ID as the primary key.\n\n\n\n\n\n\n  \n    \n      \n      Cal ID\n      Name\n      Major\n    \n  \n  \n    \n      0\n      3034619471\n      Oski\n      Data Science\n    \n    \n      1\n      3035619472\n      Ollie\n      Computer Science\n    \n    \n      2\n      3025619473\n      Orrie\n      Data Science\n    \n    \n      3\n      3046789372\n      Ollie\n      Economics\n    \n  \n\n\n\n\nThe foreign key is the column or set of columns in a table that reference primary keys in other tables. Knowing a dataset’s foreign keys can be useful when assigning the left_on and right_on parameters of .merge. In the table of office hour tickets below, \"Cal ID\" is a foreign key referencing the previous table.\n\n\n\n\n\n\n  \n    \n      \n      OH Request\n      Cal ID\n      Question\n    \n  \n  \n    \n      0\n      1\n      3034619471\n      HW 2 Q1\n    \n    \n      1\n      2\n      3035619472\n      HW 2 Q3\n    \n    \n      2\n      3\n      3025619473\n      Lab 3 Q4\n    \n    \n      3\n      4\n      3035619472\n      HW 2 Q7"
  },
  {
    "objectID": "eda/eda.html#granularity-scope-and-temporality",
    "href": "eda/eda.html#granularity-scope-and-temporality",
    "title": "5  Data Cleaning and EDA",
    "section": "5.2 Granularity, Scope, and Temporality",
    "text": "5.2 Granularity, Scope, and Temporality\nAfter understanding the structure of the dataset, the next task is to determine what exactly the data represents. We’ll do so by considering the data’s granularity, scope, and temporality.\nThe granularity of a dataset is the level of detail included in the data. To determine the data’s granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, each record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit – for example, each record may represent a group of people.\nThe scope of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with narrow scope might encompass all students enrolled in Data 100; a dataset with expansive scope might encompass all students in California.\nThe temporality of a dataset describes the time period over which the data was collected. To fully understand the temporality of the data, it may be necessary to standardize timezones or inspect recurring time-based trends in the data (Do patterns recur in 24-hour patterns? Over the course of a month? Seasonally?)."
  },
  {
    "objectID": "eda/eda.html#faithfulness",
    "href": "eda/eda.html#faithfulness",
    "title": "5  Data Cleaning and EDA",
    "section": "5.3 Faithfulness",
    "text": "5.3 Faithfulness\nAt this stage in our data cleaning and EDA workflow, we’ve achieved quite a lot: we’ve identified how our data is structured, come to terms with what information it encodes, and gained insight as to how it was generated. Throughout this process, we should always recall the original intent of our work in Data Science – to use data to better understand and model the real world. To achieve this goal, we need to ensure that the data we use is faithful to reality; that is, that our data accurately captures the “real world.”\nData used in research or industry is often “messy” – there may be errors or inaccuracies that impact the faithfulness of the dataset. Signs that data may not be faithful include:\n\nUnrealistic or “incorrect” values, such as negative counts, locations that don’t exist, or dates set in the future\nViolations of obvious dependencies, like an age that does not match a birthday\nClear signs that data was entered by hand, which can lead to spelling errors or fields that are incorrectly shifted\nSigns of data falsification, such as fake email addresses or repeated use of the same names\nDuplicated records or fields containing the same information\n\nA common issue encountered with real-world datasets is that of missing data. One strategy to resolve this is to simply drop any records with missing values from the dataset. This does, however, introduce the risk of inducing biases – it is possible that the missing or corrupt records may be systemically related to some feature of interest in the data.\nAnother method to address missing data is to perform imputation: infer the missing values using other data available in the dataset. There is a wide variety of imputation techniques that can be implemented; some of the most common are listed below.\n\nAverage imputation: replace missing values with the average value for that field\nHot deck imputation: replace missing values with some random value\nRegression imputation: develop a model to predict missing values\nMultiple imputation: replace missing values with multiple random values\n\nRegardless of the strategy used to deal with missing data, we should think carefully about why particular records or fields may be missing – this can help inform whether or not the absence of these values is signficant in some meaningful way."
  },
  {
    "objectID": "eda/eda.html#csvs-and-field-names",
    "href": "eda/eda.html#csvs-and-field-names",
    "title": "5  Data Cleaning and EDA",
    "section": "6.1 CSVs and Field Names",
    "text": "6.1 CSVs and Field Names\nSuppose Table 1 was saved as a CSV file located in data/cdc_tuberculosis.csv.\nWe can then explore the CSV (which is a text file, and does not contain binary-encoded data) in many ways: 1. Using a text editor like emacs, vim, VSCode, etc. 2. Opening the CSV directly in DataHub (read-only), Excel, Google Sheets, etc. 3. The Python file object 4. pandas, using pd.read_csv()\n1, 2. Let’s start with the first two so we really solidify the idea of a CSV as rectangular data (i.e., tabular data) stored as comma-separated values.\n\nNext, let’s try using the Python file object. Let’s check out the first three lines:\n\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(row)\n        i += 1\n        if i > 3:\n            break\n\n\n,No. of TB cases,,,TB incidence,,\n\nU.S. jurisdiction,2019,2020,2021,2019,2020,2021\n\nTotal,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\n\nAlabama,87,72,92,1.77,1.43,1.83\n\n\n\nWhoa, why are there blank lines interspaced between the lines of the CSV?\nYou may recall that all line breaks in text files are encoded as the special newline character \\n. Python’s print() prints each string (including the newline), and an additional newline on top of that.\nIf you’re curious, we can use the repr() function to return the raw string with all special characters:\n\n\nCode\nwith open(\"data/cdc_tuberculosis.csv\", \"r\") as f:\n    i = 0\n    for row in f:\n        print(repr(row)) # print raw strings\n        i += 1\n        if i > 3:\n            break\n\n\n',No. of TB cases,,,TB incidence,,\\n'\n'U.S. jurisdiction,2019,2020,2021,2019,2020,2021\\n'\n'Total,\"8,900\",\"7,173\",\"7,860\",2.71,2.16,2.37\\n'\n'Alabama,87,72,92,1.77,1.43,1.83\\n'\n\n\n\nFinally, let’s see the tried-and-true Data 100 approach: pandas.\n\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\")\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      No. of TB cases\n      Unnamed: 2\n      Unnamed: 3\n      TB incidence\n      Unnamed: 5\n      Unnamed: 6\n    \n  \n  \n    \n      0\n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.00\n      2020.00\n      2021.00\n    \n    \n      1\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      2\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      3\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      4\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n  \n\n\n\n\nWait, what’s up with the “Unnamed” column names? And the first row, for that matter?\nCongratulations – you’re ready to wrangle your data. Because of how things are stored, we’ll need to clean the data a bit to name our columns better.\nA reasonable first step is to identify the row with the right header. The pd.read_csv() function (documentation) has the convenient header parameter:\n\n\nCode\ntb_df = pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1) # row index\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      2019\n      2020\n      2021\n      2019.1\n      2020.1\n      2021.1\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nWait…but now we can’t differentiate betwen the “Number of TB cases” and “TB incidence” year columns. pandas has tried to make our lives easier by automatically adding “.1” to the latter columns, but this doesn’t help us as humans understand the data.\nWe can do this manually with df.rename() (documentation):\n\n\nCode\nrename_dict = {'2019': 'TB cases 2019',\n               '2020': 'TB cases 2020',\n               '2021': 'TB cases 2021',\n               '2019.1': 'TB incidence 2019',\n               '2020.1': 'TB incidence 2020',\n               '2021.1': 'TB incidence 2021'}\ntb_df = tb_df.rename(columns=rename_dict)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8,900\n      7,173\n      7,860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28"
  },
  {
    "objectID": "eda/eda.html#record-granularity",
    "href": "eda/eda.html#record-granularity",
    "title": "5  Data Cleaning and EDA",
    "section": "6.2 Record Granularity",
    "text": "6.2 Record Granularity\nYou might already be wondering: What’s up with that first record?\nRow 0 is what we call a rollup record, or summary record. It’s often useful when displaying tables to humans. The granularity of record 0 (Totals) vs the rest of the records (States) is different.\nOkay, EDA step two. How was the rollup record aggregated?\nLet’s check if Total TB cases is the sum of all state TB cases. If we sum over all rows, we should get 2x the total cases in each of our TB cases by year (why?).\n\n\nCode\ntb_df.sum(axis=0)\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019        8,9008758183642,111666718245583029973261085237...\nTB cases 2020        7,1737258136591,706525417194122219282169239376...\nTB cases 2021        7,8609258129691,750585443194992281064255127494...\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nWhoa, what’s going on? Check out the column types:\n\n\nCode\ntb_df.dtypes\n\n\nU.S. jurisdiction     object\nTB cases 2019         object\nTB cases 2020         object\nTB cases 2021         object\nTB incidence 2019    float64\nTB incidence 2020    float64\nTB incidence 2021    float64\ndtype: object\n\n\nLooks like those commas are causing all TB cases to be read as the object datatype, or storage type (close to the Python string datatype), so pandas is concatenating strings instead of adding integers.\nFortunately read_csv also has a thousands parameter (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):\n\n\nCode\n# improve readability: chaining method calls with outer parentheses/line breaks\ntb_df = (\n    pd.read_csv(\"data/cdc_tuberculosis.csv\", header=1, thousands=',')\n    .rename(columns=rename_dict)\n)\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      0\n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ntb_df.sum()\n\n\nU.S. jurisdiction    TotalAlabamaAlaskaArizonaArkansasCaliforniaCol...\nTB cases 2019                                                    17800\nTB cases 2020                                                    14346\nTB cases 2021                                                    15720\nTB incidence 2019                                               109.94\nTB incidence 2020                                                93.09\nTB incidence 2021                                               102.94\ndtype: object\n\n\nThe Total TB cases look right. Phew!\nLet’s just look at the records with state-level granularity:\n\n\nCode\nstate_tb_df = tb_df[1:]\nstate_tb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n  \n  \n    \n      1\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      2\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      3\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      4\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n    \n      5\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46"
  },
  {
    "objectID": "eda/eda.html#gather-more-data-census",
    "href": "eda/eda.html#gather-more-data-census",
    "title": "5  Data Cleaning and EDA",
    "section": "6.3 Gather More Data: Census",
    "text": "6.3 Gather More Data: Census\nU.S. Census population estimates source (2019), source (2020-2021).\nRunning the below cells cleans the data. There are a few new methods here: * df.convert_dtypes() (documentation) conveniently converts all float dtypes into ints and is out of scope for the class. * df.drop_na() (documentation) will be explained in more detail next time.\n\n\nCode\n# 2010s census data\ncensus_2010s_df = pd.read_csv(\"data/nst-est2019-01.csv\", header=3, thousands=\",\")\ncensus_2010s_df = (\n    census_2010s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Census\", \"Estimates Base\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2010s_df['Geographic Area'] = census_2010s_df['Geographic Area'].str.strip('.')\n\n# with pd.option_context('display.min_rows', 30): # shows more rows\n#     display(census_2010s_df)\n    \ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n  \n  \n    \n      0\n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      1\n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      2\n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      3\n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      4\n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nOccasionally, you will want to modify code that you have imported. To reimport those modifications you can either use the python importlib library:\nfrom importlib import reload\nreload(utils)\nor use iPython magic which will intelligently import code when files change:\n%load_ext autoreload\n%autoreload 2\n\n\nCode\n# census 2020s data\ncensus_2020s_df = pd.read_csv(\"data/NST-EST2022-POP.csv\", header=3, thousands=\",\")\ncensus_2020s_df = (\n    census_2020s_df\n    .reset_index()\n    .drop(columns=[\"index\", \"Unnamed: 1\"])\n    .rename(columns={\"Unnamed: 0\": \"Geographic Area\"})\n    .convert_dtypes()                 # \"smart\" converting of columns, use at your own risk\n    .dropna()                         # we'll introduce this next time\n)\ncensus_2020s_df['Geographic Area'] = census_2020s_df['Geographic Area'].str.strip('.')\n\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      Geographic Area\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      1\n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      2\n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      3\n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      4\n      West\n      78650958\n      78589763\n      78743364"
  },
  {
    "objectID": "eda/eda.html#joining-data-on-primary-keys",
    "href": "eda/eda.html#joining-data-on-primary-keys",
    "title": "5  Data Cleaning and EDA",
    "section": "6.4 Joining Data on Primary Keys",
    "text": "6.4 Joining Data on Primary Keys\nTime to merge! Here we use the DataFrame method df1.merge(right=df2, ...) on DataFrame df1 (documentation). Contrast this with the function pd.merge(left=df1, right=df2, ...) (documentation). Feel free to use either.\n\n\nCode\n# merge TB dataframe with two US census dataframes\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .merge(right=census_2020s_df,\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      Geographic Area_x\n      2010\n      2011\n      ...\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      Geographic Area_y\n      2020\n      2021\n      2022\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      Alabama\n      4785437\n      4799069\n      ...\n      4841799\n      4852347\n      4863525\n      4874486\n      4887681\n      4903185\n      Alabama\n      5031362\n      5049846\n      5074296\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      Alaska\n      713910\n      722128\n      ...\n      736283\n      737498\n      741456\n      739700\n      735139\n      731545\n      Alaska\n      732923\n      734182\n      733583\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      Arizona\n      6407172\n      6472643\n      ...\n      6730413\n      6829676\n      6941072\n      7044008\n      7158024\n      7278717\n      Arizona\n      7179943\n      7264877\n      7359197\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      Arkansas\n      2921964\n      2940667\n      ...\n      2967392\n      2978048\n      2989918\n      3001345\n      3009733\n      3017804\n      Arkansas\n      3014195\n      3028122\n      3045637\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      California\n      37319502\n      37638369\n      ...\n      38596972\n      38918045\n      39167117\n      39358497\n      39461588\n      39512223\n      California\n      39501653\n      39142991\n      39029342\n    \n  \n\n5 rows × 22 columns\n\n\n\nThis is a little unwieldy. We could either drop the unneeded columns now, or just merge on smaller census DataFrames. Let’s do the latter.\n\n\nCode\n# try merging again, but cleaner this time\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"Geographic Area\", \"2019\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n    .merge(right=census_2020s_df[[\"Geographic Area\", \"2020\", \"2021\"]],\n           left_on=\"U.S. jurisdiction\", right_on=\"Geographic Area\")\n    .drop(columns=\"Geographic Area\")\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991"
  },
  {
    "objectID": "eda/eda.html#reproducing-data-compute-incidence",
    "href": "eda/eda.html#reproducing-data-compute-incidence",
    "title": "5  Data Cleaning and EDA",
    "section": "6.5 Reproducing Data: Compute Incidence",
    "text": "6.5 Reproducing Data: Compute Incidence\nLet’s recompute incidence to make sure we know where the original CDC numbers came from.\nFrom the CDC report: TB incidence is computed as “Cases per 100,000 persons using mid-year population estimates from the U.S. Census Bureau.”\nIf we define a group as 100,000 people, then we can compute the TB incidence for a given state population as\n\\[\\text{TB incidence} = \\frac{\\text{TB cases in population}}{\\text{groups in population}} = \\frac{\\text{TB cases in population}}{\\text{population}/100000} \\]\n\\[= \\frac{\\text{TB cases in population}}{\\text{population}} \\times 100000\\]\nLet’s try this for 2019:\n\n\nCode\ntb_census_df[\"recompute incidence 2019\"] = tb_census_df[\"TB cases 2019\"]/tb_census_df[\"2019\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n    \n  \n\n\n\n\nAwesome!!!\nLet’s use a for-loop and Python format strings to compute TB incidence for all years. Python f-strings are just used for the purposes of this demo, but they’re handy to know when you explore data beyond this course (Python documentation).\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      U.S. jurisdiction\n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      0\n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      1\n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      2\n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      3\n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n    \n      4\n      California\n      2111\n      1706\n      1750\n      5.35\n      4.32\n      4.46\n      39512223\n      39501653\n      39142991\n      5.342651\n      4.318807\n      4.470788\n    \n  \n\n\n\n\nThese numbers look pretty close!!! There are a few errors in the hundredths place, particularly in 2021. It may be useful to further explore reasons behind this discrepancy.\n\n\nCode\ntb_census_df.describe()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      count\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      51.000000\n      5.100000e+01\n      5.100000e+01\n      5.100000e+01\n      51.000000\n      51.000000\n      51.000000\n    \n    \n      mean\n      174.509804\n      140.647059\n      154.117647\n      2.102549\n      1.782941\n      1.971961\n      6.436069e+06\n      6.500226e+06\n      6.510423e+06\n      2.104969\n      1.784655\n      1.969928\n    \n    \n      std\n      341.738752\n      271.055775\n      286.781007\n      1.498745\n      1.337414\n      1.478468\n      7.360660e+06\n      7.408168e+06\n      7.394300e+06\n      1.500236\n      1.338263\n      1.474929\n    \n    \n      min\n      1.000000\n      0.000000\n      2.000000\n      0.170000\n      0.000000\n      0.210000\n      5.787590e+05\n      5.776050e+05\n      5.794830e+05\n      0.172783\n      0.000000\n      0.210049\n    \n    \n      25%\n      25.500000\n      29.000000\n      23.000000\n      1.295000\n      1.210000\n      1.235000\n      1.789606e+06\n      1.820311e+06\n      1.844920e+06\n      1.297485\n      1.211433\n      1.233905\n    \n    \n      50%\n      70.000000\n      67.000000\n      69.000000\n      1.800000\n      1.520000\n      1.700000\n      4.467673e+06\n      4.507445e+06\n      4.506589e+06\n      1.808606\n      1.521612\n      1.694502\n    \n    \n      75%\n      180.500000\n      139.000000\n      150.000000\n      2.575000\n      1.990000\n      2.220000\n      7.446805e+06\n      7.451987e+06\n      7.502811e+06\n      2.577577\n      1.993607\n      2.219482\n    \n    \n      max\n      2111.000000\n      1706.000000\n      1750.000000\n      7.910000\n      7.920000\n      7.920000\n      3.951222e+07\n      3.950165e+07\n      3.914299e+07\n      7.928425\n      7.913519\n      7.899949"
  },
  {
    "objectID": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "href": "eda/eda.html#bonus-eda-reproducing-the-reported-statistic",
    "title": "5  Data Cleaning and EDA",
    "section": "6.6 Bonus EDA: Reproducing the reported statistic",
    "text": "6.6 Bonus EDA: Reproducing the reported statistic\nHow do we reproduce that reported statistic in the original CDC report?\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nThis is TB incidence computed across the entire U.S. population! How do we reproduce this * We need to reproduce the “Total” TB incidences in our rolled record. * But our current tb_census_df only has 51 entries (50 states plus Washington, D.C.). There is no rolled record. * What happened…?\nLet’s get exploring!\nBefore we keep exploring, we’ll set all indexes to more meaningful values, instead of just numbers that pertained to some row at some point. This will make our cleaning slightly easier.\n\n\nCode\ntb_df = tb_df.set_index(\"U.S. jurisdiction\")\ntb_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\n\n\nCode\ncensus_2010s_df = census_2010s_df.set_index(\"Geographic Area\")\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\ncensus_2020s_df = census_2020s_df.set_index(\"Geographic Area\")\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      United States\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\nIt turns out that our merge above only kept state records, even though our original tb_df had the “Total” rolled record:\n\n\nCode\ntb_df.head()\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n    \n    \n      U.S. jurisdiction\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n    \n  \n\n\n\n\nRecall that merge by default does an inner merge by default, meaning that it only preserves keys that are present in both DataFrames.\nThe rolled records in our census dataframes have different Geographic Area fields, which was the key we merged on:\n\n\nCode\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      United States\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\nThe Census DataFrame has several rolled records. The aggregate record we are looking for actually has the Geographic Area named “United States”.\nOne straightforward way to get the right merge is to rename the value itself. Because we now have the Geographic Area index, we’ll use df.rename() (documentation):\n\n\nCode\n# rename rolled record for 2010s\ncensus_2010s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2010s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n    \n    \n      Geographic Area\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Total\n      309321666\n      311556874\n      313830990\n      315993715\n      318301008\n      320635163\n      322941311\n      324985539\n      326687501\n      328239523\n    \n    \n      Northeast\n      55380134\n      55604223\n      55775216\n      55901806\n      56006011\n      56034684\n      56042330\n      56059240\n      56046620\n      55982803\n    \n    \n      Midwest\n      66974416\n      67157800\n      67336743\n      67560379\n      67745167\n      67860583\n      67987540\n      68126781\n      68236628\n      68329004\n    \n    \n      South\n      114866680\n      116006522\n      117241208\n      118364400\n      119624037\n      120997341\n      122351760\n      123542189\n      124569433\n      125580448\n    \n    \n      West\n      72100436\n      72788329\n      73477823\n      74167130\n      74925793\n      75742555\n      76559681\n      77257329\n      77834820\n      78347268\n    \n  \n\n\n\n\n\n\nCode\n# same, but for 2020s rename rolled record\ncensus_2020s_df.rename(index={'United States':'Total'}, inplace=True)\ncensus_2020s_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      2020\n      2021\n      2022\n    \n    \n      Geographic Area\n      \n      \n      \n    \n  \n  \n    \n      Total\n      331511512\n      332031554\n      333287557\n    \n    \n      Northeast\n      57448898\n      57259257\n      57040406\n    \n    \n      Midwest\n      68961043\n      68836505\n      68787595\n    \n    \n      South\n      126450613\n      127346029\n      128716192\n    \n    \n      West\n      78650958\n      78589763\n      78743364\n    \n  \n\n\n\n\n\nNext let’s rerun our merge. Note the different chaining, because we are now merging on indexes (df.merge() documentation).\n\n\nCode\ntb_census_df = (\n    tb_df\n    .merge(right=census_2010s_df[[\"2019\"]],\n           left_index=True, right_index=True)\n    .merge(right=census_2020s_df[[\"2020\", \"2021\"]],\n           left_index=True, right_index=True)\n)\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n    \n  \n\n\n\n\n\nFinally, let’s recompute our incidences:\n\n\nCode\n# recompute incidence for all years\nfor year in [2019, 2020, 2021]:\n    tb_census_df[f\"recompute incidence {year}\"] = tb_census_df[f\"TB cases {year}\"]/tb_census_df[f\"{year}\"]*100000\ntb_census_df.head(5)\n\n\n\n\n\n\n  \n    \n      \n      TB cases 2019\n      TB cases 2020\n      TB cases 2021\n      TB incidence 2019\n      TB incidence 2020\n      TB incidence 2021\n      2019\n      2020\n      2021\n      recompute incidence 2019\n      recompute incidence 2020\n      recompute incidence 2021\n    \n  \n  \n    \n      Total\n      8900\n      7173\n      7860\n      2.71\n      2.16\n      2.37\n      328239523\n      331511512\n      332031554\n      2.711435\n      2.163726\n      2.367245\n    \n    \n      Alabama\n      87\n      72\n      92\n      1.77\n      1.43\n      1.83\n      4903185\n      5031362\n      5049846\n      1.774357\n      1.431024\n      1.821838\n    \n    \n      Alaska\n      58\n      58\n      58\n      7.91\n      7.92\n      7.92\n      731545\n      732923\n      734182\n      7.928425\n      7.913519\n      7.899949\n    \n    \n      Arizona\n      183\n      136\n      129\n      2.51\n      1.89\n      1.77\n      7278717\n      7179943\n      7264877\n      2.514179\n      1.894165\n      1.775667\n    \n    \n      Arkansas\n      64\n      59\n      69\n      2.12\n      1.96\n      2.28\n      3017804\n      3014195\n      3028122\n      2.120747\n      1.957405\n      2.278640\n    \n  \n\n\n\n\nWe reproduced the total U.S. incidences correctly!\nWe’re almost there. Let’s revisit the quote:\n\nReported TB incidence (cases per 100,000 persons) increased 9.4%, from 2.2 during 2020 to 2.4 during 2021 but was lower than incidence during 2019 (2.7). Increases occurred among both U.S.-born and non–U.S.-born persons.\n\nRecall that percent change from \\(A\\) to \\(B\\) is computed as \\(\\text{percent change} = \\frac{B - A}{A} \\times 100\\).\n\n\nCode\nincidence_2020 = tb_census_df.loc['Total', 'recompute incidence 2020']\nincidence_2020\n\n\n2.1637257652759883\n\n\n\n\nCode\nincidence_2021 = tb_census_df.loc['Total', 'recompute incidence 2021']\nincidence_2021\n\n\n2.3672448914298068\n\n\n\n\nCode\ndifference = (incidence_2021 - incidence_2020)/incidence_2020 * 100\ndifference\n\n\n9.405957511804143"
  }
]