[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Principles and Techniques of Data Science",
    "section": "",
    "text": "Welcome"
  },
  {
    "objectID": "index.html#about-the-course-notes",
    "href": "index.html#about-the-course-notes",
    "title": "Principles and Techniques of Data Science",
    "section": "About the Course Notes",
    "text": "About the Course Notes\nThis text offers supplementary resources to accompany lectures presented in the Fall 2023 Edition of the UC Berkeley course Data 100: Principles and Techniques of Data Science.\nNew notes will be added each week to accompany live lectures. See the full calendar of lectures on the course website.\nIf you spot any typos or would like to suggest any changes, please email us.   Email: data100.instructors@berkeley.edu"
  },
  {
    "objectID": "sql_I/sql_I.html#databases",
    "href": "sql_I/sql_I.html#databases",
    "title": "20  SQL I",
    "section": "20.1 Databases",
    "text": "20.1 Databases\nWith this goal in mind, let’s go back to the very beginning of the lifecycle. We first started our work in data analysis by looking at the pandas library, which offered us powerful tools to manipulate tabular data stored in (primarily) CSV files. CSVs work well when analyzing relatively small datasets that don’t need to be shared across many users. In research and industry, however, data scientists often need to access enormous bodies of data that cannot be easily stored in a CSV format. Collaborating with others when working with CSVs can also be tricky – a real-world data scientist may run into problems when multiple users try to make modifications, or, even worse, security issues with who should and should not have access to the data.\nA database is a large, organized collection of data. Databases are administered by Database Management Systems (DBMS), which are software systems that store, manage, and facilitate access to one or more databases. Databases help mitigate many of the issues that come with using CSVs for data storage: they provide reliable storage that can survive system crashes or disk failures, are optimized to compute on data that does not fit into memory, and contain special data structures to improve performance. Using databases rather than CSVs offers further benefits from the standpoint of data management. A DBMS can apply settings that configure how data is organized, block certain data anomalies (for example, enforcing non-negative weights or ages), and determine who is allowed access to the data. It can also ensure safe concurrent operations where multiple users reading and writing to the database will not lead to fatal errors.\nAs you may have guessed, we can’t use our usual pandas methods to work with data in a database. Instead, we’ll turn to Structured Query Language."
  },
  {
    "objectID": "sql_I/sql_I.html#structured-query-language-and-database-schema",
    "href": "sql_I/sql_I.html#structured-query-language-and-database-schema",
    "title": "20  SQL I",
    "section": "20.2 Structured Query Language and Database Schema",
    "text": "20.2 Structured Query Language and Database Schema\nStructured Query Language, or SQL (commonly pronounced “sequel,” though this is the subject of fierce debate), is a special programming language designed to communicate with databases. You may have encountered it in classes like CS 61A or Data C88C before. Unlike Python, it is a declarative programming language – this means that rather than writing the exact logic needed to complete a task, a piece of SQL code “declares” what the desired final output should be and leaves the program to determine what logic should be implemented.\nIt is important to reiterate that SQL is an entirely different language from Python. However, Python does have special engines that allow us to run SQL code in a Jupyter notebook. While this is typically not how SQL is used outside of an educational setting, we will be using this workflow to illustrate how SQL queries are constructed using the tools we’ve already worked with this semester. You will learn more about how to run SQL queries in Jupyter in Lab 10.\nThe syntax below will seem unfamiliar to you; for now, just focus on understanding the output displayed. We will clarify the SQL code in a bit.\nTo start, we’ll look at a database called basic_examples.db.\n\n# Load the SQL Alchemy Python library\nimport sqlalchemy\nimport pandas as pd\n\n\n# load %%sql cell magic\n%load_ext sql\n\nConnect to the SQLite database basic_examples.db.\n\n%%sql\nsqlite:///data/basic_examples.db \n\n\n%%sql\nSELECT * \nFROM sqlite_master\nWHERE type=\"table\"\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\ntable\nsqlite_sequence\nsqlite_sequence\n7\nCREATE TABLE sqlite_sequence(name,seq)\n\n\ntable\nDragon\nDragon\n2\nCREATE TABLE Dragon (\nname TEXT PRIMARY KEY,\nyear INTEGER CHECK (year &gt;= 2000),\ncute INTEGER\n)\n\n\ntable\nDish\nDish\n4\nCREATE TABLE Dish (\nname TEXT PRIMARY KEY,\ntype TEXT,\ncost INTEGER CHECK (cost &gt;= 0)\n)\n\n\ntable\nScene\nScene\n6\nCREATE TABLE Scene (\nid INTEGER PRIMARY KEY AUTOINCREMENT,\nbiome TEXT NOT NULL,\ncity TEXT NOT NULL,\nvisitors INTEGER CHECK (visitors &gt;= 0),\ncreated_at DATETIME DEFAULT (DATETIME('now'))\n)\n\n\n\n\n\nThe summary above displays information about the database. The database contains four tables, named sqlite_sequence, Dragon, Dish, and Scene. The rightmost column above lists the command that was used to construct each table.\nLet’s look more closely at the command used to create the Dragon table (the second entry above).\nCREATE TABLE Dragon (name TEXT PRIMARY KEY,\n                     year INTEGER CHECK (year &gt;= 2000),\n                     cute INTEGER)\nThe statement CREATE TABLE is used to specify the schema of the table – a description of what logic is used to organize the table. Schema follows a set format:\n\nColName: the name of a column\nDataType: the type of data to be stored in a column. Some of the most common SQL data types are INT (integers), FLOAT (floating point numbers), TEXT (strings), BLOB (arbitrary data, such as audio/video files), and DATETIME (a date and time).\nConstraint: some restriction on the data to be stored in the column. Common constraints are CHECK (data must obey a certain condition), PRIMARY KEY (designate a column as the table’s primary key), NOT NULL (data cannot be null), and DEFAULT (a default fill value if no specific entry is given).\n\nWe see that Dragon contains three columns. The first of these, \"name\", contains text data. It is designated as the primary key of the table; that is, the data contained in \"name\" uniquely identifies each entry in the table. Because \"name\" is the primary key of the table, no two entries in the table can have the same name – a given value of \"name\" is unique to each dragon. The \"year\" column contains integer data, with the constraint that year values must be greater than or equal to 2000. The final column, \"cute\", contains integer data with no restrictions on allowable values.\nWe can verify this by viewing Dragon itself.\n\n%%sql\nSELECT *\nFROM Dragon\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\nhiccup\n2010\n10\n\n\ndrogon\n2011\n-100\n\n\ndragon 2\n2019\n0\n\n\n\n\n\nDatabase tables (also referred to as relations) are structured much like DataFrames in pandas. Each row, sometimes called a tuple, represents a single record in the dataset. Each column, sometimes called an attribute or field, describes some feature of the record."
  },
  {
    "objectID": "sql_I/sql_I.html#selecting-from-tables",
    "href": "sql_I/sql_I.html#selecting-from-tables",
    "title": "20  SQL I",
    "section": "20.3 SELECTing From Tables",
    "text": "20.3 SELECTing From Tables\nTo extract and manipulate data stored in a SQL table, we will need to familiarize ourselves with the syntax to write pieces of SQL code, which we call queries.\nThe basic unit of a SQL query is the SELECT statement. SELECT specifies what columns we would like to extract from a given table. We use FROM to tell SQL the table from which we want to SELECT our data.\n\n%%sql\nSELECT *\nFROM Dragon\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\nhiccup\n2010\n10\n\n\ndrogon\n2011\n-100\n\n\ndragon 2\n2019\n0\n\n\n\n\n\nIn SQL, * means “everything.” The query above grabs all the columns in Dragon and displays them in the outputted table. We can also specify a specific subset of columns to be SELECTed. Notice that the outputted columns appear in the order that they were SELECTed.\n\n%%sql\nSELECT cute, year\nFROM Dragon\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\ncute\nyear\n\n\n\n\n10\n2010\n\n\n-100\n2011\n\n\n0\n2019\n\n\n\n\n\nAnd just like that, we’ve already written two SQL queries. There are a few points of note in the queries above. Firstly, notice that every “verb” is written in uppercase. It is convention to write SQL operations in capital letters, but your code will run just fine even if you choose to keep things in lowercase. Second, the query above separates each statement with a new line. SQL queries are not impacted by whitespace within the query; this means that SQL code is typically written with a new line after each statement to make things more readable. The semicolon (;) indicates the end of a query. There are some “flavors” of SQL in which a query will not run if no semicolon is present; however, in Data 100, the SQL version we will use works with or without an ending semicolon. Queries in these notes will end with semicolons to build up good habits.\nThe AS keyword allows us to give a column a new name (called an alias) after it has been SELECTed. The general syntax is:\nSELECT column_name_in_database_table AS new_name_in_output_table\n\n%%sql\nSELECT cute AS cuteness, year AS birth\nFROM Dragon\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\ncuteness\nbirth\n\n\n\n\n10\n2010\n\n\n-100\n2011\n\n\n0\n2019\n\n\n\n\n\nTo SELECT only the unique values in a column, we use the DISTINCT keyword. This will cause any any duplicate entries in a column to be removed. If we want to find only the unique years in Dragon, without any repeats, we would write:\n\n%%sql\nSELECT DISTINCT year\nFROM Dragon\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nyear\n\n\n\n\n2010\n\n\n2011\n\n\n2019\n\n\n\n\n\nEvery SQL query must include both a SELECT and FROM statement. Intuitively, this makes sense – we know that we’ll want to extract some piece of information from the table; to do so, we also need to indicate what table we want to consider.\nIt is important to note that SQL enforces a strict “order of operations” – SQL clauses must always follow the same sequence. For example, the SELECT statement must always precede FROM. This means that any SQL query will follow the same structure.\nSELECT &lt;column list&gt;\nFROM &lt;table&gt;\n[additional clauses]\nThe additional clauses that we use depend on the specific task trying to be achieved. We may refine our query to filter on a certain condition, aggregate a particular column, or join several tables together. We will spend the rest of this lecture outlining some useful clauses to build up our understanding of the order of operations."
  },
  {
    "objectID": "sql_I/sql_I.html#applying-where-conditions",
    "href": "sql_I/sql_I.html#applying-where-conditions",
    "title": "20  SQL I",
    "section": "20.4 Applying WHERE Conditions",
    "text": "20.4 Applying WHERE Conditions\nThe WHERE keyword is used to select only some rows of a table, filtered on a given Boolean condition.\n\n%%sql\nSELECT name, year\nFROM Dragon\nWHERE cute &gt; 0\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\n\n\n\n\nhiccup\n2010\n\n\n\n\n\nWe can add complexity to the WHERE condition using the keywords AND, OR, and NOT, much like we would in Python.\n\n%%sql\nSELECT name, year\nFROM Dragon\nWHERE cute &gt; 0 OR year &gt; 2013\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\n\n\n\n\nhiccup\n2010\n\n\ndragon 2\n2019\n\n\n\n\n\nTo spare ourselves needing to write complicated logical expressions by combining several conditions, we can also filter for entries that are IN a specified list of values. This is similar to the use of in or .isin in Python.\n\n%%sql\nSELECT name, year\nFROM Dragon\nWHERE name IN (\"hiccup\", \"puff\")\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\n\n\n\n\nhiccup\n2010\n\n\n\n\n\nYou may have noticed earlier that our table actually has a missing value. In SQL, missing data is given the special value NULL. NULL behaves in a fundamentally different way to other data types. We can’t use the typical operators (=, &gt;, and &lt;) on NULL values (in fact, NULL == NULL returns False!); instead, we check to see if a value IS or IS NOT NULL.\n\n%%sql\nSELECT *\nFROM Dragon\nWHERE cute IS NOT NULL\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\nhiccup\n2010\n10\n\n\ndrogon\n2011\n-100\n\n\ndragon 2\n2019\n0"
  },
  {
    "objectID": "sql_I/sql_I.html#sorting-and-restricting-output",
    "href": "sql_I/sql_I.html#sorting-and-restricting-output",
    "title": "20  SQL I",
    "section": "20.5 Sorting and Restricting Output",
    "text": "20.5 Sorting and Restricting Output\nWhat if we want the output table to appear in a certain order? The ORDER BY keyword behaves similarly to .sort_values() in pandas.\n\n%%sql\nSELECT *\nFROM Dragon\nORDER BY cute\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\ndrogon\n2011\n-100\n\n\ndragon 2\n2019\n0\n\n\nhiccup\n2010\n10\n\n\n\n\n\nBy default, ORDER BY will display results in ascending order (with the lowest values at the top of the table). To sort in descending order, we use the DESC keyword after specifying the column to be used for ordering.\n\n%%sql\nSELECT *\nFROM Dragon\nORDER BY cute DESC\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\nhiccup\n2010\n10\n\n\ndragon 2\n2019\n0\n\n\ndrogon\n2011\n-100\n\n\n\n\n\nWe can also tell SQL to ORDER BY two columns at once. This will sort the table by the first listed column, then use the values in the second listed column to break any ties.\n\n%%sql\nSELECT *\nFROM Dragon\nORDER BY name, cute\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\ndragon 2\n2019\n0\n\n\ndrogon\n2011\n-100\n\n\nhiccup\n2010\n10\n\n\n\n\n\nIn many instances, we are only concerned with a certain number of rows in the output table (for example, wanting to find the first two dragons in the table). The LIMIT keyword restricts the output to a specified number of rows. It serves a function similar to that of .head() in pandas.\n\n%%sql\nSELECT *\nFROM Dragon\nLIMIT 2\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\nhiccup\n2010\n10\n\n\ndrogon\n2011\n-100\n\n\n\n\n\nThe OFFSET keyword indicates the index at which LIMIT should start. In other words, we can use OFFSET to shift where the LIMITing begins by a specified number of rows. For example, we might care about the dragons that are at positions #2 and #3 in the table.\n\n%%sql\nSELECT *\nFROM Dragon\nLIMIT 2\nOFFSET 1\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\nyear\ncute\n\n\n\n\ndrogon\n2011\n-100\n\n\ndragon 2\n2019\n0\n\n\n\n\n\nLet’s summarize what we’ve learned so far. We know that SELECT and FROM are the fundamental building blocks of any SQL query. We can augment these two keywords with additional clauses to refine the data in our output table.\nAny clauses that we include must follow a strict ordering within the query:\nSELECT &lt;column list&gt;\nFROM &lt;table&gt;\n[WHERE &lt;predicate&gt;]\n[ORDER BY &lt;column list&gt;]\n[LIMIT &lt;number of rows&gt;]\n[OFFSET &lt;number of rows&gt;]\nHere, any clause contained in square brackets [ ] is optional – we only need to use the keyword if it is relevant to the table operation we want to perform. Also note that by convention, we use all caps for keywords in SQL statements and use newlines to make code more readable."
  },
  {
    "objectID": "sql_I/sql_I.html#aggregating-with-group-by",
    "href": "sql_I/sql_I.html#aggregating-with-group-by",
    "title": "20  SQL I",
    "section": "20.6 Aggregating with GROUP BY",
    "text": "20.6 Aggregating with GROUP BY\nAt this point, we’ve seen that SQL offers much of the same functionality that was given to us by pandas. We can extract data from a table, filter it, and reorder it to suit our needs.\nIn pandas, much of our analysis work relied heavily on being able to use .groupby() to aggregate across the rows of our dataset. SQL’s answer to this task is the (very conveniently named) GROUP BY clause. While the outputs of GROUP BY are similar to those of .groupby() – in both cases, we obtain an output table where some column has been used for grouping – the syntax and logic used to group data in SQL are fairly different to the pandas implementation.\nTo illustrate GROUP BY, we will consider the Dish table from the basic_examples.db database.\n\n%%sql\nSELECT * \nFROM Dish\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nname\ntype\ncost\n\n\n\n\nravioli\nentree\n10\n\n\nramen\nentree\n13\n\n\ntaco\nentree\n7\n\n\nedamame\nappetizer\n4\n\n\nfries\nappetizer\n4\n\n\npotsticker\nappetizer\n4\n\n\nice cream\ndessert\n5\n\n\n\n\n\nSay we wanted to find the total costs of dishes of a certain type. To accomplish this, we would write the following code.\n\n%%sql\nSELECT type, SUM(cost)\nFROM Dish\nGROUP BY type\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\ntype\nSUM(cost)\n\n\n\n\nappetizer\n12\n\n\ndessert\n5\n\n\nentree\n30\n\n\n\n\n\nWhat is going on here? The statement GROUP BY type tells SQL to group the data based on the value contained in the type column (whether a record is an appetizer, entree, or dessert). SUM(cost) sums up the costs of dishes in each type and displays the result in the output table.\nYou may be wondering: why does SUM(cost) come before the command to GROUP BY type? Don’t we need to form groups before we can count the number of entries in each?\nRemember that SQL is a declarative programming language – a SQL programmer simply states what end result they would like to see, and leaves the task of figuring out how to obtain this result to SQL itself. This means that SQL queries sometimes don’t follow what a reader sees as a “logical” sequence of thought. Instead, SQL requires that we follow its set order of operations when constructing queries. So long as we follow this ordering, SQL will handle the underlying logic.\nIn practical terms: our goal with this query was to output the total costs of each type. To communicate this to SQL, we say that we want to SELECT the SUMmed cost values for each type group.\nThere are many aggregation functions that can be used to aggregate the data contained in each group. Some common examples are:\n\nCOUNT: count the number of rows associated with each group\nMIN: find the minimum value of each group\nMAX: find the maximum value of each group\nSUM: sum across all records in each group\nAVG: find the average value of each group\n\nWe can easily compute multiple aggregations, all at once (a task that was very tricky in pandas).\n\n%%sql\nSELECT type, SUM(cost), MIN(cost), MAX(name)\nFROM Dish\nGROUP BY type\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\ntype\nSUM(cost)\nMIN(cost)\nMAX(name)\n\n\n\n\nappetizer\n12\n4\npotsticker\n\n\ndessert\n5\n5\nice cream\n\n\nentree\n30\n7\ntaco\n\n\n\n\n\nTo count the number of rows associated with each group, we use the COUNT keyword. Calling COUNT(*) will compute the total number of rows in each group, including rows with null values. Its pandas equivalent is .groupby().size().\n\n%%sql\nSELECT type, COUNT(*)\nFROM Dish\nGROUP BY type\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\ntype\nCOUNT(*)\n\n\n\n\nappetizer\n3\n\n\ndessert\n1\n\n\nentree\n3\n\n\n\n\n\nTo exclude NULL values when counting the rows in each group, we explicitly call COUNT on a column in the table. This is similar to calling .groupby().count() in pandas.\n\n%%sql\nSELECT year, COUNT(cute)\nFROM Dragon\nGROUP BY year\n\n * sqlite:///data/basic_examples.db\nDone.\n\n\n\n\n\nyear\nCOUNT(cute)\n\n\n\n\n2010\n1\n\n\n2011\n1\n\n\n2019\n1\n\n\n\n\n\nWith this definition of GROUP BY in hand, let’s update our SQL order of operations. Remember: every SQL query must list clauses in this order.\nSELECT &lt;column expression list&gt;\nFROM &lt;table&gt;\n[WHERE &lt;predicate&gt;]\n[GROUP BY &lt;column list&gt;]\n[ORDER BY &lt;column list&gt;]\n[LIMIT &lt;number of rows&gt;]\n[OFFSET &lt;number of rows&gt;];\nNote that we can use the AS keyword to rename columns during the selection process and that column expressions may include aggregation functions (MAX, MIN, etc.)."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#decision-boundaries",
    "href": "logistic_regression_2/logistic_reg_2.html#decision-boundaries",
    "title": "23  Logistic Regression II",
    "section": "23.1 Decision Boundaries",
    "text": "23.1 Decision Boundaries\nIn logistic regression, we model the probability that a datapoint belongs to Class 1. Last week, we developed the logistic regression model to predict that probability, but we never actually made any classifications for whether our prediction \\(y\\) belongs in Class 0 or Class 1.\n\\[ p = P(Y=1 | x) = \\frac{1}{1 + e^{-x^T\\theta}}\\]\nA decision rule tells us how to interpret the output of the model to make a decision on how to classify a datapoint. We commonly make decision rules by specifying a threshold, \\(T\\). If the predicted probability is greater than or equal to \\(T\\), predict Class 1. Otherwise, predict Class 0.\n\\[\\hat y = \\text{classify}(x) = \\begin{cases}\n        1, & P(Y=1|x) \\ge T\\\\\n        0, & \\text{otherwise }\n    \\end{cases}\\]\nThe threshold is often set to \\(T = 0.5\\), but not always. We’ll discuss why we might want to use other thresholds \\(T \\neq 0.5\\) later in this lecture.\nUsing our decision rule, we can define a decision boundary as the “line” that splits the data into classes based on its features. For logistic regression, the decision boundary is a hyperplane – a linear combination of the features in p-dimensions – and we can recover it from the final logistic regression model. For example, if we have a model with 2 features (2D), we have \\(\\theta = [\\theta_0, \\theta_1, \\theta_2]\\) including the intercept term, and we can solve for the decision boundary like so:\n\\[\n\\begin{align}\nT &= \\frac{1}{1 + e^{\\theta_0 + \\theta_1 * \\text{feature1} +  \\theta_2 * \\text{feature2}}} \\\\\n1 + e^{\\theta_0 + \\theta_1 \\cdot \\text{feature1} +  \\theta_2  \\cdot  \\text{feature2}} &= \\frac{1}{T} \\\\\ne^{\\theta_0 + \\theta_1  \\cdot  \\text{feature1} +  \\theta_2  \\cdot  \\text{feature2}} &= \\frac{1}{T} - 1 \\\\\n\\theta_0 + \\theta_1  \\cdot  \\text{feature1} +  \\theta_2  \\cdot  \\text{feature2} &= \\log(\\frac{1}{T} - 1)\n\\end{align}\n\\]\nFor a model with 2 features, the decision boundary is a line in terms of its features. To make it easier to visualize, we’ve included an example of a 1-dimensional and a 2-dimensional decision boundary below. Notice how the decision boundary predicted by our logistic regression model perfectly separates the points into two classes.\n\n\n\nIn real life, however, that is often not the case, and we often see some overlap between points of different classes across the decision boundary. The true classes of the 2D data are shown below:\n\n\n\nAs you can see, the decision boundary predicted by our logistic regression does not perfectly separate the two classes. There’s a “muddled” region near the decision boundary where our classifier predicts the wrong class. What would the data have to look like for the classifier to make perfect predictions?"
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#linear-separability-and-regularization",
    "href": "logistic_regression_2/logistic_reg_2.html#linear-separability-and-regularization",
    "title": "23  Logistic Regression II",
    "section": "23.2 Linear Separability and Regularization",
    "text": "23.2 Linear Separability and Regularization\nA classification dataset is said to be linearly separable if there exists a hyperplane among input features \\(x\\) that separates the two classes \\(y\\).\nLinear separability in 1D can be found with a rugplot of a single feature. For example, notice how the plot on the bottom left is linearly separable along the vertical line \\(x=0\\). However, no such line perfectly separates the two classes on the bottom right.\n\n\n\nThis same definition holds in higher dimensions. If there are two features, the separating hyperplane must exist in two dimensions (any line of the form \\(y=mx+b\\)). We can visualize this using a scatter plot.\n\n\n\nThis sounds great! When the dataset is linearly separable, a logistic regression classifier can perfectly assign datapoints into classes. However, (unexpected) complications may arise. Consider the toy dataset with 2 points and only a single feature \\(x\\):\n\n\n\nThe optimal \\(\\theta\\) value that minimizes loss pushes the predicted probabilities of the data points to their true class.\n\n\\(P(Y = 1|x = -1) = \\frac{1}{1 + e^\\theta} \\rightarrow 1\\)\n\\(P(Y = 1|x = 1) = \\frac{1}{1 + e^{-\\theta}} \\rightarrow 0\\)\n\nThis happens when \\(\\theta = -\\infty\\). When \\(\\theta = -\\infty\\), we observe the following behavior for any input \\(x\\).\n\\[P(Y=1|x) = \\sigma(\\theta x) \\rightarrow \\begin{cases}\n        1, \\text{if }  x &lt; 0\\\\\n        0, \\text{if }  x \\ge 0\n    \\end{cases}\\]\nThe diverging weights cause the model to be overconfident. For example, consider the new point \\((x, y) = (0.5, 1)\\). Following the behavior above, our model will incorrectly predict \\(p=0\\), and thus, \\(\\hat y = 0\\).\n\n\n\nThe loss incurred by this misclassified point is infinite.\n\\[-(y\\text{ log}(p) + (1-y)\\text{ log}(1-p))=1\\text{log}(0)\\]\nThus, diverging weights (\\(|\\theta| \\rightarrow \\infty\\)) occur with lineary separable data. “Overconfidence” is a particularly dangerous version of overfitting.\nConsider the loss function with respect to the parameter \\(\\theta\\).\n\n\n\nThough it’s very difficult to see, the plateau for negative values of \\(\\theta\\) is slightly tilted downwards, meaning the loss approaches \\(0\\) as \\(\\theta\\) decreases and approaches \\(-\\infty\\).\n\n23.2.1 Regularized Logistic Regression\nTo avoid large weights and infinite loss (particularly on linearly separable data), we use regularization. The same principles apply as with linear regression - make sure to standardize your features first.\nFor example, \\(L2\\) (Ridge) Logistic Regression takes on the form:\n\\[\\min_{\\theta} -\\frac{1}{n} \\sum_{i=1}^{n} (y_i \\text{log}(\\sigma(x_i^T\\theta)) + (1-y_i)\\text{log}(1-\\sigma(x_i^T\\theta))) + \\lambda \\sum_{i=1}^{d} \\theta_j^2\\]\nNow, let us compare the loss functions of un-regularized and regularized logistic regression.\n\n\n\n\n\n\nAs we can see, \\(L2\\) regularization helps us prevent diverging weights and deters against “overconfidence.”\nsklearn’s logistic regression defaults to L2 regularization and C=1.0; C is the inverse of \\(\\lambda\\): \\(C = \\frac{1}{\\lambda}\\). Setting C to a large value, for example, C=300.0, results in minimal regularization.\n# sklearn defaults\nmodel = LogisticRegression(penalty='l2', C=1.0, …)\nmodel.fit()\nNote that in Data 100, we only use sklearn to fit logistic regression models. There is no closed-form solution to the optimal theta vector, and the gradient is a little messy (see the bonus section below for details).\nFrom here, the .predict function returns the predicted class \\(\\hat y\\) of the point. In the simple binary case,\n\\[\\hat y = \\begin{cases}\n        1, & P(Y=1|x) \\ge 0.5\\\\\n        0, & \\text{otherwise }\n    \\end{cases}\\]"
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#performance-metrics",
    "href": "logistic_regression_2/logistic_reg_2.html#performance-metrics",
    "title": "23  Logistic Regression II",
    "section": "23.3 Performance Metrics",
    "text": "23.3 Performance Metrics\nYou might be thinking, if we’ve already introduced cross-entropy loss, why do we need additional ways of assessing how well our models perform? In linear regression, we made numerical predictions and used a loss function to determine how “good” these predictions were. In logistic regression, our ultimate goal is to classify data – we are much more concerned with whether or not each datapoint was assigned the correct class using the decision rule. As such, we are interested in the quality of classifications, not the predicted probabilities.\nThe most basic evaluation metric is accuracy, that is, the proportion of correctly classified points.\n\\[\\text{accuracy} = \\frac{\\# \\text{ of points classified correctly}}{\\# \\text{ of total points}}\\]\nTranslated to code:\ndef accuracy(X, Y):\n    return np.mean(model.predict(X) == Y)\n    \nmodel.score(X, y) # built-in accuracy function\nHowever, accuracy is not always a great metric for classification. To understand why, let’s consider a classification problem with 100 emails where only 5 are truly spam, and the remaining 95 are truly ham. We’ll investigate two models where accuracy is a poor metric.\n\nModel 1: Our first model classifies every email as non-spam. The model’s accuracy is high (\\(\\frac{95}{100} = 0.95\\)), but it doesn’t detect any spam emails. Despite the high accuracy, this is a bad model.\nModel 2: The second model classifies every email as spam. The accuracy is low (\\(\\frac{5}{100} = 0.05\\)), but the model correctly labels every spam email. Unfortunately, it also misclassifies every non-spam email.\n\nAs this example illustrates, accuracy is not always a good metric for classification, particularly when your data could exhibit class imbalance (e.g., very few 1’s compared to 0’s).\n\n23.3.1 Types of Classification\nThere are 4 different different classifications that our model might make:\n\nTrue positive: correctly classify a positive point as being positive (\\(y=1\\) and \\(\\hat{y}=1\\))\nTrue negative: correctly classify a negative point as being negative (\\(y=0\\) and \\(\\hat{y}=0\\))\nFalse positive: incorrectly classify a negative point as being positive (\\(y=0\\) and \\(\\hat{y}=1\\))\nFalse negative: incorrectly classify a positive point as being negative (\\(y=1\\) and \\(\\hat{y}=0\\))\n\nThese classifications can be concisely summarized in a confusion matrix.\n\n\n\nAn easy way to remember this terminology is as follows:\n\nLook at the second word in the phrase. Positive means a prediction of 1. Negative means a prediction of 0.\nLook at the first word in the phrase. True means our prediction was correct. False means it was incorrect.\n\nWe can now write the accuracy calculation as \\[\\text{accuracy} = \\frac{TP + TN}{n}\\]\nIn sklearn, we use the following syntax\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_true, Y_pred)\n\n\n\n\n\n23.3.2 Accuracy, Precision, and Recall\nThe purpose of our discussion of the confusion matrix was to motivate better performance metrics for classification problems with class imbalance - namely, precision and recall.\nPrecision is defined as\n\\[\\text{precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\\]\nPrecision answers the question: “Of all observations that were predicted to be \\(1\\), what proportion was actually \\(1\\)?” It measures how accurate the classifier is when its predictions are positive.\nRecall (or sensitivity) is defined as\n\\[\\text{recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\nRecall aims to answer: “Of all observations that were actually \\(1\\), what proportion was predicted to be \\(1\\)?” It measures how many positive predictions were missed.\nHere’s a helpful graphic that summarizes our discussion above.\n\n\n\n\n\n23.3.3 Example Calculation\nIn this section, we will calculate the accuracy, precision, and recall performance metrics for our earlier spam classification example. As a reminder, we had 100 emails, 5 of which were spam. We designed two models:\n\nModel 1: Predict that every email is non-spam\nModel 2: Predict that every email is spam\n\n\n23.3.3.1 Model 1\nFirst, let’s begin by creating the confusion matrix.\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTrue Negative: 95\nFalse Positive: 0\n\n\n1\nFalse Negative: 5\nTrue Positive: 0\n\n\n\nConvince yourself of why our confusion matrix looks like so.\n\\[\\text{accuracy} = \\frac{95}{100} = 0.95\\] \\[\\text{precision} = \\frac{0}{0 + 0} = \\text{undefined}\\] \\[\\text{recall} = \\frac{0}{0 + 5} = 0\\]\nNotice how our precision is undefined because we never predicted class \\(1\\). Our recall is 0 for the same reason – the numerator is 0 (we had no positive predictions).\n\n\n23.3.3.2 Model 2\nOur confusion matrix for Model 2 looks like so.\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nTrue Negative: 0\nFalse Positive: 95\n\n\n1\nFalse Negative: 0\nTrue Positive: 5\n\n\n\n\\[\\text{accuracy} = \\frac{5}{100} = 0.05\\] \\[\\text{precision} = \\frac{5}{5 + 95} = 0.05\\] \\[\\text{recall} = \\frac{5}{5 + 0} = 1\\]\nOur precision is low because we have many false positives, and our recall is perfect - we correctly classified all spam emails (we never predicted class \\(0\\)).\n\n\n\n23.3.4 Precision vs. Recall\nPrecision (\\(\\frac{\\text{TP}}{\\text{TP} + \\textbf{ FP}}\\)) penalizes false positives, while recall (\\(\\frac{\\text{TP}}{\\text{TP} + \\textbf{ FN}}\\)) penalizes false negatives.\nIn fact, precision and recall are inversely related. This is evident in our second model – we observed a high recall and low precision. Usually, there is a tradeoff in these two (most models can either minimize the number of FP or FN; and in rare cases, both).\nThe specific performance metric(s) to prioritize depends on the context. In many medical settings, there might be a much higher cost to missing positive cases. For instance, in our breast cancer example, it is more costly to misclassify malignant tumors (false negatives) than it is to incorrectly classify a benign tumor as malignant (false positives). In the case of the latter, pathologists can conduct further studies to verify malignant tumors. As such, we should minimize the number of false negatives. This is equivalent to maximizing recall.\n\n\n23.3.5 Two More Metrics\nThe True Positive Rate (TPR) is defined as\n\\[\\text{true positive rate} = \\frac{\\text{TP}}{\\text{TP + FN}}\\]\nYou’ll notice this is equivalent to recall. In the context of our spam email classifier, it answers the question: “What proportion of spam did I mark correctly?”. We’d like this to be close to \\(1\\)\nThe False Positive Rate (FPR) is defined as\n\\[\\text{false positive rate} = \\frac{\\text{FP}}{\\text{FP + TN}}\\]\nAnother word for FPR is specificity. This answers the question: “What proportion of regular email did I mark as spam?”. We’d like this to be close to \\(0\\)\nAs we increase threshold \\(T\\), both TPR and FPR decrease. We’ve plotted this relationship below for some model on a toy dataset."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#adjusting-the-classification-threshold",
    "href": "logistic_regression_2/logistic_reg_2.html#adjusting-the-classification-threshold",
    "title": "23  Logistic Regression II",
    "section": "23.4 Adjusting the Classification Threshold",
    "text": "23.4 Adjusting the Classification Threshold\nOne way to minimize the number of FP vs. FN (equivalently, maximizing precision vs. recall) is by adjusting the classification threshold \\(T\\).\n\\[\\hat y = \\begin{cases}\n        1, & P(Y=1|x) \\ge T\\\\\n        0, & \\text{otherwise }\n    \\end{cases}\\]\nThe default threshold in sklearn is \\(T = 0.5\\). As we increase the threshold \\(T\\), we “raise the standard” of how confident our classifier needs to be to predict 1 (i.e., “positive”).\n\n\n\nAs you may notice, the choice of threshold \\(T\\) impacts our classifier’s performance.\n\nHigh \\(T\\): Most predictions are \\(0\\).\n\nLots of false negatives\nFewer false positives\n\nLow \\(T\\): Most predictions are \\(1\\).\n\nLots of false positives\nFewer false negatives\n\n\nIn fact, we can choose a threshold \\(T\\) based on our desired number, or proportion, of false positives and false negatives. We can do so using a few different tools. We’ll touch on two of the most important ones in Data 100.\n\nPrecision-Recall Curve (PR Curve)\n“Receiver Operating Characteristic” Curve (ROC Curve)\n\n\n23.4.1 Precision-Recall Curves\nA Precision-Recall Curve (PR Curve) is an alternative to the ROC curve that displays the relationship between precision and recall for various threshold values. It is constructed in a similar way as with the ROC curve.\nLet’s first consider how precision and recall change as a function of the threshold \\(T\\). We know this quite well from earlier – precision will generally increase, and recall will decrease.\n\n\n\nDisplayed below is the PR Curve for the same toy dataset. Notice how threshold values increase as we move to the left.\n\n\n\nOnce again, the perfect classifier will resemble the orange curve, this time, facing the opposite direction.\n\n\n\nWe want our PR curve to be as close to the “top right” of this graph as possible. Again, we use the AUC to determine “closeness”, with the perfect classifier exhibiting an AUC = 1 (and the worst with an AUC = 0.5).\n\n\n23.4.2 The ROC Curve\nThe “Receiver Operating Characteristic” Curve (ROC Curve) plots the tradeoff between FPR and TPR. Notice how the far-left of the curve corresponds to higher threshold \\(T\\) values.\n\n\n\nThe “perfect” classifier is the one that has a TPR of 1, and FPR of 0. This is achieved at the top-left of the plot below. More generally, it’s ROC curve resembles the curve in orange.\n\n\n\nWe want our model to be as close to this orange curve as possible. How do we quantify “closeness”?\nWe can compute the area under curve (AUC) of the ROC curve. Notice how the perfect classifier has an AUC = 1. The closer our model’s AUC is to 1, the better it is.\n\n23.4.2.1 [Extra] What is the “worst” AUC, and why is it 0.5?\nOn the other hand, a terrible model will have an AUC closer to 0.5. Random predictors randomly predict \\(P(Y = 1 | x)\\) to be uniformly between 0 and 1. This indicates the classifier is not able to distinguish between positive and negative classes, and thus, randomly predicts one of the two."
  },
  {
    "objectID": "logistic_regression_2/logistic_reg_2.html#extra-gradient-descent-for-logistic-regression",
    "href": "logistic_regression_2/logistic_reg_2.html#extra-gradient-descent-for-logistic-regression",
    "title": "23  Logistic Regression II",
    "section": "23.5 [Extra] Gradient Descent for Logistic Regression",
    "text": "23.5 [Extra] Gradient Descent for Logistic Regression\nLet’s define the following: \\[\nt_i = \\phi(x_i)^T \\theta \\\\\np_i = \\sigma(t_i) \\\\\nt_i = \\log(\\frac{p_i}{1 - p_i}) \\\\\n1 - \\sigma(t_i) = \\sigma(-t_i) \\\\\n\\frac{d}{dt}  \\sigma(t) =  \\sigma(t) \\sigma(-t)\n\\]\nNow, we can simplify the cross-entropy loss \\[\n\\begin{align}\ny_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) &= y_i \\log(\\frac{p_i}{1 - p_i}) + \\log(1 - p_i) \\\\\n&= y_i \\phi(x_i)^T + \\log(\\sigma(-\\phi(x_i)^T \\theta))\n\\end{align}\n\\]\nHence, the optimal \\(\\hat{\\theta}\\) is \\[\\text{argmin}_{\\theta} - \\frac{1}{n} \\sum_{i=1}^n (y_i \\phi(x_i)^T + \\log(\\sigma(-\\phi(x_i)^T \\theta)))\\]\nWe want to minimize \\[L(\\theta) = - \\frac{1}{n} \\sum_{i=1}^n (y_i \\phi(x_i)^T + \\log(\\sigma(-\\phi(x_i)^T \\theta)))\\]\nSo we take the derivative \\[\n\\begin{align}\n\\triangledown_{\\theta} L(\\theta) &= - \\frac{1}{n} \\sum_{i=1}^n \\triangledown_{\\theta} y_i \\phi(x_i)^T + \\triangledown_{\\theta} \\log(\\sigma(-\\phi(x_i)^T \\theta)) \\\\\n&= - \\frac{1}{n} \\sum_{i=1}^n y_i \\phi(x_i) + \\triangledown_{\\theta} \\log(\\sigma(-\\phi(x_i)^T \\theta)) \\\\\n&= - \\frac{1}{n} \\sum_{i=1}^n y_i \\phi(x_i) + \\frac{1}{\\sigma(-\\phi(x_i)^T \\theta)} \\triangledown_{\\theta} \\sigma(-\\phi(x_i)^T \\theta) \\\\\n&= - \\frac{1}{n} \\sum_{i=1}^n y_i \\phi(x_i) + \\frac{\\sigma(-\\phi(x_i)^T \\theta)}{\\sigma(-\\phi(x_i)^T \\theta)} \\sigma(\\phi(x_i)^T \\theta)\\triangledown_{\\theta} \\sigma(-\\phi(x_i)^T \\theta) \\\\\n&= - \\frac{1}{n} \\sum_{i=1}^n (y_i - \\sigma(\\phi(x_i)^T \\theta)\\phi(x_i))\n\\end{align}\n\\]\nSetting the derivative equal to 0 and solving for \\(\\hat{\\theta}\\), we find that there’s no general analytic solution. Therefore, we must solve using numeric methods.\n\n23.5.1 Gradient Descent Update Rule\n\\[\\theta^{(0)} \\leftarrow \\text{initial vector (random, zeros, ...)} \\]\nFor \\(\\tau\\) from 0 to convergence: \\[ \\theta^{(\\tau + 1)} \\leftarrow \\theta^{(\\tau)} + \\rho(\\tau)\\left( \\frac{1}{n} \\sum_{i=1}^n \\triangledown_{\\theta} L_i(\\theta) \\mid_{\\theta = \\theta^{(\\tau)}}\\right) \\]\n\n\n23.5.2 Stochastic Gradient Descent Update Rule\n\\[\\theta^{(0)} \\leftarrow \\text{initial vector (random, zeros, ...)} \\]\nFor \\(\\tau\\) from 0 to convergence, let \\(B\\) ~ \\(\\text{Random subset of indices}\\). \\[ \\theta^{(\\tau + 1)} \\leftarrow \\theta^{(\\tau)} + \\rho(\\tau)\\left( \\frac{1}{|B|} \\sum_{i \\in B} \\triangledown_{\\theta} L_i(\\theta) \\mid_{\\theta = \\theta^{(\\tau)}}\\right) \\]"
  }
]