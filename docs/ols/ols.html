<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 12&nbsp; Ordinary Least Squares</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../gradient_descent/gradient_descent.html" rel="next">
<link href="../constant_model_loss_transformations/loss_transformations.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../ols/ols.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sklearn and Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Ordinary Least Squares</h2>
   
  <ul>
  <li><a href="#linearity" id="toc-linearity" class="nav-link active" data-scroll-target="#linearity"><span class="header-section-number">12.1</span> Linearity</a></li>
  <li><a href="#terminology-for-multiple-linear-regression" id="toc-terminology-for-multiple-linear-regression" class="nav-link" data-scroll-target="#terminology-for-multiple-linear-regression"><span class="header-section-number">12.2</span> Terminology for Multiple Linear Regression</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">12.3</span> Multiple Linear Regression</a></li>
  <li><a href="#linear-algebra-approach" id="toc-linear-algebra-approach" class="nav-link" data-scroll-target="#linear-algebra-approach"><span class="header-section-number">12.4</span> Linear Algebra Approach</a></li>
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error"><span class="header-section-number">12.5</span> Mean Squared Error</a></li>
  <li><a href="#geometric-perspective" id="toc-geometric-perspective" class="nav-link" data-scroll-target="#geometric-perspective"><span class="header-section-number">12.6</span> Geometric Perspective</a></li>
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance"><span class="header-section-number">12.7</span> Evaluating Model Performance</a></li>
  <li><a href="#ols-properties" id="toc-ols-properties" class="nav-link" data-scroll-target="#ols-properties"><span class="header-section-number">12.8</span> OLS Properties</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Define linearity with respect to a vector of parameters <span class="math inline">\(\theta\)</span>.</li>
<li>Understand the use of matrix notation to express multiple linear regression.</li>
<li>Interpret ordinary least squares as the minimization of the norm of the residual vector.</li>
<li>Compute performance metrics for multiple linear regression.</li>
</ul>
</div>
</div>
</div>
<p>We’ve now spent a number of lectures exploring how to build effective models – we introduced the SLR and constant models, selected cost functions to suit our modeling task, and applied transformations to improve the linear fit.</p>
<p>Throughout all of this, we considered models of one feature (<span class="math inline">\(\hat{y}_i = \theta_0 + \theta_1 x_i\)</span>) or zero features (<span class="math inline">\(\hat{y}_i = \theta_0\)</span>). As data scientists, we usually have access to datasets containing <em>many</em> features. To make the best models we can, it will be beneficial to consider all of the variables available to us as inputs to a model, rather than just one. In today’s lecture, we’ll introduce <strong>multiple linear regression</strong> as a framework to incorporate multiple features into a model. We will also learn how to accelerate the modeling process – specifically, we’ll see how linear algebra offers us a powerful set of tools for understanding model performance.</p>
<section id="linearity" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="linearity"><span class="header-section-number">12.1</span> Linearity</h2>
<p>An expression is <strong>linear in <span class="math inline">\(\theta\)</span></strong> (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms – a <strong>vector of <span class="math inline">\(\theta\)</span></strong> s, and a matrix/vector <strong>not involving <span class="math inline">\(\theta\)</span></strong> – is a good indicator of linearity.</p>
<p>For example, consider the vector <span class="math inline">\(\theta = [\theta_0, \theta_1, \theta_2]\)</span></p>
<ol type="1">
<li><span class="math inline">\(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\)</span> is linear in theta, and we can separate it into a matrix product of two terms:</li>
</ol>
<p><span class="math display">\[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]</span></p>
<ol start="2" type="1">
<li><span class="math inline">\(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\)</span> is <em>not</em> linear in theta, as the <span class="math inline">\(\theta_1\)</span> term is squared, and the <span class="math inline">\(\theta_2\)</span> term is logged. We cannot separate it into a matrix product of two terms.</li>
</ol>
</section>
<section id="terminology-for-multiple-linear-regression" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="terminology-for-multiple-linear-regression"><span class="header-section-number">12.2</span> Terminology for Multiple Linear Regression</h2>
<p>There are several equivalent terms in the context of regression. The ones we use most often for this course are bolded.</p>
<ul>
<li><span class="math inline">\(x\)</span> can be called a
<ul>
<li><strong>Feature(s)</strong></li>
<li>Covariate(s)</li>
<li><strong>Independent variable(s)</strong></li>
<li>Explanatory variable(s)</li>
<li>Predictor(s)</li>
<li>Input(s)</li>
<li>Regressor(s)</li>
</ul></li>
<li><span class="math inline">\(y\)</span> can be called an
<ul>
<li><strong>Output</strong></li>
<li>Outcome</li>
<li><strong>Response</strong></li>
<li>Dependent variable</li>
</ul></li>
<li><span class="math inline">\(\hat{y}\)</span> can be called a
<ul>
<li><strong>Prediction</strong></li>
<li>Predicted response</li>
<li>Estimated value</li>
</ul></li>
<li><span class="math inline">\(\theta\)</span> can be called a
<ul>
<li><strong>Weight(s)</strong></li>
<li><strong>Parameter(s)</strong></li>
<li>Coefficient(s)</li>
</ul></li>
<li><span class="math inline">\(\hat{\theta}\)</span> can be called a
<ul>
<li><strong>Estimator(s)</strong></li>
<li><strong>Optimal parameter(s)</strong></li>
</ul></li>
<li>A datapoint <span class="math inline">\((x, y)\)</span> is also called an observation.</li>
</ul>
</section>
<section id="multiple-linear-regression" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">12.3</span> Multiple Linear Regression</h2>
<p>Multiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:</p>
<p><span class="math display">\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]</span></p>
<p>Our predicted value of <span class="math inline">\(y\)</span>, <span class="math inline">\(\hat{y}\)</span>, is a linear combination of the single <strong>observations</strong> (features), <span class="math inline">\(x_i\)</span>, and the parameters, <span class="math inline">\(\theta_i\)</span>.</p>
<p>We can explore this idea further by looking at a dataset containing aggregate per-player data from the 2018-19 NBA season, downloaded from <a href="https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019">Kaggle</a>.</p>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>nba <span class="op">=</span> pd.read_csv(<span class="st">'data/nba18-19.csv'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nba.index.name <span class="op">=</span> <span class="va">None</span> <span class="co"># Drops name of index (players are ordered by rank)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>nba.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Player</th>
<th data-quarto-table-cell-role="th">Pos</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">Tm</th>
<th data-quarto-table-cell-role="th">G</th>
<th data-quarto-table-cell-role="th">GS</th>
<th data-quarto-table-cell-role="th">MP</th>
<th data-quarto-table-cell-role="th">FG</th>
<th data-quarto-table-cell-role="th">FGA</th>
<th data-quarto-table-cell-role="th">FG%</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">FT%</th>
<th data-quarto-table-cell-role="th">ORB</th>
<th data-quarto-table-cell-role="th">DRB</th>
<th data-quarto-table-cell-role="th">TRB</th>
<th data-quarto-table-cell-role="th">AST</th>
<th data-quarto-table-cell-role="th">STL</th>
<th data-quarto-table-cell-role="th">BLK</th>
<th data-quarto-table-cell-role="th">TOV</th>
<th data-quarto-table-cell-role="th">PF</th>
<th data-quarto-table-cell-role="th">PTS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>Álex Abrines\abrinal01</td>
<td>SG</td>
<td>25</td>
<td>OKC</td>
<td>31</td>
<td>2</td>
<td>19.0</td>
<td>1.8</td>
<td>5.1</td>
<td>0.357</td>
<td>...</td>
<td>0.923</td>
<td>0.2</td>
<td>1.4</td>
<td>1.5</td>
<td>0.6</td>
<td>0.5</td>
<td>0.2</td>
<td>0.5</td>
<td>1.7</td>
<td>5.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2</td>
<td>Quincy Acy\acyqu01</td>
<td>PF</td>
<td>28</td>
<td>PHO</td>
<td>10</td>
<td>0</td>
<td>12.3</td>
<td>0.4</td>
<td>1.8</td>
<td>0.222</td>
<td>...</td>
<td>0.700</td>
<td>0.3</td>
<td>2.2</td>
<td>2.5</td>
<td>0.8</td>
<td>0.1</td>
<td>0.4</td>
<td>0.4</td>
<td>2.4</td>
<td>1.7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>Jaylen Adams\adamsja01</td>
<td>PG</td>
<td>22</td>
<td>ATL</td>
<td>34</td>
<td>1</td>
<td>12.6</td>
<td>1.1</td>
<td>3.2</td>
<td>0.345</td>
<td>...</td>
<td>0.778</td>
<td>0.3</td>
<td>1.4</td>
<td>1.8</td>
<td>1.9</td>
<td>0.4</td>
<td>0.1</td>
<td>0.8</td>
<td>1.3</td>
<td>3.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">4</td>
<td>Steven Adams\adamsst01</td>
<td>C</td>
<td>25</td>
<td>OKC</td>
<td>80</td>
<td>80</td>
<td>33.4</td>
<td>6.0</td>
<td>10.1</td>
<td>0.595</td>
<td>...</td>
<td>0.500</td>
<td>4.9</td>
<td>4.6</td>
<td>9.5</td>
<td>1.6</td>
<td>1.5</td>
<td>1.0</td>
<td>1.7</td>
<td>2.6</td>
<td>13.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5</td>
<td>Bam Adebayo\adebaba01</td>
<td>C</td>
<td>21</td>
<td>MIA</td>
<td>82</td>
<td>28</td>
<td>23.3</td>
<td>3.4</td>
<td>5.9</td>
<td>0.576</td>
<td>...</td>
<td>0.735</td>
<td>2.0</td>
<td>5.3</td>
<td>7.3</td>
<td>2.2</td>
<td>0.9</td>
<td>0.8</td>
<td>1.5</td>
<td>2.5</td>
<td>8.9</td>
</tr>
</tbody>
</table>

<p>5 rows × 29 columns</p>
</div>
</div>
</div>
<p>Let’s say we are interested in predicting the number of points (<code>PTS</code>) an athlete will score in a basketball game this season.</p>
<p>Suppose we want to fit a linear model by using some characteristics, or <strong>features</strong> of a player. Specifically, we’ll focus on field goals, assists, and 3-point attempts.</p>
<ul>
<li><code>FG</code>, the number of (2-point) field goals per game</li>
<li><code>AST</code>, the average number of assists per game</li>
<li><code>3PA</code>, the number of 3-point field goals attempted per game</li>
</ul>
<div class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>nba[[<span class="st">'FG'</span>, <span class="st">'AST'</span>, <span class="st">'3PA'</span>, <span class="st">'PTS'</span>]].head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">FG</th>
<th data-quarto-table-cell-role="th">AST</th>
<th data-quarto-table-cell-role="th">3PA</th>
<th data-quarto-table-cell-role="th">PTS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">1</td>
<td>1.8</td>
<td>0.6</td>
<td>4.1</td>
<td>5.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">2</td>
<td>0.4</td>
<td>0.8</td>
<td>1.5</td>
<td>1.7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">3</td>
<td>1.1</td>
<td>1.9</td>
<td>2.2</td>
<td>3.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">4</td>
<td>6.0</td>
<td>1.6</td>
<td>0.0</td>
<td>13.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5</td>
<td>3.4</td>
<td>2.2</td>
<td>0.2</td>
<td>8.9</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Because we are now dealing with many parameter values, we’ve collected them all into a <strong>parameter vector</strong> with dimensions <span class="math inline">\((p+1) \times 1\)</span> to keep things tidy. Remember that <span class="math inline">\(p\)</span> represents the number of features we have (in this case, 3).</p>
<p><span class="math display">\[\theta = \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix}\]</span></p>
<p>We are working with two vectors here: a row vector representing the observed data, and a column vector containing the model parameters. The multiple linear regression model is <strong>equivalent to the dot (scalar) product of the observation vector and parameter vector</strong>.</p>
<p><span class="math display">\[[1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \theta = [1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}] \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}\]</span></p>
<p>Notice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with <span class="math inline">\(\theta_0\)</span> to give the intercept of the regression model. We call this 1 entry the <strong>intercept</strong> or <strong>bias</strong> term.</p>
<p>Given that we have three features here, we can express this model as: <span class="math display">\[\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:\theta_3 x_{3}\]</span></p>
<p>Our features are represented by <span class="math inline">\(x_1\)</span> (<code>FG</code>), <span class="math inline">\(x_2\)</span> (<code>AST</code>), and <span class="math inline">\(x_3\)</span> (<code>3PA</code>) with each having correpsonding parameters, <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>.</p>
<p>In statistics, this model + loss is called <strong>Ordinary Least Squares (OLS)</strong>. The solution to OLS is the minimizing loss for parameters <span class="math inline">\(\hat{\theta}\)</span>, also called the <strong>least squares estimate</strong>.</p>
</section>
<section id="linear-algebra-approach" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="linear-algebra-approach"><span class="header-section-number">12.4</span> Linear Algebra Approach</h2>
<p>We now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple linear regression. We want to use the tools of linear algebra.</p>
<p>Let’s think about how we can apply what we did above. To accomodate for the fact that we’re considering several feature variables, we’ll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of <span class="math inline">\(p\)</span> features.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/observation.png" alt="observation" width="550">
</td>
</tr>

</tbody></table>
</div>
<p>To make a prediction from the first observation in the data, we take the dot product of the parameter vector and first observation vector. To make a prediction from the <em>second</em> observation, we would repeat this process to find the dot product of the parameter vector and the <em>second</em> observation vector. If we wanted to find the model predictions for each observation in the dataset, we’d repeat this process for all <span class="math inline">\(n\)</span> observations in the data.</p>
<p><span class="math display">\[\hat{y}_1 = \theta_0 + \theta_1 x_{11} + \theta_2 x_{12} + ... + \theta_p x_{1p} = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\]</span> <span class="math display">\[\hat{y}_2 = \theta_0 + \theta_1 x_{21} + \theta_2 x_{22} + ... + \theta_p x_{2p} = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}] \theta\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[\hat{y}_n = \theta_0 + \theta_1 x_{n1} + \theta_2 x_{n2} + ... + \theta_p x_{np} = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]</span></p>
<p>Our observed data is represented by <span class="math inline">\(n\)</span> row vectors, each with dimension <span class="math inline">\((p+1)\)</span>. We can collect them all into a single matrix, which we call <span class="math inline">\(\mathbb{X}\)</span>.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/design_matrix.png" alt="design_matrix" width="400">
</td>
</tr>

</tbody></table>
</div>
<p>The matrix <span class="math inline">\(\mathbb{X}\)</span> is known as the <strong>design matrix</strong>. It contains all observed data for each of our <span class="math inline">\(p\)</span> features, where each <strong>row</strong> corresponds to one <strong>observation</strong>, and each <strong>column</strong> corresponds to a <strong>feature</strong>. It often (but not always) contains an additional column of all ones to represent the <strong>intercept</strong> or <strong>bias column</strong>.</p>
<p>To review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/row_col.png" alt="row_col" width="600">
</td>
</tr>

</tbody></table>
</div>
<p>The multiple linear regression model can then be restated in terms of matrices: <span class="math display">\[
\Large
\mathbb{\hat{Y}} = \mathbb{X} \theta
\]</span></p>
<p>Here, <span class="math inline">\(\mathbb{\hat{Y}}\)</span> is the <strong>prediction vector</strong> with <span class="math inline">\(n\)</span> elements (<span class="math inline">\(\mathbb{\hat{Y}} \in \mathbb{R}^{n}\)</span>); it contains the prediction made by the model for each of the <span class="math inline">\(n\)</span> input observations. <span class="math inline">\(\mathbb{X}\)</span> is the <strong>design matrix</strong> with dimensions <span class="math inline">\(\mathbb{X} \in \mathbb{R}^(n \times (p + 1))\)</span>, and <span class="math inline">\(\theta\)</span> is the <strong>parameter vector</strong> with dimensions <span class="math inline">\(\theta \in \mathbb{R}^{(p + 1)}\)</span></p>
<p>As a refresher, let’s also review the <strong>dot product (or inner product)</strong>. This is a vector operation that:</p>
<ul>
<li>Can only be carried out on two vectors of the <strong>same length</strong></li>
<li>Sums up the products of the corresponding entries of the two vectors</li>
<li>Returns a single number</li>
</ul>
<p>While this is not in scope, note that we can also interpret the dot product geometrically:</p>
<ul>
<li>It is the product of three things: the <strong>magnitude</strong> of both vectors, and the <strong>cosine</strong> of the angles between them: <span class="math display">\[\vec{u} \cdot \vec{v} = ||\vec{u}|| \cdot ||\vec{v}|| \cdot {cos \theta}\]</span></li>
</ul>
</section>
<section id="mean-squared-error" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="mean-squared-error"><span class="header-section-number">12.5</span> Mean Squared Error</h2>
<p>We now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of risk functions and model fitting.</p>
<p>Recall our definition of MSE: <span class="math display">\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p>
<p>At its heart, the MSE is a measure of <em>distance</em> – it gives an indication of how “far away” the predictions are from the true values, on average.</p>
<p>When working with vectors, this idea of “distance” or the vector’s <strong>size/length</strong> is represented by the <strong>norm</strong>. More precisely, the distance between two vectors <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span> can be expressed as: <span class="math display">\[||\vec{a} - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\]</span></p>
<p>The double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.</p>
<p>The two norms we need to know for Data 100 are the L1 and L2 norms (sound familiar?). In this note, we’ll focus on L2 norm. We’ll dive into L1 norm in future lectures.</p>
<p>For the n-dimensional vector <span class="math display">\[\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}\]</span>, the <strong>L2 vector norm</strong> is</p>
<p><span class="math display">\[||\vec{x}||_2 = \sqrt{(x_1)^2 + (x_2)^2 + \ldots + (x_n)^2} = \sqrt{\sum_{i=1}^n (x_i)^2}\]</span></p>
<p>The L2 vector norm is a generalization of the Pythagorean theorem in <span class="math inline">\(n\)</span> dimensions. Thus, it can be used as a measure of the <strong>length</strong> of a vector or even as a measure of the <strong>distance</strong> between two vectors.</p>
<p>We can express the MSE as a squared L2 norm if we rewrite it in terms of the prediction vector, <span class="math inline">\(\hat{\mathbb{Y}}\)</span>, and true target vector, <span class="math inline">\(\mathbb{Y}\)</span>:</p>
<p><span class="math display">\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y} - \hat{\mathbb{Y}}||_2^2\]</span></p>
<p>Here, the superscript 2 outside of the norm double bars means that we are <em>squaring</em> the norm. If we plug in our linear model <span class="math inline">\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)</span>, we find the MSE cost function in vector notation:</p>
<p><span class="math display">\[R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2\]</span></p>
<p>Under the linear algebra perspective, our new task is to fit the optimal parameter vector <span class="math inline">\(\theta\)</span> such that the cost function is minimized. Equivalently, we wish to minimize the norm <span class="math display">\[||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2.\]</span></p>
<p>We can restate this goal in two ways:</p>
<ul>
<li>Minimize the <strong>distance</strong> between the vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, and the vector of predicted values, <span class="math inline">\(\mathbb{\hat{Y}}\)</span></li>
<li>Minimize the <strong>length</strong> of the <strong>residual vector</strong>, defined as: <span class="math display">\[e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}
         y_1 - \hat{y}_1 \\
         y_2 - \hat{y}_2 \\
         \vdots \\
         y_n - \hat{y}_n
       \end{bmatrix}\]</span></li>
</ul>
</section>
<section id="geometric-perspective" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="geometric-perspective"><span class="header-section-number">12.6</span> Geometric Perspective</h2>
<p>To derive the best parameter vector to meet this goal, we can turn to the geometric properties of our modeling setup.</p>
<p>Up until now, we’ve mostly thought of our model as a scalar product between horizontally stacked observations and the parameter vector. We can also think of <span class="math inline">\(\hat{\mathbb{Y}}\)</span> as a <strong>linear combination of feature vectors</strong>, scaled by the <strong>parameters</strong>. We use the notation <span class="math inline">\(\mathbb{X}_{:, i}\)</span> to denote the <span class="math inline">\(i\)</span>th column of the design matrix. You can think of this as following the same convention as used when calling <code>.iloc</code> and <code>.loc</code>. “:” means that we are taking all entries in the <span class="math inline">\(i\)</span>th column.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/columns.png" alt="columns" width="500">
</td>
</tr>

</tbody></table>
</div>
<p><span class="math display">\[
\hat{\mathbb{Y}} =
\theta_0 \begin{bmatrix}
           1 \\
           1 \\
           \vdots \\
           1
         \end{bmatrix} + \theta_1 \begin{bmatrix}
           x_{11} \\
           x_{21} \\
           \vdots \\
           x_{n1}
         \end{bmatrix} + \ldots + \theta_p \begin{bmatrix}
           x_{1p} \\
           x_{2p} \\
           \vdots \\
           x_{np}
         \end{bmatrix}
         = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p \mathbb{X}_{:,\:p+1}\]</span></p>
<p>This new approach is useful because it allows us to take advantage of the properties of linear combinations.</p>
<p>Recall that the <strong>span</strong> or <strong>column space</strong> of a matrix <span class="math inline">\(\mathbb{X}\)</span> (denoted <span class="math inline">\(span(\mathbb{X})\)</span>) is the set of all possible linear combinations of the matrix’s columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns. Additionally, if each column of <span class="math inline">\(\mathbb{X}\)</span> has length <span class="math inline">\(n\)</span>, <span class="math inline">\(span(\mathbb{X})\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
<p>Because the prediction vector, <span class="math inline">\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)</span>, is a <strong>linear combination</strong> of the columns of <span class="math inline">\(\mathbb{X}\)</span>, we know that the <strong>predictions are contained in the span of <span class="math inline">\(\mathbb{X}\)</span></strong>. That is, we know that <span class="math inline">\(\mathbb{\hat{Y}} \in \text{Span}(\mathbb{X})\)</span>.</p>
<p>The diagram below is a simplified view of <span class="math inline">\(\text{Span}(\mathbb{X})\)</span>, assuming that each column of <span class="math inline">\(\mathbb{X}\)</span> has length <span class="math inline">\(n\)</span>. Notice that the columns of <span class="math inline">\(\mathbb{X}\)</span> define a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, where each point in the subspace can be reached by a linear combination of <span class="math inline">\(\mathbb{X}\)</span>’s columns. The prediction vector <span class="math inline">\(\mathbb{\hat{Y}}\)</span> lies somewhere in this subspace.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/span.png" alt="span" width="600">
</td>
</tr>

</tbody></table>
</div>
<p>Examining this diagram, we find a problem. The vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, could theoretically lie <em>anywhere</em> in <span class="math inline">\(\mathbb{R}^n\)</span> space – its exact location depends on the data we collect out in the real world. However, our multiple linear regression model can only make predictions in the subspace of <span class="math inline">\(\mathbb{R}^n\)</span> spanned by <span class="math inline">\(\mathbb{X}\)</span>. Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, and the vector of predicted values, <span class="math inline">\(\mathbb{\hat{Y}}\)</span>, is minimized. This means that <strong>we want <span class="math inline">\(\mathbb{\hat{Y}}\)</span> to be the vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span> that is closest to <span class="math inline">\(\mathbb{Y}\)</span></strong>.</p>
<p>Another way of rephrasing this goal is to say that we wish to minimize the length of the residual vector <span class="math inline">\(e\)</span>, as measured by its <span class="math inline">\(L_2\)</span> norm.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/residual.png" alt="residual" width="600">
</td>
</tr>

</tbody></table>
</div>
<p>The vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span> that is closest to <span class="math inline">\(\mathbb{Y}\)</span> is always the <strong>orthogonal projection</strong> of <span class="math inline">\(\mathbb{Y}\)</span> onto <span class="math inline">\(\text{Span}(\mathbb{X}).\)</span> Thus, we should choose the parameter vector <span class="math inline">\(\theta\)</span> that makes the <strong>residual vector orthogonal to any vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span></strong>. You can visualize this as the vector created by dropping a perpendicular line from <span class="math inline">\(\mathbb{Y}\)</span> onto the span of <span class="math inline">\(\mathbb{X}\)</span>.</p>
<p>How does this help us identify the optimal parameter vector, <span class="math inline">\(\hat{\theta}\)</span>? Recall that two vectors <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are orthogonal if their dot product is zero: <span class="math inline">\({a}^{T}b = 0\)</span>. A vector <span class="math inline">\(v\)</span> is <strong>orthogonal</strong> to the span of a matrix <span class="math inline">\(M\)</span> if and only if <span class="math inline">\(v\)</span> is orthogonal to <strong>each column</strong> in <span class="math inline">\(M\)</span>. Put together, a vector <span class="math inline">\(v\)</span> is orthogonal to <span class="math inline">\(\text{Span}(M)\)</span> if:</p>
<p><span class="math display">\[M^Tv = \vec{0}\]</span></p>
<p>Note that <span class="math inline">\(\vec{0}\)</span> represents the <strong>zero vector</strong>, a <span class="math inline">\(d\)</span>-length vector full of 0s.</p>
<p>Remember our goal is to find <span class="math inline">\(\hat{\theta}\)</span> such that we minimize the objective function <span class="math inline">\(R(\theta)\)</span>. Equivalently, this is the <span class="math inline">\(\hat{\theta}\)</span> such that the residual vector <span class="math inline">\(e = \mathbb{Y} - \mathbb{X} \theta\)</span> is orthogonal to <span class="math inline">\(\text{Span}(\mathbb{X})\)</span>.</p>
<p>Looking at the definition of orthogonality of <span class="math inline">\(\mathbb{Y} - \mathbb{X}\hat{\theta}\)</span> to <span class="math inline">\(span(\mathbb{X})\)</span> (0 is the <span class="math inline">\(\vec{0}\)</span> vector), we can write: <span class="math display">\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]</span></p>
<p>Let’s then rearrange the terms: <span class="math display">\[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} = \vec{0}\]</span></p>
<p>And finally, we end up with the <strong>normal equation</strong>: <span class="math display">\[\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]</span></p>
<p>Any vector <span class="math inline">\(\theta\)</span> that minimizes MSE on a dataset must satisfy this equation.</p>
<p>If <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is invertible, we can conclude: <span class="math display">\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}\]</span></p>
<p>This is called the <strong>least squares estimate</strong> of <span class="math inline">\(\theta\)</span>: it is the value of <span class="math inline">\(\theta\)</span> that minimizes the squared loss.</p>
<p>Note that the least squares estimate was derived under the assumption that <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is <em>invertible</em>. This condition holds true when <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is full column rank, which, in turn, happens when <span class="math inline">\(\mathbb{X}\)</span> is full column rank. We will explore the consequences of this fact in lab and homework.</p>
</section>
<section id="evaluating-model-performance" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="evaluating-model-performance"><span class="header-section-number">12.7</span> Evaluating Model Performance</h2>
<p>Our geometric view of multiple linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features.</p>
<p>Now, we want to understand how well our fitted model performs. One measure of model performance is the <strong>Root Mean Squared Error</strong>, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of <span class="math inline">\(y_i\)</span>, which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is a lower average loss across the dataset.</p>
<p><span class="math display">\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]</span></p>
<p>When working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple linear model performs well if its residual plot shows no patterns.</p>
<div data-align="middle">

<table style="width:100%">
<tbody><tr align="center">
<td>
<img src="images/residual_plot.png" alt="residual_plot" width="500">
</td>
</tr>

</tbody></table>
</div>
<p>For SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple linear model setting, we will need a performance metric that can account for multiple features at once. <strong>Multiple <span class="math inline">\(R^2\)</span></strong>, also called the <strong>coefficient of determination</strong>, is the <strong>proportion of variance</strong> of our <strong>fitted values</strong> (predictions) <span class="math inline">\(\hat{y}_i\)</span> to our true values <span class="math inline">\(y_i\)</span>. It ranges from 0 to 1 and is effectively the <em>proportion</em> of variance in the observations the <strong>model explains</strong>.</p>
<p><span class="math display">\[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]</span></p>
<p>Note that for OLS with an intercept term, for example <span class="math inline">\(\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_px_p\)</span>, <span class="math inline">\(\mathbb{R}^2\)</span> is equal to the square of the correlation between <span class="math inline">\(y\)</span> and <span class="math inline">\(\hat{y}\)</span>. On the other hand for SLR, <span class="math inline">\(\mathbb{R}^2\)</span> is equal to <span class="math inline">\(r^2\)</span>, the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The proof of these last two properties is out of scope for this course.</p>
<p>Additionally, as we add more features, our fitted values tend to become closer and closer to our actual values. Thus, <span class="math inline">\(\mathbb{R}^2\)</span> increases.</p>
<p>Adding more features doesn’t always mean our model is better though! We’ll see why later in the course.</p>
</section>
<section id="ols-properties" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="ols-properties"><span class="header-section-number">12.8</span> OLS Properties</h2>
<ol type="1">
<li>When using the optimal parameter vector, our residuals <span class="math inline">\(e = \mathbb{Y} - \hat{\mathbb{Y}}\)</span> are orthogonal to <span class="math inline">\(span(\mathbb{X})\)</span>.</li>
</ol>
<p><span class="math display">\[\mathbb{X}^Te = 0 \]</span></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<ul>
<li>The optimal parameter vector, <span class="math inline">\(\hat{\theta}\)</span>, solves the normal equations <span class="math inline">\(\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)</span></li>
</ul>
<p><span class="math display">\[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]</span></p>
<p><span class="math display">\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{X}\hat{\theta}\]</span></p>
<ul>
<li>Any matrix multiplied with its own inverse is the identity matrix <span class="math inline">\(\mathbb{I}\)</span></li>
</ul>
<p><span class="math display">\[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y} = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]</span></p>
</div>
</div>
</div>
<ol start="2" type="1">
<li>For all linear models with an <strong>intercept term</strong>, the <strong>sum of residuals is zero</strong>.</li>
</ol>
<p><span class="math display">\[\sum_i^n e_i = 0\]</span></p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<ul>
<li>For all linear models with an <strong>intercept term</strong>, the average of the predicted <span class="math inline">\(y\)</span> values is equal to the average of the true <span class="math inline">\(y\)</span> values. <span class="math display">\[\bar{y} = \bar{\hat{y}}\]</span></li>
<li>Rewriting the sum of residuals as two separate sums, <span class="math display">\[\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i\]</span></li>
<li>Each respective sum is a multiple of the average of the sum. <span class="math display">\[\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0\]</span></li>
</ul>
</div>
</div>
</div>
<ol start="3" type="1">
<li>The Least Squares estimate <span class="math inline">\(\hat{\theta}\)</span> is <strong>unique</strong> if and only if <span class="math inline">\(\mathbb{X}\)</span> is <strong>full column rank</strong>.</li>
</ol>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<ul>
<li>We know the solution to the normal equation <span class="math inline">\(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{Y}\)</span> is the least square estimate that fulfills the prior equality.</li>
<li><span class="math inline">\(\hat{\theta}\)</span> has a <strong>unique</strong> solution <span class="math inline">\(\iff\)</span> the square matrix <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> is <strong>invertible</strong>.
<ul>
<li>The <strong>column</strong> rank of a square matrix is the number of linearly independent columns it contains.</li>
<li>An <span class="math inline">\(n\)</span> x <span class="math inline">\(n\)</span> square matrix is deemed full column rank when all of its columns are linearly independent. That is, its rank would be equal to <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has shape <span class="math inline">\((p + 1) \times (p + 1)\)</span>, and therefore has max rank <span class="math inline">\(p + 1\)</span>.</li>
</ul></li>
<li><span class="math inline">\(rank(\mathbb{X}^T\mathbb{X})\)</span> = <span class="math inline">\(rank(\mathbb{X})\)</span> (proof out of scope).</li>
<li>Therefore, <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has rank <span class="math inline">\(p + 1\)</span> <span class="math inline">\(\iff\)</span> <span class="math inline">\(\mathbb{X}\)</span> has rank <span class="math inline">\(p + 1\)</span> <span class="math inline">\(\iff \mathbb{X}\)</span> is full column rank.</li>
</ul>
</div>
</div>
</div>
<p>To summarize:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Model</th>
<th>Estimate</th>
<th>Unique?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Constant Model + MSE</td>
<td><span class="math inline">\(\hat{y} = \theta_0\)</span></td>
<td><span class="math inline">\(\hat{\theta_0} = mean(y) = \bar{y}\)</span></td>
<td><strong>Yes</strong>. Any set of values has a unique mean.</td>
</tr>
<tr class="even">
<td>Constant Model + MAE</td>
<td><span class="math inline">\(\hat{y} = \theta_0\)</span></td>
<td><span class="math inline">\(\hat{\theta_0} = median(y)\)</span></td>
<td><strong>Yes</strong>, if odd. <strong>No</strong>, if even. Return the average of the middle 2 values.</td>
</tr>
<tr class="odd">
<td>Simple Linear Regression + MSE</td>
<td><span class="math inline">\(\hat{y} = \theta_0 + \theta_1x\)</span></td>
<td><span class="math inline">\(\hat{\theta_0} = \bar{y} - \hat{\theta_1}\hat{x}\)</span> <span class="math inline">\(\hat{\theta_1} = r\frac{\sigma_y}{\sigma_x}\)</span></td>
<td><strong>Yes</strong>. Any set of non-constant* values has a unique mean, SD, and correlation coefficient.</td>
</tr>
<tr class="even">
<td><strong>OLS</strong> (Linear Model + MSE)</td>
<td><span class="math inline">\(\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}\)</span></td>
<td><span class="math inline">\(\hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)</span></td>
<td><strong>Yes</strong>, if <span class="math inline">\(\mathbb{X}\)</span> is full column rank (all columns are linearly independent, # of datapoints &gt;&gt;&gt; # of features).</td>
</tr>
</tbody>
</table>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../constant_model_loss_transformations/loss_transformations.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../gradient_descent/gradient_descent.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Ordinary Least Squares</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Ordinary Least Squares</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Define linearity with respect to a vector of parameters $\theta$.</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Understand the use of matrix notation to express multiple linear regression.</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Interpret ordinary least squares as the minimization of the norm of the residual vector.</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute performance metrics for multiple linear regression.</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>We've now spent a number of lectures exploring how to build effective models – we introduced the SLR and constant models, selected cost functions to suit our modeling task, and applied transformations to improve the linear fit.</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>Throughout all of this, we considered models of one feature ($\hat{y}_i = \theta_0 + \theta_1 x_i$) or zero features ($\hat{y}_i = \theta_0$). As data scientists, we usually have access to datasets containing *many* features. To make the best models we can, it will be beneficial to consider all of the variables available to us as inputs to a model, rather than just one. In today's lecture, we'll introduce **multiple linear regression** as a framework to incorporate multiple features into a model. We will also learn how to accelerate the modeling process – specifically, we'll see how linear algebra offers us a powerful set of tools for understanding model performance.</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linearity </span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>An expression is **linear in $\theta$** (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms -- a **vector of $\theta$** s, and a matrix/vector **not involving $\theta$** -- is a good indicator of linearity.</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>For example, consider the vector $\theta = <span class="co">[</span><span class="ot">\theta_0, \theta_1, \theta_2</span><span class="co">]</span>$</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2$ is linear in theta, and we can separate it into a matrix product of two terms:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix} \theta_0 <span class="sc">\\</span> \theta_1 <span class="sc">\\</span> \theta_2 \end{bmatrix}$$</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)$ is *not* linear in theta, as the $\theta_1$ term is squared, and the $\theta_2$ term is logged. We cannot separate it into a matrix product of two terms.</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="fu">## Terminology for Multiple Linear Regression</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>There are several equivalent terms in the context of regression. The ones we use most often for this course are bolded.</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$x$ can be called a</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Feature(s)**</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Covariate(s)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Independent variable(s)**</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Explanatory variable(s)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Predictor(s)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Input(s)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Regressor(s)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$y$ can be called an</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Output**</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Outcome</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Response**</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Dependent variable</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\hat{y}$ can be called a</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Prediction**</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Predicted response</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Estimated value</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\theta$ can be called a</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Weight(s)**</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Parameter(s)**</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Coefficient(s)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\hat{\theta}$ can be called a</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Estimator(s)**</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Optimal parameter(s)**</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A datapoint $(x, y)$ is also called an observation.</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multiple Linear Regression</span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>Multiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}$$</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>Our predicted value of $y$, $\hat{y}$, is a linear combination of the single **observations** (features), $x_i$, and the parameters, $\theta_i$. </span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>We can explore this idea further by looking at a dataset containing aggregate per-player data from the 2018-19 NBA season, downloaded from <span class="co">[</span><span class="ot">Kaggle</span><span class="co">](https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019)</span>.</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a><span class="co">#| vscode: {languageId: python}</span></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>nba <span class="op">=</span> pd.read_csv(<span class="st">'data/nba18-19.csv'</span>, index_col<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>nba.index.name <span class="op">=</span> <span class="va">None</span> <span class="co"># Drops name of index (players are ordered by rank)</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a><span class="co">#| vscode: {languageId: python}</span></span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>nba.head(<span class="dv">5</span>)</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>Let's say we are interested in predicting the number of points (<span class="in">`PTS`</span>) an athlete will score in a basketball game this season.</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>Suppose we want to fit a linear model by using some characteristics, or **features** of a player. Specifically, we'll focus on field goals, assists, and 3-point attempts.</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`FG`</span>, the number of (2-point) field goals per game</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`AST`</span>, the average number of assists per game</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`3PA`</span>, the number of 3-point field goals attempted per game</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a><span class="co">#| vscode: {languageId: python}</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>nba[[<span class="st">'FG'</span>, <span class="st">'AST'</span>, <span class="st">'3PA'</span>, <span class="st">'PTS'</span>]].head()</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>Because we are now dealing with many parameter values, we've collected them all into a **parameter vector** with dimensions $(p+1) \times 1$ to keep things tidy. Remember that $p$ represents the number of features we have (in this case, 3).</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>$$\theta = \begin{bmatrix}</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>           \theta_{0} <span class="sc">\\</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>           \theta_{1} <span class="sc">\\</span></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>           \theta_{p}</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}$$</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>We are working with two vectors here: a row vector representing the observed data, and a column vector containing the model parameters. The multiple linear regression model is **equivalent to the dot (scalar) product of the observation vector and parameter vector**. </span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>$$<span class="co">[</span><span class="ot">1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}</span><span class="co">]</span> \theta = <span class="co">[</span><span class="ot">1,\:x_{1},\:x_{2},\:x_{3},\:...,\:x_{p}</span><span class="co">]</span> \begin{bmatrix}</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>           \theta_{0} <span class="sc">\\</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>           \theta_{1} <span class="sc">\\</span></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>           \theta_{p}</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:...\:+\:\theta_p x_{p}$$</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>Notice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with $\theta_0$ to give the intercept of the regression model. We call this 1 entry the **intercept** or **bias** term.</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>Given that we have three features here, we can express this model as:</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \theta_0\:+\:\theta_1x_{1}\:+\:\theta_2 x_{2}\:+\:\theta_3 x_{3}$$</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a>Our features are represented by $x_1$ (<span class="in">`FG`</span>), $x_2$ (<span class="in">`AST`</span>), and $x_3$ (<span class="in">`3PA`</span>) with each having correpsonding parameters, $\theta_1$, $\theta_2$, and  $\theta_3$.</span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>In statistics, this model + loss is called **Ordinary Least Squares (OLS)**. The solution to OLS is the minimizing loss for parameters $\hat{\theta}$, also called the **least squares estimate**.</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a><span class="fu">## Linear Algebra Approach</span></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>We now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple linear regression. We want to use the tools of linear algebra.</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>Let's think about how we can apply what we did above. To accomodate for the fact that we're considering several feature variables, we'll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of $p$ features.</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/observation.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'observation'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'550'</span><span class="kw">&gt;</span></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>To make a prediction from the first observation in the data, we take the dot product of the parameter vector and first observation vector. To make a prediction from the *second* observation, we would repeat this process to find the dot product of the parameter vector and the *second* observation vector. If we wanted to find the model predictions for each observation in the dataset, we'd repeat this process for all $n$ observations in the data. </span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_1 = \theta_0 + \theta_1 x_{11} + \theta_2 x_{12} + ... + \theta_p x_{1p} = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta$$</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_2 = \theta_0 + \theta_1 x_{21} + \theta_2 x_{22} + ... + \theta_p x_{2p} = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}] \theta$$</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>$$\vdots$$</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_n = \theta_0 + \theta_1 x_{n1} + \theta_2 x_{n2} + ... + \theta_p x_{np} = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta$$</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>Our observed data is represented by $n$ row vectors, each with dimension $(p+1)$. We can collect them all into a single matrix, which we call $\mathbb{X}$.</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/design_matrix.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'design_matrix'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'400'</span><span class="kw">&gt;</span></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>The matrix $\mathbb{X}$ is known as the **design matrix**. It contains all observed data for each of our $p$ features, where each **row** corresponds to one **observation**, and each **column** corresponds to a **feature**. It often (but not always) contains an additional column of all ones to represent the **intercept** or **bias column**. </span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>To review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;</span></span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>        <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/row_col.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'row_col'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>The multiple linear regression model can then be restated in terms of matrices:</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>\Large</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>\mathbb{\hat{Y}} = \mathbb{X} \theta</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>Here, $\mathbb{\hat{Y}}$ is the **prediction vector** with $n$ elements ($\mathbb{\hat{Y}} \in \mathbb{R}^{n}$); it contains the prediction made by the model for each of the $n$ input observations. $\mathbb{X}$ is the **design matrix** with dimensions $\mathbb{X} \in \mathbb{R}^(n \times (p + 1))$, and $\theta$ is the **parameter vector** with dimensions $\theta \in \mathbb{R}^{(p + 1)}$</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>As a refresher, let's also review the **dot product (or inner product)**. This is a vector operation that:</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Can only be carried out on two vectors of the **same length**</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Sums up the products of the corresponding entries of the two vectors</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Returns a single number</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>While this is not in scope, note that we can also interpret the dot product geometrically:</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>It is the product of three things: the **magnitude** of both vectors, and the **cosine** of the angles between them: $$\vec{u} \cdot \vec{v} = ||\vec{u}|| \cdot ||\vec{v}|| \cdot {cos \theta}$$</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mean Squared Error</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>We now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of risk functions and model fitting.</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>Recall our definition of MSE:</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>$$R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>At its heart, the MSE is a measure of *distance* – it gives an indication of how "far away" the predictions are from the true values, on average. </span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>When working with vectors, this idea of "distance" or the vector's **size/length** is represented by the **norm**. More precisely, the distance between two vectors $\vec{a}$ and $\vec{b}$ can be expressed as:</span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>$$||\vec{a} - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}$$</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>The double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>The two norms we need to know for Data 100 are the L1 and L2 norms (sound familiar?). In this note, we'll focus on L2 norm. We'll dive into L1 norm in future lectures. </span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>For the n-dimensional vector $$\vec{x} = \begin{bmatrix} x_1 <span class="sc">\\</span> x_2 <span class="sc">\\</span> \vdots <span class="sc">\\</span> x_n \end{bmatrix}$$, the **L2 vector norm** is</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>$$||\vec{x}||_2 = \sqrt{(x_1)^2 + (x_2)^2 + \ldots + (x_n)^2} = \sqrt{\sum_{i=1}^n (x_i)^2}$$</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>The L2 vector norm is a generalization of the Pythagorean theorem in $n$ dimensions. Thus, it can be used as a measure of the **length** of a vector or even as a measure of the **distance** between two vectors. </span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>We can express the MSE as a squared L2 norm if we rewrite it in terms of the prediction vector, $\hat{\mathbb{Y}}$, and true target vector, $\mathbb{Y}$:</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>$$R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y} - \hat{\mathbb{Y}}||_2^2$$</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>Here, the superscript 2 outside of the norm double bars means that we are *squaring* the norm. If we plug in our linear model $\hat{\mathbb{Y}} = \mathbb{X} \theta$, we find the MSE cost function in vector notation:</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>$$R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2$$</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>Under the linear algebra perspective, our new task is to fit the optimal parameter vector $\theta$ such that the cost function is minimized. Equivalently, we wish to minimize the norm $$||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2.$$ </span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>We can restate this goal in two ways:</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minimize the **distance** between the vector of true values, $\mathbb{Y}$, and the vector of predicted values, $\mathbb{\hat{Y}}$</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minimize the **length** of the **residual vector**, defined as:</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>$$e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>           y_1 - \hat{y}_1 <span class="sc">\\</span></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>           y_2 - \hat{y}_2 <span class="sc">\\</span></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>           y_n - \hat{y}_n</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}$$</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometric Perspective</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>To derive the best parameter vector to meet this goal, we can turn to the geometric properties of our modeling setup.</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>Up until now, we've mostly thought of our model as a scalar product between horizontally stacked observations and the parameter vector. We can also think of $\hat{\mathbb{Y}}$ as a **linear combination of feature vectors**, scaled by the **parameters**. We use the notation $\mathbb{X}_{:, i}$ to denote the $i$th column of the design matrix. You can think of this as following the same convention as used when calling <span class="in">`.iloc`</span> and <span class="in">`.loc`</span>. ":" means that we are taking all entries in the $i$th column.</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;</span></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>        <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/columns.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'columns'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'500'</span><span class="kw">&gt;</span></span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>\hat{\mathbb{Y}} = </span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>\theta_0 \begin{bmatrix}</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>           1 <span class="sc">\\</span></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>           1 <span class="sc">\\</span></span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>           1</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix} + \theta_1 \begin{bmatrix}</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>           x_{11} <span class="sc">\\</span></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>           x_{21} <span class="sc">\\</span></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>           x_{n1}</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix} + \ldots + \theta_p \begin{bmatrix}</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>           x_{1p} <span class="sc">\\</span></span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>           x_{2p} <span class="sc">\\</span></span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>           \vdots <span class="sc">\\</span></span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>           x_{np}</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>         \end{bmatrix}</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>         = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p \mathbb{X}_{:,\:p+1}$$</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>This new approach is useful because it allows us to take advantage of the properties of linear combinations.</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>Recall that the **span** or **column space** of a matrix $\mathbb{X}$ (denoted $span(\mathbb{X})$) is the set of all possible linear combinations of the matrix's columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns. Additionally, if each column of $\mathbb{X}$ has length $n$, $span(\mathbb{X})$ is a subspace of $\mathbb{R}^{n}$.</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>Because the prediction vector, $\hat{\mathbb{Y}} = \mathbb{X} \theta$, is a **linear combination** of the columns of $\mathbb{X}$, we know that the **predictions are contained in the span of $\mathbb{X}$**. That is, we know that $\mathbb{\hat{Y}} \in \text{Span}(\mathbb{X})$.</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>The diagram below is a simplified view of $\text{Span}(\mathbb{X})$, assuming that each column of $\mathbb{X}$ has length $n$. Notice that the columns of $\mathbb{X}$ define a subspace of $\mathbb{R}^n$, where each point in the subspace can be reached by a linear combination of $\mathbb{X}$'s columns. The prediction vector $\mathbb{\hat{Y}}$ lies somewhere in this subspace.</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;</span></span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>        <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/span.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'span'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>Examining this diagram, we find a problem. The vector of true values, $\mathbb{Y}$, could theoretically lie *anywhere* in $\mathbb{R}^n$ space – its exact location depends on the data we collect out in the real world. However, our multiple linear regression model can only make predictions in the subspace of $\mathbb{R}^n$ spanned by $\mathbb{X}$. Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, $\mathbb{Y}$, and the vector of predicted values, $\mathbb{\hat{Y}}$, is minimized. This means that **we want $\mathbb{\hat{Y}}$ to be the vector in $\text{Span}(\mathbb{X})$ that is closest to $\mathbb{Y}$**. </span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>Another way of rephrasing this goal is to say that we wish to minimize the length of the residual vector $e$, as measured by its $L_2$ norm. </span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;</span></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>        <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/residual.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'residual'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>The vector in $\text{Span}(\mathbb{X})$ that is closest to $\mathbb{Y}$ is always the **orthogonal projection** of $\mathbb{Y}$ onto $\text{Span}(\mathbb{X}).$ Thus, we should choose the parameter vector $\theta$ that makes the **residual vector orthogonal to any vector in $\text{Span}(\mathbb{X})$**. You can visualize this as the vector created by dropping a perpendicular line from $\mathbb{Y}$ onto the span of $\mathbb{X}$. </span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>How does this help us identify the optimal parameter vector, $\hat{\theta}$? Recall that two vectors $a$ and $b$ are orthogonal if their dot product is zero: ${a}^{T}b = 0$. A vector $v$ is **orthogonal** to the span of a matrix $M$ if and only if $v$ is orthogonal to **each column** in $M$. Put together, a vector $v$ is orthogonal to $\text{Span}(M)$ if:</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>$$M^Tv = \vec{0}$$</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>Note that $\vec{0}$ represents the **zero vector**, a $d$-length vector full of 0s.</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>Remember our goal is to find $\hat{\theta}$ such that we minimize the objective function $R(\theta)$. Equivalently, this is the $\hat{\theta}$ such that the residual vector $e = \mathbb{Y} - \mathbb{X} \theta$ is orthogonal to $\text{Span}(\mathbb{X})$. </span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>Looking at the definition of orthogonality of $\mathbb{Y} - \mathbb{X}\hat{\theta}$ to $span(\mathbb{X})$ (0 is the $\vec{0}$ vector), we can write: </span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}$$ </span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>Let's then rearrange the terms: </span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} = \vec{0}$$</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>And finally, we end up with the **normal equation**:</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}$$</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>Any vector $\theta$ that minimizes MSE on a dataset must satisfy this equation.</span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>If $\mathbb{X}^T \mathbb{X}$ is invertible, we can conclude:</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}$$</span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a>This is called the **least squares estimate** of $\theta$: it is the value of $\theta$ that minimizes the squared loss. </span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>Note that the least squares estimate was derived under the assumption that $\mathbb{X}^T \mathbb{X}$ is *invertible*. This condition holds true when $\mathbb{X}^T \mathbb{X}$ is full column rank, which, in turn, happens when $\mathbb{X}$ is full column rank. We will explore the consequences of this fact in lab and homework.</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluating Model Performance</span></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a>Our geometric view of multiple linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features.</span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a>Now, we want to understand how well our fitted model performs. One measure of model performance is the **Root Mean Squared Error**, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of $y_i$, which is useful for understanding the model's performance. A low RMSE indicates more "accurate" predictions – that there is a lower average loss across the dataset.</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a>$$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$$</span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a>When working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple linear model performs well if its residual plot shows no patterns.</span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">align</span><span class="ot">=</span><span class="st">"middle"</span><span class="kw">&gt;</span></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;table</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width:100%"</span><span class="kw">&gt;</span></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;tr</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;td&gt;</span></span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a>        <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/residual_plot.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'residual_plot'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'500'</span><span class="kw">&gt;</span></span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a>      <span class="kw">&lt;/td&gt;</span></span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a>    <span class="kw">&lt;/tr&gt;</span></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;/table&gt;</span></span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/div&gt;</span></span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>For SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple linear model setting, we will need a performance metric that can account for multiple features at once. **Multiple $R^2$**, also called the **coefficient of determination**, is the **proportion of variance** of our **fitted values** (predictions) $\hat{y}_i$ to our true values $y_i$. It ranges from 0 to 1 and is effectively the *proportion* of variance in the observations the **model explains**. </span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>$$R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}$$</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>Note that for OLS with an intercept term, for example $\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_px_p$, $\mathbb{R}^2$ is equal to the square of the correlation between $y$ and $\hat{y}$. On the other hand for SLR, $\mathbb{R}^2$ is equal to $r^2$, the correlation between $x$ and $y$. The proof of these last two properties is out of scope for this course.</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>Additionally, as we add more features, our fitted values tend to become closer and closer to our actual values. Thus, $\mathbb{R}^2$ increases.</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>Adding more features doesn't always mean our model is better though! We'll see why later in the course.</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a><span class="fu">## OLS Properties</span></span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>When using the optimal parameter vector, our residuals $e = \mathbb{Y} - \hat{\mathbb{Y}}$ are orthogonal to $span(\mathbb{X})$.</span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^Te = 0 $$</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>::: {.callout}</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>Proof: </span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The optimal parameter vector, $\hat{\theta}$, solves the normal equations $\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}$</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) $$</span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{X}\hat{\theta}$$</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Any matrix multiplied with its own inverse is the identity matrix $\mathbb{I}$</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>$$\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y} = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0$$</span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For all linear models with an **intercept term**, the **sum of residuals is zero**.</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>$$\sum_i^n e_i = 0$$</span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>::: {.callout}</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>Proof:</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>For all linear models with an **intercept term**, the average of the predicted $y$ values is equal to the average of the true $y$ values.</span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>$$\bar{y} = \bar{\hat{y}}$$</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Rewriting the sum of residuals as two separate sums,</span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>$$\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i$$</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Each respective sum is a multiple of the average of the sum.</span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>$$\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0$$</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The Least Squares estimate $\hat{\theta}$ is **unique** if and only if $\mathbb{X}$ is **full column rank**.</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>::: {.callout}</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>Proof: </span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>We know the solution to the normal equation $\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{Y}$ is the least square estimate that fulfills the prior equality.</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\hat{\theta}$ has a **unique** solution $\iff$ the square matrix $\mathbb{X}^T\mathbb{X}$ is **invertible**.</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>The **column** rank of a square matrix is the number of linearly independent columns it contains.</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>An $n$ x $n$ square matrix is deemed full column rank when all of its columns are linearly independent. That is, its rank would be equal to $n$.</span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>$\mathbb{X}^T\mathbb{X}$ has shape $(p + 1) \times (p + 1)$, and therefore has max rank $p + 1$. </span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$rank(\mathbb{X}^T\mathbb{X})$ = $rank(\mathbb{X})$ (proof out of scope).</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Therefore, $\mathbb{X}^T\mathbb{X}$ has rank $p + 1$ $\iff$  $\mathbb{X}$ has rank $p + 1$ $\iff \mathbb{X}$ is full column rank.</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>To summarize:</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>|   | Model | Estimate | Unique? |</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>| -- | -- | -- |  -- | </span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>| Constant Model + MSE | $\hat{y} = \theta_0$| $\hat{\theta_0} = mean(y) = \bar{y}$ | **Yes**. Any set of values has a unique mean.</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a>| Constant Model + MAE | $\hat{y} = \theta_0$  | $\hat{\theta_0} = median(y)$ | **Yes**, if odd. **No**, if even. Return the average of the middle 2 values.</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>| Simple Linear Regression + MSE | $\hat{y} = \theta_0 + \theta_1x$| $\hat{\theta_0} = \bar{y} - \hat{\theta_1}\hat{x}$ $\hat{\theta_1} = r\frac{\sigma_y}{\sigma_x}$| **Yes**. Any set of non-constant* values has a unique mean, SD, and correlation coefficient.</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a>| **OLS** (Linear Model + MSE) | $\mathbb{\hat{Y}} = \mathbb{X}\mathbb{\theta}$| $\hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}$  | **Yes**, if $\mathbb{X}$ is full column rank (all columns are linearly independent, # of datapoints &gt;&gt;&gt; # of features).</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>