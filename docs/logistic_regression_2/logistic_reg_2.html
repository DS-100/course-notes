<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 24&nbsp; Logistic Regression II</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../pca_1/pca_1.html" rel="next">
<link href="../logistic_regression_1/logistic_reg_1.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="../Principles-and-Techniques-of-Data-Science.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Random Variables</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_climate/case_study_climate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Case Study in Climate and Physical Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bias, Variance, and Inference</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">SQL II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">PCA II</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#logistic-regression-model-continued" id="toc-logistic-regression-model-continued" class="nav-link active" data-scroll-target="#logistic-regression-model-continued"><span class="toc-section-number">24.1</span>  Logistic Regression Model (continued)</a>
  <ul>
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="toc-section-number">24.1.1</span>  Maximum Likelihood Estimation</a></li>
  </ul></li>
  <li><a href="#a-tangent-note" id="toc-a-tangent-note" class="nav-link" data-scroll-target="#a-tangent-note"><span class="toc-section-number">24.2</span>  A (Tangent) Note</a>
  <ul>
  <li><a href="#linear-seperability-and-regularization" id="toc-linear-seperability-and-regularization" class="nav-link" data-scroll-target="#linear-seperability-and-regularization"><span class="toc-section-number">24.2.1</span>  Linear Seperability and Regularization</a>
  <ul>
  <li><a href="#regularized-logistic-regression" id="toc-regularized-logistic-regression" class="nav-link" data-scroll-target="#regularized-logistic-regression"><span class="toc-section-number">24.2.1.1</span>  Regularized Logistic Regression</a></li>
  </ul></li>
  <li><a href="#logistic-regression-model-implementation" id="toc-logistic-regression-model-implementation" class="nav-link" data-scroll-target="#logistic-regression-model-implementation"><span class="toc-section-number">24.2.2</span>  Logistic Regression Model Implementation</a></li>
  </ul></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="toc-section-number">24.3</span>  Performance Metrics</a>
  <ul>
  <li><a href="#the-confusion-matrix" id="toc-the-confusion-matrix" class="nav-link" data-scroll-target="#the-confusion-matrix"><span class="toc-section-number">24.3.1</span>  The Confusion Matrix</a></li>
  <li><a href="#accuracy-precision-and-recall" id="toc-accuracy-precision-and-recall" class="nav-link" data-scroll-target="#accuracy-precision-and-recall"><span class="toc-section-number">24.3.2</span>  Accuracy, Precision, and Recall</a>
  <ul>
  <li><a href="#example-calculation" id="toc-example-calculation" class="nav-link" data-scroll-target="#example-calculation"><span class="toc-section-number">24.3.2.1</span>  Example Calculation</a>
  <ul class="collapse">
  <li><a href="#model-1" id="toc-model-1" class="nav-link" data-scroll-target="#model-1"><span class="toc-section-number">24.3.2.1.1</span>  Model 1</a></li>
  <li><a href="#model-2" id="toc-model-2" class="nav-link" data-scroll-target="#model-2"><span class="toc-section-number">24.3.2.1.2</span>  Model 2</a></li>
  </ul></li>
  <li><a href="#precision-vs-recall" id="toc-precision-vs-recall" class="nav-link" data-scroll-target="#precision-vs-recall"><span class="toc-section-number">24.3.2.2</span>  Precision vs Recall</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#adjusting-the-classification-threshold" id="toc-adjusting-the-classification-threshold" class="nav-link" data-scroll-target="#adjusting-the-classification-threshold"><span class="toc-section-number">24.4</span>  Adjusting the Classification Threshold</a>
  <ul>
  <li><a href="#two-more-metrics" id="toc-two-more-metrics" class="nav-link" data-scroll-target="#two-more-metrics"><span class="toc-section-number">24.4.1</span>  Two More Metrics</a></li>
  <li><a href="#the-roc-curve" id="toc-the-roc-curve" class="nav-link" data-scroll-target="#the-roc-curve"><span class="toc-section-number">24.4.2</span>  The ROC Curve</a></li>
  </ul></li>
  <li><a href="#extra-content" id="toc-extra-content" class="nav-link" data-scroll-target="#extra-content"><span class="toc-section-number">24.5</span>  Extra Content</a>
  <ul>
  <li><a href="#precision-recall-curves" id="toc-precision-recall-curves" class="nav-link" data-scroll-target="#precision-recall-curves"><span class="toc-section-number">24.5.1</span>  Precision-Recall Curves</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Today, we will continue studying the Logistic Regression model. First, we’ll examine logisitc regression through the lens of maxmium likelihood estimation and understand its connection to cross-entropy loss. Then, we’ll pick up from last lecture’s discussion of cross-entropy loss, study a few of its pitfalls, and learn potential remedies. Finally, we will provide an implementation of <code>sklearn</code>’s logistic regression model.</p>
<p>This will introduce us to the process of <strong>thresholding</strong> – a technique used to <em>classify</em> data from our model’s predicted probabilities, or <span class="math inline">\(P(Y=1|x)\)</span>. In doing so, we’ll focus on how these thresholding decisions affect the behavior of our model. We will learn various evaluation metrics useful for binary classification, and apply them to our study of logistic regression.</p>
<section id="logistic-regression-model-continued" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="logistic-regression-model-continued"><span class="header-section-number">24.1</span> Logistic Regression Model (continued)</h2>
<section id="maximum-likelihood-estimation" class="level3" data-number="24.1.1">
<h3 data-number="24.1.1" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">24.1.1</span> Maximum Likelihood Estimation</h3>
<p>In our earlier coin toss example, we had data on 10 coin flips, and wanted to estimate <span class="math inline">\(\hat \theta\)</span>, the probability of a heads.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>flips <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>flips</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>[0, 0, 1, 1, 1, 1, 0, 0, 0, 0]</code></pre>
</div>
</div>
<p><span class="math inline">\(\hat \theta = 0.4\)</span> is the most intuitive two reasons:</p>
<ol type="1">
<li>It is the frequency of heads in our data</li>
<li>It maximizes the <strong>likelihood</strong> of our data</li>
</ol>
<p><span class="math display">\[\hat \theta = \text{argmax}_\theta (\theta^4(1-\theta)^6)\]</span></p>
<p>More generally, we can apply this notion of likelihood to any random binary sample. For example, we can find the likelihood of the data observed in our breast cancer study. We will show how the likelihood is intrinsically related to cross-entropy loss.</p>
<p>As a quick refresher on likelihood:</p>
<p>For some Bernoulli(<span class="math inline">\(p\)</span>) random variable <span class="math inline">\(Y\)</span> the probability distribution, or likelihood is:</p>
<p><span class="math display">\[P(Y = y) = \begin{cases}
        1, \text{with probability }  p\\
        0, \text{with probability }  1 - p
    \end{cases} \]</span></p>
<p>Equivalently, this can be written in a compact way:</p>
<p><span class="math display">\[P(Y=y) = p^y(1-p)^{1-y}\]</span></p>
<ul>
<li>When <span class="math inline">\(y = 1\)</span>, this reads <span class="math inline">\(P(Y=y) = p\)</span></li>
<li>When <span class="math inline">\(y = 0\)</span>, this reads <span class="math inline">\(P(Y=y) = (1-p)\)</span></li>
</ul>
<p>In our example, a Bernoulli random variable is analagous to a single data point, or tumor (from the previous chapter). All together, our breast cancer study consist of multiple IID Bernoulli(<span class="math inline">\(p\)</span>) random variables. To find the likelihood of independent events in succession, simply multiply their likelihoods.</p>
<p><span class="math display">\[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]</span></p>
<p>As with the coin example, we want to find the parameter <span class="math inline">\(p\)</span> that maximizes this likelihood - this technique is known as <strong>maximum likelihood estimation</strong>. Earlier, we gave an intuitive graphical solution, but let’s take the derivative of the likelihood to find this maximum.</p>
<p>From a first glance, this derivative will be complicated! We will have to use the product rule, followed by the chain rule. Instead, we can make an observation that simplifies the problem.</p>
<p>Finding the <span class="math inline">\(p\)</span> that maximizes <span class="math display">\[\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i}\]</span> is equivalent to the <span class="math inline">\(p\)</span> that maximizes <span class="math display">\[\text{log}(\prod_{i=1}^{n} p^{y_i} (1-p)^{1-y_i})\]</span></p>
<p>This is because <span class="math inline">\(\text{log}\)</span> is a strictly <em>increasing</em> function. It won’t change the maximum or minimum of the function it was applied to. From <span class="math inline">\(\text{log}\)</span> properties, <span class="math inline">\(\text{log}(a*b)\)</span> = <span class="math inline">\(\text{log}(a) + \text{log}(b)\)</span>. We can apply this to our equation above to get:</p>
<p><span class="math display">\[\text{argmax}_p \sum_{i=1}^{n} \text{log}(p^{y_i} (1-p)^{1-y_i})\]</span></p>
<p><span class="math display">\[= \text{argmax}_p \sum_{i=1}^{n} \text{log}(p^{y_i}) + \text{log}((1-p)^{1-y_i})\]</span></p>
<p><span class="math display">\[= \text{argmax}_p \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)\]</span></p>
<p>We can add a constant factor of <span class="math inline">\(\frac{1}{n}\)</span> out front. It won’t affect the <span class="math inline">\(p\)</span> that maximizes our likelihood.</p>
<p><span class="math display">\[=\text{argmax}_p  \frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)\]</span></p>
<p>One last “trick” we can do is change this to a minimization problem by negating the result. This works because we are dealing with a <em>concave</em> function, which can be made <em>convex</em>.</p>
<p><span class="math display">\[= \text{argmin}_p -\frac{1}{n} \sum_{i=1}^{n} y_i\text{log}(p) + (1-y_i)\text{log}(1-p)\]</span></p>
<p>This is exactly our average cross-entropy loss minimization problem from before!</p>
<p>Why did we do all this complicated math? We have shown that <em>minimizing</em> cross-entropy loss is equivalent to <em>maximizing</em> the likelihood of the training data.</p>
<ul>
<li>By minimizing cross-entropy loss, we are choosing the model parameters that are “most likely” for the data we observed.</li>
</ul>
</section>
</section>
<section id="a-tangent-note" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="a-tangent-note"><span class="header-section-number">24.2</span> A (Tangent) Note</h2>
<p>You will study MLE further in probability and ML classes. But now you know it exists. It turns out that many of the model + loss combinations we’ve seen can be motivated using MLE (OLS, Ridge Regression, etc.)</p>
<p>The two important takeways from this section are</p>
<ol type="1">
<li>Formulating the Logistic Regression Model</li>
<li>Motivating the Cross-Entropy Loss</li>
</ol>
<p>We will now continue to learn how to evaluate the strength of logistic regression models.</p>
<p>Above, we proved that <em>minimizing</em> cross-entropy loss is equivalent to <em>maximizing</em> likelihood of the training data.</p>
<p>Intuitively, this means that the optimal <span class="math inline">\(\hat \theta\)</span> that minimizes cross-entropy loss “pushes” all the probabilities in the direction of their true, underlying class.</p>
<ul>
<li>For points that belong to the <span class="math inline">\(0\)</span> class, <span class="math inline">\(\sigma(x^T\theta) \rightarrow 0\)</span></li>
<li>For points that belong to the <span class="math inline">\(1\)</span> class, <span class="math inline">\(\sigma(x^T\theta) \rightarrow 1\)</span></li>
</ul>
<p>However, something interesting happens when our data is perfectly classifiable; in other words, linearly seperable.</p>
<section id="linear-seperability-and-regularization" class="level3" data-number="24.2.1">
<h3 data-number="24.2.1" class="anchored" data-anchor-id="linear-seperability-and-regularization"><span class="header-section-number">24.2.1</span> Linear Seperability and Regularization</h3>
<p>A classification dataset is said to be <strong>linearly separable</strong> if there exists a hyperplane among input features <span class="math inline">\(x\)</span> that separates the two classes <span class="math inline">\(y\)</span>. For example, notice how the plot on the bottom left is linearly seperable along the vertical line <span class="math inline">\(x=0\)</span>. No such line perfectly seperates the two classes on the bottom right.</p>
<ul>
<li>Linear seperability in 1D can be found with a rugplot of a single feature.</li>
</ul>
<p><img src="images/linear_seperability_1D.png" alt="linear_seperability_1D" width="800"></p>
<p>This same definition holds in higher dimensions. If there are two features, the seperating hyperplane must exist in two dimensions (any line of the form <span class="math inline">\(y=mx+b\)</span>)</p>
<ul>
<li>Linear seperability among 2 features is evident from a two-dimensional visualization, or scatter plot.</li>
</ul>
<p><img src="images/linear_seperability_2D.png" alt="linear_seperability_1D" width="800"></p>
<p>Complications may arise when data is linearly seperable. Consider the toy dataset with 2 points and only a single feature <span class="math inline">\(x\)</span>:</p>
<p><img src="images/toy_2_point.png" alt="toy_linear_seperability" width="500"></p>
<p>The optimal <span class="math inline">\(\theta\)</span> value that minimizes loss pushes the predicted probabilities of the data points to their true class.</p>
<ul>
<li><span class="math inline">\(P(Y = 1|x = -1) = \frac{1}{1 + e^\theta} \rightarrow 1\)</span></li>
<li><span class="math inline">\(P(Y = 1|x = 1) = \frac{1}{1 + e^{-\theta}} \rightarrow 0\)</span></li>
</ul>
<p>This happens when <span class="math inline">\(\theta = -\infty\)</span>. When <span class="math inline">\(\theta = -\infty\)</span>, we observe the following behavior for any input <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[P(Y=1|x) = \sigma(\theta x) \rightarrow \begin{cases}
        1, \text{if }  x &lt; 0\\
        0, \text{if }  x \ge 0
    \end{cases}\]</span></p>
<p>The diverging weights cause the model to be overconfident. For example, consider the new point <span class="math inline">\((x, y) = (0.5, 1)\)</span>. Following the behavior above, our model will incorrectly predict <span class="math inline">\(p=0\)</span>, and a thus, <span class="math inline">\(\hat y = 0\)</span>.</p>
<p><img src="images/toy_3_point.png" alt="toy_linear_seperability" width="500"></p>
<p>The loss incurred by this misclassified point is infinite.</p>
<p><span class="math display">\[-(y\text{ log}(p) + (1-y)\text{ log}(1-p))\]</span></p>
<p><span class="math display">\[=1\text{log}(0)\]</span></p>
<p>Thus, diverging weights (<span class="math inline">\(|\theta| \rightarrow \infty\)</span>) occur with <strong>lineary separable</strong> data. “Overconfidence” is a particularly dangerous version of overfitting.</p>
<p>Consider the loss function with respect to the parameter <span class="math inline">\(\theta\)</span>.</p>
<p><img src="images/unreg_loss.png" alt="unreg_loss" width="500"></p>
<p>Although impossible to see, the plateau for negative values of <span class="math inline">\(\theta\)</span> is slightly tilted downwards, meaning the loss approaches <span class="math inline">\(0\)</span> as <span class="math inline">\(\theta\)</span> decreases and approaches <span class="math inline">\(-\infty\)</span>.</p>
<section id="regularized-logistic-regression" class="level4" data-number="24.2.1.1">
<h4 data-number="24.2.1.1" class="anchored" data-anchor-id="regularized-logistic-regression"><span class="header-section-number">24.2.1.1</span> Regularized Logistic Regression</h4>
<p>To avoid large weights, and thus, infinite loss (particularly on linearly seperable data), regularization is used. The same principles apply as with linear regression - make sure to standardize your features first.</p>
<p>For example, L2 (Ridge) Logistic Regression takes on the form</p>
<p><span class="math display">\[\min_{\theta} -\frac{1}{n} \sum_{i=1}^{n} (y_i \text{log}(\sigma(x_i^T\theta)) + (1-y_i)\text{log}(1-\sigma(x_i^T\theta))) + \lambda \sum_{i=1}^{d} \theta_j^2\]</span></p>
<p>Now, let us compare the loss functions of un-regularized and regularized logistic regression.</p>
<p><img src="images/unreg_loss.png" alt="unreg_loss" width="500"></p>
<p><img src="images/reg_loss.png" alt="reg_loss" width="500"></p>
<p>As we can see, <span class="math inline">\(L2\)</span> regularization helps us prevent diverging weights and deters against “overconfidence.”</p>
</section>
</section>
<section id="logistic-regression-model-implementation" class="level3" data-number="24.2.2">
<h3 data-number="24.2.2" class="anchored" data-anchor-id="logistic-regression-model-implementation"><span class="header-section-number">24.2.2</span> Logistic Regression Model Implementation</h3>
<p>The implementation of logistic regression in <code>sklearn</code> is simple. We’ll begin by fitting a model on the breast cancer dataset from last lecture.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sklearn.datasets</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>data_dict <span class="op">=</span> sklearn.datasets.load_breast_cancer()</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame(data_dict[<span class="st">'data'</span>], columns<span class="op">=</span>data_dict[<span class="st">'feature_names'</span>])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>data[<span class="st">'malignant'</span>] <span class="op">=</span> (data_dict[<span class="st">'target'</span>] <span class="op">==</span> <span class="dv">0</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data[[<span class="st">'mean radius'</span>]]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> data[<span class="st">'malignant'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>By default, <code>sklearn</code> applies regularization to the logistic regression class. This is to avoid diverging weights with seperable data. The code above can be written more expliclty as follows.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sklearn defaults</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="st">'l2'</span>, C<span class="op">=</span><span class="fl">1.0</span>, fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>model.fit(X, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The parameter <code>C</code> controls the amount of regularization – <code>C</code> is the inverse of the regularization hyperparameter <span class="math inline">\(\lambda\)</span>. Set <code>C</code> big for minimal regularization, and vice versa.</p>
<p>The <code>.predict_proba</code> method returns the predicted probabilities of belonging to each class. The first element corresponds to class <span class="math inline">\(0\)</span>, the second to class <span class="math inline">\(1\)</span>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here are the first 5 predicted probabilities</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>model.predict_proba(X)[:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[0.03507844, 0.96492156],
       [0.00257756, 0.99742244],
       [0.00632723, 0.99367277],
       [0.96826813, 0.03173187],
       [0.00343123, 0.99656877]])</code></pre>
</div>
</div>
<p>From here, the <code>.predict</code> function returns the predicted class <span class="math inline">\(\hat y\)</span> of the point. In the simple binary case,</p>
<p><span class="math display">\[\hat y = \begin{cases}
        1, &amp; P(Y=1|x) \ge 0.5\\
        0, &amp; \text{otherwise }
    \end{cases}\]</span></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here are the first 5 predicted classes</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>model.predict(X)[:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>array([1, 1, 1, 0, 1])</code></pre>
</div>
</div>
</section>
</section>
<section id="performance-metrics" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">24.3</span> Performance Metrics</h2>
<p>Now that we have our classifier, let’s quantify how well it performs. The most basic evaluation metric is <strong>accuracy</strong> – the proportion of correctly classified points.</p>
<p><span class="math display">\[\text{accuracy} = \frac{\# \text{ of points classified correctly}}{\# \text{ of total points}}\]</span></p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model.score(X, y) <span class="co"># built-in accuracy function</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>0.8787346221441125</code></pre>
</div>
</div>
<p>However, accuracy is not always a great metric for classification, particularily when the data has class imbalance.</p>
<p>To understand why, let’s consider a classification problem with 100 emails, 5 of which are spam. We’ll investigate two models where accuracy is a poor metric.</p>
<ul>
<li><strong>Model 1</strong>: Our first model classifies every email as non-spam. The model’s accuracy is high (<span class="math inline">\(\frac{95}{100} = 0.95\)</span>), but it doesn’t detect any spam emails. Despite the high accuracy, this is a bad model.</li>
<li><strong>Model 2</strong>: The second model classifies every email as spam. The accuracy is low (<span class="math inline">\(\frac{5}{100} = 0.05\)</span>), but the model correctly labels every spam email. Unfortunately, it also misclassifies every non-spam email.</li>
</ul>
<section id="the-confusion-matrix" class="level3" data-number="24.3.1">
<h3 data-number="24.3.1" class="anchored" data-anchor-id="the-confusion-matrix"><span class="header-section-number">24.3.1</span> The Confusion Matrix</h3>
<p>Model 1 from above has 5 <strong>false negatives (FN)</strong> – data points which were predicted to belong to class <span class="math inline">\(0\)</span> (non-spam), but their true class was <span class="math inline">\(1\)</span> (spam). In a similar vein, Model 2 has 95 <strong>false positives (FP)</strong> – that is, “false alarms” where we predict class <span class="math inline">\(1\)</span>, but the true class was <span class="math inline">\(0\)</span>. <strong>True positives (TP)</strong> and <strong>true negatives (TN)</strong> are when we correctly classify observations as being positive or negative, respectively.</p>
<p>These classifications can be concisely summarized in a <strong>confusion matrix</strong>.</p>
<p><img src="images/confusion_matrix.png" alt="confusion_matrix" width="500"></p>
<p>An easy way to remember this terminology is as follows:</p>
<ol type="1">
<li>Look at the second word in the phrase. <em>Positive</em> means a prediction of 1. <em>Negative</em> means a prediction of 0.</li>
<li>Look at the first word in the phrase. <em>True</em> means our prediction was correct. <em>False</em> means it was incorrect.</li>
</ol>
<p>A confusion matrix for a particular classifier may be found programatically. For our breast cancer data, it looks like this:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>confusion_matrix(y, y_pred)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([[333,  24],
       [ 45, 167]])</code></pre>
</div>
</div>
</section>
<section id="accuracy-precision-and-recall" class="level3" data-number="24.3.2">
<h3 data-number="24.3.2" class="anchored" data-anchor-id="accuracy-precision-and-recall"><span class="header-section-number">24.3.2</span> Accuracy, Precision, and Recall</h3>
<p>The purpose of our discussion of the confusion matrix was to motivate better performance metrics for classification problems with class imbalance - namely, precision and recall.</p>
<p><strong>Precision</strong> is defined as</p>
<p><span class="math display">\[\frac{\text{TP}}{\text{TP + FP}}\]</span></p>
<p>Precision answers the question: “of all observations that were predicted to be <span class="math inline">\(1\)</span>, what proportion were actually <span class="math inline">\(1\)</span>?” It measures how accurate the classifier is when its predictions are positive.</p>
<p><strong>Recall</strong> (or <strong>sensitivity</strong>) is defined as</p>
<p><span class="math display">\[\frac{\text{TP}}{\text{TP + FN}}\]</span></p>
<p>Recall aims to answer: “of all observations that were actually <span class="math inline">\(1\)</span>, what proportion were predicted to be <span class="math inline">\(1\)</span>?” It measures how many positive predictions were missed.</p>
<p>Here’s a helpful graphic that summarizes our discussion above.</p>
<p><img src="images/precision_recall_graphic.png" alt="confusion_matrix" width="700"></p>
<section id="example-calculation" class="level4" data-number="24.3.2.1">
<h4 data-number="24.3.2.1" class="anchored" data-anchor-id="example-calculation"><span class="header-section-number">24.3.2.1</span> Example Calculation</h4>
<p>In this section, we will calculate the accuracy, precision, and recall performance metrics for our earlier spam classification example. As a reminder, we had a 100 emails, 5 of which were spam. We designed two models:</p>
<ul>
<li>Model 1: Predict that every email is <em>non-spam</em></li>
<li>Model 2: Predict that every email is <em>spam</em></li>
</ul>
<section id="model-1" class="level5" data-number="24.3.2.1.1">
<h5 data-number="24.3.2.1.1" class="anchored" data-anchor-id="model-1"><span class="header-section-number">24.3.2.1.1</span> Model 1</h5>
<p>First, let’s begin by creating the confusion matrix.</p>
<table class="table">
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>True Negative: 95</td>
<td>False Positive: 0</td>
</tr>
<tr class="even">
<td>1</td>
<td>False Negative: 5</td>
<td>True Positive: 0</td>
</tr>
</tbody>
</table>
<p>Convince yourself of why our confusion matrix looks like so.</p>
<p><span class="math display">\[\text{accuracy} = \frac{95}{100} = 0.95\]</span> <span class="math display">\[\text{precision} = \frac{0}{0 + 0} = \text{undefined}\]</span> <span class="math display">\[\text{recall} = \frac{0}{0 + 5} = 0\]</span></p>
<ul>
<li>Notice how our precision is undefined because we never predicted class <span class="math inline">\(1\)</span></li>
<li>Our recall is 0 for the same reason – the numerator is 0 (we had no positive predictions)</li>
</ul>
</section>
<section id="model-2" class="level5" data-number="24.3.2.1.2">
<h5 data-number="24.3.2.1.2" class="anchored" data-anchor-id="model-2"><span class="header-section-number">24.3.2.1.2</span> Model 2</h5>
<p>Our confusion matrix for Model 2 looks like so.</p>
<table class="table">
<colgroup>
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>True Negative: 0</td>
<td>False Positive: 95</td>
</tr>
<tr class="even">
<td>1</td>
<td>False Negative: 0</td>
<td>True Positive: 5</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[\text{accuracy} = \frac{5}{100} = 0.05\]</span> <span class="math display">\[\text{precision} = \frac{5}{5 + 95} = 0.05\]</span> <span class="math display">\[\text{recall} = \frac{5}{5 + 0} = 1\]</span></p>
<ul>
<li>Our precision is low because we have many false positives</li>
<li>Our recall is perfect - we correctly classified all spam emails (we never predicted class <span class="math inline">\(0\)</span>)</li>
</ul>
</section>
</section>
<section id="precision-vs-recall" class="level4" data-number="24.3.2.2">
<h4 data-number="24.3.2.2" class="anchored" data-anchor-id="precision-vs-recall"><span class="header-section-number">24.3.2.2</span> Precision vs Recall</h4>
<p>Precision (<span class="math inline">\(\frac{\text{TP}}{\text{TP} + \textbf{ FP}}\)</span>) penalizes false positives, while recall (<span class="math inline">\(\frac{\text{TP}}{\text{TP} + \textbf{ FN}}\)</span>) penalizes false negatives.</p>
<p>In fact, precision and recall are <em>inversely related</em>. This is evident in our second model – we observed a high recall and low precision. Usually, there is a tradeoff in these two (most models can either minimize the number of FP or FN; and in rare cases, both).</p>
<p>The specific performance metric(s) to prioritize depends on the context. In many medical settings, there might be a much higher cost to missing positive cases. For instance, in our breast cancer example, it is more costly to misclassify malignant tumors (false negatives) than it is to incorrectly classify a benign tumor as malignant (false positives). In the case of the latter, pathologists can conduct further study to verify malignant tumors. As such, we should minimize the number of false negatives. This is equivalent to maximizing recall.</p>
</section>
</section>
</section>
<section id="adjusting-the-classification-threshold" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="adjusting-the-classification-threshold"><span class="header-section-number">24.4</span> Adjusting the Classification Threshold</h2>
<p>One way to minimize the number of FP vs.&nbsp;FN (equivalently, maximizing precision vs.&nbsp;recall) is by adjusting the classification threshold <span class="math inline">\(T\)</span>.</p>
<p><span class="math display">\[\hat y = \begin{cases}
        1, &amp; P(Y=1|x) \ge T\\
        0, &amp; \text{otherwise }
    \end{cases}\]</span></p>
<p>The default threshold in <code>sklearn</code> is <span class="math inline">\(T = 0.5\)</span>. As we increase the threshold <span class="math inline">\(T\)</span>, we “raise the standard” of how confident our classifier needs to be to predict 1 (i.e., “positive”).</p>
<p><img src="images/varying_threshold.png" alt="varying_threshold" width="800"></p>
<p>As you may notice, the choice of threshold <span class="math inline">\(T\)</span> impacts our classifier’s performance.</p>
<ul>
<li>High <span class="math inline">\(T\)</span>: Most predictions are <span class="math inline">\(0\)</span>.
<ul>
<li>Lots of false negatives</li>
<li>Fewer false positives</li>
</ul></li>
<li>Low <span class="math inline">\(T\)</span>: Most predictions are <span class="math inline">\(1\)</span>.
<ul>
<li>Lots of false positives</li>
<li>Fewer false negatives</li>
</ul></li>
</ul>
<p>In fact, we can choose a threshold <span class="math inline">\(T\)</span> based on our desired number, or proportion, of false positives and false negatives. We can do so using a few different tools. We’ll touch on two of the most important ones in Data 100.</p>
<ol type="1">
<li>Precision-Recall Curve (PR Curve). [Covered in Extra Content]</li>
<li>“Receiver Operating Characteristic” Curve (ROC Curve)</li>
</ol>
<p>To motivate the ROC Curve, let’s first consider two more metrics - true positive rate (TPR) and false positive rate (FPR).</p>
<section id="two-more-metrics" class="level3" data-number="24.4.1">
<h3 data-number="24.4.1" class="anchored" data-anchor-id="two-more-metrics"><span class="header-section-number">24.4.1</span> Two More Metrics</h3>
<p>The <strong>True Positive Rate (TPR)</strong> is defined as</p>
<p><span class="math display">\[\frac{\text{TP}}{\text{TP + FN}}\]</span></p>
<p>You’ll notice this is equivalent to <em>recall</em>. In the context of our spam email classifier, it answers the question: “what proportion of spam did I mark correctly?”.</p>
<ul>
<li>We’d like this to be close to <span class="math inline">\(1\)</span></li>
</ul>
<p>The <strong>False Positive Rate (FPR)</strong> is defined as</p>
<p><span class="math display">\[\frac{\text{FP}}{\text{FP + TN}}\]</span></p>
<p>Another word for FPR is <em>specificity</em>. This answers the question: “what proportion of regular email did I mark as spam?”</p>
<ul>
<li>We’d like this to be close to <span class="math inline">\(0\)</span></li>
</ul>
<p>As we increase threshold <span class="math inline">\(T\)</span>, both TPR and FPR decrease. We’ve plotted this relationship below for some model on a toy dataset.</p>
<p><img src="images/tpr_fpr.png" alt="tpr_fpr" width="800"></p>
</section>
<section id="the-roc-curve" class="level3" data-number="24.4.2">
<h3 data-number="24.4.2" class="anchored" data-anchor-id="the-roc-curve"><span class="header-section-number">24.4.2</span> The ROC Curve</h3>
<p>The “Receiver Operating Characteristic” Curve (<strong>ROC Curve</strong>) plots the tradeoff between FPR and TPR. Notice how the far-left of the curve corresponds to higher threshold <span class="math inline">\(T\)</span> values.</p>
<p><img src="images/roc_curve.png" alt="roc_curve" width="700"></p>
<p>The “perfect” classifier is the one that has a TPR of 1, and FPR of 0. This is achieved at the top-left of the plot below. More generally, it’s ROC curve resembles the curve in orange.</p>
<p><img src="images/roc_curve_perfect.png" alt="roc_curve_perfect" width="700"></p>
<p>We want our model to be as close to this orange curve as possible. How do we quantify “closeness”?</p>
<p>We can compute the <strong>area under curve (AUC)</strong> of the ROC curve. Notice how the perfect classifier has an AUC = 1. The closer our model’s AUC is to 1, the better it is. On the other hand, a terrible model will have an AUC closer to 0.5. This indicates the classifier is not able to distinguish between positive and negative classes, and thus, randomly predicts one of the two.</p>
<p><img src="images/roc_curve_worst_predictor.png" alt="roc_curve_worst_predictor" width="900"></p>
</section>
</section>
<section id="extra-content" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="extra-content"><span class="header-section-number">24.5</span> Extra Content</h2>
<section id="precision-recall-curves" class="level3" data-number="24.5.1">
<h3 data-number="24.5.1" class="anchored" data-anchor-id="precision-recall-curves"><span class="header-section-number">24.5.1</span> Precision-Recall Curves</h3>
<p>A <strong>Precision-Recall Curve (PR Curve)</strong> is an alternative to the ROC curve that displays the relationship between precision and recall for various threshold values. It is constructed in a similar way as with the ROC curve.</p>
<p>Let’s first consider how precision and recall change as a function of the threshold <span class="math inline">\(T\)</span>. We know this quite well from earlier – precision will generally increase, and recall will decrease.</p>
<p><img src="images/precision-recall-thresh.png" alt="precision-recall-thresh" width="750"></p>
<p>Displayed below is the PR-Curve for the same toy dataset. Notice how threshold values increase as we move to the left.</p>
<p><img src="images/pr_curve_thresholds.png" alt="pr_curve_thresholds" width="685"></p>
<p>Once again, the perfect classifier will resemble the orange curve, this time, facing the opposite direction.</p>
<p><img src="images/pr_curve_perfect.png" alt="pr_curve_perfect" width="675"></p>
<p>We want our PR-Curve to be as close to the “top right” of this graph as possible. Again, we use the AUC to determine “closeness”, with the perfect classifier exhibiting an AUC = 1 (and the worst with an AUC = 0.5).</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../logistic_regression_1/logistic_reg_1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../pca_1/pca_1.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA I</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>