<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Principles and Techniques of Data Science - 17&nbsp; Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../probability_2/probability_2.html" rel="next">
<link href="../cv_regularization/cv_reg.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../probability_1/probability_1.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_to_modeling/intro_to_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Introduction to Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causal Inference and Confounding</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../decision_tree/decision_tree.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Decision Trees</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Random Variables</h2>
   
  <ul>
  <li><a href="#random-variables-and-distributions" id="toc-random-variables-and-distributions" class="nav-link active" data-scroll-target="#random-variables-and-distributions"><span class="header-section-number">17.1</span> Random Variables and Distributions</a>
  <ul>
  <li><a href="#example-tossing-a-coin" id="toc-example-tossing-a-coin" class="nav-link" data-scroll-target="#example-tossing-a-coin"><span class="header-section-number">17.1.1</span> Example: Tossing a Coin</a></li>
  <li><a href="#example-sampling-data-100-students" id="toc-example-sampling-data-100-students" class="nav-link" data-scroll-target="#example-sampling-data-100-students"><span class="header-section-number">17.1.2</span> Example: Sampling Data 100 Students</a></li>
  <li><a href="#distributions" id="toc-distributions" class="nav-link" data-scroll-target="#distributions"><span class="header-section-number">17.1.3</span> Distributions</a></li>
  </ul></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance"><span class="header-section-number">17.2</span> Expectation and Variance</a>
  <ul>
  <li><a href="#expectation" id="toc-expectation" class="nav-link" data-scroll-target="#expectation"><span class="header-section-number">17.2.1</span> Expectation</a>
  <ul>
  <li><a href="#example-1-coin-toss" id="toc-example-1-coin-toss" class="nav-link" data-scroll-target="#example-1-coin-toss"><span class="header-section-number">17.2.1.1</span> Example 1: Coin Toss</a></li>
  <li><a href="#example-2" id="toc-example-2" class="nav-link" data-scroll-target="#example-2"><span class="header-section-number">17.2.1.2</span> Example 2</a></li>
  </ul></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">17.2.2</span> Variance</a></li>
  <li><a href="#example-die" id="toc-example-die" class="nav-link" data-scroll-target="#example-die"><span class="header-section-number">17.2.3</span> Example: Die</a></li>
  </ul></li>
  <li><a href="#sums-of-random-variables" id="toc-sums-of-random-variables" class="nav-link" data-scroll-target="#sums-of-random-variables"><span class="header-section-number">17.3</span> Sums of Random Variables</a>
  <ul>
  <li><a href="#properties-of-expectation" id="toc-properties-of-expectation" class="nav-link" data-scroll-target="#properties-of-expectation"><span class="header-section-number">17.3.1</span> Properties of Expectation</a></li>
  <li><a href="#properties-of-variance" id="toc-properties-of-variance" class="nav-link" data-scroll-target="#properties-of-variance"><span class="header-section-number">17.3.2</span> Properties of Variance</a></li>
  <li><a href="#covariance-and-correlation" id="toc-covariance-and-correlation" class="nav-link" data-scroll-target="#covariance-and-correlation"><span class="header-section-number">17.3.3</span> Covariance and Correlation</a></li>
  <li><a href="#equal-vs.-identically-distributed-vs.-i.i.d" id="toc-equal-vs.-identically-distributed-vs.-i.i.d" class="nav-link" data-scroll-target="#equal-vs.-identically-distributed-vs.-i.i.d"><span class="header-section-number">17.3.4</span> Equal vs.&nbsp;Identically Distributed vs.&nbsp;i.i.d</a></li>
  <li><a href="#example-bernoulli-random-variable" id="toc-example-bernoulli-random-variable" class="nav-link" data-scroll-target="#example-bernoulli-random-variable"><span class="header-section-number">17.3.5</span> Example: Bernoulli Random Variable</a></li>
  <li><a href="#example-binomial-random-variable" id="toc-example-binomial-random-variable" class="nav-link" data-scroll-target="#example-binomial-random-variable"><span class="header-section-number">17.3.6</span> Example: Binomial Random Variable</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">17.3.7</span> Summary</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Define a random variable in terms of its distribution</li>
<li>Compute the expectation and variance of a random variable</li>
<li>Gain familiarity with the Bernoulli and binomial random variables</li>
</ul>
</div>
</div>
</div>
<p>In the past few lectures, we’ve examined the role of complexity in influencing model performance. We’ve considered model complexity in the context of a tradeoff between two competing factors: model variance and training error.</p>
<p>So far, our analysis has been mostly qualitative. We’ve acknowledged that our choice of model complexity needs to strike a balance between model variance and training error, but we haven’t yet discussed <em>why</em> exactly this tradeoff exists.</p>
<p>To better understand the origin of this tradeoff, we will need to dive into <strong>random variables</strong>. The next two course notes on probability will be a brief digression from our work on modeling so we can build up the concepts needed to understand this so-called <strong>bias-variance tradeoff</strong>. In specific, we will cover:</p>
<ol type="1">
<li>Random Variables: introduce random variables, considering the concepts of expectation, variance, and covariance</li>
<li>Estimators, Bias, and Variance: re-express the ideas of model variance and training error in terms of random variables and use this new perspective to investigate our choice of model complexity</li>
</ol>
<p>We’ll go over just enough probability to help you understand its implications for modeling, but if you want to go a step further, take Data 140, CS 70, and/or EECS 126.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data 8 Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Recall the following concepts from Data 8:</p>
<ol type="1">
<li><p>Sample mean: The mean of the random sample</p></li>
<li><p>Central Limit Theorem: If you draw a large random sample with replacement, then, regardless of the population distribution, the probability distribution of the sample mean</p>
<ol type="a">
<li><p>is roughly normal</p></li>
<li><p>is centered at the population mean</p></li>
<li><p>has an <span class="math inline">\(SD = \frac{\text{population SD}}{\sqrt{\text{sample size}}}\)</span></p></li>
</ol></li>
</ol>
</div>
</div>
</div>
<p>In Data 100, we want to understand the broader relationship between the following:</p>
<ul>
<li><strong>Population parameter</strong>: a number that describes something about the population</li>
<li><strong>Sample statistic</strong>: an estimate of the number computed on a sample</li>
</ul>
<section id="random-variables-and-distributions" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="random-variables-and-distributions"><span class="header-section-number">17.1</span> Random Variables and Distributions</h2>
<p>Suppose we generate a set of random data, like a random sample from some population. A <strong>random variable</strong> is a <em>function</em> from the outcome of a random event to a number.</p>
<p>It is <em>random</em> since our sample was drawn at random; it is <em>variable</em> because its exact value depends on how this random sample came out. As such, the domain or input of our random variable is all possible outcomes for some random event in a <em>sample space</em>, and its range or output is the real number line. We typically denote random variables with uppercase letters, such as <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>. In contrast, note that regular variables tend to be denoted using lowercase letters. Sometimes we also use uppercase letters to refer to matrices (such as your design matrix <span class="math inline">\(\mathbb{X}\)</span>), but we will do our best to be clear with the notation.</p>
<p>To motivate what this (rather abstract) definition means, let’s consider the following examples:</p>
<section id="example-tossing-a-coin" class="level3" data-number="17.1.1">
<h3 data-number="17.1.1" class="anchored" data-anchor-id="example-tossing-a-coin"><span class="header-section-number">17.1.1</span> Example: Tossing a Coin</h3>
<p>Let’s formally define a fair coin toss. A fair coin can land on heads (<span class="math inline">\(H\)</span>) or tails (<span class="math inline">\(T\)</span>), each with a probability of 0.5. With these possible outcomes, we can define a random variable <span class="math inline">\(X\)</span> as: <span class="math display">\[X = \begin{cases}
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails}
   \end{cases}\]</span></p>
<p><span class="math inline">\(X\)</span> is a function with a domain, or input, of <span class="math inline">\(\{H, T\}\)</span> and a range, or output, of <span class="math inline">\(\{1, 0\}\)</span>. In practice, while we don’t use the following function notation, you could write the above as <span class="math display">\[X = \begin{cases}  X(H) = 1 \\ X(T) = 0 \end{cases}\]</span></p>
</section>
<section id="example-sampling-data-100-students" class="level3" data-number="17.1.2">
<h3 data-number="17.1.2" class="anchored" data-anchor-id="example-sampling-data-100-students"><span class="header-section-number">17.1.2</span> Example: Sampling Data 100 Students</h3>
<p>Suppose we draw a random sample <span class="math inline">\(s\)</span> of size 3 from all students enrolled in Data 100.</p>
<p>We can define <span class="math inline">\(Y\)</span> as the number of data science students in our sample. Its domain is all possible samples of size 3, and its range is <span class="math inline">\(\{0, 1, 2, 3\}\)</span>.</p>
<p align="center">
<img src="images/rv.png" alt="rv" width="600" class="center">
</p>
<p>Note that we can use random variables in mathematical expressions to create new random variables.</p>
<p>For example, let’s say we sample 3 students at random from lecture and look at their midterm scores. Let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span> represent each student’s midterm grade.</p>
<p>We can use these random variables to create a new random variable, <span class="math inline">\(Y\)</span>, which represents the average of the 3 scores: <span class="math inline">\(Y = (X_1 + X_2 + X_3)/3\)</span>.</p>
<p>As we’re creating this random variable, a few questions arise:</p>
<ul>
<li>What can we say about the distribution of <span class="math inline">\(Y\)</span>?</li>
<li>How does it depend on the distribution of <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, and <span class="math inline">\(X_3\)</span>?</li>
</ul>
<p>But, what exactly is a distribution? Let’s dive into this!</p>
</section>
<section id="distributions" class="level3" data-number="17.1.3">
<h3 data-number="17.1.3" class="anchored" data-anchor-id="distributions"><span class="header-section-number">17.1.3</span> Distributions</h3>
<p>To define any random variable <span class="math inline">\(X\)</span>, we need to be able to specify 2 things:</p>
<ol type="1">
<li><strong>Possible values</strong>: the set of values the random variable can take on.</li>
<li><strong>Probabilities</strong>: the set of probabilities describing how the total probability of 100% is split over the possible values.</li>
</ol>
<p>If <span class="math inline">\(X\)</span> is discrete (has a finite number of possible values), the probability that a random variable <span class="math inline">\(X\)</span> takes on the value <span class="math inline">\(x\)</span> is given by <span class="math inline">\(P(X=x)\)</span>, and probabilities must sum to 1: <span class="math inline">\(\sum_{\text{all } x} P(X=x) = 1\)</span>,</p>
<p>We can often display this using a <strong>probability distribution table</strong>. In the coin toss example, the probability distribution table of <span class="math inline">\(X\)</span> is given by.</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(P(X=x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
</tr>
</tbody>
</table>
<p>The <strong>distribution</strong> of a random variable <span class="math inline">\(X\)</span> describes how the total probability of 100% is split across all the possible values of <span class="math inline">\(X\)</span>, and it fully defines a random variable. If you know the distribution of a random variable you can:</p>
<ul>
<li>compute properties of the random variables and derived variables</li>
<li>simulate the random variables by randomly picking values of <span class="math inline">\(X\)</span> according to its distribution using <code>np.random.choice</code>, <code>df.sample</code>, or <code>scipy.stats.&lt;dist&gt;.rvs(...)</code></li>
</ul>
<p>The distribution of a discrete random variable can also be represented using a histogram. If a variable is <strong>continuous</strong>, meaning it can take on infinitely many values, we can illustrate its distribution using a density curve.</p>
<p align="center">
<img src="images/discrete_continuous.png" alt="discrete_continuous" width="700">
</p>
<p>We often don’t know the (true) distribution and instead compute an empirical distribution. If you flip a coin 3 times and get {H, H, T}, you may ask —— what is the probability that the coin will land heads? We can come up with an <strong>empirical estimate</strong> of <span class="math inline">\(\frac{2}{3}\)</span>, though the true probability might be <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p>Probabilities are areas. For discrete random variables, the <em>area of the red bars</em> represents the probability that a discrete random variable <span class="math inline">\(X\)</span> falls within those values. For continuous random variables, the <em>area under the curve</em> represents the probability that a discrete random variable <span class="math inline">\(Y\)</span> falls within those values.</p>
<p align="center">
<img src="images/probability_areas.png" alt="discrete_continuous" width="600">
</p>
<p>If we sum up the total area of the bars/under the density curve, we should get 100%, or 1.</p>
<p>We can show the distribution of <span class="math inline">\(Y\)</span> in the following tables. The table on the left lists all possible samples of <span class="math inline">\(s\)</span> and the number of times they can appear (<span class="math inline">\(Y(s)\)</span>). We can use this to calculate the values for the table on the right, a <strong>probability distribution table</strong>.</p>
<p align="center">
<img src="images/distribution.png" alt="distribution" width="600">
</p>
<p>Rather than fully write out a probability distribution or show a histogram, there are some common distributions that come up frequently when doing data science. These distributions are specified by some <strong>parameters</strong>, which are constants that specify the shape of the distribution. In terms of notation, the ‘~’ means “has the probability distribution of”.</p>
<p>These common distributions are listed below:</p>
<ol type="1">
<li>Bernoulli(<span class="math inline">\(p\)</span>): If <span class="math inline">\(X\)</span> ~ Bernoulli(<span class="math inline">\(p\)</span>), then <span class="math inline">\(X\)</span> takes on a value 1 with probability <span class="math inline">\(p\)</span>, and 0 with probability <span class="math inline">\(1 - p\)</span>. Bernoulli random variables are also termed the “indicator” random variables.</li>
<li>Binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>): If <span class="math inline">\(X\)</span> ~ Binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>), then <span class="math inline">\(X\)</span> counts the number of 1s in <span class="math inline">\(n\)</span> independent Bernoulli(<span class="math inline">\(p\)</span>) trials.</li>
<li>Categorical(<span class="math inline">\(p_1, ..., p_k\)</span>) of values: The probability of each value is 1 / (number of possible values).</li>
<li>Uniform on the unit interval (0, 1): The density is flat at 1 on (0, 1) and 0 elsewhere. We won’t get into what density means as much here, but intuitively, this is saying that there’s an equally likely chance of getting any value on the interval (0, 1).</li>
<li>Normal(<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>): The probability density is specified by <span class="math inline">\(\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\)</span>. This bell-shaped distribution comes up fairly often in data, in part due to the Central Limit Theorem you saw back in Data 8.</li>
</ol>
</section>
</section>
<section id="expectation-and-variance" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="expectation-and-variance"><span class="header-section-number">17.2</span> Expectation and Variance</h2>
<p>There are several ways to describe a random variable. The methods shown above —— a table of all samples <span class="math inline">\(s, X(s)\)</span>, distribution table <span class="math inline">\(P(X=x)\)</span>, and histograms —— are all definitions that <em>fully describe</em> a random variable. Often, it is easier to describe a random variable using some <em>numerical summary</em> rather than fully defining its distribution. These numerical summaries are numbers that characterize some properties of the random variable. Because they give a “summary” of how the variable tends to behave, they are <em>not</em> random. Instead, think of them as a static number that describes a certain property of the random variable. In Data 100, we will focus our attention on the expectation and variance of a random variable.</p>
<section id="expectation" class="level3" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="expectation"><span class="header-section-number">17.2.1</span> Expectation</h3>
<p>The <strong>expectation</strong> of a random variable <span class="math inline">\(X\)</span> is the <strong>weighted average</strong> of the values of <span class="math inline">\(X\)</span>, where the weights are the probabilities of each value occurring. There are two equivalent ways to compute the expectation:</p>
<ol type="1">
<li>Apply the weights one <em>sample</em> at a time: <span class="math display">\[\mathbb{E}[X] = \sum_{\text{all possible } s} X(s) P(s)\]</span>.</li>
<li>Apply the weights one possible <em>value</em> at a time: <span class="math display">\[\mathbb{E}[X] = \sum_{\text{all possible } x} x P(X=x)\]</span></li>
</ol>
<p>The latter is more commonly used as we are usually just given the distribution, not all possible samples.</p>
<p>We want to emphasize that the expectation is a <em>number</em>, not a random variable. Expectation is a generalization of the average, and it has the same units as the random variable. It is also the center of gravity of the probability distribution histogram, meaning if we simulate the variable many times, it is the long-run average of the simulated values.</p>
<section id="example-1-coin-toss" class="level4" data-number="17.2.1.1">
<h4 data-number="17.2.1.1" class="anchored" data-anchor-id="example-1-coin-toss"><span class="header-section-number">17.2.1.1</span> Example 1: Coin Toss</h4>
<p>Going back to our coin toss example, we define a random variable <span class="math inline">\(X\)</span> as: <span class="math display">\[X = \begin{cases}
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails}
   \end{cases}\]</span></p>
<p>We can calculate its expectation <span class="math inline">\(\mathbb{E}[X]\)</span> using the second method of applying the weights one possible value at a time: <span class="math display">\[\begin{align}
\mathbb{E}[X] &amp;= \sum_{x} x P(X=x) \\
&amp;= 1 * 0.5 + 0 * 0.5 \\
&amp;= 0.5
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\mathbb{E}[X] = 0.5\)</span> is not a possible value of <span class="math inline">\(X\)</span>; it’s an average. <strong>The expectation of X does not need to be a possible value of X</strong>.</p>
</section>
<section id="example-2" class="level4" data-number="17.2.1.2">
<h4 data-number="17.2.1.2" class="anchored" data-anchor-id="example-2"><span class="header-section-number">17.2.1.2</span> Example 2</h4>
<p>Consider the random variable <span class="math inline">\(X\)</span>:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(P(X=x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>To calculate it’s expectation, <span class="math display">\[\begin{align}
\mathbb{E}[X] &amp;= \sum_{x} x P(X=x) \\
&amp;= 3 * 0.1 + 4 * 0.2 + 6 * 0.4 + 8 * 0.3 \\
&amp;= 0.3 + 0.8 + 2.4 + 2.4 \\
&amp;= 5.9
\end{align}\]</span></p>
<p>Again, note that <span class="math inline">\(\mathbb{E}[X] = 5.9\)</span> is not a possible value of <span class="math inline">\(X\)</span>; it’s an average. <strong>The expectation of X does not need to be a possible value of X</strong>.</p>
</section>
</section>
<section id="variance" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="variance"><span class="header-section-number">17.2.2</span> Variance</h3>
<p>The <strong>variance</strong> of a random variable is a measure of its chance error. It is defined as the expected squared deviation from the expectation of <span class="math inline">\(X\)</span>. Put more simply, variance asks: how far does <span class="math inline">\(X\)</span> typically vary from its average value, just by chance? What is the spread of <span class="math inline">\(X\)</span>’s distribution?</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</span></p>
<p>The units of variance are the square of the units of <span class="math inline">\(X\)</span>. To get it back to the right scale, use the standard deviation of <span class="math inline">\(X\)</span>: <span class="math display">\[\text{SD}(X) = \sqrt{\text{Var}(X)}\]</span></p>
<p>Like with expectation, <strong>variance and standard deviation are numbers, not random variables</strong>! Variance helps us describe the variability of a random variable. It is the expected squared error between the random variable and its expected value. As you will see shortly, we can use variance to help us quantify the chance error that arises when using a sample <span class="math inline">\(X\)</span> to estimate the population mean.</p>
<p>By <a href="https://www.inferentialthinking.com/chapters/14/2/Variability.html#Chebychev's-Bounds">Chebyshev’s inequality</a>, which you saw in Data 8, no matter what the shape of the distribution of <span class="math inline">\(X\)</span> is, the vast majority of the probability lies in the interval “expectation plus or minus a few SDs.”</p>
<p>If we expand the square and use properties of expectation, we can re-express variance as the <strong>computational formula for variance</strong>.</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]</span></p>
<p>This form is often more convenient to use when computing the variance of a variable by hand, and it is also useful in Mean Squared Error calculations, as <span class="math inline">\(\mathbb{E}[X^2] = \text{Var}(X)\)</span> if <span class="math inline">\(X\)</span> is centered and <span class="math inline">\(E(X)=0\)</span>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
   \text{Var}(X) &amp;= \mathbb{E}[(X-\mathbb{E}[X])^2] \\
   &amp;= \mathbb{E}(X^2 - 2X\mathbb{E}(X) + (\mathbb{E}(X))^2) \\
   &amp;= \mathbb{E}(X^2) - 2 \mathbb{E}(X)\mathbb{E}(X) +( \mathbb{E}(X))^2\\
   &amp;= \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{align}\]</span></p>
</div>
</div>
</div>
<p>How do we compute <span class="math inline">\(\mathbb{E}[X^2]\)</span>? Any function of a random variable is <em>also</em> a random variable. That means that by squaring <span class="math inline">\(X\)</span>, we’ve created a new random variable. To compute <span class="math inline">\(\mathbb{E}[X^2]\)</span>, we can simply apply our definition of expectation to the random variable <span class="math inline">\(X^2\)</span>.</p>
<p><span class="math display">\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x)\]</span></p>
</section>
<section id="example-die" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="example-die"><span class="header-section-number">17.2.3</span> Example: Die</h3>
<p>Let <span class="math inline">\(X\)</span> be the outcome of a single fair die roll. <span class="math inline">\(X\)</span> is a random variable defined as <span class="math display">\[X = \begin{cases}
      \frac{1}{6}, \text{if } x \in \{1,2,3,4,5,6\} \\
      0, \text{otherwise}
   \end{cases}\]</span></p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What’s the expectation, <span class="math inline">\(\mathbb{E}[X]?\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[ \begin{align}
         \mathbb{E}[X] &amp;= 1\big(\frac{1}{6}\big) + 2\big(\frac{1}{6}\big) + 3\big(\frac{1}{6}\big) + 4\big(\frac{1}{6}\big) + 5\big(\frac{1}{6}\big) + 6\big(\frac{1}{6}\big) \\
         &amp;= \big(\frac{1}{6}\big)( 1 + 2 + 3 + 4 + 5 + 6) \\
         &amp;= \frac{7}{2}
      \end{align}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What’s the variance, <span class="math inline">\(\text{Var}(X)?\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using Approach 1 (definition): <span class="math display">\[\begin{align}
      \text{Var}(X) &amp;= \big(\frac{1}{6}\big)((1 - \frac{7}{2})^2 + (2 - \frac{7}{2})^2 + (3 - \frac{7}{2})^2 + (4 - \frac{7}{2})^2 + (5 - \frac{7}{2})^2 + (6 - \frac{7}{2})^2) \\
      &amp;= \frac{35}{12}
   \end{align}\]</span></p>
<p>Using Approach 2 (property): <span class="math display">\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x) = \frac{91}{6}\]</span> <span class="math display">\[\text{Var}(X) = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12}\]</span></p>
</div>
</div>
</div>
<p>We can summarize our discussion so far in the following diagram:</p>
<p align="center">
<img src="images/exp_var.png" alt="distribution" width="800">
</p>
</section>
</section>
<section id="sums-of-random-variables" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="sums-of-random-variables"><span class="header-section-number">17.3</span> Sums of Random Variables</h2>
<p>Often, we will work with multiple random variables at the same time. A function of a random variable is also a random variable. If you create multiple random variables based on your sample, then functions of those random variables are also random variables.</p>
<p>For example, if <span class="math inline">\(X_1, X_2, ..., X_n\)</span> are random variables, then so are all of these:</p>
<ul>
<li><span class="math inline">\(X_n^2\)</span></li>
<li><span class="math inline">\(\#\{i : X_i &gt; 10\}\)</span></li>
<li><span class="math inline">\(\text{max}(X_1, X_2, ..., X_n)\)</span></li>
<li><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (X_i - c)^2\)</span></li>
<li><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n X_i\)</span></li>
</ul>
<p>Many functions of random variables that we are interested in (e.g., counts, means) involve sums of random variables, so let’s dive deeper into the properties of sums of random variables.</p>
<section id="properties-of-expectation" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="properties-of-expectation"><span class="header-section-number">17.3.1</span> Properties of Expectation</h3>
<p>Instead of simulating full distributions, we often just compute expectation and variance directly. Recall the definition of expectation: <span class="math display">\[\mathbb{E}[X] = \sum_{x} x P(X=x)\]</span></p>
<p>From it, we can derive some useful properties:</p>
<ol type="1">
<li><strong>Linearity of expectation</strong>. The expectation of the linear transformation <span class="math inline">\(aX+b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, is:</li>
</ol>
<p><span class="math display">\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
        \mathbb{E}[aX+b] &amp;= \sum_{x} (ax + b) P(X=x) \\
        &amp;= \sum_{x} (ax P(X=x) + bP(X=x)) \\
        &amp;= a\sum_{x}P(X=x) + b\sum_{x}P(X=x)\\
        &amp;= a\mathbb{E}(X) + b * 1
    \end{align}\]</span></p>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Expectation is also linear in <em>sums</em> of random variables.</li>
</ol>
<p><span class="math display">\[\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
    \mathbb{E}[X+Y] &amp;= \sum_{s} (X+Y)(s) P(s) \\
    &amp;= \sum_{s} (X(s)P(s) + Y(s)P(s)) \\
    &amp;= \sum_{s} X(s)P(s) + \sum_{s} Y(s)P(s)\\
    &amp;= \mathbb{E}[X] + \mathbb{E}[Y]
\end{align}\]</span></p>
</div>
</div>
</div>
<ol start="3" type="1">
<li>If <span class="math inline">\(g\)</span> is a non-linear function, then in general, <span class="math display">\[\mathbb{E}[g(X)] \neq g(\mathbb{E}[X])\]</span> For example, if <span class="math inline">\(X\)</span> is -1 or 1 with equal probability, then <span class="math inline">\(\mathbb{E}[X] = 0\)</span>, but <span class="math inline">\(\mathbb{E}[X^2] = 1 \neq 0\)</span>.</li>
</ol>
</section>
<section id="properties-of-variance" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="properties-of-variance"><span class="header-section-number">17.3.2</span> Properties of Variance</h3>
<p>Let’s now get into the properties of variance. Recall the definition of variance: <span class="math display">\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</span></p>
<p>Combining it with the properties of expectation, we can derive some useful properties:</p>
<ol type="1">
<li>Unlike expectation, variance is <em>non-linear</em>. The variance of the linear transformation <span class="math inline">\(aX+b\)</span> is: <span class="math display">\[\text{Var}(aX+b) = a^2 \text{Var}(X)\]</span></li>
</ol>
<ul>
<li>Subsequently, <span class="math display">\[\text{SD}(aX+b) = |a| \text{SD}(X)\]</span></li>
<li>The full proof of this fact can be found using the definition of variance. As general intuition, consider that <span class="math inline">\(aX+b\)</span> scales the variable <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(a\)</span>, then shifts the distribution of <span class="math inline">\(X\)</span> by <span class="math inline">\(b\)</span> units.</li>
</ul>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We know that <span class="math display">\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]</span></p>
<p>In order to compute <span class="math inline">\(\text{Var}(aX+b)\)</span>, consider that a shift by <span class="math inline">\(b\)</span> units does not affect spread, so <span class="math inline">\(\text{Var}(aX+b) = \text{Var}(aX)\)</span>.</p>
<p>Then, <span class="math display">\[\begin{align}
    \text{Var}(aX+b) &amp;= \text{Var}(aX) \\
    &amp;= E((aX)^2) - (E(aX))^2 \\
    &amp;= E(a^2 X^2) - (aE(X))^2\\
    &amp;= a^2 (E(X^2) - (E(X))^2) \\
    &amp;= a^2 \text{Var}(X)
\end{align}\]</span></p>
</div>
</div>
</div>
<ul>
<li>Shifting the distribution by <span class="math inline">\(b\)</span> <em>does not</em> impact the <em>spread</em> of the distribution. Thus, <span class="math inline">\(\text{Var}(aX+b) = \text{Var}(aX)\)</span>.</li>
<li>Scaling the distribution by <span class="math inline">\(a\)</span> <em>does</em> impact the spread of the distribution.</li>
</ul>
<p align="center">
<img src="images/transformation.png" alt="transformation" width="600">
</p>
<ol start="2" type="1">
<li>Variance of sums of random variables is affected by the (in)dependence of the random variables. <span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{cov}(X,Y)\]</span> <span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}\]</span></li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The variance of a sum is affected by the dependence between the two random variables that are being added. Let’s expand the definition of <span class="math inline">\(\text{Var}(X + Y)\)</span> to see what’s going on.</p>
<p>To simplify the math, let <span class="math inline">\(\mu_x = \mathbb{E}[X]\)</span> and <span class="math inline">\(\mu_y = \mathbb{E}[Y]\)</span>.</p>
<p><span class="math display">\[ \begin{align}
\text{Var}(X + Y) &amp;= \mathbb{E}[(X+Y- \mathbb{E}(X+Y))^2] \\
&amp;= \mathbb{E}[((X - \mu_x) + (Y - \mu_y))^2] \\
&amp;= \mathbb{E}[(X - \mu_x)^2 + 2(X - \mu_x)(Y - \mu_y) + (Y - \mu_y)^2] \\
&amp;= \mathbb{E}[(X - \mu_x)^2] + \mathbb{E}[(Y - \mu_y)^2] + \mathbb{E}[(X - \mu_x)(Y - \mu_y)] \\
&amp;= \text{Var}(X) + \text{Var}(Y) + \mathbb{E}[(X - \mu_x)(Y - \mu_y)]
\end{align}\]</span></p>
</div>
</div>
</div>
</section>
<section id="covariance-and-correlation" class="level3" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="covariance-and-correlation"><span class="header-section-number">17.3.3</span> Covariance and Correlation</h3>
<p>We define the <strong>covariance</strong> of two random variables as the expected product of deviations from expectation. Put more simply, covariance is a generalization of variance to variance:</p>
<p><span class="math display">\[\text{Cov}(X, X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X)\]</span></p>
<p><span class="math display">\[\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\]</span></p>
<p>We can treat the covariance as a measure of association. Remember the definition of correlation given when we first established SLR?</p>
<p><span class="math display">\[r(X, Y) = \mathbb{E}\left[\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right] = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}\]</span></p>
<p>It turns out we’ve been quietly using covariance for some time now! If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{Cov}(X, Y) =0\)</span> and <span class="math inline">\(r(X, Y) = 0\)</span>. Note, however, that the converse is not always true: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> could have <span class="math inline">\(\text{Cov}(X, Y) = r(X, Y) = 0\)</span> but not be independent.</p>
</section>
<section id="equal-vs.-identically-distributed-vs.-i.i.d" class="level3" data-number="17.3.4">
<h3 data-number="17.3.4" class="anchored" data-anchor-id="equal-vs.-identically-distributed-vs.-i.i.d"><span class="header-section-number">17.3.4</span> Equal vs.&nbsp;Identically Distributed vs.&nbsp;i.i.d</h3>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<ul>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>equal</strong> if <span class="math inline">\(X(s) = Y(s)\)</span> for every sample <span class="math inline">\(s\)</span>. Regardless of the exact sample drawn, <span class="math inline">\(X\)</span> is always equal to <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>identically distributed</strong> if the distribution of <span class="math inline">\(X\)</span> is equal to the distribution of <span class="math inline">\(Y\)</span>. We say “<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are equal in distribution.” That is, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> take on the same set of possible values, and each of these possible values is taken with the same probability. On any specific sample <span class="math inline">\(s\)</span>, identically distributed variables do <em>not</em> necessarily share the same value. If <span class="math inline">\(X = Y\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are identically distributed; however, the converse is not true (ex: <span class="math inline">\(Y = 7 - X\)</span>, <span class="math inline">\(X\)</span> is a die)</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent and identically distributed (i.i.d)</strong> if
<ol type="1">
<li>The variables are identically distributed.</li>
<li>Knowing the outcome of one variable does not influence our belief of the outcome of the other.</li>
</ol></li>
</ul>
<p>Note that in Data 100, you’ll never be expected to prove that random variables are i.i.d.</p>
<p>Now let’s walk through an example. Say <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be numbers on rolls of two fair die. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are i.i.d, so <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the same distribution. However, the sums <span class="math inline">\(Y = X_1 + X_1 = 2X_1\)</span> and <span class="math inline">\(Z=X_1+X_2\)</span> have different distributions but the same expectation.</p>
<p align="center">
<img src="images/yz_distribution.png" alt="distribution" width="=500">
</p>
<p>However, <span class="math inline">\(Y = X_1\)</span> has a larger variance.</p>
<p align="center">
<img src="images/yz.png" alt="distribution" width="200">
</p>
</section>
<section id="example-bernoulli-random-variable" class="level3" data-number="17.3.5">
<h3 data-number="17.3.5" class="anchored" data-anchor-id="example-bernoulli-random-variable"><span class="header-section-number">17.3.5</span> Example: Bernoulli Random Variable</h3>
<p>To get some practice with the formulas discussed so far, let’s derive the expectation and variance for a Bernoulli(<span class="math inline">\(p\)</span>) random variable. If <span class="math inline">\(X\)</span> ~ Bernoulli(<span class="math inline">\(p\)</span>),</p>
<p><span class="math inline">\(\mathbb{E}[X] = 1 \cdot p + 0 \cdot (1 - p) = p\)</span></p>
<p>To compute the variance, we will use the computational formula. We first find that: <span class="math inline">\(\mathbb{E}[X^2] = 1^2 \cdot p + 0^2 \cdot (1 - p) = p\)</span></p>
<p>From there, let’s calculate our variance: <span class="math inline">\(\text{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2 = p - p^2 = p(1-p)\)</span></p>
</section>
<section id="example-binomial-random-variable" class="level3" data-number="17.3.6">
<h3 data-number="17.3.6" class="anchored" data-anchor-id="example-binomial-random-variable"><span class="header-section-number">17.3.6</span> Example: Binomial Random Variable</h3>
<p>Let <span class="math inline">\(Y\)</span> ~ Binomial(<span class="math inline">\(n\)</span>, <span class="math inline">\(p\)</span>). We can think of <span class="math inline">\(Y\)</span> as being the sum of <span class="math inline">\(n\)</span> i.i.d. Bernoulli(<span class="math inline">\(p\)</span>) random variables. Mathematically, this translates to</p>
<p><span class="math display">\[Y = \sum_{i=1}^n X_i\]</span></p>
<p>where <span class="math inline">\(X_i\)</span> is the indicator of a success on trial <span class="math inline">\(i\)</span>.</p>
<p>Using linearity of expectation,</p>
<p><span class="math display">\[\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\]</span></p>
<p>For the variance, since each <span class="math inline">\(X_i\)</span> is independent of the other, <span class="math inline">\(\text{Cov}(X_i, X_j) = 0\)</span>,</p>
<p><span class="math display">\[\text{Var}(Y) =  \sum_{i=1}^n \text{Var}[X_i] = np(1-p)\]</span></p>
</section>
<section id="summary" class="level3" data-number="17.3.7">
<h3 data-number="17.3.7" class="anchored" data-anchor-id="summary"><span class="header-section-number">17.3.7</span> Summary</h3>
<ul>
<li>Let <span class="math inline">\(X\)</span> be a random variable with distribution <span class="math inline">\(P(X=x)\)</span>.
<ul>
<li><span class="math inline">\(\mathbb{E}[X] = \sum_{x} x P(X=x)\)</span></li>
<li><span class="math inline">\(\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\)</span></li>
</ul></li>
<li>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be scalar values.
<ul>
<li><span class="math inline">\(\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\)</span></li>
<li><span class="math inline">\(\text{Var}(aX+b) = a^2 \text{Var}(X)\)</span></li>
</ul></li>
<li>Let <span class="math inline">\(Y\)</span> be another random variable.
<ul>
<li><span class="math inline">\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)</span></li>
<li><span class="math inline">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)\)</span></li>
</ul></li>
</ul>
<p>Note that <span class="math inline">\(\text{Cov}(X,Y)\)</span> would equal 0 if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../cv_regularization/cv_reg.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../probability_2/probability_2.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Random Variables</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Random Variables</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.16.1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Define a random variable in terms of its distribution</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Compute the expectation and variance of a random variable</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Gain familiarity with the Bernoulli and binomial random variables</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>In the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of a tradeoff between two competing factors: model variance and training error. </span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>So far, our analysis has been mostly qualitative. We've acknowledged that our choice of model complexity needs to strike a balance between model variance and training error, but we haven't yet discussed *why* exactly this tradeoff exists.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>To better understand the origin of this tradeoff, we will need to dive into **random variables**. The next two course notes on probability will be a brief digression from our work on modeling so we can build up the concepts needed to understand this so-called **bias-variance tradeoff**. In specific, we will cover:</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Random Variables: introduce random variables, considering the concepts of expectation, variance, and covariance</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Estimators, Bias, and Variance: re-express the ideas of model variance and training error in terms of random variables and use this new perspective to investigate our choice of model complexity</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>We'll go over just enough probability to help you understand its implications for modeling, but if you want to go a step further, take Data 140, CS 70, and/or EECS 126.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="false"}</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data 8 Recap</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Recall the following concepts from Data 8: </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample mean: The mean of the random sample</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Central Limit Theorem: If you draw a large random sample with replacement, then, regardless of the population distribution, the probability distribution of the sample mean</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    a. is roughly normal</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    b. is centered at the population mean</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    c. has an $SD = \frac{\text{population SD}}{\sqrt{\text{sample size}}}$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>In Data 100, we want to understand the broader relationship between the following:</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Population parameter**: a number that describes something about the population</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sample statistic**: an estimate of the number computed on a sample</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Variables and Distributions</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>Suppose we generate a set of random data, like a random sample from some population. A **random variable** is a *function* from the outcome of a random event to a number.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>It is *random* since our sample was drawn at random; it is *variable* because its exact value depends on how this random sample came out. As such, the domain or input of our random variable is all possible outcomes for some random event in a *sample space*, and its range or output is the real number line. We typically denote random variables with uppercase letters, such as $X$ or $Y$. In contrast, note that regular variables tend to be denoted using lowercase letters. Sometimes we also use uppercase letters to refer to matrices (such as your design matrix $\mathbb{X}$), but we will do our best to be clear with the notation.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>To motivate what this (rather abstract) definition means, let's consider the following examples:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Tossing a Coin</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>Let's formally define a fair coin toss. A fair coin can land on heads ($H$) or tails ($T$), each with a probability of 0.5. With these possible outcomes, we can define a random variable $X$ as: </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>$$X = \begin{cases} </span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>      1, \text{if the coin lands heads} <span class="sc">\\</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>      0, \text{if the coin lands tails} </span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>   \end{cases}$$</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>$X$ is a function with a domain, or input, of $<span class="sc">\{</span>H, T<span class="sc">\}</span>$ and a range, or output, of $<span class="sc">\{</span>1, 0<span class="sc">\}</span>$. In practice, while we don't use the following function notation, you could write the above as </span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>$$X = \begin{cases}  X(H) = 1 <span class="sc">\\</span> X(T) = 0 \end{cases}$$</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Sampling Data 100 Students</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Suppose we draw a random sample $s$ of size 3 from all students enrolled in Data 100. </span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>We can define $Y$ as the number of data science students in our sample. Its domain is all possible samples of size 3, and its range is $<span class="sc">\{</span>0, 1, 2, 3<span class="sc">\}</span>$.</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>  <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/rv.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'rv'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span> <span class="er">class</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>Note that we can use random variables in mathematical expressions to create new random variables. </span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>For example, let's say we sample 3 students at random from lecture and look at their midterm scores. Let $X_1$, $X_2$, and $X_3$ represent each student's midterm grade. </span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>We can use these random variables to create a new random variable, $Y$, which represents the average of the 3 scores: $Y = (X_1 + X_2 + X_3)/3$.</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>As we're creating this random variable, a few questions arise:</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What can we say about the distribution of $Y$?</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How does it depend on the distribution of $X_1$, $X_2$, and $X_3$?</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>But, what exactly is a distribution? Let's dive into this! </span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="fu">### Distributions</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>To define any random variable $X$, we need to be able to specify 2 things: </span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Possible values**: the set of values the random variable can take on.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Probabilities**: the set of probabilities describing how the total probability of 100% is split over the possible values.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>If $X$ is discrete (has a finite number of possible values), the probability that a random variable $X$ takes on the value $x$ is given by $P(X=x)$, and probabilities must sum to 1: $\sum_{\text{all } x} P(X=x) = 1$,</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>We can often display this using a **probability distribution table**. In the coin toss example, the probability distribution table of $X$ is given by.</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>| $x$ | $P(X=x)$ | </span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>| --- | -------- |</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>| 0 | $\frac{1}{2}$ | </span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>| 1 | $\frac{1}{2}$ |</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>The **distribution** of a random variable $X$ describes how the total probability of 100% is split across all the possible values of $X$, and it fully defines a random variable. If you know the distribution of a random variable you can:</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>compute properties of the random variables and derived variables</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>simulate the random variables by randomly picking values of $X$ according to its distribution using <span class="in">`np.random.choice`</span>, <span class="in">`df.sample`</span>, or <span class="in">`scipy.stats.&lt;dist&gt;.rvs(...)`</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>The distribution of a discrete random variable can also be represented using a histogram. If a variable is **continuous**, meaning it can take on infinitely many values, we can illustrate its distribution using a density curve. </span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/discrete_continuous.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'discrete_continuous'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'700'</span><span class="kw">&gt;</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>We often don’t know the (true) distribution and instead compute an empirical distribution. If you flip a coin 3 times and get {H, H, T}, you may ask —— what is the probability that the coin will land heads? We can come up with an **empirical estimate** of $\frac{2}{3}$, though the true probability might be $\frac{1}{2}$.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>Probabilities are areas. For discrete random variables, the *area of the red bars* represents the probability that a discrete random variable $X$ falls within those values. For continuous random variables, the *area under the curve* represents the probability that a discrete random variable $Y$ falls within those values.</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/probability_areas.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'discrete_continuous'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>If we sum up the total area of the bars/under the density curve, we should get 100%, or 1.</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>We can show the distribution of $Y$ in the following tables. The table on the left lists all possible samples of $s$ and the number of times they can appear ($Y(s)$). We can use this to calculate the values for the table on the right, a **probability distribution table**. </span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/distribution.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'distribution'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>Rather than fully write out a probability distribution or show a histogram, there are some common distributions that come up frequently when doing data science. These distributions are specified by some **parameters**, which are constants that specify the shape of the distribution. In terms of notation, the '~' means "has the probability distribution of". </span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>These common distributions are listed below:</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Bernoulli($p$): If $X$ ~ Bernoulli($p$), then $X$ takes on a value 1 with probability $p$, and 0 with probability $1 - p$. Bernoulli random variables are also termed the "indicator" random variables.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Binomial($n$, $p$): If $X$ ~ Binomial($n$, $p$), then $X$ counts the number of 1s in $n$ independent Bernoulli($p$) trials.</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Categorical($p_1, ..., p_k$) of values: The probability of each value is 1 / (number of possible values).</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Uniform on the unit interval (0, 1): The density is flat at 1 on (0, 1) and 0 elsewhere. We won't get into what density means as much here, but intuitively, this is saying that there's an equally likely chance of getting any value on the interval (0, 1).</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Normal($\mu$, $\sigma^2$): The probability density is specified by $\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}$. This bell-shaped distribution comes up fairly often in data, in part due to the Central Limit Theorem you saw back in Data 8.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a><span class="fu">## Expectation and Variance</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>There are several ways to describe a random variable. The methods shown above —— a table of all samples $s, X(s)$, distribution table $P(X=x)$, and histograms —— are all definitions that *fully describe* a random variable. Often, it is easier to describe a random variable using some *numerical summary* rather than fully defining its distribution. These numerical summaries are numbers that characterize some properties of the random variable. Because they give a "summary" of how the variable tends to behave, they are *not* random. Instead, think of them as a static number that describes a certain property of the random variable. In Data 100, we will focus our attention on the expectation and variance of a random variable.</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="fu">### Expectation</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>The **expectation** of a random variable $X$ is the **weighted average** of the values of $X$, where the weights are the probabilities of each value occurring. There are two equivalent ways to compute the expectation: </span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Apply the weights one *sample* at a time: $$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \sum_{\text{all possible } s} X(s) P(s)$$.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Apply the weights one possible *value* at a time: $$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \sum_{\text{all possible } x} x P(X=x)$$</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>The latter is more commonly used as we are usually just given the distribution, not all possible samples.</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>We want to emphasize that the expectation is a *number*, not a random variable. Expectation is a generalization of the average, and it has the same units as the random variable. It is also the center of gravity of the probability distribution histogram, meaning if we simulate the variable many times, it is the long-run average of the simulated values.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Example 1: Coin Toss</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>Going back to our coin toss example, we define a random variable $X$ as: </span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>$$X = \begin{cases} </span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>      1, \text{if the coin lands heads} <span class="sc">\\</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>      0, \text{if the coin lands tails} </span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>   \end{cases}$$</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>We can calculate its expectation $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$ using the second method of applying the weights one possible value at a time: </span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a> \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp;= \sum_{x} x P(X=x) <span class="sc">\\</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a> &amp;= 1 * 0.5 + 0 * 0.5 <span class="sc">\\</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a> &amp;= 0.5</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>Note that $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = 0.5$ is not a possible value of $X$; it's an average. **The expectation of X does not need to be a possible value of X**.</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Example 2</span></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>Consider the random variable $X$: </span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>| $x$ | $P(X=x)$ | </span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>| --- | -------- |</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>| 3 | 0.1 | </span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>| 4 | 0.2 |</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>| 6 | 0.4 | </span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>| 8 | 0.3 |</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>To calculate it's expectation, </span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a> \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp;= \sum_{x} x P(X=x) <span class="sc">\\</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a> &amp;= 3 * 0.1 + 4 * 0.2 + 6 * 0.4 + 8 * 0.3 <span class="sc">\\</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a> &amp;= 0.3 + 0.8 + 2.4 + 2.4 <span class="sc">\\</span></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a> &amp;= 5.9</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>Again, note that $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = 5.9$ is not a possible value of $X$; it's an average. **The expectation of X does not need to be a possible value of X**.</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variance</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>The **variance** of a random variable is a measure of its chance error. It is defined as the expected squared deviation from the expectation of $X$. Put more simply, variance asks: how far does $X$ typically vary from its average value, just by chance? What is the spread of $X$'s distribution?</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(X) = \mathbb{E}<span class="co">[</span><span class="ot">(X-\mathbb{E}[X])^2</span><span class="co">]</span>$$</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>The units of variance are the square of the units of $X$. To get it back to the right scale, use the standard deviation of $X$: $$\text{SD}(X) = \sqrt{\text{Var}(X)}$$</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>Like with expectation, **variance and standard deviation are numbers, not random variables**! Variance helps us describe the variability of a random variable. It is the expected squared error between the random variable and its expected value. As you will see shortly, we can use variance to help us quantify the chance error that arises when using a sample $X$ to estimate the population mean.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>By <span class="co">[</span><span class="ot">Chebyshev’s inequality</span><span class="co">](https://www.inferentialthinking.com/chapters/14/2/Variability.html#Chebychev's-Bounds)</span>, which you saw in Data 8, no matter what the shape of the distribution of $X$ is, the vast majority of the probability lies in the interval “expectation plus or minus a few SDs.”</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>If we expand the square and use properties of expectation, we can re-express variance as the **computational formula for variance**.</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(X) = \mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> - (\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>)^2$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>This form is often more convenient to use when computing the variance of a variable by hand, and it is also useful in Mean Squared Error calculations, as $\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> = \text{Var}(X)$ if $X$ is centered and $E(X)=0$.</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>   \text{Var}(X) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X-\mathbb{E}[X])^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>   &amp;= \mathbb{E}(X^2 - 2X\mathbb{E}(X) + (\mathbb{E}(X))^2) <span class="sc">\\</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>   &amp;= \mathbb{E}(X^2) - 2 \mathbb{E}(X)\mathbb{E}(X) +( \mathbb{E}(X))^2<span class="sc">\\</span></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>   &amp;= \mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> - (\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>)^2</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>How do we compute $\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span>$? Any function of a random variable is *also* a random variable. That means that by squaring $X$, we've created a new random variable. To compute $\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span>$, we can simply apply our definition of expectation to the random variable $X^2$.</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> = \sum_{x} x^2 P(X = x)$$ </span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Die</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>Let $X$ be the outcome of a single fair die roll. $X$ is a random variable defined as </span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>$$X = \begin{cases} </span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>      \frac{1}{6}, \text{if } x \in <span class="sc">\{</span>1,2,3,4,5,6<span class="sc">\}</span> <span class="sc">\\</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>      0, \text{otherwise} </span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>   \end{cases}$$</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution collapse="true"}</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## What's the expectation, $\mathbb{E}[X]?$</span></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>$$ \begin{align} </span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>         \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> &amp;= 1\big(\frac{1}{6}\big) + 2\big(\frac{1}{6}\big) + 3\big(\frac{1}{6}\big) + 4\big(\frac{1}{6}\big) + 5\big(\frac{1}{6}\big) + 6\big(\frac{1}{6}\big) <span class="sc">\\</span></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>         &amp;= \big(\frac{1}{6}\big)( 1 + 2 + 3 + 4 + 5 + 6) <span class="sc">\\</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>         &amp;= \frac{7}{2}</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>      \end{align}$$</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>::: {.callout-caution collapse="true"}</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## What's the variance, $\text{Var}(X)?$</span></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Using Approach 1 (definition): </span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>   $$\begin{align} </span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>      \text{Var}(X) &amp;= \big(\frac{1}{6}\big)((1 - \frac{7}{2})^2 + (2 - \frac{7}{2})^2 + (3 - \frac{7}{2})^2 + (4 - \frac{7}{2})^2 + (5 - \frac{7}{2})^2 + (6 - \frac{7}{2})^2) <span class="sc">\\</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>      &amp;= \frac{35}{12}</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>   \end{align}$$</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>Using Approach 2 (property): </span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> = \sum_{x} x^2 P(X = x) = \frac{91}{6}$$</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(X) = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12}$$</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>We can summarize our discussion so far in the following diagram:</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/exp_var.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'distribution'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'800'</span><span class="kw">&gt;</span></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sums of Random Variables</span></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>Often, we will work with multiple random variables at the same time. A function of a random variable is also a random variable. If you create multiple random variables based on your sample, then functions of those random variables are also random variables.</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>For example, if $X_1, X_2, ..., X_n$ are random variables, then so are all of these: </span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X_n^2$</span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$<span class="sc">\#\{</span>i : X_i &gt; 10<span class="sc">\}</span>$</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\text{max}(X_1, X_2, ..., X_n)$</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\frac{1}{n} \sum_{i=1}^n (X_i - c)^2$</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\frac{1}{n} \sum_{i=1}^n X_i$</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a>Many functions of random variables that we are interested in (e.g., counts, means) involve sums of random variables, so let's dive deeper into the properties of sums of random variables.</span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Expectation </span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>Instead of simulating full distributions, we often just compute expectation and variance directly. Recall the definition of expectation: $$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \sum_{x} x P(X=x)$$</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>From it, we can derive some useful properties: </span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Linearity of expectation**. The expectation of the linear transformation $aX+b$, where $a$ and $b$ are constants, is:</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">aX+b</span><span class="co">]</span> = aE<span class="co">[</span><span class="ot">\mathbb{X}</span><span class="co">]</span> + b$$</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb1-304"><a href="#cb1-304" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-305"><a href="#cb1-305" aria-hidden="true" tabindex="-1"></a>        \mathbb{E}<span class="co">[</span><span class="ot">aX+b</span><span class="co">]</span> &amp;= \sum_{x} (ax + b) P(X=x) <span class="sc">\\</span></span>
<span id="cb1-306"><a href="#cb1-306" aria-hidden="true" tabindex="-1"></a>        &amp;= \sum_{x} (ax P(X=x) + bP(X=x)) <span class="sc">\\</span></span>
<span id="cb1-307"><a href="#cb1-307" aria-hidden="true" tabindex="-1"></a>        &amp;= a\sum_{x}P(X=x) + b\sum_{x}P(X=x)<span class="sc">\\</span></span>
<span id="cb1-308"><a href="#cb1-308" aria-hidden="true" tabindex="-1"></a>        &amp;= a\mathbb{E}(X) + b * 1</span>
<span id="cb1-309"><a href="#cb1-309" aria-hidden="true" tabindex="-1"></a>    \end{align}$$</span>
<span id="cb1-310"><a href="#cb1-310" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-311"><a href="#cb1-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-312"><a href="#cb1-312" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Expectation is also linear in *sums* of random variables. </span>
<span id="cb1-313"><a href="#cb1-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-314"><a href="#cb1-314" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">X+Y</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$$</span>
<span id="cb1-315"><a href="#cb1-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-316"><a href="#cb1-316" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb1-317"><a href="#cb1-317" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb1-318"><a href="#cb1-318" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-319"><a href="#cb1-319" aria-hidden="true" tabindex="-1"></a>    \mathbb{E}<span class="co">[</span><span class="ot">X+Y</span><span class="co">]</span> &amp;= \sum_{s} (X+Y)(s) P(s) <span class="sc">\\</span></span>
<span id="cb1-320"><a href="#cb1-320" aria-hidden="true" tabindex="-1"></a>    &amp;= \sum_{s} (X(s)P(s) + Y(s)P(s)) <span class="sc">\\</span></span>
<span id="cb1-321"><a href="#cb1-321" aria-hidden="true" tabindex="-1"></a>    &amp;= \sum_{s} X(s)P(s) + \sum_{s} Y(s)P(s)<span class="sc">\\</span></span>
<span id="cb1-322"><a href="#cb1-322" aria-hidden="true" tabindex="-1"></a>    &amp;= \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span></span>
<span id="cb1-323"><a href="#cb1-323" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-324"><a href="#cb1-324" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-325"><a href="#cb1-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-326"><a href="#cb1-326" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>If $g$ is a non-linear function, then in general, </span>
<span id="cb1-327"><a href="#cb1-327" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">g(X)</span><span class="co">]</span> \neq g(\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>)$$ For example, if $X$ is -1 or 1 with equal probability, then $\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = 0$, but $\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> = 1 \neq 0$.</span>
<span id="cb1-328"><a href="#cb1-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-329"><a href="#cb1-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### Properties of Variance</span></span>
<span id="cb1-330"><a href="#cb1-330" aria-hidden="true" tabindex="-1"></a>Let's now get into the properties of variance. Recall the definition of variance: $$\text{Var}(X) = \mathbb{E}<span class="co">[</span><span class="ot">(X-\mathbb{E}[X])^2</span><span class="co">]</span>$$</span>
<span id="cb1-331"><a href="#cb1-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-332"><a href="#cb1-332" aria-hidden="true" tabindex="-1"></a>Combining it with the properties of expectation, we can derive some useful properties: </span>
<span id="cb1-333"><a href="#cb1-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-334"><a href="#cb1-334" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Unlike expectation, variance is *non-linear*. The variance of the linear transformation $aX+b$ is:</span>
<span id="cb1-335"><a href="#cb1-335" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(aX+b) = a^2 \text{Var}(X)$$</span>
<span id="cb1-336"><a href="#cb1-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-337"><a href="#cb1-337" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Subsequently, $$\text{SD}(aX+b) = |a| \text{SD}(X)$$</span>
<span id="cb1-338"><a href="#cb1-338" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The full proof of this fact can be found using the definition of variance. As general intuition, consider that $aX+b$ scales the variable $X$ by a factor of $a$, then shifts the distribution of $X$ by $b$ units. </span>
<span id="cb1-339"><a href="#cb1-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-340"><a href="#cb1-340" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb1-341"><a href="#cb1-341" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb1-342"><a href="#cb1-342" aria-hidden="true" tabindex="-1"></a>We know that $$\mathbb{E}<span class="co">[</span><span class="ot">aX+b</span><span class="co">]</span> = aE<span class="co">[</span><span class="ot">\mathbb{X}</span><span class="co">]</span> + b$$</span>
<span id="cb1-343"><a href="#cb1-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-344"><a href="#cb1-344" aria-hidden="true" tabindex="-1"></a>In order to compute $\text{Var}(aX+b)$, consider that a shift by $b$ units does not affect spread, so $\text{Var}(aX+b) = \text{Var}(aX)$.</span>
<span id="cb1-345"><a href="#cb1-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-346"><a href="#cb1-346" aria-hidden="true" tabindex="-1"></a>Then, </span>
<span id="cb1-347"><a href="#cb1-347" aria-hidden="true" tabindex="-1"></a>$$\begin{align}</span>
<span id="cb1-348"><a href="#cb1-348" aria-hidden="true" tabindex="-1"></a>    \text{Var}(aX+b) &amp;= \text{Var}(aX) <span class="sc">\\</span></span>
<span id="cb1-349"><a href="#cb1-349" aria-hidden="true" tabindex="-1"></a>    &amp;= E((aX)^2) - (E(aX))^2 <span class="sc">\\</span></span>
<span id="cb1-350"><a href="#cb1-350" aria-hidden="true" tabindex="-1"></a>    &amp;= E(a^2 X^2) - (aE(X))^2<span class="sc">\\</span></span>
<span id="cb1-351"><a href="#cb1-351" aria-hidden="true" tabindex="-1"></a>    &amp;= a^2 (E(X^2) - (E(X))^2) <span class="sc">\\</span></span>
<span id="cb1-352"><a href="#cb1-352" aria-hidden="true" tabindex="-1"></a>    &amp;= a^2 \text{Var}(X)</span>
<span id="cb1-353"><a href="#cb1-353" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-354"><a href="#cb1-354" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-355"><a href="#cb1-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-356"><a href="#cb1-356" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Shifting the distribution by $b$ *does not* impact the *spread* of the distribution. Thus, $\text{Var}(aX+b) = \text{Var}(aX)$.</span>
<span id="cb1-357"><a href="#cb1-357" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Scaling the distribution by $a$ *does* impact the spread of the distribution.</span>
<span id="cb1-358"><a href="#cb1-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-359"><a href="#cb1-359" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-360"><a href="#cb1-360" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/transformation.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'transformation'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'600'</span><span class="kw">&gt;</span></span>
<span id="cb1-361"><a href="#cb1-361" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-362"><a href="#cb1-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-363"><a href="#cb1-363" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Variance of sums of random variables is affected by the (in)dependence of the random variables.</span>
<span id="cb1-364"><a href="#cb1-364" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{cov}(X,Y)$$</span>
<span id="cb1-365"><a href="#cb1-365" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}$$</span>
<span id="cb1-366"><a href="#cb1-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-367"><a href="#cb1-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-368"><a href="#cb1-368" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb1-369"><a href="#cb1-369" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof</span></span>
<span id="cb1-370"><a href="#cb1-370" aria-hidden="true" tabindex="-1"></a>The variance of a sum is affected by the dependence between the two random variables that are being added. Let’s expand the definition of $\text{Var}(X + Y)$ to see what’s going on.</span>
<span id="cb1-371"><a href="#cb1-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-372"><a href="#cb1-372" aria-hidden="true" tabindex="-1"></a>To simplify the math, let $\mu_x = \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>$ and $\mu_y = \mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$.</span>
<span id="cb1-373"><a href="#cb1-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-374"><a href="#cb1-374" aria-hidden="true" tabindex="-1"></a>$$ \begin{align}</span>
<span id="cb1-375"><a href="#cb1-375" aria-hidden="true" tabindex="-1"></a>\text{Var}(X + Y) &amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X+Y- \mathbb{E}(X+Y))^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-376"><a href="#cb1-376" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">((X - \mu_x) + (Y - \mu_y))^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-377"><a href="#cb1-377" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_x)^2 + 2(X - \mu_x)(Y - \mu_y) + (Y - \mu_y)^2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-378"><a href="#cb1-378" aria-hidden="true" tabindex="-1"></a>&amp;= \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_x)^2</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">(Y - \mu_y)^2</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_x)(Y - \mu_y)</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-379"><a href="#cb1-379" aria-hidden="true" tabindex="-1"></a>&amp;= \text{Var}(X) + \text{Var}(Y) + \mathbb{E}<span class="co">[</span><span class="ot">(X - \mu_x)(Y - \mu_y)</span><span class="co">]</span> </span>
<span id="cb1-380"><a href="#cb1-380" aria-hidden="true" tabindex="-1"></a>\end{align}$$</span>
<span id="cb1-381"><a href="#cb1-381" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-382"><a href="#cb1-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-383"><a href="#cb1-383" aria-hidden="true" tabindex="-1"></a><span class="fu">### Covariance and Correlation</span></span>
<span id="cb1-384"><a href="#cb1-384" aria-hidden="true" tabindex="-1"></a>We define the **covariance** of two random variables as the expected product of deviations from expectation. Put more simply, covariance is a generalization of variance to variance: </span>
<span id="cb1-385"><a href="#cb1-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-386"><a href="#cb1-386" aria-hidden="true" tabindex="-1"></a>$$\text{Cov}(X, X) = \mathbb{E}<span class="co">[</span><span class="ot">(X - \mathbb{E}[X])^2</span><span class="co">]</span> = \text{Var}(X)$$</span>
<span id="cb1-387"><a href="#cb1-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-388"><a href="#cb1-388" aria-hidden="true" tabindex="-1"></a>$$\text{Cov}(X, Y) = \mathbb{E}<span class="co">[</span><span class="ot">(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])</span><span class="co">]</span>$$</span>
<span id="cb1-389"><a href="#cb1-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-390"><a href="#cb1-390" aria-hidden="true" tabindex="-1"></a>We can treat the covariance as a measure of association. Remember the definition of correlation given when we first established SLR?</span>
<span id="cb1-391"><a href="#cb1-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-392"><a href="#cb1-392" aria-hidden="true" tabindex="-1"></a>$$r(X, Y) = \mathbb{E}\left<span class="co">[</span><span class="ot">\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right</span><span class="co">]</span> = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}$$</span>
<span id="cb1-393"><a href="#cb1-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-394"><a href="#cb1-394" aria-hidden="true" tabindex="-1"></a>It turns out we've been quietly using covariance for some time now! If $X$ and $Y$ are independent, then $\text{Cov}(X, Y) =0$ and $r(X, Y) = 0$. Note, however, that the converse is not always true: $X$ and $Y$ could have $\text{Cov}(X, Y) = r(X, Y) = 0$ but not be independent. </span>
<span id="cb1-395"><a href="#cb1-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-396"><a href="#cb1-396" aria-hidden="true" tabindex="-1"></a><span class="fu">### Equal vs. Identically Distributed vs. i.i.d</span></span>
<span id="cb1-397"><a href="#cb1-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-398"><a href="#cb1-398" aria-hidden="true" tabindex="-1"></a>Suppose that we have two random variables $X$ and $Y$:</span>
<span id="cb1-399"><a href="#cb1-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-400"><a href="#cb1-400" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X$ and $Y$ are **equal** if $X(s) = Y(s)$ for every sample $s$. Regardless of the exact sample drawn, $X$ is always equal to $Y$.</span>
<span id="cb1-401"><a href="#cb1-401" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X$ and $Y$ are **identically distributed** if the distribution of $X$ is equal to the distribution of $Y$. We say “$X$ and $Y$ are equal in distribution.” That is, $X$ and $Y$ take on the same set of possible values, and each of these possible values is taken with the same probability. On any specific sample $s$, identically distributed variables do *not* necessarily share the same value. If $X = Y$, then $X$ and $Y$ are identically distributed; however, the converse is not true (ex: $Y = 7 - X$, $X$ is a die)</span>
<span id="cb1-402"><a href="#cb1-402" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$X$ and $Y$ are **independent and identically distributed (i.i.d)** if </span>
<span id="cb1-403"><a href="#cb1-403" aria-hidden="true" tabindex="-1"></a><span class="ss">    1. </span>The variables are identically distributed. </span>
<span id="cb1-404"><a href="#cb1-404" aria-hidden="true" tabindex="-1"></a><span class="ss">    2. </span>Knowing the outcome of one variable does not influence our belief of the outcome of the other.</span>
<span id="cb1-405"><a href="#cb1-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-406"><a href="#cb1-406" aria-hidden="true" tabindex="-1"></a>Note that in Data 100, you'll never be expected to prove that random variables are i.i.d.</span>
<span id="cb1-407"><a href="#cb1-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-408"><a href="#cb1-408" aria-hidden="true" tabindex="-1"></a>Now let's walk through an example. Say $X_1$ and $X_2$ be numbers on rolls of two fair die. $X_1$ and $X_2$ are i.i.d, so  $X_1$ and $X_2$ have the same distribution. However, the sums $Y = X_1 + X_1 = 2X_1$ and $Z=X_1+X_2$ have different distributions but the same expectation.</span>
<span id="cb1-409"><a href="#cb1-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-410"><a href="#cb1-410" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-411"><a href="#cb1-411" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/yz_distribution.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'distribution'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'=500'</span><span class="kw">&gt;</span></span>
<span id="cb1-412"><a href="#cb1-412" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-413"><a href="#cb1-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-414"><a href="#cb1-414" aria-hidden="true" tabindex="-1"></a>However, $Y = X_1$ has a larger variance.</span>
<span id="cb1-415"><a href="#cb1-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-416"><a href="#cb1-416" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;p</span> <span class="er">align</span><span class="ot">=</span><span class="st">"center"</span><span class="kw">&gt;</span></span>
<span id="cb1-417"><a href="#cb1-417" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"images/yz.png"</span> <span class="er">alt</span><span class="ot">=</span><span class="st">'distribution'</span> <span class="er">width</span><span class="ot">=</span><span class="st">'200'</span><span class="kw">&gt;</span></span>
<span id="cb1-418"><a href="#cb1-418" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;/p&gt;</span></span>
<span id="cb1-419"><a href="#cb1-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-420"><a href="#cb1-420" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Bernoulli Random Variable</span></span>
<span id="cb1-421"><a href="#cb1-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-422"><a href="#cb1-422" aria-hidden="true" tabindex="-1"></a>To get some practice with the formulas discussed so far, let's derive the expectation and variance for a Bernoulli($p$) random variable. If $X$ ~ Bernoulli($p$),</span>
<span id="cb1-423"><a href="#cb1-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-424"><a href="#cb1-424" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = 1 \cdot p + 0 \cdot (1 - p) = p$</span>
<span id="cb1-425"><a href="#cb1-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-426"><a href="#cb1-426" aria-hidden="true" tabindex="-1"></a>To compute the variance, we will use the computational formula. We first find that:</span>
<span id="cb1-427"><a href="#cb1-427" aria-hidden="true" tabindex="-1"></a>$\mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> = 1^2 \cdot p + 0^2 \cdot (1 - p) = p$</span>
<span id="cb1-428"><a href="#cb1-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-429"><a href="#cb1-429" aria-hidden="true" tabindex="-1"></a>From there, let's calculate our variance: </span>
<span id="cb1-430"><a href="#cb1-430" aria-hidden="true" tabindex="-1"></a>$\text{Var}(X) = \mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>^2 = p - p^2 = p(1-p)$</span>
<span id="cb1-431"><a href="#cb1-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-432"><a href="#cb1-432" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example: Binomial Random Variable</span></span>
<span id="cb1-433"><a href="#cb1-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-434"><a href="#cb1-434" aria-hidden="true" tabindex="-1"></a>Let $Y$ ~ Binomial($n$, $p$). We can think of $Y$ as being the sum of $n$ i.i.d. Bernoulli($p$) random variables. Mathematically, this translates to </span>
<span id="cb1-435"><a href="#cb1-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-436"><a href="#cb1-436" aria-hidden="true" tabindex="-1"></a>$$Y = \sum_{i=1}^n X_i$$</span>
<span id="cb1-437"><a href="#cb1-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-438"><a href="#cb1-438" aria-hidden="true" tabindex="-1"></a>where $X_i$ is the indicator of a success on trial $i$. </span>
<span id="cb1-439"><a href="#cb1-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-440"><a href="#cb1-440" aria-hidden="true" tabindex="-1"></a>Using linearity of expectation,</span>
<span id="cb1-441"><a href="#cb1-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-442"><a href="#cb1-442" aria-hidden="true" tabindex="-1"></a>$$\mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span> = \sum_{i=1}^n \mathbb{E}<span class="co">[</span><span class="ot">X_i</span><span class="co">]</span> = np$$</span>
<span id="cb1-443"><a href="#cb1-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-444"><a href="#cb1-444" aria-hidden="true" tabindex="-1"></a>For the variance, since each $X_i$ is independent of the other, $\text{Cov}(X_i, X_j) = 0$,</span>
<span id="cb1-445"><a href="#cb1-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-446"><a href="#cb1-446" aria-hidden="true" tabindex="-1"></a>$$\text{Var}(Y) =  \sum_{i=1}^n \text{Var}<span class="co">[</span><span class="ot">X_i</span><span class="co">]</span> = np(1-p)$$</span>
<span id="cb1-447"><a href="#cb1-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-448"><a href="#cb1-448" aria-hidden="true" tabindex="-1"></a><span class="fu">### Summary </span></span>
<span id="cb1-449"><a href="#cb1-449" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Let $X$ be a random variable with distribution $P(X=x)$. </span>
<span id="cb1-450"><a href="#cb1-450" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> = \sum_{x} x P(X=x)$</span>
<span id="cb1-451"><a href="#cb1-451" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\text{Var}(X) = \mathbb{E}<span class="co">[</span><span class="ot">(X-\mathbb{E}[X])^2</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">X^2</span><span class="co">]</span> - (\mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span>)^2$</span>
<span id="cb1-452"><a href="#cb1-452" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Let $a$ and $b$ be scalar values. </span>
<span id="cb1-453"><a href="#cb1-453" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\mathbb{E}<span class="co">[</span><span class="ot">aX+b</span><span class="co">]</span> = aE<span class="co">[</span><span class="ot">\mathbb{X}</span><span class="co">]</span> + b$</span>
<span id="cb1-454"><a href="#cb1-454" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\text{Var}(aX+b) = a^2 \text{Var}(X)$</span>
<span id="cb1-455"><a href="#cb1-455" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Let $Y$ be another random variable. </span>
<span id="cb1-456"><a href="#cb1-456" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\mathbb{E}<span class="co">[</span><span class="ot">X+Y</span><span class="co">]</span> = \mathbb{E}<span class="co">[</span><span class="ot">X</span><span class="co">]</span> + \mathbb{E}<span class="co">[</span><span class="ot">Y</span><span class="co">]</span>$</span>
<span id="cb1-457"><a href="#cb1-457" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span>$\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y)$</span>
<span id="cb1-458"><a href="#cb1-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-459"><a href="#cb1-459" aria-hidden="true" tabindex="-1"></a>Note that $\text{Cov}(X,Y)$ would equal 0 if $X$ and $Y$ are independent.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>