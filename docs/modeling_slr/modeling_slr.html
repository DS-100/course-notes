<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Modeling &amp; SLR – Principles and Techniques of Data Science</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../constant_model_loss_transformations/loss_transformations.html" rel="next">
<link href="../sampling/sampling.html" rel="prev">
<link href="../data100_logo.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5c5377781c2bbb9bcadb5cf09ec76441.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../modeling_slr/modeling_slr.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modeling &amp; SLR</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../data100_logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Principles and Techniques of Data Science</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/DS-100/course-notes" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../intro_lec/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_1/pandas_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Pandas I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_2/pandas_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Pandas II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pandas_3/pandas_3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Pandas III</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../eda/eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Cleaning and EDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../regex/regex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Regular Expressions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_1/visualization_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Visualization I</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../visualization_2/visualization_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Visualization II</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../modeling_slr/modeling_slr.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modeling &amp; SLR</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../constant_model_loss_transformations/loss_transformations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../ols/ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Ordinary Least Squares</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../gradient_descent/gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">sklearn and Gradient Descent</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../feature_engineering/feature_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Gradient Descent Continuation, Feature Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../case_study_HCE/case_study_HCE.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Case Study in Human Contexts and Ethics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../cv_regularization/cv_reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Cross Validation and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_1/probability_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability_2/probability_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Estimators, Bias, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../inference_causality/inference_causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Parameter Inference and Bootstrapping (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_I/sql_I.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">SQL I (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../sql_II/sql_II.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">SQL II (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_1/logistic_reg_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Logistic Regression I (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../logistic_regression_2/logistic_reg_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Logistic Regression II (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_1/pca_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">PCA I (from Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pca_2/pca_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">PCA II (Spring 2025)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../clustering/clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Clustering (from Spring 2025)</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Modeling &amp; SLR</h2>
   
  <ul>
  <li><a href="#what-is-a-model" id="toc-what-is-a-model" class="nav-link active" data-scroll-target="#what-is-a-model"><span class="header-section-number">10.1</span> What is a Model?</a>
  <ul>
  <li><a href="#reasons-for-building-models" id="toc-reasons-for-building-models" class="nav-link" data-scroll-target="#reasons-for-building-models"><span class="header-section-number">10.1.1</span> Reasons for Building Models</a></li>
  <li><a href="#not-in-scope-common-types-of-models" id="toc-not-in-scope-common-types-of-models" class="nav-link" data-scroll-target="#not-in-scope-common-types-of-models"><span class="header-section-number">10.1.2</span> [NOT IN SCOPE] Common Types of Models</a></li>
  </ul></li>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression"><span class="header-section-number">10.2</span> Simple Linear Regression</a>
  <ul>
  <li><a href="#notations-and-definitions" id="toc-notations-and-definitions" class="nav-link" data-scroll-target="#notations-and-definitions"><span class="header-section-number">10.2.1</span> Notations and Definitions</a>
  <ul>
  <li><a href="#standard-units" id="toc-standard-units" class="nav-link" data-scroll-target="#standard-units"><span class="header-section-number">10.2.1.1</span> Standard Units</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="header-section-number">10.2.1.2</span> Correlation</a></li>
  </ul></li>
  <li><a href="#alternate-form" id="toc-alternate-form" class="nav-link" data-scroll-target="#alternate-form"><span class="header-section-number">10.2.2</span> Alternate Form</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation"><span class="header-section-number">10.2.3</span> Derivation</a></li>
  </ul></li>
  <li><a href="#the-modeling-process" id="toc-the-modeling-process" class="nav-link" data-scroll-target="#the-modeling-process"><span class="header-section-number">10.3</span> The Modeling Process</a></li>
  <li><a href="#choosing-a-model" id="toc-choosing-a-model" class="nav-link" data-scroll-target="#choosing-a-model"><span class="header-section-number">10.4</span> Choosing a Model</a></li>
  <li><a href="#choosing-a-loss-function" id="toc-choosing-a-loss-function" class="nav-link" data-scroll-target="#choosing-a-loss-function"><span class="header-section-number">10.5</span> Choosing a Loss Function</a></li>
  <li><a href="#fitting-the-model" id="toc-fitting-the-model" class="nav-link" data-scroll-target="#fitting-the-model"><span class="header-section-number">10.6</span> Fitting the Model</a></li>
  <li><a href="#evaluating-the-slr-model" id="toc-evaluating-the-slr-model" class="nav-link" data-scroll-target="#evaluating-the-slr-model"><span class="header-section-number">10.7</span> Evaluating the SLR Model</a>
  <ul>
  <li><a href="#four-mysterious-datasets-anscombes-quartet" id="toc-four-mysterious-datasets-anscombes-quartet" class="nav-link" data-scroll-target="#four-mysterious-datasets-anscombes-quartet"><span class="header-section-number">10.7.1</span> Four Mysterious Datasets (Anscombe’s quartet)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Modeling &amp; SLR</span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Understand what models are and how to carry out the four-step modeling process.</li>
<li>Define the concept of loss and gain familiarity with <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span> loss.</li>
<li>Fit the Simple Linear Regression model using minimization techniques.</li>
</ul>
</div>
</div>
</div>
<p>Up until this point in the semester, we’ve focused on analyzing datasets. We’ve looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.</p>
<p>This lecture marks a shift in focus. We will move away from examining datasets to actually <em>using</em> our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some predictions about the world around us. In this lecture, we’ll introduce the conceptual framework for setting up a modeling task. In the next few lectures, we’ll put this framework into practice by implementing various kinds of models.</p>
<section id="what-is-a-model" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="what-is-a-model"><span class="header-section-number">10.1</span> What is a Model?</h2>
<p>A model is an <strong>idealized representation</strong> of a system. A system is a set of principles or procedures according to which something functions. We live in a world full of systems: the procedure of turning on a light happens according to a specific set of rules dictating the flow of electricity. The truth behind how any event occurs is usually complex, and many times the specifics are unknown. The workings of the world can be viewed as its own giant procedure. Models seek to simplify the world and distill them into workable pieces.</p>
<p>Example: We model the fall of an object on Earth as subject to a constant acceleration of <span class="math inline">\(9.81 m/s^2\)</span> due to gravity.</p>
<ul>
<li>This is an <strong>approximate</strong> description of a system</li>
<li>It doesn’t account for air resistance, topography, etc</li>
<li>But in practice, it’s <strong>accurate enough</strong> to be useful!</li>
</ul>
<section id="reasons-for-building-models" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="reasons-for-building-models"><span class="header-section-number">10.1.1</span> Reasons for Building Models</h3>
<p>Why do we want to build models? As far as data scientists and statisticians are concerned, there are three reasons, and each implies a different focus on modeling.</p>
<ol type="1">
<li><p><strong>Inference</strong>: Make sense of <em>phenomena</em>. For example,</p>
<ul>
<li>How do parents’ heights <ins>relate</ins> to children’s heights?</li>
<li>What is the <ins>correlation</ins> of income and education?</li>
</ul>
<p>We often want <em>simple</em> and <em>interpretable</em> models to help us understand relationships</p></li>
<li><p><strong>Prediction</strong>: Make accurate predictions about unseen data. Some examples include:</p>
<ul>
<li>Is an email spam or not?</li>
<li>Generate a summary of a 10-page long article</li>
</ul>
<p>When making prediction, we care more about making extremely <em>accurate</em> predictions, at the cost of having a <em>less interpretable</em> or <em>black-box</em> model. Uninterpretable models are common in fields like deep learning.</p></li>
<li><p><strong>Causality</strong>: Assess whether one thing <em>causes</em> something else. For example,</p>
<ul>
<li>Does smoking <ins>cause</ins> lung cancer?</li>
<li>Does a job training program <ins>increase</ins> in employment and wages?</li>
</ul>
<p>This is a much harder question! Most statistical tools are designed to infer association, not causation. We will not focus on this task in Data 100, but you can take other advanced classes on causal inference (e.g., Stat 156, Data 102) if you are intrigued!</p></li>
</ol>
<p>Most of the time, we aim to strike a balance between building <strong>interpretable</strong> models and building <strong>accurate models</strong>.</p>
<p>Note that these three reasons can overlap! The distinctions are not always clear cut.</p>
</section>
<section id="not-in-scope-common-types-of-models" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="not-in-scope-common-types-of-models"><span class="header-section-number">10.1.2</span> [NOT IN SCOPE] Common Types of Models</h3>
<p>In general, models can be split into two categories:</p>
<ol type="1">
<li><p>Deterministic physical (mechanistic) models: Laws that govern how the world works.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion#Third_law">Kepler’s Third Law of Planetary Motion (1619)</a>: The ratio of the square of an object’s orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.
<ul>
<li><span class="math inline">\(T^2 \propto R^3\)</span></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion">Newton’s Laws: motion and gravitation (1687)</a>: Newton’s second law of motion models the relationship between the mass of an object and the force required to accelerate it.
<ul>
<li><span class="math inline">\(F = ma\)</span></li>
<li><span class="math inline">\(F_g = G \frac{m_1 m_2}{r^2}\)</span> <br></li>
</ul></li>
</ul></li>
<li><p>Probabilistic models: Models that attempt to understand how random processes evolve. These are more general and can be used to describe many phenomena in the real world. These models commonly make simplifying assumptions about the nature of the world.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson Process models</a>: Used to model random events that happen with some probability at any point in time and are strictly increasing in count, such as the arrival of customers at a store.</li>
</ul></li>
</ol>
<p>Note: These specific models are not in the scope of Data 100 and exist to serve as motivation.</p>
</section>
</section>
<section id="simple-linear-regression" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="simple-linear-regression"><span class="header-section-number">10.2</span> Simple Linear Regression</h2>
<p>The <strong>regression line</strong> is the unique straight line that minimizes the <strong>mean squared error</strong> of estimation among all straight lines. As with any straight line, it can be defined by a slope and a y-intercept:</p>
<ul>
<li><p><span class="math inline">\(r = \text{\textbf{correlation}} \text{ between }x \text{ and } y\)</span></p></li>
<li><p><span class="math inline">\(\text{slope} = r \cdot \frac{\text{Standard Deviation of } y}{\text{Standard Deviation of }x}\)</span></p></li>
<li><p><span class="math inline">\(y\text{-intercept} = \text{average of }y - \text{slope}\cdot\text{average of }x\)</span></p></li>
<li><p><span class="math inline">\(\text{regression estimate} = y\text{-intercept} + \text{slope}\cdot\text{}x\)</span></p></li>
<li><p><span class="math inline">\(\text{residual} =\text{observed }y - \text{regression estimate}\)</span></p></li>
</ul>
<div id="5c6c1989" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for consistency </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">43</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>) </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random noise for plotting</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">*</span> <span class="fl">0.5</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot regression line</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x,y<span class="op">=</span>y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="modeling_slr_files/figure-html/cell-2-output-1.png" width="559" height="413" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="notations-and-definitions" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="notations-and-definitions"><span class="header-section-number">10.2.1</span> Notations and Definitions</h3>
<p>For a pair of variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> representing our data <span class="math inline">\(\mathcal{D} = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)\}\)</span>, we denote their means/averages as <span class="math inline">\(\bar x\)</span> and <span class="math inline">\(\bar y\)</span> and standard deviations as <span class="math inline">\(\sigma_x\)</span> and <span class="math inline">\(\sigma_y\)</span>.</p>
<section id="standard-units" class="level4" data-number="10.2.1.1">
<h4 data-number="10.2.1.1" class="anchored" data-anchor-id="standard-units"><span class="header-section-number">10.2.1.1</span> Standard Units</h4>
<p>A variable is represented in standard units if the following are true:</p>
<ol type="1">
<li>0 in standard units is equal to the mean (<span class="math inline">\(\bar{x}\)</span>) in the original variable’s units.</li>
<li>An increase of 1 standard unit is an increase of 1 standard deviation (<span class="math inline">\(\sigma_x\)</span>) in the original variable’s units.</li>
</ol>
<p>To convert a variable <span class="math inline">\(x_i\)</span> into standard units, we subtract its mean from it and divide it by its standard deviation. For example, <span class="math inline">\(x_i\)</span> in standard units is <span class="math inline">\(\frac{x_i - \bar x}{\sigma_x}\)</span>.</p>
</section>
<section id="correlation" class="level4" data-number="10.2.1.2">
<h4 data-number="10.2.1.2" class="anchored" data-anchor-id="correlation"><span class="header-section-number">10.2.1.2</span> Correlation</h4>
<p>The correlation (<span class="math inline">\(r\)</span>) is the average of the product of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, both measured in <em>standard units</em>.</p>
<p>In general,</p>
<p><span class="math display">\[r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i - \bar{x}}{\sigma_x})(\frac{y_i - \bar{y}}{\sigma_y})\]</span></p>
<p>However, when <span class="math inline">\(\bar{x} = 0\)</span>, <span class="math inline">\(\bar{y} = 0\)</span>, <span class="math inline">\(\sigma_x = 1\)</span>, or <span class="math inline">\(\sigma_y = 1\)</span> (which are all satisfied when x and y are both in standard units),</p>
<p><span class="math display">\[r = \frac{1}{n} \sum_{i=1}^{n} x_i y_i\]</span></p>
<p>This simpler formula is convenient to work with when possible.</p>
<ol type="1">
<li>Correlation measures the strength of a <strong>linear association</strong> between two variables.</li>
<li>Correlations range between -1 and 1: <span class="math inline">\(|r| \leq 1\)</span>, with <span class="math inline">\(r=1\)</span> indicating perfect positive linear association, and <span class="math inline">\(r=-1\)</span> indicating perfect negative association. The closer <span class="math inline">\(r\)</span> is to <span class="math inline">\(0\)</span>, the weaker the linear association is.</li>
<li>Correlation says nothing about causation and non-linear association. Correlation does <strong>not</strong> imply causation. When <span class="math inline">\(r = 0\)</span>, the two variables are uncorrelated. However, they could still be related through some non-linear relationship.</li>
</ol>
<p>For an intuitive understanding of correlation, when <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> have the same sign (in standard units), the (<span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>) pair contributes <ins>positively</ins> to correlation. <em>Opposite</em> signs contrinbute <ins>negatively</ins>.</p>
<div id="5abc8cce" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_and_get_corr(ax, x, y, title):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x, y, alpha <span class="op">=</span> <span class="fl">0.73</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title <span class="op">+</span> <span class="st">" (corr: </span><span class="sc">{}</span><span class="st">)"</span>.<span class="bu">format</span>(r.<span class="bu">round</span>(<span class="dv">2</span>)))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> r</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Just noise</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>x1, y1 <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>corr1 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">0</span>, <span class="dv">0</span>], x1, y1, title <span class="op">=</span> <span class="st">"noise"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Strong linear</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> x2 <span class="op">*</span> <span class="fl">0.5</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>corr2 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">0</span>, <span class="dv">1</span>], x2, y2, title <span class="op">=</span> <span class="st">"strong linear"</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Unequal spread</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>x3 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> <span class="op">-</span> x3<span class="op">/</span><span class="dv">3</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">*</span>(x3)<span class="op">/</span><span class="fl">2.5</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>corr3 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">1</span>, <span class="dv">0</span>], x3, y3, title <span class="op">=</span> <span class="st">"strong linear"</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>extent <span class="op">=</span> axs[<span class="dv">1</span>, <span class="dv">0</span>].get_window_extent().transformed(fig.dpi_scale_trans.inverted())</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Strong non-linear</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>x4 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>y4 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.sin(x3 <span class="op">-</span> <span class="fl">1.5</span>) <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>corr4 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">1</span>, <span class="dv">1</span>], x4, y4, title <span class="op">=</span> <span class="st">"strong non-linear"</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="modeling_slr_files/figure-html/cell-3-output-1.png" width="794" height="811" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="alternate-form" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="alternate-form"><span class="header-section-number">10.2.2</span> Alternate Form</h3>
<p>When the variables <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are measured in <em>standard units</em>, the regression line for predicting <span class="math inline">\(y\)</span> based on <span class="math inline">\(x\)</span> has slope <span class="math inline">\(r\)</span> and passes through the origin.</p>
<p><span class="math display">\[\hat{y}_{su} = r \cdot x_{su}\]</span></p>
<p>Notice that when r = 1, we have perfect prediction!</p>
<p><img src="images/reg_line_1.png" class="img-fluid"></p>
<ul>
<li>In the original units, this becomes</li>
</ul>
<p><span class="math display">\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]</span></p>
<p><img src="images/reg_line_2.png" class="img-fluid"></p>
</section>
<section id="derivation" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="derivation"><span class="header-section-number">10.2.3</span> Derivation</h3>
<p>Starting from the top, we have our claimed form of the regression line, and we want to show that it is equivalent to the optimal linear regression line: <span class="math inline">\(\hat{y} = \hat{a} + \hat{b}x\)</span>.</p>
<p>Recall:</p>
<ul>
<li><span class="math inline">\(\hat{b} = r \cdot \frac{\text{Standard Deviation of }y}{\text{Standard Deviation of }x}\)</span></li>
<li><span class="math inline">\(\hat{a} = \text{average of }y - \text{slope}\cdot\text{average of }x\)</span></li>
</ul>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<p><span class="math display">\[\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}\]</span></p>
<p>Multiply by <span class="math inline">\(\sigma_y\)</span>, and add <span class="math inline">\(\bar{y}\)</span> on both sides.</p>
<p><span class="math display">\[\hat{y} = \sigma_y \cdot r \cdot \frac{x - \bar{x}}{\sigma_x} + \bar{y}\]</span></p>
<p>Distribute coefficient <span class="math inline">\(\sigma_{y}\cdot r\)</span> to the <span class="math inline">\(\frac{x - \bar{x}}{\sigma_x}\)</span> term</p>
<p><span class="math display">\[\hat{y} = (\frac{r\sigma_y}{\sigma_x} ) \cdot x + (\bar{y} - (\frac{r\sigma_y}{\sigma_x} ) \bar{x})\]</span></p>
<p>We now see that we have a line that matches our claim:</p>
<ul>
<li>slope: <span class="math inline">\(r\cdot\frac{\text{SD of y}}{\text{SD of x}} = r\cdot\frac{\sigma_y}{\sigma_x}\)</span></li>
<li>intercept: <span class="math inline">\(\bar{y} - \text{slope}\cdot \bar{x}\)</span></li>
</ul>
<p>Note that the error for the i-th datapoint is: <span class="math inline">\(e_i = y_i - \hat{y_i}\)</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="the-modeling-process" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="the-modeling-process"><span class="header-section-number">10.3</span> The Modeling Process</h2>
<p>At a high level, a model is a way of representing a system. In Data 100, we’ll treat a model as some mathematical rule we use to describe the relationship between variables.</p>
<p>What variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset <span class="math inline">\(\mathcal{D}\)</span>:</p>
<p><span class="math display">\[\mathcal{D} = \{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\]</span></p>
<p>Each pair of values <span class="math inline">\((x_i, y_i)\)</span> represents a datapoint. In a modeling setting, we call these <strong>observations</strong>. <span class="math inline">\(y_i\)</span> is the dependent variable we are trying to model, also called an <strong>output</strong> or <strong>response</strong>. <span class="math inline">\(x_i\)</span> is the independent variable inputted into the model to make predictions, also known as a <strong>feature</strong>.</p>
<p>Our goal in modeling is to use the observed data <span class="math inline">\(\mathcal{D}\)</span> to predict the output variable <span class="math inline">\(y_i\)</span>. We denote each prediction as <span class="math inline">\(\hat{y}_i\)</span> (read: “y hat sub i”).</p>
<p>How do we generate these predictions? Some examples of models we’ll encounter in the next few lectures are given below:</p>
<p><span class="math display">\[\hat{y}_i = \theta\]</span> <span class="math display">\[\hat{y}_i = \theta_0 + \theta_1 x_i\]</span></p>
<p>The examples above are known as <strong>parametric models</strong>. They relate the collected data, <span class="math inline">\(x_i\)</span>, to the prediction we make, <span class="math inline">\(\hat{y}_i\)</span>. A few parameters (<span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\theta_1\)</span>) are used to describe the relationship between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\hat{y}_i\)</span>.</p>
<p>Notice that we don’t immediately know the values of these parameters. While the features, <span class="math inline">\(x_i\)</span>, are taken from our observed data, we need to decide what values to give <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\theta_0\)</span>, and <span class="math inline">\(\theta_1\)</span> ourselves. This is the heart of parametric modeling: <em>what parameter values should we choose so our model makes the best possible predictions?</em></p>
<p><span class="math inline">\(\hat{\theta}\)</span> is an estimate of a parameter <span class="math inline">\(\theta\)</span> based on a sample. The “hat” denotes an estimated or predicted quantity. For example, <span class="math inline">\(\hat{y}\)</span> is a prediction. <span class="math inline">\(\hat{\theta}\)</span> is estimated from data.</p>
<p>Before we move on, note that not all statistical models have parameters! k-Nearest Neighbor classifiers (from Data 8) and KDEs are <strong>non-parametric</strong> models.</p>
<p>To choose our model parameters, we’ll work through the <strong>modeling process</strong>.</p>
<ol type="1">
<li><strong>Choose a model</strong>: How should we <ins>represent</ins> the world?</li>
<li><strong>Choose a loss function</strong>: How do we quantify prediction <ins>error</ins>?</li>
<li><strong>Fit the model</strong>: How do we <ins>choose</ins> the best parameters of our model given our data?</li>
<li><strong>Evaluate model performance</strong>: How do we <ins>evaluate</ins> whether this process gave rise to a good model?</li>
</ol>
</section>
<section id="choosing-a-model" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="choosing-a-model"><span class="header-section-number">10.4</span> Choosing a Model</h2>
<p>Our first step is choosing a model: defining the mathematical rule that describes the relationship between the features, <span class="math inline">\(x_i\)</span>, and predictions <span class="math inline">\(\hat{y}_i\)</span>.</p>
<p>In <a href="https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html">Data 8</a>, you learned about the <strong>Simple Linear Regression (SLR) model</strong>. You learned that the model takes the form: <span class="math display">\[\hat{y}_i = a + bx_i\]</span></p>
<p>In Data 100, we’ll use slightly different notation: we will replace <span class="math inline">\(a\)</span> with <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(b\)</span> with <span class="math inline">\(\theta_1\)</span>. This will allow us to use the same notation when we explore more complex models later on in the course.</p>
<p><span class="math display">\[\hat{y}_i = \theta_0 + \theta_1 x_i\]</span></p>
<p>The parameters of the SLR model are <span class="math inline">\(\theta_0\)</span>, also called the intercept term, and <span class="math inline">\(\theta_1\)</span>, also called the slope term. To create an effective model, we want to choose values for <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> that most accurately predict the output variable. The “best” fitting model parameters are given the special names: <span class="math inline">\(\hat{\theta}_0\)</span> and <span class="math inline">\(\hat{\theta}_1\)</span>; they are the specific parameter values that allow our model to generate the best possible predictions.</p>
<p>In Data 8, you learned that the best SLR model parameters are: <span class="math display">\[\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} \qquad \qquad \hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]</span></p>
<p>A quick reminder on notation:</p>
<ul>
<li><span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> indicate the mean value of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, respectively</li>
<li><span class="math inline">\(\sigma_y\)</span> and <span class="math inline">\(\sigma_x\)</span> indicate the standard deviations of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(r\)</span> is the <a href="https://inferentialthinking.com/chapters/15/1/Correlation.html#the-correlation-coefficient">correlation coefficient</a>, defined as the average of the product of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> measured in standard units: <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y})\)</span></li>
</ul>
<p>In Data 100, we want to understand <em>how</em> to derive these best model coefficients. To do so, we’ll introduce the concept of a loss function.</p>
</section>
<section id="choosing-a-loss-function" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="choosing-a-loss-function"><span class="header-section-number">10.5</span> Choosing a Loss Function</h2>
<p>We’ve talked about the idea of creating the “best” possible predictions. This begs the question: how do we decide how “good” or “bad” our model’s predictions are?</p>
<p>A <strong>loss function</strong> characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, <span class="math inline">\(L(y, \hat{y})\)</span>, quantifies how “bad” or “far off” a single prediction by our model is from a true, observed value in our collected data.</p>
<p>The choice of loss function for a particular model will affect the accuracy and computational cost of estimation, and it’ll also depend on the estimation task at hand. For example,</p>
<ul>
<li>Are outputs quantitative or qualitative?</li>
<li>Do outliers matter?</li>
<li>Are all errors equally costly? (e.g., a false negative on a cancer test is arguably more dangerous than a false positive)</li>
</ul>
<p>Regardless of the specific function used, a loss function should follow two basic principles:</p>
<ul>
<li>If the prediction <span class="math inline">\(\hat{y}_i\)</span> is <em>close</em> to the actual value <span class="math inline">\(y_i\)</span>, loss should be low.</li>
<li>If the prediction <span class="math inline">\(\hat{y}_i\)</span> is <em>far</em> from the actual value <span class="math inline">\(y_i\)</span>, loss should be high.</li>
</ul>
<p>Two common choices of loss function are squared loss and absolute loss.</p>
<p><strong>Squared loss</strong>, also known as <strong>L2 loss</strong>, computes loss as the square of the difference between the observed <span class="math inline">\(y_i\)</span> and predicted <span class="math inline">\(\hat{y}_i\)</span>: <span class="math display">\[L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2\]</span></p>
<p><strong>Absolute loss</strong>, also known as <strong>L1 loss</strong>, computes loss as the absolute difference between the observed <span class="math inline">\(y_i\)</span> and predicted <span class="math inline">\(\hat{y}_i\)</span>: <span class="math display">\[L(y_i, \hat{y}_i) = |y_i - \hat{y}_i|\]</span></p>
<p>L1 and L2 loss give us a tool for quantifying our model’s performance on a single data point. This is a good start, but ideally, we want to understand how our model performs across our <em>entire</em> dataset. A natural way to do this is to compute the average loss across all data points in the dataset. This is known as the <strong>cost function</strong>, <span class="math inline">\(\hat{R}(\theta)\)</span>: <span class="math display">\[\hat{R}(\theta) = \frac{1}{n} \sum^n_{i=1} L(y_i, \hat{y}_i)\]</span></p>
<p>The cost function has many names in the statistics literature. You may also encounter the terms:</p>
<ul>
<li>Empirical risk (this is why we give the cost function the name <span class="math inline">\(R\)</span>)</li>
<li>Error function</li>
<li>Average loss</li>
</ul>
<p>We can substitute our L1 and L2 loss into the cost function definition. The <strong>Mean Squared Error (MSE)</strong> is the average squared loss across a dataset: <span class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p>
<p>The <strong>Mean Absolute Error (MAE)</strong> is the average absolute loss across a dataset: <span class="math display">\[\text{MAE}= \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|\]</span></p>
</section>
<section id="fitting-the-model" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="fitting-the-model"><span class="header-section-number">10.6</span> Fitting the Model</h2>
<p>Now that we’ve established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model’s cost on our dataset. This process is called fitting the model.</p>
<p>We know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the <strong>objective function</strong> (our objective is to find its minimum).</p>
<p>To find the optimal model parameter, we:</p>
<ol type="1">
<li>Take the derivative of the cost function with respect to that parameter</li>
<li>Set the derivative equal to 0</li>
<li>Solve for the parameter</li>
</ol>
<p>We repeat this process for each parameter present in the model. For now, we’ll disregard the second derivative condition.</p>
<p>To help us make sense of this process, let’s put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!</p>
<p>Step 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, <span class="math inline">\(\hat{y}_i = \theta_0+\theta_1 x_i\)</span>, into the definition of MSE above and differentiate with respect to <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span>. <span class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)^2\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} y_i - \theta_0 - \theta_1 x_i\]</span></p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)x_i\]</span></p>
<p>Let’s walk through these derivations in more depth, starting with the derivative of MSE with respect to <span class="math inline">\(\theta_0\)</span>.</p>
<p>Given our MSE above, we know that: <span class="math display">\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{\partial}{\partial \theta_0} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]</span></p>
<p>Noting that the derivative of sum is equivalent to the sum of derivatives, this then becomes: <span class="math display">\[ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_0} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]</span></p>
<p>We can then apply the chain rule.</p>
<p><span class="math display">\[ = \frac{1}{n} \sum_{i=1}^{n} 2 \cdot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-1)\]</span></p>
<p>Finally, we can simplify the constants, leaving us with our answer.</p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n}{(y_i - \theta_0 - \theta_1 x_i)}\]</span></p>
<p>Following the same procedure, we can take the derivative of MSE with respect to <span class="math inline">\(\theta_1\)</span>.</p>
<p><span class="math display">\[\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{\partial}{\partial \theta_1} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]</span></p>
<p><span class="math display">\[ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_1} {(y_i - \theta_0 - \theta_1 x_i)}^{2}\]</span></p>
<p><span class="math display">\[ = \frac{1}{n} \sum_{i=1}^{n} 2 \dot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-x_i)\]</span></p>
<p><span class="math display">\[= \frac{-2}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}x_i\]</span></p>
<p>Step 2: set the derivatives equal to 0. After simplifying terms, this produces two <strong>estimating equations</strong>. The best set of model parameters <span class="math inline">\((\hat{\theta}_0, \hat{\theta}_1)\)</span> <em>must</em> satisfy these two optimality conditions. <span class="math display">\[0 = \frac{-2}{n} \sum_{i=1}^{n} y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} y_i - \hat{y}_i = 0\]</span> <span class="math display">\[0 = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i)x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)x_i = 0\]</span></p>
<p>Step 3: solve the estimating equations to compute estimates for <span class="math inline">\(\hat{\theta}_0\)</span> and <span class="math inline">\(\hat{\theta}_1\)</span>.</p>
<p>Taking the first equation gives the estimate of <span class="math inline">\(\hat{\theta}_0\)</span>: <span class="math display">\[\frac{1}{n} \sum_{i=1}^n y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i = 0 \]</span></p>
<p><span class="math display">\[\left(\frac{1}{n} \sum_{i=1}^n y_i \right) - \hat{\theta}_0 - \hat{\theta}_1\left(\frac{1}{n} \sum_{i=1}^n x_i \right) = 0\]</span></p>
<p><span class="math display">\[ \hat{\theta}_0 = \bar{y} - \hat{\theta}_1 \bar{x}\]</span></p>
<p>With a bit more maneuvering, the second equation gives the estimate of <span class="math inline">\(\hat{\theta}_1\)</span>. Start by multiplying the first estimating equation by <span class="math inline">\(\bar{x}\)</span>, then subtracting the result from the second estimating equation.</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)x_i - \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)\bar{x} = 0 \]</span></p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)(x_i - \bar{x}) = 0 \]</span></p>
<p>Next, plug in <span class="math inline">\(\hat{y}_i = \hat{\theta}_0 + \hat{\theta}_1 x_i = \bar{y} + \hat{\theta}_1(x_i - \bar{x})\)</span>:</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y} - \hat{\theta}_1(x - \bar{x}))(x_i - \bar{x}) = 0 \]</span></p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x}) = \hat{\theta}_1 \times \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\]</span></p>
<p>By using the definition of correlation <span class="math inline">\(\left(r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y}) \right)\)</span> and standard deviation <span class="math inline">\(\left(\sigma_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \right)\)</span>, we can conclude: <span class="math display">\[r \sigma_x \sigma_y = \hat{\theta}_1 \times \sigma_x^2\]</span> <span class="math display">\[\hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}\]</span></p>
<p>Just as was given in Data 8!</p>
<p>Remember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can <em>always</em> follow these three steps to fit the model.</p>
</section>
<section id="evaluating-the-slr-model" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="evaluating-the-slr-model"><span class="header-section-number">10.7</span> Evaluating the SLR Model</h2>
<p>Now that we’ve explored the mathematics behind (1) choosing a model, (2) choosing a loss function, and (3) fitting the model, we’re left with one final question – how “good” are the predictions made by this “best” fitted model? To determine this, we can:</p>
<ol type="1">
<li><p>Visualize data and compute statistics:</p>
<ul>
<li>Plot the original data.</li>
<li>Compute each column’s mean and standard deviation. If the mean and standard deviation of our predictions are close to those of the original observed <span class="math inline">\(y_i\)</span>’s, we might be inclined to say that our model has done well.</li>
<li>(If we’re fitting a linear model) Compute the correlation <span class="math inline">\(r\)</span>. A large magnitude for the correlation coefficient between the feature and response variables could also indicate that our model has done well.</li>
</ul></li>
<li><p>Performance metrics:</p>
<ul>
<li>We can take the <strong>Root Mean Squared Error (RMSE)</strong>.
<ul>
<li>It’s the square root of the mean squared error (MSE), which is the average loss that we’ve been minimizing to determine optimal model parameters.</li>
<li>RMSE is in the same units as <span class="math inline">\(y\)</span>.</li>
<li>A lower RMSE indicates more “accurate” predictions, as we have a lower “average loss” across the data.</li>
</ul></li>
</ul>
<p><span class="math display">\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]</span></p></li>
<li><p>Visualization:</p>
<ul>
<li>Look at the residual plot of <span class="math inline">\(e_i = y_i - \hat{y_i}\)</span> to visualize the difference between actual and predicted values. The good residual plot should not show any pattern between input/features <span class="math inline">\(x_i\)</span> and residual values <span class="math inline">\(e_i\)</span>.</li>
</ul></li>
</ol>
<p>To illustrate this process, let’s take a look at <strong>Anscombe’s quartet</strong>.</p>
<section id="four-mysterious-datasets-anscombes-quartet" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="four-mysterious-datasets-anscombes-quartet"><span class="header-section-number">10.7.1</span> Four Mysterious Datasets (Anscombe’s quartet)</h3>
<p>Let’s take a look at four different datasets.</p>
<div id="2a3d3ae7" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="26edc14d" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Big font helper</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_fontsize(size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    SMALL_SIZE <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    MEDIUM_SIZE <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    BIGGER_SIZE <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> size <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        SMALL_SIZE <span class="op">=</span> MEDIUM_SIZE <span class="op">=</span> BIGGER_SIZE <span class="op">=</span> size</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"font"</span>, size<span class="op">=</span>SMALL_SIZE)  <span class="co"># controls default text sizes</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, titlesize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the axes title</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, labelsize<span class="op">=</span>MEDIUM_SIZE)  <span class="co"># fontsize of the x and y labels</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"xtick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"ytick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"legend"</span>, fontsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># legend fontsize</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"figure"</span>, titlesize<span class="op">=</span>BIGGER_SIZE)  <span class="co"># fontsize of the figure title</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper functions</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standard_units(x):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">-</span> np.mean(x)) <span class="op">/</span> np.std(x)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> correlation(x, y):</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(standard_units(x) <span class="op">*</span> standard_units(y))</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> slope(x, y):</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correlation(x, y) <span class="op">*</span> np.std(y) <span class="op">/</span> np.std(x)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> intercept(x, y):</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(y) <span class="op">-</span> slope(x, y) <span class="op">*</span> np.mean(x)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_least_squares(x, y):</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    theta_0 <span class="op">=</span> intercept(x, y)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    theta_1 <span class="op">=</span> slope(x, y)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_0, theta_1</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x, theta_0, theta_1):</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mse(y, yhat):</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y <span class="op">-</span> yhat) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d4d7d31e" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>NO_VIZ, RESID, RESID_SCATTER <span class="op">=</span> <span class="bu">range</span>(<span class="dv">3</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> least_squares_evaluation(x, y, visualize<span class="op">=</span>NO_VIZ):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># statistics</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x_mean : </span><span class="sc">{</span>np<span class="sc">.</span>mean(x)<span class="sc">:.2f}</span><span class="ss">, y_mean : </span><span class="sc">{</span>np<span class="sc">.</span>mean(y)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x_stdev: </span><span class="sc">{</span>np<span class="sc">.</span>std(x)<span class="sc">:.2f}</span><span class="ss">, y_stdev: </span><span class="sc">{</span>np<span class="sc">.</span>std(y)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"r = Correlation(x, y): </span><span class="sc">{</span>correlation(x, y)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Performance metrics</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\t</span><span class="ss">heta_0: </span><span class="sc">{</span>ahat<span class="sc">:.2f}</span><span class="ss">, </span><span class="ch">\t</span><span class="ss">heta_1: </span><span class="sc">{</span>bhat<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(compute_mse(y, yhat))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># visualization</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    fig, ax_resid <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize <span class="op">==</span> RESID_SCATTER:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].scatter(x, y)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].plot(x, yhat)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_title(<span class="st">"LS fit"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        ax_resid <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> visualize <span class="op">==</span> RESID:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        ax_resid <span class="op">=</span> plt.gca()</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax_resid <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        ax_resid.scatter(x, y <span class="op">-</span> yhat, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        ax_resid.plot([<span class="dv">4</span>, <span class="dv">14</span>], [<span class="dv">0</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        ax_resid.set_title(<span class="st">"Residuals"</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="178a85b2" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in four different datasets: I, II, III, IV</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">8</span>, <span class="dv">13</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">14</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">12</span>, <span class="dv">7</span>, <span class="dv">5</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> [<span class="fl">8.04</span>, <span class="fl">6.95</span>, <span class="fl">7.58</span>, <span class="fl">8.81</span>, <span class="fl">8.33</span>, <span class="fl">9.96</span>, <span class="fl">7.24</span>, <span class="fl">4.26</span>, <span class="fl">10.84</span>, <span class="fl">4.82</span>, <span class="fl">5.68</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> [<span class="fl">9.14</span>, <span class="fl">8.14</span>, <span class="fl">8.74</span>, <span class="fl">8.77</span>, <span class="fl">9.26</span>, <span class="fl">8.10</span>, <span class="fl">6.13</span>, <span class="fl">3.10</span>, <span class="fl">9.13</span>, <span class="fl">7.26</span>, <span class="fl">4.74</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> [<span class="fl">7.46</span>, <span class="fl">6.77</span>, <span class="fl">12.74</span>, <span class="fl">7.11</span>, <span class="fl">7.81</span>, <span class="fl">8.84</span>, <span class="fl">6.08</span>, <span class="fl">5.39</span>, <span class="fl">8.15</span>, <span class="fl">6.42</span>, <span class="fl">5.73</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>x4 <span class="op">=</span> [<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">19</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>y4 <span class="op">=</span> [<span class="fl">6.58</span>, <span class="fl">5.76</span>, <span class="fl">7.71</span>, <span class="fl">8.84</span>, <span class="fl">8.47</span>, <span class="fl">7.04</span>, <span class="fl">5.25</span>, <span class="fl">12.50</span>, <span class="fl">5.56</span>, <span class="fl">7.91</span>, <span class="fl">6.89</span>]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>anscombe <span class="op">=</span> {</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y1)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"II"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y2)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"III"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y3)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"IV"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x4, y4)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the scatter plot and line of best fit</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, dataset <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>]</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].scatter(x, y, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">"red"</span>)  <span class="co"># plot the x, y points</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].plot(x, yhat)  <span class="co"># plot the line of best fit</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_xlabel(<span class="ss">f"$x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_ylabel(<span class="ss">f"$y_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_title(<span class="ss">f"Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="modeling_slr_files/figure-html/cell-7-output-1.png" width="841" height="854" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>While these four sets of datapoints look very different, they actually all have identical means <span class="math inline">\(\bar x\)</span>, <span class="math inline">\(\bar y\)</span>, standard deviations <span class="math inline">\(\sigma_x\)</span>, <span class="math inline">\(\sigma_y\)</span>, correlation <span class="math inline">\(r\)</span>, and RMSE! If we only look at these statistics, we would probably be inclined to say that these datasets are similar.</p>
<div id="20be20df" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dataset <span class="kw">in</span> [<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"&gt;&gt;&gt; Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> least_squares_evaluation(ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>], visualize<span class="op">=</span>NO_VIZ)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&gt;&gt;&gt; Dataset I:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.119


&gt;&gt;&gt; Dataset II:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.119


&gt;&gt;&gt; Dataset III:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.816
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.118


&gt;&gt;&gt; Dataset IV:
x_mean : 9.00, y_mean : 7.50
x_stdev: 3.16, y_stdev: 1.94
r = Correlation(x, y): 0.817
    heta_0: 3.00,   heta_1: 0.50
RMSE: 1.118

</code></pre>
</div>
</div>
<p>We may also wish to visualize the model’s <strong>residuals</strong>, defined as the difference between the observed and predicted <span class="math inline">\(y_i\)</span> value (<span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>). This gives a high-level view of how “off” each prediction is from the true observed value. Recall that you explored this concept in <a href="https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity">Data 8</a>: a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe’s quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points.</p>
<!-- <img src="images/residual.png" alt='residual' width='600'> -->
<div id="d7ad75aa" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Residual visualization</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, dataset <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].scatter(</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">-</span> yhat, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">"red"</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># plot the x, y points</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].plot(</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        x, np.zeros_like(x), color<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># plot the residual line</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_xlabel(<span class="ss">f"$x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_ylabel(<span class="ss">f"$e_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_title(<span class="ss">f"Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss"> Residuals"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="modeling_slr_files/figure-html/cell-9-output-1.png" width="858" height="854" class="figure-img"></p>
</figure>
</div>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../sampling/sampling.html" class="pagination-link" aria-label="Sampling">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Sampling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../constant_model_loss_transformations/loss_transformations.html" class="pagination-link" aria-label="Constant Model, Loss, and Transformations">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Constant Model, Loss, and Transformations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Modeling &amp; SLR</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-title: Modeling &amp; SLR</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">    theme:</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - cosmo</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - cerulean</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co">    callout-icon: false</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="false"}</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning Outcomes</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Understand what models are and how to carry out the four-step modeling process.</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Define the concept of loss and gain familiarity with $L_1$ and $L_2$ loss.</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Fit the Simple Linear Regression model using minimization techniques.</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>Up until this point in the semester, we've focused on analyzing datasets. We've looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>This lecture marks a shift in focus. We will move away from examining datasets to actually *using* our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some predictions about the world around us. In this lecture, we'll introduce the conceptual framework for setting up a modeling task. In the next few lectures, we'll put this framework into practice by implementing various kinds of models.</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is a Model?</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>A model is an **idealized representation** of a system. A system is a set of principles or procedures according to which something functions. We live in a world full of systems: the procedure of turning on a light happens according to a specific set of rules dictating the flow of electricity. The truth behind how any event occurs is usually complex, and many times the specifics are unknown. The workings of the world can be viewed as its own giant procedure. Models seek to simplify the world and distill them into workable pieces.  </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>Example:</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>We model the fall of an object on Earth as subject to a constant acceleration of $9.81 m/s^2$ due to gravity.</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is an **approximate** description of a system</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It doesn't account for air resistance, topography, etc</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But in practice, it's **accurate enough** to be useful!</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reasons for Building Models</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>Why do we want to build models? As far as data scientists and statisticians are concerned, there are three reasons, and each implies a different focus on modeling.</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Inference**: Make sense of *phenomena*. For example,</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>How do parents' heights <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>relate<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> to children's heights?</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>What is the <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>correlation<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> of income and education?</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    We often want *simple* and *interpretable* models to help us understand relationships</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Prediction**: Make accurate predictions about unseen data. Some examples include:</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Is an email spam or not?</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Generate a summary of a 10-page long article</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    When making prediction, we care more about making extremely *accurate* predictions, at the cost of having a *less interpretable* or *black-box* model. Uninterpretable models are common in fields like deep learning.</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Causality**: Assess whether one thing *causes* something else. For example,</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Does smoking <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>cause<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> lung cancer?</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Does a job training program <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>increase<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> in employment and wages?</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    This is a much harder question! Most statistical tools are designed to infer association, not causation. We will not focus on this task in Data 100, but you can take other advanced classes on causal inference (e.g., Stat 156, Data 102) if you are intrigued! </span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>Most of the time, we aim to strike a balance between building **interpretable** models and building **accurate models**.</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>Note that these three reasons can overlap! The distinctions are not always clear cut.</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a><span class="fu">### [NOT IN SCOPE] Common Types of Models </span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>In general, models can be split into two categories:</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Deterministic physical (mechanistic) models: Laws that govern how the world works.</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Kepler's Third Law of Planetary Motion (1619)</span><span class="co">](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion#Third_law)</span>: The ratio of the square of an object's orbital period with the cube of the semi-major axis of its orbit is the same for all objects orbiting the same primary.</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>$T^2 \propto R^3$</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Newton's Laws: motion and gravitation (1687)</span><span class="co">](https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion)</span>: Newton’s second law of motion models the relationship between the mass of an object and the force required to accelerate it.</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>$F = ma$</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>$F_g = G \frac{m_1 m_2}{r^2}$</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">br</span><span class="dt">&gt;</span></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Probabilistic models: Models that attempt to understand how random processes evolve. These are more general and can be used to describe many phenomena in the real world. These models commonly make simplifying assumptions about the nature of the world.</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Poisson Process models</span><span class="co">](https://en.wikipedia.org/wiki/Poisson_point_process)</span>: Used to model random events that happen with some probability at any point in time and are strictly increasing in count, such as the arrival of customers at a store. </span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>Note: These specific models are not in the scope of Data 100 and exist to serve as motivation.</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simple Linear Regression </span></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>The **regression line** is the unique straight line that minimizes the **mean squared error** of estimation among all straight lines. As with any straight line, it can be defined by a slope and a y-intercept:</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r = \text{\textbf{correlation}} \text{ between }x \text{ and } y$</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{slope} = r \cdot \frac{\text{Standard Deviation of } y}{\text{Standard Deviation of }x}$</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$y\text{-intercept} = \text{average of }y - \text{slope}\cdot\text{average of }x$</span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{regression estimate} = y\text{-intercept} + \text{slope}\cdot\text{}x$</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{residual} =\text{observed }y - \text{regression estimate}$</span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed for consistency </span></span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">43</span>)</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'default'</span>) </span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random noise for plotting</span></span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">*</span> <span class="fl">0.5</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot regression line</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>x,y<span class="op">=</span>y)<span class="op">;</span></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Notations and Definitions</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>For a pair of variables $x$ and $y$ representing our data $\mathcal{D} = <span class="sc">\{</span>(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)<span class="sc">\}</span>$, we denote their means/averages as $\bar x$ and $\bar y$ and standard deviations as $\sigma_x$ and $\sigma_y$.</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Standard Units</span></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>A variable is represented in standard units if the following are true:</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>0 in standard units is equal to the mean ($\bar{x}$) in the original variable's units.</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>An increase of 1 standard unit is an increase of 1 standard deviation ($\sigma_x$) in the original variable's units.</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>To convert a variable $x_i$ into standard units, we subtract its mean from it and divide it by its standard deviation. For example, $x_i$ in standard units is $\frac{x_i - \bar x}{\sigma_x}$.</span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Correlation</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>The correlation ($r$) is the average of the product of $x$ and $y$, both measured in *standard units*.</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>In general,</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>$$r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i - \bar{x}}{\sigma_x})(\frac{y_i - \bar{y}}{\sigma_y})$$</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>However, when $\bar{x} = 0$,  $\bar{y} = 0$,  $\sigma_x = 1$, or $\sigma_y = 1$ (which are all satisfied when x and y are both in standard units),</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>$$r = \frac{1}{n} \sum_{i=1}^{n} x_i y_i$$</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>This simpler formula is convenient to work with when possible.</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Correlation measures the strength of a **linear association** between two variables.</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Correlations range between -1 and 1: $|r| \leq 1$, with $r=1$ indicating perfect positive linear association, and $r=-1$ indicating perfect negative association. The closer $r$ is to $0$, the weaker the linear association is.</span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Correlation says nothing about causation and non-linear association. Correlation does **not** imply causation. When $r = 0$, the two variables are uncorrelated. However, they could still be related through some non-linear relationship.</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>For an intuitive understanding of correlation, when $x_i$ and $y_i$ have the same sign (in standard units), the ($x_i$, $y_i$) pair contributes <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>positively<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> to correlation. *Opposite* signs contrinbute <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>negatively<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span>.</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_and_get_corr(ax, x, y, title):</span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x, y, alpha <span class="op">=</span> <span class="fl">0.73</span>)</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> np.corrcoef(x, y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title <span class="op">+</span> <span class="st">" (corr: </span><span class="sc">{}</span><span class="st">)"</span>.<span class="bu">format</span>(r.<span class="bu">round</span>(<span class="dv">2</span>)))</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> r</span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a><span class="co"># Just noise</span></span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>x1, y1 <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>corr1 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">0</span>, <span class="dv">0</span>], x1, y1, title <span class="op">=</span> <span class="st">"noise"</span>)</span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Strong linear</span></span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> x2 <span class="op">*</span> <span class="fl">0.5</span> <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a>corr2 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">0</span>, <span class="dv">1</span>], x2, y2, title <span class="op">=</span> <span class="st">"strong linear"</span>)</span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Unequal spread</span></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>x3 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> <span class="op">-</span> x3<span class="op">/</span><span class="dv">3</span> <span class="op">+</span> np.random.randn(<span class="dv">100</span>)<span class="op">*</span>(x3)<span class="op">/</span><span class="fl">2.5</span></span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>corr3 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">1</span>, <span class="dv">0</span>], x3, y3, title <span class="op">=</span> <span class="st">"strong linear"</span>)</span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>extent <span class="op">=</span> axs[<span class="dv">1</span>, <span class="dv">0</span>].get_window_extent().transformed(fig.dpi_scale_trans.inverted())</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a><span class="co"># Strong non-linear</span></span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>x4 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>y4 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>np.sin(x3 <span class="op">-</span> <span class="fl">1.5</span>) <span class="op">+</span> np.random.randn(<span class="dv">100</span>) <span class="op">*</span> <span class="fl">0.3</span></span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a>corr4 <span class="op">=</span> plot_and_get_corr(axs[<span class="dv">1</span>, <span class="dv">1</span>], x4, y4, title <span class="op">=</span> <span class="st">"strong non-linear"</span>)</span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a><span class="fu">### Alternate Form</span></span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a>When the variables $y$ and $x$ are measured in *standard units*, the regression line for predicting $y$ based on $x$ has slope $r$ and passes through the origin.</span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a> $$\hat{y}_{su} = r \cdot x_{su}$$</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a> Notice that when r = 1, we have perfect prediction!</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/reg_line_1.png)</span></span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the original units, this becomes</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>$$\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}$$</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/reg_line_2.png)</span></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### Derivation</span></span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>Starting from the top, we have our claimed form of the regression line, and we want to show that it is equivalent to the optimal linear regression line: $\hat{y} = \hat{a} + \hat{b}x$.</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a>Recall: </span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{b} = r \cdot \frac{\text{Standard Deviation of }y}{\text{Standard Deviation of }x}$</span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{a} = \text{average of }y - \text{slope}\cdot\text{average of }x$</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>:::{.callout}</span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>Proof: </span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>$$\frac{\hat{y} - \bar{y}}{\sigma_y} = r \cdot \frac{x - \bar{x}}{\sigma_x}$$</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>Multiply by $\sigma_y$, and add $\bar{y}$ on both sides.</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = \sigma_y \cdot r \cdot \frac{x - \bar{x}}{\sigma_x} + \bar{y}$$</span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>Distribute coefficient $\sigma_{y}\cdot r$ to the $\frac{x - \bar{x}}{\sigma_x}$ term</span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a>$$\hat{y} = (\frac{r\sigma_y}{\sigma_x} ) \cdot x + (\bar{y} - (\frac{r\sigma_y}{\sigma_x} ) \bar{x})$$</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a>We now see that we have a line that matches our claim:</span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>slope: $r\cdot\frac{\text{SD of y}}{\text{SD of x}} = r\cdot\frac{\sigma_y}{\sigma_x}$</span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>intercept: $\bar{y} - \text{slope}\cdot \bar{x}$</span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a>Note that the error for the i-th datapoint is: $e_i = y_i - \hat{y_i}$</span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Modeling Process</span></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>At a high level, a model is a way of representing a system. In Data 100, we'll treat a model as some mathematical rule we use to describe the relationship between variables. </span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a>What variables are we modeling? Typically, we use a subset of the variables in our sample of collected data to model another variable in this data. To put this more formally, say we have the following dataset $\mathcal{D}$:</span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a>$$\mathcal{D} = <span class="sc">\{</span>(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)<span class="sc">\}</span>$$</span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a>Each pair of values $(x_i, y_i)$ represents a datapoint. In a modeling setting, we call these **observations**. $y_i$ is the dependent variable we are trying to model, also called an **output** or **response**. $x_i$ is the independent variable inputted into the model to make predictions, also known as a **feature**. </span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>Our goal in modeling is to use the observed data $\mathcal{D}$ to predict the output variable $y_i$. We denote each prediction as $\hat{y}_i$ (read: "y hat sub i").</span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a>How do we generate these predictions? Some examples of models we'll encounter in the next few lectures are given below:</span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_i = \theta$$</span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_i = \theta_0 + \theta_1 x_i$$</span>
<span id="cb10-254"><a href="#cb10-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-255"><a href="#cb10-255" aria-hidden="true" tabindex="-1"></a>The examples above are known as **parametric models**. They relate the collected data, $x_i$, to the prediction we make, $\hat{y}_i$. A few parameters ($\theta$, $\theta_0$, $\theta_1$) are used to describe the relationship between $x_i$ and $\hat{y}_i$.</span>
<span id="cb10-256"><a href="#cb10-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-257"><a href="#cb10-257" aria-hidden="true" tabindex="-1"></a>Notice that we don't immediately know the values of these parameters. While the features, $x_i$, are taken from our observed data, we need to decide what values to give $\theta$, $\theta_0$, and $\theta_1$ ourselves. This is the heart of parametric modeling: *what parameter values should we choose so our model makes the best possible predictions?*</span>
<span id="cb10-258"><a href="#cb10-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-259"><a href="#cb10-259" aria-hidden="true" tabindex="-1"></a>$\hat{\theta}$ is an estimate of a parameter $\theta$ based on a sample. The "hat" denotes an estimated or predicted quantity. For example, $\hat{y}$ is a prediction. $\hat{\theta}$ is estimated from data.</span>
<span id="cb10-260"><a href="#cb10-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-261"><a href="#cb10-261" aria-hidden="true" tabindex="-1"></a>Before we move on, note that not all statistical models have parameters! k-Nearest Neighbor classifiers (from Data 8) and KDEs are **non-parametric** models.</span>
<span id="cb10-262"><a href="#cb10-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-263"><a href="#cb10-263" aria-hidden="true" tabindex="-1"></a>To choose our model parameters, we'll work through the **modeling process**. </span>
<span id="cb10-264"><a href="#cb10-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-265"><a href="#cb10-265" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Choose a model**: How should we <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>represent<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> the world?</span>
<span id="cb10-266"><a href="#cb10-266" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Choose a loss function**: How do we quantify prediction <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>error<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span>?</span>
<span id="cb10-267"><a href="#cb10-267" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Fit the model**: How do we <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>choose<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> the best parameters of our model given our data?</span>
<span id="cb10-268"><a href="#cb10-268" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Evaluate model performance**: How do we <span class="dt">&lt;</span><span class="kw">ins</span><span class="dt">&gt;</span>evaluate<span class="dt">&lt;/</span><span class="kw">ins</span><span class="dt">&gt;</span> whether this process gave rise to a good model?</span>
<span id="cb10-269"><a href="#cb10-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-270"><a href="#cb10-270" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choosing a Model</span></span>
<span id="cb10-271"><a href="#cb10-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-272"><a href="#cb10-272" aria-hidden="true" tabindex="-1"></a>Our first step is choosing a model: defining the mathematical rule that describes the relationship between the features, $x_i$, and predictions $\hat{y}_i$. </span>
<span id="cb10-273"><a href="#cb10-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-274"><a href="#cb10-274" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">Data 8</span><span class="co">](https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html)</span>, you learned about the **Simple Linear Regression (SLR) model**. You learned that the model takes the form:</span>
<span id="cb10-275"><a href="#cb10-275" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_i = a + bx_i$$</span>
<span id="cb10-276"><a href="#cb10-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-277"><a href="#cb10-277" aria-hidden="true" tabindex="-1"></a>In Data 100, we'll use slightly different notation: we will replace $a$ with $\theta_0$ and $b$ with $\theta_1$. This will allow us to use the same notation when we explore more complex models later on in the course.</span>
<span id="cb10-278"><a href="#cb10-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-279"><a href="#cb10-279" aria-hidden="true" tabindex="-1"></a>$$\hat{y}_i = \theta_0 + \theta_1 x_i$$</span>
<span id="cb10-280"><a href="#cb10-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-281"><a href="#cb10-281" aria-hidden="true" tabindex="-1"></a>The parameters of the SLR model are $\theta_0$, also called the intercept term, and $\theta_1$, also called the slope term. To create an effective model, we want to choose values for $\theta_0$ and $\theta_1$ that most accurately predict the output variable. The "best" fitting model parameters are given the special names: $\hat{\theta}_0$ and $\hat{\theta}_1$; they are the specific parameter values that allow our model to generate the best possible predictions.</span>
<span id="cb10-282"><a href="#cb10-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-283"><a href="#cb10-283" aria-hidden="true" tabindex="-1"></a>In Data 8, you learned that the best SLR model parameters are:</span>
<span id="cb10-284"><a href="#cb10-284" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_0 = \bar{y} - \hat{\theta}_1\bar{x} \qquad \qquad \hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}$$</span>
<span id="cb10-285"><a href="#cb10-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-286"><a href="#cb10-286" aria-hidden="true" tabindex="-1"></a>A quick reminder on notation:</span>
<span id="cb10-287"><a href="#cb10-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-288"><a href="#cb10-288" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\bar{y}$ and $\bar{x}$ indicate the mean value of $y$ and $x$, respectively</span>
<span id="cb10-289"><a href="#cb10-289" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\sigma_y$ and $\sigma_x$ indicate the standard deviations of $y$ and $x$</span>
<span id="cb10-290"><a href="#cb10-290" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$r$ is the <span class="co">[</span><span class="ot">correlation coefficient</span><span class="co">](https://inferentialthinking.com/chapters/15/1/Correlation.html#the-correlation-coefficient)</span>, defined as the average of the product of $x$ and $y$ measured in standard units: $\frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y})$</span>
<span id="cb10-291"><a href="#cb10-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-292"><a href="#cb10-292" aria-hidden="true" tabindex="-1"></a>In Data 100, we want to understand *how* to derive these best model coefficients. To do so, we'll introduce the concept of a loss function.</span>
<span id="cb10-293"><a href="#cb10-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-294"><a href="#cb10-294" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choosing a Loss Function</span></span>
<span id="cb10-295"><a href="#cb10-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-296"><a href="#cb10-296" aria-hidden="true" tabindex="-1"></a>We've talked about the idea of creating the "best" possible predictions. This begs the question: how do we decide how "good" or "bad" our model's predictions are?</span>
<span id="cb10-297"><a href="#cb10-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-298"><a href="#cb10-298" aria-hidden="true" tabindex="-1"></a>A **loss function** characterizes the cost, error, or fit resulting from a particular choice of model or model parameters. This function, $L(y, \hat{y})$, quantifies how "bad" or "far off" a single prediction by our model is from a true, observed value in our collected data. </span>
<span id="cb10-299"><a href="#cb10-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-300"><a href="#cb10-300" aria-hidden="true" tabindex="-1"></a>The choice of loss function for a particular model will affect the accuracy and computational cost of estimation, and it'll also depend on the estimation task at hand. For example, </span>
<span id="cb10-301"><a href="#cb10-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-302"><a href="#cb10-302" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Are outputs quantitative or qualitative? </span>
<span id="cb10-303"><a href="#cb10-303" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Do outliers matter? </span>
<span id="cb10-304"><a href="#cb10-304" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Are all errors equally costly? (e.g., a false negative on a cancer test is arguably more dangerous than a false positive) </span>
<span id="cb10-305"><a href="#cb10-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-306"><a href="#cb10-306" aria-hidden="true" tabindex="-1"></a>Regardless of the specific function used, a loss function should follow two basic principles:</span>
<span id="cb10-307"><a href="#cb10-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-308"><a href="#cb10-308" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If the prediction $\hat{y}_i$ is *close* to the actual value $y_i$, loss should be low.</span>
<span id="cb10-309"><a href="#cb10-309" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>If the prediction $\hat{y}_i$ is *far* from the actual value $y_i$, loss should be high.</span>
<span id="cb10-310"><a href="#cb10-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-311"><a href="#cb10-311" aria-hidden="true" tabindex="-1"></a>Two common choices of loss function are squared loss and absolute loss. </span>
<span id="cb10-312"><a href="#cb10-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-313"><a href="#cb10-313" aria-hidden="true" tabindex="-1"></a>**Squared loss**, also known as **L2 loss**, computes loss as the square of the difference between the observed $y_i$ and predicted $\hat{y}_i$:</span>
<span id="cb10-314"><a href="#cb10-314" aria-hidden="true" tabindex="-1"></a>$$L(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2$$</span>
<span id="cb10-315"><a href="#cb10-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-316"><a href="#cb10-316" aria-hidden="true" tabindex="-1"></a>**Absolute loss**, also known as **L1 loss**, computes loss as the absolute difference between the observed $y_i$ and predicted $\hat{y}_i$:</span>
<span id="cb10-317"><a href="#cb10-317" aria-hidden="true" tabindex="-1"></a>$$L(y_i, \hat{y}_i) = |y_i - \hat{y}_i|$$</span>
<span id="cb10-318"><a href="#cb10-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-319"><a href="#cb10-319" aria-hidden="true" tabindex="-1"></a>L1 and L2 loss give us a tool for quantifying our model's performance on a single data point. This is a good start, but ideally, we want to understand how our model performs across our *entire* dataset. A natural way to do this is to compute the average loss across all data points in the dataset. This is known as the **cost function**, $\hat{R}(\theta)$:</span>
<span id="cb10-320"><a href="#cb10-320" aria-hidden="true" tabindex="-1"></a>$$\hat{R}(\theta) = \frac{1}{n} \sum^n_{i=1} L(y_i, \hat{y}_i)$$</span>
<span id="cb10-321"><a href="#cb10-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-322"><a href="#cb10-322" aria-hidden="true" tabindex="-1"></a>The cost function has many names in the statistics literature. You may also encounter the terms:</span>
<span id="cb10-323"><a href="#cb10-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-324"><a href="#cb10-324" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Empirical risk (this is why we give the cost function the name $R$)</span>
<span id="cb10-325"><a href="#cb10-325" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Error function</span>
<span id="cb10-326"><a href="#cb10-326" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Average loss</span>
<span id="cb10-327"><a href="#cb10-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-328"><a href="#cb10-328" aria-hidden="true" tabindex="-1"></a>We can substitute our L1 and L2 loss into the cost function definition. The **Mean Squared Error (MSE)** is the average squared loss across a dataset:</span>
<span id="cb10-329"><a href="#cb10-329" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$</span>
<span id="cb10-330"><a href="#cb10-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-331"><a href="#cb10-331" aria-hidden="true" tabindex="-1"></a>The **Mean Absolute Error (MAE)** is the average absolute loss across a dataset:</span>
<span id="cb10-332"><a href="#cb10-332" aria-hidden="true" tabindex="-1"></a>$$\text{MAE}= \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$$</span>
<span id="cb10-333"><a href="#cb10-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-334"><a href="#cb10-334" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fitting the Model</span></span>
<span id="cb10-335"><a href="#cb10-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-336"><a href="#cb10-336" aria-hidden="true" tabindex="-1"></a>Now that we've established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want to choose the best set of model parameters that will minimize the model's cost on our dataset. This process is called fitting the model.</span>
<span id="cb10-337"><a href="#cb10-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-338"><a href="#cb10-338" aria-hidden="true" tabindex="-1"></a>We know from calculus that a function is minimized when (1) its first derivative is equal to zero and (2) its second derivative is positive. We often call the function being minimized the **objective function** (our objective is to find its minimum).</span>
<span id="cb10-339"><a href="#cb10-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-340"><a href="#cb10-340" aria-hidden="true" tabindex="-1"></a>To find the optimal model parameter, we:</span>
<span id="cb10-341"><a href="#cb10-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-342"><a href="#cb10-342" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Take the derivative of the cost function with respect to that parameter</span>
<span id="cb10-343"><a href="#cb10-343" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Set the derivative equal to 0</span>
<span id="cb10-344"><a href="#cb10-344" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Solve for the parameter</span>
<span id="cb10-345"><a href="#cb10-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-346"><a href="#cb10-346" aria-hidden="true" tabindex="-1"></a>We repeat this process for each parameter present in the model. For now, we'll disregard the second derivative condition. </span>
<span id="cb10-347"><a href="#cb10-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-348"><a href="#cb10-348" aria-hidden="true" tabindex="-1"></a>To help us make sense of this process, let's put it into action by deriving the optimal model parameters for simple linear regression using the mean squared error as our cost function. Remember: although the notation may look tricky, all we are doing is following the three steps above!</span>
<span id="cb10-349"><a href="#cb10-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-350"><a href="#cb10-350" aria-hidden="true" tabindex="-1"></a>Step 1: take the derivative of the cost function with respect to each model parameter. We substitute the SLR model, $\hat{y}_i = \theta_0+\theta_1 x_i$, into the definition of MSE above and differentiate with respect to $\theta_0$ and $\theta_1$.</span>
<span id="cb10-351"><a href="#cb10-351" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)^2$$</span>
<span id="cb10-352"><a href="#cb10-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-353"><a href="#cb10-353" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} y_i - \theta_0 - \theta_1 x_i$$</span>
<span id="cb10-354"><a href="#cb10-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-355"><a href="#cb10-355" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \theta_0 - \theta_1 x_i)x_i$$</span>
<span id="cb10-356"><a href="#cb10-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-357"><a href="#cb10-357" aria-hidden="true" tabindex="-1"></a>Let's walk through these derivations in more depth, starting with the derivative of MSE with respect to $\theta_0$.</span>
<span id="cb10-358"><a href="#cb10-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-359"><a href="#cb10-359" aria-hidden="true" tabindex="-1"></a>Given our MSE above, we know that:</span>
<span id="cb10-360"><a href="#cb10-360" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{\partial}{\partial \theta_0} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}$$</span>
<span id="cb10-361"><a href="#cb10-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-362"><a href="#cb10-362" aria-hidden="true" tabindex="-1"></a>Noting that the derivative of sum is equivalent to the sum of derivatives, this then becomes:</span>
<span id="cb10-363"><a href="#cb10-363" aria-hidden="true" tabindex="-1"></a>$$ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_0} {(y_i - \theta_0 - \theta_1 x_i)}^{2}$$</span>
<span id="cb10-364"><a href="#cb10-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-365"><a href="#cb10-365" aria-hidden="true" tabindex="-1"></a>We can then apply the chain rule.</span>
<span id="cb10-366"><a href="#cb10-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-367"><a href="#cb10-367" aria-hidden="true" tabindex="-1"></a>$$ = \frac{1}{n} \sum_{i=1}^{n} 2 \cdot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-1)$$</span>
<span id="cb10-368"><a href="#cb10-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-369"><a href="#cb10-369" aria-hidden="true" tabindex="-1"></a>Finally, we can simplify the constants, leaving us with our answer. </span>
<span id="cb10-370"><a href="#cb10-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-371"><a href="#cb10-371" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta_0} \text{MSE} = \frac{-2}{n} \sum_{i=1}^{n}{(y_i - \theta_0 - \theta_1 x_i)}$$</span>
<span id="cb10-372"><a href="#cb10-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-373"><a href="#cb10-373" aria-hidden="true" tabindex="-1"></a>Following the same procedure, we can take the derivative of MSE with respect to  $\theta_1$.</span>
<span id="cb10-374"><a href="#cb10-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-375"><a href="#cb10-375" aria-hidden="true" tabindex="-1"></a>$$\frac{\partial}{\partial \theta_1} \text{MSE} = \frac{\partial}{\partial \theta_1} \frac{1}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}^{2}$$</span>
<span id="cb10-376"><a href="#cb10-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-377"><a href="#cb10-377" aria-hidden="true" tabindex="-1"></a>$$ = \frac{1}{n} \sum_{i=1}^{n} \frac{\partial}{\partial \theta_1} {(y_i - \theta_0 - \theta_1 x_i)}^{2}$$</span>
<span id="cb10-378"><a href="#cb10-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-379"><a href="#cb10-379" aria-hidden="true" tabindex="-1"></a>$$ = \frac{1}{n} \sum_{i=1}^{n} 2 \dot{(y_i - \theta_0 - \theta_1 x_i)}\dot(-x_i)$$</span>
<span id="cb10-380"><a href="#cb10-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-381"><a href="#cb10-381" aria-hidden="true" tabindex="-1"></a>$$= \frac{-2}{n} \sum_{i=1}^{n} {(y_i - \theta_0 - \theta_1 x_i)}x_i$$</span>
<span id="cb10-382"><a href="#cb10-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-383"><a href="#cb10-383" aria-hidden="true" tabindex="-1"></a>Step 2: set the derivatives equal to 0. After simplifying terms, this produces two **estimating equations**. The best set of model parameters $(\hat{\theta}_0, \hat{\theta}_1)$ *must* satisfy these two optimality conditions.</span>
<span id="cb10-384"><a href="#cb10-384" aria-hidden="true" tabindex="-1"></a>$$0 = \frac{-2}{n} \sum_{i=1}^{n} y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} y_i - \hat{y}_i = 0$$</span>
<span id="cb10-385"><a href="#cb10-385" aria-hidden="true" tabindex="-1"></a>$$0 = \frac{-2}{n} \sum_{i=1}^{n} (y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i)x_i \Longleftrightarrow \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)x_i = 0$$</span>
<span id="cb10-386"><a href="#cb10-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-387"><a href="#cb10-387" aria-hidden="true" tabindex="-1"></a>Step 3: solve the estimating equations to compute estimates for $\hat{\theta}_0$ and $\hat{\theta}_1$.</span>
<span id="cb10-388"><a href="#cb10-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-389"><a href="#cb10-389" aria-hidden="true" tabindex="-1"></a>Taking the first equation gives the estimate of $\hat{\theta}_0$:</span>
<span id="cb10-390"><a href="#cb10-390" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n y_i - \hat{\theta}_0 - \hat{\theta}_1 x_i = 0 $$ </span>
<span id="cb10-391"><a href="#cb10-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-392"><a href="#cb10-392" aria-hidden="true" tabindex="-1"></a>$$\left(\frac{1}{n} \sum_{i=1}^n y_i \right) - \hat{\theta}_0 - \hat{\theta}_1\left(\frac{1}{n} \sum_{i=1}^n x_i \right) = 0$$</span>
<span id="cb10-393"><a href="#cb10-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-394"><a href="#cb10-394" aria-hidden="true" tabindex="-1"></a>$$ \hat{\theta}_0 = \bar{y} - \hat{\theta}_1 \bar{x}$$</span>
<span id="cb10-395"><a href="#cb10-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-396"><a href="#cb10-396" aria-hidden="true" tabindex="-1"></a>With a bit more maneuvering, the second equation gives the estimate of $\hat{\theta}_1$. Start by multiplying the first estimating equation by $\bar{x}$, then subtracting the result from the second estimating equation.</span>
<span id="cb10-397"><a href="#cb10-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-398"><a href="#cb10-398" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)x_i - \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)\bar{x} = 0 $$</span>
<span id="cb10-399"><a href="#cb10-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-400"><a href="#cb10-400" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)(x_i - \bar{x}) = 0 $$</span>
<span id="cb10-401"><a href="#cb10-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-402"><a href="#cb10-402" aria-hidden="true" tabindex="-1"></a>Next, plug in $\hat{y}_i = \hat{\theta}_0 + \hat{\theta}_1 x_i = \bar{y} + \hat{\theta}_1(x_i - \bar{x})$:</span>
<span id="cb10-403"><a href="#cb10-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-404"><a href="#cb10-404" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y} - \hat{\theta}_1(x - \bar{x}))(x_i - \bar{x}) = 0 $$</span>
<span id="cb10-405"><a href="#cb10-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-406"><a href="#cb10-406" aria-hidden="true" tabindex="-1"></a>$$\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x}) = \hat{\theta}_1 \times \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2</span>
<span id="cb10-407"><a href="#cb10-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb10-408"><a href="#cb10-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-409"><a href="#cb10-409" aria-hidden="true" tabindex="-1"></a>By using the definition of correlation $\left(r = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\sigma_x})(\frac{y_i-\bar{y}}{\sigma_y}) \right)$ and standard deviation $\left(\sigma_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \right)$, we can conclude:</span>
<span id="cb10-410"><a href="#cb10-410" aria-hidden="true" tabindex="-1"></a>$$r \sigma_x \sigma_y = \hat{\theta}_1 \times \sigma_x^2$$</span>
<span id="cb10-411"><a href="#cb10-411" aria-hidden="true" tabindex="-1"></a>$$\hat{\theta}_1 = r \frac{\sigma_y}{\sigma_x}$$</span>
<span id="cb10-412"><a href="#cb10-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-413"><a href="#cb10-413" aria-hidden="true" tabindex="-1"></a>Just as was given in Data 8! </span>
<span id="cb10-414"><a href="#cb10-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-415"><a href="#cb10-415" aria-hidden="true" tabindex="-1"></a>Remember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function, we likely would have found different values for the best model parameters. However, regardless of the model and loss used, we can *always* follow these three steps to fit the model.</span>
<span id="cb10-416"><a href="#cb10-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-417"><a href="#cb10-417" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluating the SLR Model</span></span>
<span id="cb10-418"><a href="#cb10-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-419"><a href="#cb10-419" aria-hidden="true" tabindex="-1"></a>Now that we've explored the mathematics behind (1) choosing a model, (2) choosing a loss function, and (3) fitting the model, we're left with one final question – how "good" are the predictions made by this "best" fitted model? To determine this, we can:</span>
<span id="cb10-420"><a href="#cb10-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-421"><a href="#cb10-421" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Visualize data and compute statistics:</span>
<span id="cb10-422"><a href="#cb10-422" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Plot the original data.</span>
<span id="cb10-423"><a href="#cb10-423" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Compute each column's mean and standard deviation. If the mean and standard deviation of our predictions are close to those of the original observed $y_i$'s, we might be inclined to say that our model has done well.</span>
<span id="cb10-424"><a href="#cb10-424" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(If we're fitting a linear model) Compute the correlation $r$. A large magnitude for the correlation coefficient between the feature and response variables could also indicate that our model has done well.    </span>
<span id="cb10-425"><a href="#cb10-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-426"><a href="#cb10-426" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Performance metrics:</span>
<span id="cb10-427"><a href="#cb10-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-428"><a href="#cb10-428" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>We can take the **Root Mean Squared Error (RMSE)**.</span>
<span id="cb10-429"><a href="#cb10-429" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>It's the square root of the mean squared error (MSE), which is the average loss that we've been minimizing to determine optimal model parameters.</span>
<span id="cb10-430"><a href="#cb10-430" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>RMSE is in the same units as $y$.</span>
<span id="cb10-431"><a href="#cb10-431" aria-hidden="true" tabindex="-1"></a><span class="ss">     - </span>A lower RMSE indicates more "accurate" predictions, as we have a lower "average loss" across the data.</span>
<span id="cb10-432"><a href="#cb10-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-433"><a href="#cb10-433" aria-hidden="true" tabindex="-1"></a>   $$\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$$</span>
<span id="cb10-434"><a href="#cb10-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-435"><a href="#cb10-435" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Visualization:</span>
<span id="cb10-436"><a href="#cb10-436" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Look at the residual plot of $e_i = y_i - \hat{y_i}$ to visualize the difference between actual and predicted values. The good residual plot should not show any pattern between input/features $x_i$ and residual values $e_i$.</span>
<span id="cb10-437"><a href="#cb10-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-438"><a href="#cb10-438" aria-hidden="true" tabindex="-1"></a>To illustrate this process, let's take a look at **Anscombe's quartet**.</span>
<span id="cb10-439"><a href="#cb10-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-440"><a href="#cb10-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Four Mysterious Datasets (Anscombe’s quartet)</span></span>
<span id="cb10-441"><a href="#cb10-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-442"><a href="#cb10-442" aria-hidden="true" tabindex="-1"></a>Let's take a look at four different datasets.</span>
<span id="cb10-443"><a href="#cb10-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-446"><a href="#cb10-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-447"><a href="#cb10-447" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-448"><a href="#cb10-448" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-449"><a href="#cb10-449" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-450"><a href="#cb10-450" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-451"><a href="#cb10-451" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb10-452"><a href="#cb10-452" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb10-453"><a href="#cb10-453" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb10-454"><a href="#cb10-454" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb10-455"><a href="#cb10-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-456"><a href="#cb10-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-459"><a href="#cb10-459" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-460"><a href="#cb10-460" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-461"><a href="#cb10-461" aria-hidden="true" tabindex="-1"></a><span class="co"># Big font helper</span></span>
<span id="cb10-462"><a href="#cb10-462" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adjust_fontsize(size<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-463"><a href="#cb10-463" aria-hidden="true" tabindex="-1"></a>    SMALL_SIZE <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb10-464"><a href="#cb10-464" aria-hidden="true" tabindex="-1"></a>    MEDIUM_SIZE <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-465"><a href="#cb10-465" aria-hidden="true" tabindex="-1"></a>    BIGGER_SIZE <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb10-466"><a href="#cb10-466" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> size <span class="op">!=</span> <span class="va">None</span>:</span>
<span id="cb10-467"><a href="#cb10-467" aria-hidden="true" tabindex="-1"></a>        SMALL_SIZE <span class="op">=</span> MEDIUM_SIZE <span class="op">=</span> BIGGER_SIZE <span class="op">=</span> size</span>
<span id="cb10-468"><a href="#cb10-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-469"><a href="#cb10-469" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"font"</span>, size<span class="op">=</span>SMALL_SIZE)  <span class="co"># controls default text sizes</span></span>
<span id="cb10-470"><a href="#cb10-470" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, titlesize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the axes title</span></span>
<span id="cb10-471"><a href="#cb10-471" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"axes"</span>, labelsize<span class="op">=</span>MEDIUM_SIZE)  <span class="co"># fontsize of the x and y labels</span></span>
<span id="cb10-472"><a href="#cb10-472" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"xtick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb10-473"><a href="#cb10-473" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"ytick"</span>, labelsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># fontsize of the tick labels</span></span>
<span id="cb10-474"><a href="#cb10-474" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"legend"</span>, fontsize<span class="op">=</span>SMALL_SIZE)  <span class="co"># legend fontsize</span></span>
<span id="cb10-475"><a href="#cb10-475" aria-hidden="true" tabindex="-1"></a>    plt.rc(<span class="st">"figure"</span>, titlesize<span class="op">=</span>BIGGER_SIZE)  <span class="co"># fontsize of the figure title</span></span>
<span id="cb10-476"><a href="#cb10-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-477"><a href="#cb10-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-478"><a href="#cb10-478" aria-hidden="true" tabindex="-1"></a><span class="co"># Helper functions</span></span>
<span id="cb10-479"><a href="#cb10-479" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standard_units(x):</span>
<span id="cb10-480"><a href="#cb10-480" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">-</span> np.mean(x)) <span class="op">/</span> np.std(x)</span>
<span id="cb10-481"><a href="#cb10-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-482"><a href="#cb10-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-483"><a href="#cb10-483" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> correlation(x, y):</span>
<span id="cb10-484"><a href="#cb10-484" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(standard_units(x) <span class="op">*</span> standard_units(y))</span>
<span id="cb10-485"><a href="#cb10-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-486"><a href="#cb10-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-487"><a href="#cb10-487" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> slope(x, y):</span>
<span id="cb10-488"><a href="#cb10-488" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correlation(x, y) <span class="op">*</span> np.std(y) <span class="op">/</span> np.std(x)</span>
<span id="cb10-489"><a href="#cb10-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-490"><a href="#cb10-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-491"><a href="#cb10-491" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> intercept(x, y):</span>
<span id="cb10-492"><a href="#cb10-492" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(y) <span class="op">-</span> slope(x, y) <span class="op">*</span> np.mean(x)</span>
<span id="cb10-493"><a href="#cb10-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-494"><a href="#cb10-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-495"><a href="#cb10-495" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_least_squares(x, y):</span>
<span id="cb10-496"><a href="#cb10-496" aria-hidden="true" tabindex="-1"></a>    theta_0 <span class="op">=</span> intercept(x, y)</span>
<span id="cb10-497"><a href="#cb10-497" aria-hidden="true" tabindex="-1"></a>    theta_1 <span class="op">=</span> slope(x, y)</span>
<span id="cb10-498"><a href="#cb10-498" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_0, theta_1</span>
<span id="cb10-499"><a href="#cb10-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-500"><a href="#cb10-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-501"><a href="#cb10-501" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x, theta_0, theta_1):</span>
<span id="cb10-502"><a href="#cb10-502" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_0 <span class="op">+</span> theta_1 <span class="op">*</span> x</span>
<span id="cb10-503"><a href="#cb10-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-504"><a href="#cb10-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-505"><a href="#cb10-505" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mse(y, yhat):</span>
<span id="cb10-506"><a href="#cb10-506" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean((y <span class="op">-</span> yhat) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb10-507"><a href="#cb10-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-508"><a href="#cb10-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-509"><a href="#cb10-509" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span>
<span id="cb10-510"><a href="#cb10-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-511"><a href="#cb10-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-514"><a href="#cb10-514" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-515"><a href="#cb10-515" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"default"</span>)  <span class="co"># Revert style to default mpl</span></span>
<span id="cb10-516"><a href="#cb10-516" aria-hidden="true" tabindex="-1"></a>NO_VIZ, RESID, RESID_SCATTER <span class="op">=</span> <span class="bu">range</span>(<span class="dv">3</span>)</span>
<span id="cb10-517"><a href="#cb10-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-518"><a href="#cb10-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-519"><a href="#cb10-519" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> least_squares_evaluation(x, y, visualize<span class="op">=</span>NO_VIZ):</span>
<span id="cb10-520"><a href="#cb10-520" aria-hidden="true" tabindex="-1"></a>    <span class="co"># statistics</span></span>
<span id="cb10-521"><a href="#cb10-521" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x_mean : </span><span class="sc">{</span>np<span class="sc">.</span>mean(x)<span class="sc">:.2f}</span><span class="ss">, y_mean : </span><span class="sc">{</span>np<span class="sc">.</span>mean(y)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb10-522"><a href="#cb10-522" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"x_stdev: </span><span class="sc">{</span>np<span class="sc">.</span>std(x)<span class="sc">:.2f}</span><span class="ss">, y_stdev: </span><span class="sc">{</span>np<span class="sc">.</span>std(y)<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb10-523"><a href="#cb10-523" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"r = Correlation(x, y): </span><span class="sc">{</span>correlation(x, y)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-524"><a href="#cb10-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-525"><a href="#cb10-525" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Performance metrics</span></span>
<span id="cb10-526"><a href="#cb10-526" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb10-527"><a href="#cb10-527" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb10-528"><a href="#cb10-528" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\t</span><span class="ss">heta_0: </span><span class="sc">{</span>ahat<span class="sc">:.2f}</span><span class="ss">, </span><span class="ch">\t</span><span class="ss">heta_1: </span><span class="sc">{</span>bhat<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb10-529"><a href="#cb10-529" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(compute_mse(y, yhat))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-530"><a href="#cb10-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-531"><a href="#cb10-531" aria-hidden="true" tabindex="-1"></a>    <span class="co"># visualization</span></span>
<span id="cb10-532"><a href="#cb10-532" aria-hidden="true" tabindex="-1"></a>    fig, ax_resid <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb10-533"><a href="#cb10-533" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize <span class="op">==</span> RESID_SCATTER:</span>
<span id="cb10-534"><a href="#cb10-534" aria-hidden="true" tabindex="-1"></a>        fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">3</span>))</span>
<span id="cb10-535"><a href="#cb10-535" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].scatter(x, y)</span>
<span id="cb10-536"><a href="#cb10-536" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].plot(x, yhat)</span>
<span id="cb10-537"><a href="#cb10-537" aria-hidden="true" tabindex="-1"></a>        axs[<span class="dv">0</span>].set_title(<span class="st">"LS fit"</span>)</span>
<span id="cb10-538"><a href="#cb10-538" aria-hidden="true" tabindex="-1"></a>        ax_resid <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb10-539"><a href="#cb10-539" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> visualize <span class="op">==</span> RESID:</span>
<span id="cb10-540"><a href="#cb10-540" aria-hidden="true" tabindex="-1"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb10-541"><a href="#cb10-541" aria-hidden="true" tabindex="-1"></a>        ax_resid <span class="op">=</span> plt.gca()</span>
<span id="cb10-542"><a href="#cb10-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-543"><a href="#cb10-543" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> ax_resid <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-544"><a href="#cb10-544" aria-hidden="true" tabindex="-1"></a>        ax_resid.scatter(x, y <span class="op">-</span> yhat, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb10-545"><a href="#cb10-545" aria-hidden="true" tabindex="-1"></a>        ax_resid.plot([<span class="dv">4</span>, <span class="dv">14</span>], [<span class="dv">0</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">"black"</span>)</span>
<span id="cb10-546"><a href="#cb10-546" aria-hidden="true" tabindex="-1"></a>        ax_resid.set_title(<span class="st">"Residuals"</span>)</span>
<span id="cb10-547"><a href="#cb10-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-548"><a href="#cb10-548" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb10-549"><a href="#cb10-549" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-550"><a href="#cb10-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-553"><a href="#cb10-553" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-554"><a href="#cb10-554" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-555"><a href="#cb10-555" aria-hidden="true" tabindex="-1"></a><span class="co"># Load in four different datasets: I, II, III, IV</span></span>
<span id="cb10-556"><a href="#cb10-556" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">8</span>, <span class="dv">13</span>, <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">14</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">12</span>, <span class="dv">7</span>, <span class="dv">5</span>]</span>
<span id="cb10-557"><a href="#cb10-557" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> [<span class="fl">8.04</span>, <span class="fl">6.95</span>, <span class="fl">7.58</span>, <span class="fl">8.81</span>, <span class="fl">8.33</span>, <span class="fl">9.96</span>, <span class="fl">7.24</span>, <span class="fl">4.26</span>, <span class="fl">10.84</span>, <span class="fl">4.82</span>, <span class="fl">5.68</span>]</span>
<span id="cb10-558"><a href="#cb10-558" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> [<span class="fl">9.14</span>, <span class="fl">8.14</span>, <span class="fl">8.74</span>, <span class="fl">8.77</span>, <span class="fl">9.26</span>, <span class="fl">8.10</span>, <span class="fl">6.13</span>, <span class="fl">3.10</span>, <span class="fl">9.13</span>, <span class="fl">7.26</span>, <span class="fl">4.74</span>]</span>
<span id="cb10-559"><a href="#cb10-559" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> [<span class="fl">7.46</span>, <span class="fl">6.77</span>, <span class="fl">12.74</span>, <span class="fl">7.11</span>, <span class="fl">7.81</span>, <span class="fl">8.84</span>, <span class="fl">6.08</span>, <span class="fl">5.39</span>, <span class="fl">8.15</span>, <span class="fl">6.42</span>, <span class="fl">5.73</span>]</span>
<span id="cb10-560"><a href="#cb10-560" aria-hidden="true" tabindex="-1"></a>x4 <span class="op">=</span> [<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">19</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">8</span>]</span>
<span id="cb10-561"><a href="#cb10-561" aria-hidden="true" tabindex="-1"></a>y4 <span class="op">=</span> [<span class="fl">6.58</span>, <span class="fl">5.76</span>, <span class="fl">7.71</span>, <span class="fl">8.84</span>, <span class="fl">8.47</span>, <span class="fl">7.04</span>, <span class="fl">5.25</span>, <span class="fl">12.50</span>, <span class="fl">5.56</span>, <span class="fl">7.91</span>, <span class="fl">6.89</span>]</span>
<span id="cb10-562"><a href="#cb10-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-563"><a href="#cb10-563" aria-hidden="true" tabindex="-1"></a>anscombe <span class="op">=</span> {</span>
<span id="cb10-564"><a href="#cb10-564" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y1)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb10-565"><a href="#cb10-565" aria-hidden="true" tabindex="-1"></a>    <span class="st">"II"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y2)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb10-566"><a href="#cb10-566" aria-hidden="true" tabindex="-1"></a>    <span class="st">"III"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x, y3)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb10-567"><a href="#cb10-567" aria-hidden="true" tabindex="-1"></a>    <span class="st">"IV"</span>: pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(x4, y4)), columns<span class="op">=</span>[<span class="st">"x"</span>, <span class="st">"y"</span>]),</span>
<span id="cb10-568"><a href="#cb10-568" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb10-569"><a href="#cb10-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-570"><a href="#cb10-570" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the scatter plot and line of best fit</span></span>
<span id="cb10-571"><a href="#cb10-571" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb10-572"><a href="#cb10-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-573"><a href="#cb10-573" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, dataset <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]):</span>
<span id="cb10-574"><a href="#cb10-574" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb10-575"><a href="#cb10-575" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>]</span>
<span id="cb10-576"><a href="#cb10-576" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb10-577"><a href="#cb10-577" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb10-578"><a href="#cb10-578" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].scatter(x, y, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">"red"</span>)  <span class="co"># plot the x, y points</span></span>
<span id="cb10-579"><a href="#cb10-579" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].plot(x, yhat)  <span class="co"># plot the line of best fit</span></span>
<span id="cb10-580"><a href="#cb10-580" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_xlabel(<span class="ss">f"$x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb10-581"><a href="#cb10-581" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_ylabel(<span class="ss">f"$y_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb10-582"><a href="#cb10-582" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_title(<span class="ss">f"Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-583"><a href="#cb10-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-584"><a href="#cb10-584" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-585"><a href="#cb10-585" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-586"><a href="#cb10-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-587"><a href="#cb10-587" aria-hidden="true" tabindex="-1"></a>While these four sets of datapoints look very different, they actually all have identical means $\bar x$, $\bar y$, standard deviations $\sigma_x$, $\sigma_y$, correlation $r$, and RMSE! If we only look at these statistics, we would probably be inclined to say that these datasets are similar.</span>
<span id="cb10-588"><a href="#cb10-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-591"><a href="#cb10-591" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-592"><a href="#cb10-592" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-593"><a href="#cb10-593" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dataset <span class="kw">in</span> [<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]:</span>
<span id="cb10-594"><a href="#cb10-594" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"&gt;&gt;&gt; Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb10-595"><a href="#cb10-595" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb10-596"><a href="#cb10-596" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> least_squares_evaluation(ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>], visualize<span class="op">=</span>NO_VIZ)</span>
<span id="cb10-597"><a href="#cb10-597" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb10-598"><a href="#cb10-598" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb10-599"><a href="#cb10-599" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb10-600"><a href="#cb10-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-601"><a href="#cb10-601" aria-hidden="true" tabindex="-1"></a>We may also wish to visualize the model's **residuals**, defined as the difference between the observed and predicted $y_i$ value ($e_i = y_i - \hat{y}_i$). This gives a high-level view of how "off" each prediction is from the true observed value. Recall that you explored this concept in <span class="co">[</span><span class="ot">Data 8</span><span class="co">](https://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity)</span>: a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe's quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points.</span>
<span id="cb10-602"><a href="#cb10-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-603"><a href="#cb10-603" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- &lt;img src="images/residual.png" alt='residual' width='600'&gt; --&gt;</span></span>
<span id="cb10-604"><a href="#cb10-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-607"><a href="#cb10-607" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb10-608"><a href="#cb10-608" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb10-609"><a href="#cb10-609" aria-hidden="true" tabindex="-1"></a><span class="co"># Residual visualization</span></span>
<span id="cb10-610"><a href="#cb10-610" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb10-611"><a href="#cb10-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-612"><a href="#cb10-612" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, dataset <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="st">"I"</span>, <span class="st">"II"</span>, <span class="st">"III"</span>, <span class="st">"IV"</span>]):</span>
<span id="cb10-613"><a href="#cb10-613" aria-hidden="true" tabindex="-1"></a>    ans <span class="op">=</span> anscombe[dataset]</span>
<span id="cb10-614"><a href="#cb10-614" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> ans[<span class="st">"x"</span>], ans[<span class="st">"y"</span>]</span>
<span id="cb10-615"><a href="#cb10-615" aria-hidden="true" tabindex="-1"></a>    ahat, bhat <span class="op">=</span> fit_least_squares(x, y)</span>
<span id="cb10-616"><a href="#cb10-616" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> predict(x, ahat, bhat)</span>
<span id="cb10-617"><a href="#cb10-617" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].scatter(</span>
<span id="cb10-618"><a href="#cb10-618" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">-</span> yhat, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">"red"</span></span>
<span id="cb10-619"><a href="#cb10-619" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># plot the x, y points</span></span>
<span id="cb10-620"><a href="#cb10-620" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].plot(</span>
<span id="cb10-621"><a href="#cb10-621" aria-hidden="true" tabindex="-1"></a>        x, np.zeros_like(x), color<span class="op">=</span><span class="st">"black"</span></span>
<span id="cb10-622"><a href="#cb10-622" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># plot the residual line</span></span>
<span id="cb10-623"><a href="#cb10-623" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_xlabel(<span class="ss">f"$x_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb10-624"><a href="#cb10-624" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_ylabel(<span class="ss">f"$e_</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb10-625"><a href="#cb10-625" aria-hidden="true" tabindex="-1"></a>    axs[i <span class="op">//</span> <span class="dv">2</span>, i <span class="op">%</span> <span class="dv">2</span>].set_title(<span class="ss">f"Dataset </span><span class="sc">{</span>dataset<span class="sc">}</span><span class="ss"> Residuals"</span>)</span>
<span id="cb10-626"><a href="#cb10-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-627"><a href="#cb10-627" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-628"><a href="#cb10-628" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>