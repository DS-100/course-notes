<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Ordinary Least Squares</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="ols_files/libs/clipboard/clipboard.min.js"></script>
<script src="ols_files/libs/quarto-html/quarto.js"></script>
<script src="ols_files/libs/quarto-html/popper.min.js"></script>
<script src="ols_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="ols_files/libs/quarto-html/anchor.min.js"></script>
<link href="ols_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="ols_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="ols_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="ols_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="ols_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Ordinary Least Squares</h2>
   
  <ul>
  <li><a href="#linearity" id="toc-linearity" class="nav-link active" data-scroll-target="#linearity">Linearity</a></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression">Multiple Linear Regression</a></li>
  <li><a href="#linear-algebra-approach" id="toc-linear-algebra-approach" class="nav-link" data-scroll-target="#linear-algebra-approach">Linear Algebra Approach</a></li>
  <li><a href="#geometric-perspective" id="toc-geometric-perspective" class="nav-link" data-scroll-target="#geometric-perspective">Geometric Perspective</a></li>
  <li><a href="#evaluating-model-performance" id="toc-evaluating-model-performance" class="nav-link" data-scroll-target="#evaluating-model-performance">Evaluating Model Performance</a></li>
  <li><a href="#ols-properties" id="toc-ols-properties" class="nav-link" data-scroll-target="#ols-properties">OLS Properties</a></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Ordinary Least Squares</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Define linearity with respect to a vector of parameters <span class="math inline">\(\theta\)</span></li>
<li>Understand the use of matrix notation to express multiple linear regression</li>
<li>Interpret ordinary least squares as the minimization of the norm of the residual vector</li>
<li>Compute performance metrics for multiple linear regression</li>
</ul>
</div>
</div>
</div>
<p>We’ve now spent a number of lectures exploring how to build effective models – we introduced the SLR and constant models, selected cost functions to suit our modeling task, and applied transformations to improve the linear fit.</p>
<p>Throughout all of this, we considered models of one feature (<span class="math inline">\(\hat{y}_i = \theta_0 + \theta_1 x_i\)</span>) or zero features (<span class="math inline">\(\hat{y}_i = \theta\)</span>). As data scientists, we usually have access to datasets containing <em>many</em> features. To make the best models we can, it will be beneficial to consider all of the variables available to us as inputs to a model, rather than just one. In today’s lecture, we’ll introduce <strong>multiple Linear regression</strong> as a framework to incorporate multiple features into a model. We will also learn how to accelerate the modeling process – specifically, we’ll see how linear algebra offers us a powerful set of tools for understanding model performance.</p>
<section id="linearity" class="level2">
<h2 class="anchored" data-anchor-id="linearity">Linearity</h2>
<p>An expression is <strong>linear in <span class="math inline">\(\theta\)</span></strong> (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms: a <strong>vector of <span class="math inline">\(\theta\)</span></strong> s, and a matrix/vector <strong>not involving <span class="math inline">\(\theta\)</span></strong>.</p>
<p>Example: <span class="math inline">\(\theta = [\theta_1, \theta_2, ... \theta_p]\)</span></p>
<ol type="1">
<li>Linear in theta: <span class="math inline">\(\hat{y} = \theta_0 + 2\theta_1 + 3\theta_2\)</span></li>
</ol>
<p><span class="math display">\[\hat{y} = \begin{bmatrix} 1 \space 2 \space 3 \end{bmatrix} \begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \end{bmatrix}\]</span></p>
<ol start="2" type="1">
<li>Not linear in theta: <span class="math inline">\(\hat{y} = \theta_0\theta_1 + 2\theta_1^2 + 3log(\theta_2)\)</span></li>
</ol>
</section>
<section id="multiple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="multiple-linear-regression">Multiple Linear Regression</h2>
<p>Multiple Linear regression is an extension of simple linear regression that adds additional features into the model. Say we collect information on several variables when making an observation. For example, we may record the age, height, and weekly hours of sleep for a student in Data 100. This single observation now contains data for multiple features. To accommodate for the fact that we now consider several feature variables, we’ll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of <span class="math inline">\(p\)</span> features.</p>
<p><img src="images/observation.png" alt="observation" width="600"></p>
<p>The multiple Linear regression model takes the form:</p>
<p><span class="math display">\[\hat{y}_i = \theta_0\:+\:\theta_1x_{i1}\:+\:\theta_2 x_{i2}\:+\:...\:+\:\theta_p x_{ip}\]</span></p>
<p>Our <span class="math inline">\(i\)</span>th prediction, <span class="math inline">\(\hat{y}_i\)</span>, is a linear combination of the parameters, <span class="math inline">\(\theta_i\)</span>. Because we are now dealing with many parameter values, we’ll collect them all into a <strong>parameter vector</strong> with dimensions <span class="math inline">\((p+1) \times 1\)</span> to keep things tidy.</p>
<p><span class="math display">\[\theta = \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix}\]</span></p>
<p>We are now working with two vectors: a row vector representing the observed data, and a column vector containing the model parameters. The multiple Linear regression model given above is <strong>equivalent to the dot (scalar) product of the observation vector and parameter vector</strong>.</p>
<p><span class="math display">\[[1,\:x_{i1},\:x_{i2},\:x_{i3},\:...,\:x_{ip}] \theta = [1,\:x_{i1},\:x_{i2},\:x_{i3},\:...,\:x_{ip}] \begin{bmatrix}
           \theta_{0} \\
           \theta_{1} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix} = \theta_0\:+\:\theta_1x_{i1}\:+\:\theta_2 x_{i2}\:+\:...\:+\:\theta_p x_{ip}\]</span></p>
<p>Notice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with <span class="math inline">\(\theta_0\)</span> to give the intercept of the regression model. We call this 1 entry the <strong>intercept</strong> or <strong>bias</strong> term.</p>
</section>
<section id="linear-algebra-approach" class="level2">
<h2 class="anchored" data-anchor-id="linear-algebra-approach">Linear Algebra Approach</h2>
<p>We now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple Linear regression. We want to use the tools of linear algebra.</p>
<p>Let’s think carefully about what we did to generate the single prediction above. To make a prediction from the first observation in the data, we took the scalar product of the parameter vector and first observation vector. To make a prediction from the <em>second</em> observation, we would repeat this process to find the scalar product of the parameter vector and the <em>second</em> observation vector. If we wanted to find the model predictions for each observation in the dataset, we’d repeat this process for all <span class="math inline">\(n\)</span> observations in the data.</p>
<p><span class="math display">\[\hat{y}_1 = [1,\:x_{11},\:x_{12},\:x_{13},\:...,\:x_{1p}] \theta\]</span> <span class="math display">\[\hat{y}_2 = [1,\:x_{21},\:x_{22},\:x_{23},\:...,\:x_{2p}] \theta\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[\hat{y}_n = [1,\:x_{n1},\:x_{n2},\:x_{n3},\:...,\:x_{np}] \theta\]</span></p>
<p>Our observed data is represented by <span class="math inline">\(n\)</span> row vectors, each with dimension <span class="math inline">\((p+1)\)</span>. We can collect them all into a single matrix, which we call <span class="math inline">\(\mathbb{X}\)</span>.</p>
<p><img src="images/design_matrix.png" alt="design_matrix" width="400"></p>
<p>The matrix <span class="math inline">\(\mathbb{X}\)</span> is known as the <strong>design matrix</strong>. It contains all observed data for each of our <span class="math inline">\(p\)</span> features. It often (but not always) contains an additional column of all ones to represent the <strong>intercept</strong> or <strong>bias column</strong>.</p>
<p>To review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.</p>
<p><img src="images/row_col.png" alt="row_col" width="600"></p>
<p>The multiple Linear regression model can then be restated in terms of matrices: <span class="math display">\[\mathbb{\hat{Y}} = \mathbb{X} \theta\]</span></p>
<p>Here, <span class="math inline">\(\mathbb{\hat{Y}}\)</span> is the <strong>prediction vector</strong> with dimensions <span class="math inline">\((n \times 1)\)</span>. It contains the prediction made by the model for each of <span class="math inline">\(n\)</span> input observations.</p>
<p>We now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of cost functions and model fitting.</p>
<p>Recall our definition of MSE: <span class="math display">\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</span></p>
<p>At its heart, the MSE is a measure of <em>distance</em> – it gives an indication of how “far away” the predictions are from the true values, on average.</p>
<p>When working with vectors, this idea of “distance” is represented by the <strong>norm</strong>. More precisely, the distance between two vectors <span class="math inline">\(\vec{a}\)</span> and <span class="math inline">\(\vec{b}\)</span> can be expressed as: <span class="math display">\[||\vec{a} - \vec{b}||_2 = \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \ldots + (a_n - b_n)^2} = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}\]</span></p>
<p>The double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.</p>
<p>Looks pretty familiar! We can rewrite the MSE to express it as a squared L2 norm in terms of the prediction vector, <span class="math inline">\(\hat{\mathbb{Y}}\)</span>, and true target vector, <span class="math inline">\(\mathbb{Y}\)</span>:</p>
<p><span class="math display">\[R(\theta) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} ||\mathbb{Y} - \hat{\mathbb{Y}}||_2^2\]</span></p>
<p>Here, the superscript 2 outside of the norm double bars means that we are <em>squaring</em> the norm. If we plug in our linear model <span class="math inline">\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)</span>, we find the MSE cost function in vector notation:</p>
<p><span class="math display">\[R(\theta) = \frac{1}{n} ||\mathbb{Y} - \mathbb{X} \theta||_2^2\]</span></p>
<p>Under the linear algebra perspective, our new task is to fit the optimal parameter vector <span class="math inline">\(\theta\)</span> such that the cost function is minimized. Equivalently, we wish to minimize the norm <span class="math display">\[||\mathbb{Y} - \mathbb{X} \theta||_2 = ||\mathbb{Y} - \hat{\mathbb{Y}}||_2\]</span>.</p>
<p>We can restate this goal in two ways:</p>
<ul>
<li>Minimize the <em>distance</em> between the vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, and the vector of predicted values, <span class="math inline">\(\mathbb{\hat{Y}}\)</span></li>
<li>Minimize the <em>length</em> of the <strong>residual vector</strong>, defined as: <span class="math display">\[e = \mathbb{Y} - \mathbb{\hat{Y}} = \begin{bmatrix}
         y_1 - \hat{y}_1 \\
         y_2 - \hat{y}_2 \\
         \vdots \\
         y_n - \hat{y}_n
       \end{bmatrix}\]</span></li>
</ul>
</section>
<section id="geometric-perspective" class="level2">
<h2 class="anchored" data-anchor-id="geometric-perspective">Geometric Perspective</h2>
<p>To derive the best parameter vector to meet this goal, we can turn to the geometric properties of our modeling set-up.</p>
<p>Up until now, we’ve mostly thought of our model as a scalar product between horizontally-stacked observations and the parameter vector. We can also think of <span class="math inline">\(\hat{\mathbb{Y}}\)</span> as a <strong>linear combination of feature vectors</strong>, scaled by the parameters. We use the notation <span class="math inline">\(\mathbb{X}_{:, i}\)</span> to denote the <span class="math inline">\(i\)</span>th column of the design matrix. You can think of this as following the same convention as used when calling <code>.iloc</code> and <code>.loc</code>. “:” means that we are taking all entries in the <span class="math inline">\(i\)</span>th column.</p>
<p><img src="images/columns.png" alt="columns" width="500"></p>
<p><span class="math display">\[
\hat{\mathbb{Y}} =
\theta_0 \begin{bmatrix}
           1 \\
           1 \\
           \vdots \\
           1
         \end{bmatrix} + \theta_1 \begin{bmatrix}
           x_{11} \\
           x_{21} \\
           \vdots \\
           x_{n1}
         \end{bmatrix} + \ldots + \theta_p \begin{bmatrix}
           x_{1p} \\
           x_{2p} \\
           \vdots \\
           x_{np}
         \end{bmatrix}
         = \theta_0 \mathbb{X}_{:,\:1} + \theta_1 \mathbb{X}_{:,\:2} + \ldots + \theta_p \mathbb{X}_{:,\:p+1}\]</span></p>
<p>This new approach is useful because it allows us to take advantage of the properties of linear combinations.</p>
<p>Recall that the <strong>span</strong> or <strong>column space</strong> of a matrix is the set of all possible linear combinations of the matrix’s columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns.</p>
<p>Because the prediction vector, <span class="math inline">\(\hat{\mathbb{Y}} = \mathbb{X} \theta\)</span>, is a linear combination of the columns of <span class="math inline">\(\mathbb{X}\)</span>, we know that the <strong>predictions are contained in the span of <span class="math inline">\(\mathbb{X}\)</span></strong>. That is, we know that <span class="math inline">\(\mathbb{\hat{Y}} \in \text{Span}(\mathbb{X})\)</span>.</p>
<p>The diagram below is a simplified view of <span class="math inline">\(\text{Span}(\mathbb{X})\)</span>, assuminh that each column of <span class="math inline">\(\mathbb{X}\)</span> has length <span class="math inline">\(n\)</span>. Notice that the columns of <span class="math inline">\(\mathbb{X}\)</span> define a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, where each point in the subspace can be reached by a linear combination of <span class="math inline">\(\mathbb{X}\)</span>’s columns. The prediction vector <span class="math inline">\(\mathbb{\hat{Y}}\)</span> lies somewhere in this subspace.</p>
<p><img src="images/span.png" alt="span" width="600"></p>
<p>Examining this diagram, we find a problem. The vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, could theoretically lie <em>anywhere</em> in <span class="math inline">\(\mathbb{R}^n\)</span> space – its exact location depends on the data we collect out in the real world. However, our multiple Linear regression model can only make predictions in the subspace of <span class="math inline">\(\mathbb{R}^n\)</span> spanned by <span class="math inline">\(\mathbb{X}\)</span>. Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, <span class="math inline">\(\mathbb{Y}\)</span>, and the vector of predicted values, <span class="math inline">\(\mathbb{\hat{Y}}\)</span>, is minimized. This means that <strong>we want <span class="math inline">\(\mathbb{\hat{Y}}\)</span> to be the vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span> that is closest to <span class="math inline">\(\mathbb{Y}\)</span></strong>.</p>
<p>Another way of rephrasing this goal is to say that we wish to minimize the length of the residual vector <span class="math inline">\(e\)</span>, as measured by its <span class="math inline">\(L_2\)</span> norm.</p>
<p><img src="images/residual.png" alt="residual" width="600"></p>
<p>The vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span> that is closest to <span class="math inline">\(\mathbb{Y}\)</span> is always the <strong>orthogonal projection</strong> of <span class="math inline">\(\mathbb{Y}\)</span> onto <span class="math inline">\(\text{Span}(\mathbb{X})\)</span>. Thus, we should choose the parameter vector <span class="math inline">\(\theta\)</span> that makes the <strong>residual vector orthogonal to any vector in <span class="math inline">\(\text{Span}(\mathbb{X})\)</span></strong>. You can visualize this as the vector created by dropping a perpendicular line from <span class="math inline">\(\mathbb{Y}\)</span> onto the span of <span class="math inline">\(\mathbb{X}\)</span>.</p>
<p>How does this help us identify the optimal parameter vector, <span class="math inline">\(\hat{\theta}\)</span>? Recall that two vectors are orthogonal if their dot product is zero. A vector <span class="math inline">\(\vec{v}\)</span> is orthogonal to the span of a matrix <span class="math inline">\(M\)</span> if <span class="math inline">\(\vec{v}\)</span> is orthogonal to each column in <span class="math inline">\(M\)</span>. Put together, a vector <span class="math inline">\(\vec{v}\)</span> is orthogonal to <span class="math inline">\(\text{Span}(M)\)</span> if:</p>
<p><span class="math display">\[M^T \vec{v} = \vec{0}\]</span></p>
<p>Because our goal is to find <span class="math inline">\(\hat{\theta}\)</span> such that the residual vector <span class="math inline">\(e = \mathbb{Y} - \mathbb{X} \theta\)</span> is orthogonal to <span class="math inline">\(\text{Span}(\mathbb{X})\)</span>, we can write:</p>
<p><span class="math display">\[\mathbb{X}^T e = \vec{0}\]</span> <span class="math display">\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \vec{0}\]</span> <span class="math display">\[\mathbb{X}^T \mathbb{Y} - \mathbb{X}^T \mathbb{X} \hat{\theta} = \vec{0}\]</span> <span class="math display">\[\mathbb{X}^T \mathbb{X} \hat{\theta} = \mathbb{X}^T \mathbb{Y}\]</span></p>
<p>This last line is known as the <strong>normal equation</strong>. Any vector <span class="math inline">\(\theta\)</span> that minimizes MSE on a dataset must satisfy this equation.</p>
<p>If <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is invertible, we can conclude: <span class="math display">\[\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1} \mathbb{X}^T \mathbb{Y}\]</span></p>
<p>This is called the <strong>least squares estimate</strong> of <span class="math inline">\(\theta\)</span>: it is the value of <span class="math inline">\(\theta\)</span> that minimizes the squared loss.</p>
<p>Note that the least squares estimate was derived under the assumption that <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is <em>invertible</em>. This condition holds true when <span class="math inline">\(\mathbb{X}^T \mathbb{X}\)</span> is full column rank, which, in turn, happens when <span class="math inline">\(\mathbb{X}\)</span> is full column rank. We will explore the consequences of this fact in lab and homework.</p>
</section>
<section id="evaluating-model-performance" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-model-performance">Evaluating Model Performance</h2>
<p>Our geometric view of multiple Linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features.</p>
<p>Now, we want to understand how well our fitted model performs. One measure of model performance is the <strong>Root Mean Squared Error</strong>, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of <span class="math inline">\(y_i\)</span>, which is useful for understanding the model’s performance. A low RMSE indicates more “accurate” predictions – that there is lower average loss across the dataset.</p>
<p><span class="math display">\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}\]</span></p>
<p>When working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple Linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple Linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple Linear model performs well if its residual plot shows no patterns.</p>
<p><img src="images/residual_plot.png" alt="residual_plot" width="500"></p>
<p>For SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple Linear setting, we will need a performance metric that can account for multiple features at once. <strong>Multiple <span class="math inline">\(R^2\)</span></strong>, also called the <strong>coefficient of determination</strong>, is the ratio of the variance of the predicted values <span class="math inline">\(\hat{y}_i\)</span> to the variance of the true values <span class="math inline">\(y_i\)</span>. It can be interpreted as the <em>proportion</em> of variance in the observations that is explained by the model.</p>
<p><span class="math display">\[R^2 = \frac{\text{variance of } \hat{y}_i}{\text{variance of } y_i} = \frac{\sigma^2_{\hat{y}}}{\sigma^2_y}\]</span></p>
<p>As we add more features, our fitted values tend to become closer and closer to our actual values. Thus, <span class="math inline">\(\mathbb{R}^2\)</span> increases.</p>
</section>
<section id="ols-properties" class="level2">
<h2 class="anchored" data-anchor-id="ols-properties">OLS Properties</h2>
<ol type="1">
<li>When using the optimal parameter vector, our residuals $e = - $ are orthogonal to <span class="math inline">\(span(\mathbb{X})\)</span></li>
</ol>
<p><span class="math display">\[\mathbb{X}^Te = 0 \]</span></p>
<div class="callout callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<p>The optimal parameter vector, <span class="math inline">\(\hat{\theta}\)</span>, solves the normal equations <span class="math inline">\(\implies \hat{\theta} = \mathbb{X}^T\mathbb{X}^{-1}\mathbb{X}^T\mathbb{Y}\)</span></p>
<p><span class="math display">\[\mathbb{X}^Te = \mathbb{X}^T (\mathbb{Y} - \mathbb{\hat{Y}}) \]</span></p>
<p><span class="math display">\[\mathbb{X}^T (\mathbb{Y} - \mathbb{X}\hat{\theta}) = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{X}\hat{\theta}\]</span></p>
<p>Any matrix multiplied with its own inverse is the identity matrix <span class="math inline">\(\mathbb{I}\)</span></p>
<p><span class="math display">\[\mathbb{X}^T\mathbb{Y} - (\mathbb{X}^T\mathbb{X})(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y} = \mathbb{X}^T\mathbb{Y} - \mathbb{X}^T\mathbb{Y} = 0\]</span></p>
</div>
</div>
</div>
<ol start="2" type="1">
<li>For all linear models <em>with an intercept term</em>, the sum of residuals is zero.</li>
</ol>
<p><span class="math display">\[\sum_i^n e_i = 0\]</span></p>
<div class="callout callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<p>For all linear models <em>with an intercept term</em>, the average of the predicted <span class="math inline">\(y\)</span> values is equal to the average of the true <span class="math inline">\(y\)</span> values.</p>
<p><span class="math display">\[\bar{y} = \bar{\hat{y}}\]</span></p>
<p>Rewriting the sum of residuals as two separate sums,</p>
<p><span class="math display">\[\sum_i^n e_i = \sum_i^n y_i - \sum_i^n\hat{y}_i\]</span></p>
<p>Each respective sum is a multiple of the average of the sum.</p>
<p><span class="math display">\[\sum_i^n e_i = n\bar{y} - n\bar{y} = n(\bar{y} - \bar{y}) = 0\]</span></p>
</div>
</div>
</div>
<ol start="3" type="1">
<li>The Least Squares estimate <span class="math inline">\(\hat{\theta}\)</span> is unique if and only if <span class="math inline">\(\mathbb{X}\)</span> is full column rank.</li>
</ol>
<div class="callout callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Proof:</p>
<p>We know the solution to the normal equation <span class="math inline">\(\mathbb{X}^T\mathbb{X}\hat{\theta} = \mathbb{Y}\)</span> is the least square estimate that fulfills the prior equality.</p>
<p><span class="math inline">\(\hat{\theta}\)</span> has a unique solution <span class="math inline">\(\iff\)</span> the square matrix <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> is invertible.</p>
<p>The rank of a square matrix is the maximum number of of linearly independent columns it contains. <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has shape <span class="math inline">\((p + 1) \times (p + 1)\)</span>, and therefore has max rank p + 1.</p>
<p><span class="math inline">\(rank(\mathbb{X}^T\mathbb{X})\)</span> = <span class="math inline">\(rank(\mathbb{X})\)</span> (proof out of scope).</p>
<p>Therefore <span class="math inline">\(\mathbb{X}^T\mathbb{X}\)</span> has rank p + 1 <span class="math inline">\(\iff\)</span> <span class="math inline">\(\mathbb{X}\)</span> has rank p + 1 <span class="math inline">\(\iff \mathbb{X}\)</span> is full column rank.</p>
</div>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>