{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463bc03e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Ordinary Least Squares (from Spring 2025)\n",
    "execute:\n",
    "  echo: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-title: Ordinary Least Squares\n",
    "    page-layout: full\n",
    "    theme:\n",
    "      - cosmo\n",
    "      - cerulean\n",
    "    callout-icon: false\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "::: {.callout-note collapse=\"false\"}\n",
    "## Learning Outcomes\n",
    "* Define linearity with respect to a vector of parameters $\\theta$.\n",
    "* Understand the use of matrix notation to express multiple linear regression.\n",
    "* Interpret ordinary least squares as the minimization of the norm of the residual vector.\n",
    "* Compute performance metrics for multiple linear regression.\n",
    ":::\n",
    "\n",
    "\n",
    "We've now spent a number of lectures exploring how to build effective models – we introduced the SLR and constant models, selected cost functions to suit our modeling task, and applied transformations to improve the linear fit.\n",
    "\n",
    "Throughout all of this, we considered models of one feature ($\\hat{y}_i = \\theta_0 + \\theta_1 x_i$) or zero features ($\\hat{y}_i = \\theta_0$). As data scientists, we usually have access to datasets containing *many* features. To make the best models we can, it will be beneficial to consider all of the variables available to us as inputs to a model, rather than just one. In today's lecture, we'll introduce **multiple linear regression** as a framework to incorporate multiple features into a model. We will also learn how to accelerate the modeling process – specifically, we'll see how linear algebra offers us a powerful set of tools for understanding model performance.\n",
    "\n",
    "## OLS Problem Formulation\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:\n",
    "\n",
    "$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:...\\:+\\:\\theta_p x_{p}$$\n",
    "\n",
    "Our predicted value of $y$, $\\hat{y}$, is a linear combination of the single **observations** (features), $x_i$, and the parameters, $\\theta_i$. \n",
    "\n",
    "We can explore this idea further by looking at a dataset containing aggregate per-player data from the 2018-19 NBA season, downloaded from [Kaggle](https://www.kaggle.com/schmadam97/nba-regular-season-stats-20182019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61b67b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "import pandas as pd\n",
    "nba = pd.read_csv('data/nba18-19.csv', index_col=0)\n",
    "nba.index.name = None # Drops name of index (players are ordered by rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28eb70ad",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>FG%</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Álex Abrines\\abrinal01</td>\n",
       "      <td>SG</td>\n",
       "      <td>25</td>\n",
       "      <td>OKC</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quincy Acy\\acyqu01</td>\n",
       "      <td>PF</td>\n",
       "      <td>28</td>\n",
       "      <td>PHO</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jaylen Adams\\adamsja01</td>\n",
       "      <td>PG</td>\n",
       "      <td>22</td>\n",
       "      <td>ATL</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steven Adams\\adamsst01</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>OKC</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>33.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bam Adebayo\\adebaba01</td>\n",
       "      <td>C</td>\n",
       "      <td>21</td>\n",
       "      <td>MIA</td>\n",
       "      <td>82</td>\n",
       "      <td>28</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Player Pos  Age   Tm   G  GS    MP   FG   FGA    FG%  ...  \\\n",
       "1  Álex Abrines\\abrinal01  SG   25  OKC  31   2  19.0  1.8   5.1  0.357  ...   \n",
       "2      Quincy Acy\\acyqu01  PF   28  PHO  10   0  12.3  0.4   1.8  0.222  ...   \n",
       "3  Jaylen Adams\\adamsja01  PG   22  ATL  34   1  12.6  1.1   3.2  0.345  ...   \n",
       "4  Steven Adams\\adamsst01   C   25  OKC  80  80  33.4  6.0  10.1  0.595  ...   \n",
       "5   Bam Adebayo\\adebaba01   C   21  MIA  82  28  23.3  3.4   5.9  0.576  ...   \n",
       "\n",
       "     FT%  ORB  DRB  TRB  AST  STL  BLK  TOV   PF   PTS  \n",
       "1  0.923  0.2  1.4  1.5  0.6  0.5  0.2  0.5  1.7   5.3  \n",
       "2  0.700  0.3  2.2  2.5  0.8  0.1  0.4  0.4  2.4   1.7  \n",
       "3  0.778  0.3  1.4  1.8  1.9  0.4  0.1  0.8  1.3   3.2  \n",
       "4  0.500  4.9  4.6  9.5  1.6  1.5  1.0  1.7  2.6  13.9  \n",
       "5  0.735  2.0  5.3  7.3  2.2  0.9  0.8  1.5  2.5   8.9  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40761787",
   "metadata": {},
   "source": [
    "Let's say we are interested in predicting the number of points (`PTS`) an athlete will score in a basketball game this season.\n",
    "\n",
    "Suppose we want to fit a linear model by using some characteristics, or **features** of a player. Specifically, we'll focus on field goals, assists, and 3-point attempts.\n",
    "\n",
    "* `FG`, the average number of (2-point) field goals per game\n",
    "* `AST`, the average number of assists per game\n",
    "* `3PA`, the average number of 3-point field goals attempted per game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb86c42",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG</th>\n",
       "      <th>AST</th>\n",
       "      <th>3PA</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FG  AST  3PA   PTS\n",
       "1  1.8  0.6  4.1   5.3\n",
       "2  0.4  0.8  1.5   1.7\n",
       "3  1.1  1.9  2.2   3.2\n",
       "4  6.0  1.6  0.0  13.9\n",
       "5  3.4  2.2  0.2   8.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba[['FG', 'AST', '3PA', 'PTS']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b502eec",
   "metadata": {},
   "source": [
    "Because we are now dealing with many parameter values, we've collected them all into a **parameter vector** with dimensions $(p+1) \\times 1$ to keep things tidy. Remember that $p$ represents the number of features we have (in this case, 3).\n",
    "\n",
    "$$\\theta = \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{p}\n",
    "         \\end{bmatrix}$$\n",
    "\n",
    "We are working with two vectors here: a row vector representing the observed data, and a column vector containing the model parameters. The multiple linear regression model is **equivalent to the dot (scalar) product of the observation vector and parameter vector**. \n",
    "\n",
    "$$[1,\\:x_{1},\\:x_{2},\\:x_{3},\\:...,\\:x_{p}] \\theta = [1,\\:x_{1},\\:x_{2},\\:x_{3},\\:...,\\:x_{p}] \\begin{bmatrix}\n",
    "           \\theta_{0} \\\\\n",
    "           \\theta_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{p}\n",
    "         \\end{bmatrix} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:...\\:+\\:\\theta_p x_{p}$$\n",
    "\n",
    "Notice that we have inserted 1 as the first value in the observation vector. When the dot product is computed, this 1 will be multiplied with $\\theta_0$ to give the intercept of the regression model. We call this 1 entry the **intercept** or **bias** term.\n",
    "\n",
    "Given that we have three features here, we can express this model as:\n",
    "$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:\\theta_3 x_{3}$$\n",
    "\n",
    "Our features are represented by $x_1$ (`FG`), $x_2$ (`AST`), and $x_3$ (`3PA`) with each having correpsonding parameters, $\\theta_1$, $\\theta_2$, and  $\\theta_3$.\n",
    "\n",
    "In statistics, this model + loss is called **Ordinary Least Squares (OLS)**. The solution to OLS is the minimizing loss for parameters $\\hat{\\theta}$, also called the **least squares estimate**.\n",
    "\n",
    "### Linear Algebra Approach\n",
    "\n",
    "::: {.callout-tip collapse=\"false\"}\n",
    "#### Linear Algebra Review: Vector Dot Product\n",
    "The **dot product (or inner product)** is a vector operation that:\n",
    "\n",
    "* Can only be carried out on two vectors of the **same length**\n",
    "* Sums up the products of the corresponding entries of the two vectors\n",
    "* Returns a single number\n",
    "\n",
    "For example, let\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{u} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3\\end{bmatrix}, \\vec{v} = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The dot product between $\\vec{u}$ and $\\vec{v}$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{u} \\cdot \\vec{v} &= \\vec{u}^T \\vec{v} = \\vec{v}^T \\vec{u} \\\\\n",
    "  &= 1 \\cdot 1 + 2 \\cdot 1 + 3 \\cdot 1 \\\\ \n",
    "  &= 6\n",
    "\\end{align}\n",
    "\n",
    "While not in scope, note that we can also interpret the dot product geometrically:\n",
    "\n",
    "* It is the product of three things: the **magnitude** of both vectors, and the **cosine** of the angles between them: $$\\vec{u} \\cdot \\vec{v} = ||\\vec{u}|| \\cdot ||\\vec{v}|| \\cdot {cos \\theta}$$\n",
    ":::\n",
    "\n",
    "We now know how to generate a single prediction from multiple observed features. Data scientists usually work at scale – that is, they want to build models that can produce many predictions, all at once. The vector notation we introduced above gives us a hint on how we can expedite multiple linear regression. We want to use the tools of linear algebra.\n",
    "\n",
    "Let's think about how we can apply what we did above. To accommodate for the fact that we're considering several feature variables, we'll adjust our notation slightly. Each observation can now be thought of as a row vector with an entry for each of $p$ features.\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td><img src=\"images/observation.png\" alt='observation' width='550'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "To make a prediction from the *first* observation in the data, we take the dot product of the parameter vector and *first* observation vector. To make a prediction from the *second* observation, we would repeat this process to find the dot product of the parameter vector and the *second* observation vector. If we wanted to find the model predictions for each observation in the dataset, we'd repeat this process for all $n$ observations in the data. \n",
    "\n",
    "$$\\hat{y}_1 = \\theta_0 + \\theta_1 x_{11} + \\theta_2 x_{12} + ... + \\theta_p x_{1p} = [1,\\:x_{11},\\:x_{12},\\:x_{13},\\:...,\\:x_{1p}] \\theta$$\n",
    "$$\\hat{y}_2 = \\theta_0 + \\theta_1 x_{21} + \\theta_2 x_{22} + ... + \\theta_p x_{2p} = [1,\\:x_{21},\\:x_{22},\\:x_{23},\\:...,\\:x_{2p}] \\theta$$\n",
    "$$\\vdots$$\n",
    "$$\\hat{y}_n = \\theta_0 + \\theta_1 x_{n1} + \\theta_2 x_{n2} + ... + \\theta_p x_{np} = [1,\\:x_{n1},\\:x_{n2},\\:x_{n3},\\:...,\\:x_{np}] \\theta$$\n",
    "\n",
    "Our observed data is represented by $n$ row vectors, each with dimension $(p+1)$. We can collect them all into a single matrix, which we call $\\mathbb{X}$.\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td><img src=\"images/design_matrix.png\" alt='design_matrix' width='400'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "The matrix $\\mathbb{X}$ is known as the **design matrix**. It contains all observed data for each of our $p$ features, where each **row** corresponds to one **observation**, and each **column** corresponds to a **feature**. It often (but not always) contains an additional column of all ones to represent the **intercept** or **bias column**. \n",
    "\n",
    "To review what is happening in the design matrix: each row represents a single observation. For example, a student in Data 100. Each column represents a feature. For example, the ages of students in Data 100. This convention allows us to easily transfer our previous work in DataFrames over to this new linear algebra perspective.\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/row_col.png\" alt='row_col' width='600'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "The multiple linear regression model can then be restated in terms of matrices:\n",
    "$$\n",
    "\\Large\n",
    "\\mathbb{\\hat{Y}} = \\mathbb{X} \\theta\n",
    "$$\n",
    "\n",
    "Here, $\\mathbb{\\hat{Y}}$ is the **prediction vector** with $n$ elements ($\\mathbb{\\hat{Y}} \\in \\mathbb{R}^{n}$); it contains the prediction made by the model for each of the $n$ input observations. $\\mathbb{X}$ is the **design matrix** with dimensions $\\mathbb{X} \\in \\mathbb{R}^{n \\times (p + 1)}$, and $\\theta$ is the **parameter vector** with dimensions $\\theta \\in \\mathbb{R}^{(p + 1)}$. Note that our **true output** $\\mathbb{Y}$ is also a vector with $n$ elements ($\\mathbb{Y} \\in \\mathbb{R}^{n}$).\n",
    "\n",
    "::: {.callout-tip collaps=\"false\"}\n",
    "#### Linear Algebra Review: Linearity \n",
    "\n",
    "An expression is **linear in $\\theta$** (a set of parameters) if it is a linear combination of the elements of the set. Checking if an expression can separate into a matrix product of two terms -- a **vector of $\\theta$** s, and a matrix/vector **not involving $\\theta$** -- is a good indicator of linearity.\n",
    "\n",
    "\n",
    "For example, consider the vector $\\theta = [\\theta_0, \\theta_1, \\theta_2]$\n",
    "\n",
    "* $\\hat{y} = \\theta_0 + 2\\theta_1 + 3\\theta_2$ is linear in theta, and we can separate it into a matrix product of two terms:\n",
    "\n",
    "$$\\hat{y} = \\begin{bmatrix} 1 \\space 2 \\space 3 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$$\n",
    "\n",
    "* $\\hat{y} = \\theta_0\\theta_1 + 2\\theta_1^2 + 3log(\\theta_2)$ is *not* linear in theta, as the $\\theta_1$ term is squared, and the $\\theta_2$ term is logged. We cannot separate it into a matrix product of two terms.\n",
    ":::\n",
    "\n",
    "### Mean Squared Error\n",
    "\n",
    "We now have a new approach to understanding models in terms of vectors and matrices. To accompany this new convention, we should update our understanding of risk functions and model fitting.\n",
    "\n",
    "Recall our definition of MSE:\n",
    "$$R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "At its heart, the MSE is a measure of *distance* – it gives an indication of how \"far away\" the predictions are from the true values, on average. \n",
    "\n",
    "::: {.callout-tip collaps=\"false\"}\n",
    "#### Linear Algebra: L2 Norm\n",
    "When working with vectors, this idea of \"distance\" or the vector's **size/length** is represented by the **norm**. More precisely, the distance between two vectors $\\vec{a}$ and $\\vec{b}$ can be expressed as:\n",
    "$$||\\vec{a} - \\vec{b}||_2 = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\ldots + (a_n - b_n)^2} = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}$$\n",
    "\n",
    "The double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.\n",
    "\n",
    "The two norms we need to know for Data 100 are the L1 and L2 norms (sound familiar?). In this note, we'll focus on L2 norm. We'll dive into L1 norm in future lectures. \n",
    "\n",
    "For the n-dimensional vector $$\\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$$ its **L2 vector norm** is\n",
    "\n",
    "$$||\\vec{x}||_2 = \\sqrt{(x_1)^2 + (x_2)^2 + \\ldots + (x_n)^2} = \\sqrt{\\sum_{i=1}^n (x_i)^2}$$\n",
    "\n",
    "The L2 vector norm is a generalization of the Pythagorean theorem in $n$ dimensions. Thus, it can be used as a measure of the **length** of a vector or even as a measure of the **distance** between two vectors. \n",
    "::: \n",
    "\n",
    "We can express the MSE as a squared L2 norm if we rewrite it in terms of the prediction vector, $\\hat{\\mathbb{Y}}$, and true target vector, $\\mathbb{Y}$:\n",
    "\n",
    "$$R(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} (||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2)^2$$\n",
    "\n",
    "Here, the superscript 2 outside of the parentheses means that we are *squaring* the norm. If we plug in our linear model $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, we find the MSE cost function in vector notation:\n",
    "\n",
    "$$R(\\theta) = \\frac{1}{n} (||\\mathbb{Y} - \\mathbb{X} \\theta||_2)^2$$\n",
    "\n",
    "Under the linear algebra perspective, our new task is to fit the optimal parameter vector $\\theta$ such that the cost function is minimized. Equivalently, we wish to minimize the norm $$||\\mathbb{Y} - \\mathbb{X} \\theta||_2 = ||\\mathbb{Y} - \\hat{\\mathbb{Y}}||_2.$$ \n",
    "\n",
    "We can restate this goal in two ways:\n",
    "\n",
    "* Minimize the **distance** between the vector of true values, $\\mathbb{Y}$, and the vector of predicted values, $\\mathbb{\\hat{Y}}$\n",
    "* Minimize the **length** of the **residual vector**, defined as:\n",
    "$$e = \\mathbb{Y} - \\mathbb{\\hat{Y}} = \\begin{bmatrix}\n",
    "           y_1 - \\hat{y}_1 \\\\\n",
    "           y_2 - \\hat{y}_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           y_n - \\hat{y}_n\n",
    "         \\end{bmatrix}$$\n",
    "\n",
    "### A Note on Terminology for Multiple Linear Regression\n",
    "\n",
    "There are several equivalent terms in the context of regression. The ones we use most often for this course are bolded.\n",
    "\n",
    "* $x$ can be called a\n",
    "  - **Feature(s)**\n",
    "  - Covariate(s)\n",
    "  - **Independent variable(s)**\n",
    "  - Explanatory variable(s)\n",
    "  - Predictor(s)\n",
    "  - Input(s)\n",
    "  - Regressor(s)\n",
    "* $y$ can be called an\n",
    "  - **Output**\n",
    "  - Outcome\n",
    "  - **Response**\n",
    "  - Dependent variable\n",
    "* $\\hat{y}$ can be called a\n",
    "  - **Prediction**\n",
    "  - Predicted response\n",
    "  - Estimated value\n",
    "* $\\theta$ can be called a\n",
    "  - **Weight(s)**\n",
    "  - **Parameter(s)**\n",
    "  - Coefficient(s)\n",
    "* $\\hat{\\theta}$ can be called a\n",
    "  - **Estimator(s)**\n",
    "  - **Optimal parameter(s)**\n",
    "* A datapoint $(x, y)$ is also called an observation.\n",
    "\n",
    "\n",
    "## Geometric Derivation\n",
    "::: {.callout-tip collaps=\"false\"}\n",
    "### Linear Algebra: Span \n",
    "Recall that the **span** or **column space** of a matrix $\\mathbb{X}$ (denoted $span(\\mathbb{X})$) is the set of all possible linear combinations of the matrix's columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns. Additionally, if each column of $\\mathbb{X}$ has length $n$, $span(\\mathbb{X})$ is a subspace of $\\mathbb{R}^{n}$.\n",
    ":::\n",
    "\n",
    "::: {.callout-tip collaps=\"false\"}\n",
    "### Linear Algebra: Matrix-Vector Multiplication\n",
    "There are 2 ways we can think about matrix-vector multiplication \n",
    "\n",
    "1. So far, we’ve thought of our model as horizontally stacked predictions per datapoint\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/matmul1.png\" alt='row_col' width='300'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "2. However, it is helpful sometimes to think of matrix-vector multiplication as performed by columns. We can also think of $\\mathbb{Y}$ as a *linear combination of feature vectors*, scaled by *parameters*.\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/matmul2.png\" alt='row_col' width='500'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    ":::\n",
    "\n",
    "Up until now, we've mostly thought of our model as a scalar product between horizontally stacked observations and the parameter vector. We can also think of $\\hat{\\mathbb{Y}}$ as a **linear combination of feature vectors**, scaled by the **parameters**. We use the notation $\\mathbb{X}_{:, i}$ to denote the $i$th column of the design matrix. You can think of this as following the same convention as used when calling `.iloc` and `.loc`. \":\" means that we are taking all entries in the $i$th column.\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/columns.png\" alt='columns' width='500'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{Y}} = \n",
    "\\theta_0 \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           1\n",
    "         \\end{bmatrix} + \\theta_1 \\begin{bmatrix}\n",
    "           x_{11} \\\\\n",
    "           x_{21} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n1}\n",
    "         \\end{bmatrix} + \\ldots + \\theta_p \\begin{bmatrix}\n",
    "           x_{1p} \\\\\n",
    "           x_{2p} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{np}\n",
    "         \\end{bmatrix}\n",
    "         = \\theta_0 \\mathbb{X}_{:,\\:1} + \\theta_1 \\mathbb{X}_{:,\\:2} + \\ldots + \\theta_p \\mathbb{X}_{:,\\:p+1}$$\n",
    "\n",
    "This new approach is useful because it allows us to take advantage of the properties of linear combinations.\n",
    "\n",
    "Because the prediction vector, $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, is a **linear combination** of the columns of $\\mathbb{X}$, we know that the **predictions are contained in the span of $\\mathbb{X}$**. That is, we know that $\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})$.\n",
    "\n",
    "The diagram below is a simplified view of $\\text{Span}(\\mathbb{X})$, assuming that each column of $\\mathbb{X}$ has length $n$. Notice that the columns of $\\mathbb{X}$ define a subspace of $\\mathbb{R}^n$, where each point in the subspace can be reached by a linear combination of $\\mathbb{X}$'s columns. The prediction vector $\\mathbb{\\hat{Y}}$ lies somewhere in this subspace.\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/span.png\" alt='span' width='600'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "Examining this diagram, we find a problem. The vector of true values, $\\mathbb{Y}$, could theoretically lie *anywhere* in $\\mathbb{R}^n$ space – its exact location depends on the data we collect out in the real world. However, our multiple linear regression model can only make predictions in the subspace of $\\mathbb{R}^n$ spanned by $\\mathbb{X}$. Remember the model fitting goal we established in the previous section: we want to generate predictions such that the distance between the vector of true values, $\\mathbb{Y}$, and the vector of predicted values, $\\mathbb{\\hat{Y}}$, is minimized. This means that **we want $\\mathbb{\\hat{Y}}$ to be the vector in $\\text{Span}(\\mathbb{X})$ that is closest to $\\mathbb{Y}$**. \n",
    "\n",
    "Another way of rephrasing this goal is to say that we wish to minimize the length of the residual vector $e$, as measured by its $L_2$ norm. \n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/residual.png\" alt='residual' width='600'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "The vector in $\\text{Span}(\\mathbb{X})$ that is closest to $\\mathbb{Y}$ is always the **orthogonal projection** of $\\mathbb{Y}$ onto $\\text{Span}(\\mathbb{X}).$ Thus, we should choose the parameter vector $\\theta$ that makes the **residual vector orthogonal to any vector in $\\text{Span}(\\mathbb{X})$**. You can visualize this as the vector created by dropping a perpendicular line from $\\mathbb{Y}$ onto the span of $\\mathbb{X}$. \n",
    "\n",
    "::: {.callout-tip collaps=\"false\"}\n",
    "### Linear Algebra: Orthogonality\n",
    "Recall that two vectors $\\vec{a}$ and $\\vec{b}$ are orthogonal if their dot product is zero: $\\vec{a}^{T}\\vec{b} = 0$. \n",
    "\n",
    "A vector $v$ is **orthogonal** to the span of a matrix $M$ if and only if $v$ is orthogonal to **each column** in $M$. Put together, a vector $v$ is orthogonal to $\\text{Span}(M)$ if:\n",
    "\n",
    "$$M^Tv = \\vec{0}$$\n",
    "\n",
    "Note that $\\vec{0}$ represents the **zero vector**, a $d$-length vector full of 0s.\n",
    ":::\n",
    "\n",
    "Remember our goal is to find $\\hat{\\theta}$ such that we minimize the objective function $R(\\theta)$. Equivalently, this is the $\\hat{\\theta}$ such that the residual vector $e = \\mathbb{Y} - \\mathbb{X} \\hat{\\theta}$ is orthogonal to $\\text{Span}(\\mathbb{X})$. \n",
    "\n",
    "Looking at the definition of orthogonality of $\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}$ to $span(\\mathbb{X})$, we can write: \n",
    "$$\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\vec{0}$$ \n",
    "\n",
    "Let's then rearrange the terms: \n",
    "$$\\mathbb{X}^T \\mathbb{Y} - \\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\vec{0}$$\n",
    "\n",
    "And finally, we end up with the **normal equation**:\n",
    "$$\\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "Any vector $\\theta$ that minimizes MSE on a dataset must satisfy this equation.\n",
    "\n",
    "If $\\mathbb{X}^T \\mathbb{X}$ is invertible, we can conclude:\n",
    "$$\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^{-1} \\mathbb{X}^T \\mathbb{Y}$$\n",
    "\n",
    "This is called the **least squares estimate** of $\\theta$: it is the value of $\\theta$ that minimizes the squared loss. \n",
    "\n",
    "Note that the least squares estimate was derived under the assumption that $\\mathbb{X}^T \\mathbb{X}$ is *invertible*. This condition holds true when $\\mathbb{X}^T \\mathbb{X}$ is full column rank, which, in turn, happens when $\\mathbb{X}$ is full column rank. We will get to the proof for why $\\mathbb{X}$ needs to be full column rank in the OLS Properties section.\n",
    "\n",
    "## OLS Properties\n",
    "\n",
    "### Residuals\n",
    "When using the optimal parameter vector, our residuals $e = \\mathbb{Y} - \\hat{\\mathbb{Y}}$ are orthogonal to $span(\\mathbb{X})$.\n",
    "\n",
    "$$\\mathbb{X}^Te = 0 $$\n",
    "\n",
    "::: {.callout}\n",
    "Proof: \n",
    "\n",
    "* The optimal parameter vector, $\\hat{\\theta}$, solves the normal equations $\\implies \\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}$\n",
    "\n",
    "$$\\mathbb{X}^Te = \\mathbb{X}^T (\\mathbb{Y} - \\mathbb{\\hat{Y}}) $$\n",
    "\n",
    "$$\\mathbb{X}^T (\\mathbb{Y} - \\mathbb{X}\\hat{\\theta}) = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{X}\\hat{\\theta}$$\n",
    "\n",
    "* Any matrix multiplied with its own inverse is the identity matrix $\\mathbb{I}$\n",
    "\n",
    "$$\\mathbb{X}^T\\mathbb{Y} - (\\mathbb{X}^T\\mathbb{X})(\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y} = \\mathbb{X}^T\\mathbb{Y} - \\mathbb{X}^T\\mathbb{Y} = 0$$\n",
    ":::\n",
    "\n",
    "### The Bias/Intercept Term\n",
    "For all linear models with an **intercept term**, the **sum of residuals is zero**.\n",
    "\n",
    "$$\\sum_i^n e_i = 0$$\n",
    "\n",
    "::: {.callout}\n",
    "Proof:\n",
    "\n",
    "* For all linear models with an **intercept term**, the average of the predicted $y$ values is equal to the average of the true $y$ values.\n",
    "$$\\bar{y} = \\bar{\\hat{y}}$$\n",
    "* Rewriting the sum of residuals as two separate sums,\n",
    "$$\\sum_i^n e_i = \\sum_i^n y_i - \\sum_i^n\\hat{y}_i$$\n",
    "* Each respective sum is a multiple of the average of the sum.\n",
    "$$\\sum_i^n e_i = n\\bar{y} - n\\bar{y} = n(\\bar{y} - \\bar{y}) = 0$$\n",
    ":::\n",
    "\n",
    "### Existence of a Unique Solution\n",
    "To summarize:\n",
    "\n",
    "|   | Model | Estimate | Unique? |\n",
    "| -- | -- | -- |  -- | \n",
    "| Constant Model + MSE | $\\hat{y} = \\theta_0$| $\\hat{\\theta}_0 = mean(y) = \\bar{y}$ | **Yes**. Any set of values has a unique mean.\n",
    "| Constant Model + MAE | $\\hat{y} = \\theta_0$  | $\\hat{\\theta}_0 = median(y)$ | **Yes**, if odd. **No**, if even. Return the average of the middle 2 values.\n",
    "| Simple Linear Regression + MSE | $\\hat{y} = \\theta_0 + \\theta_1x$| $\\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x}$ $\\hat{\\theta}_1 = r\\frac{\\sigma_y}{\\sigma_x}$| **Yes**. Any set of non-constant* values has a unique mean, SD, and correlation coefficient.\n",
    "| **OLS** (Linear Model + MSE) | $\\mathbb{\\hat{Y}} = \\mathbb{X}\\mathbb{\\theta}$| $\\hat{\\theta} = (\\mathbb{X}^T\\mathbb{X})^{-1}\\mathbb{X}^T\\mathbb{Y}$  | **Yes**, if $\\mathbb{X}$ is full column rank (all columns are linearly independent, # of datapoints >>> # of features).\n",
    "\n",
    "### Uniqueness of the OLS Solution\n",
    "\n",
    "In most settings, the number of observations (n) is much greater than the number of features (p).\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/solution_matrices.png\" alt='solution_matrices' width='500'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "In practice, instead of directly inverting matrices, we can use more efficient numerical solvers to directly solve a system of linear equations using the normal equation shown below. Note that at least one solution always exists because intuitively, we can always draw a line of best fit for a given set of data, but there may be multiple lines that are “equally good”. (Formal proof is beyond this course.)\n",
    "\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/normal_equation.png\" alt='normal_equation' width='400'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "The Least Squares estimate $\\hat{\\theta}$ is **unique** if and only if $\\mathbb{X}$ is **full column rank**.\n",
    "\n",
    "::: {.callout}\n",
    "Proof: \n",
    "\n",
    "* We know the solution to the normal equation $\\mathbb{X}^T\\mathbb{X}\\hat{\\theta} = \\mathbb{X}^T\\mathbb{Y}$ is the least square estimate that minimizes the squared loss.\n",
    "* $\\hat{\\theta}$ has a **unique** solution $\\iff$ the square matrix $\\mathbb{X}^T\\mathbb{X}$ is **invertible** $\\iff$ $\\mathbb{X}^T\\mathbb{X}$ is full rank.\n",
    "  * The **column rank** of a square matrix is the max number of linearly independent columns it contains.\n",
    "  * An $n$ x $n$ square matrix is deemed full column rank when all of its columns are linearly independent. That is, its rank would be equal to $n$.\n",
    "  * $\\mathbb{X}^T\\mathbb{X}$ has shape $(p + 1) \\times (p + 1)$, and therefore has max rank $p + 1$. \n",
    "* $rank(\\mathbb{X}^T\\mathbb{X})$ = $rank(\\mathbb{X})$ (proof out of scope).\n",
    "* Therefore, $\\mathbb{X}^T\\mathbb{X}$ has rank $p + 1$ $\\iff$  $\\mathbb{X}$ has rank $p + 1$ $\\iff \\mathbb{X}$ is full column rank.\n",
    ":::\n",
    "\n",
    "Therefore, if $\\mathbb{X}$ is not full column rank, we will not have unique estimates. This can happen for two major reasons.\n",
    "\n",
    "1. If our design matrix $\\mathbb{X}$ is \"**wide**\":\n",
    "    * If n < p, then we have way more features (columns) than observations (rows).\n",
    "    * Then $rank(\\mathbb{X})$ = min(n, p+1) < p+1, so $\\hat{\\theta}$ is not unique.\n",
    "    * Typically we have n >> p so this is less of an issue.\n",
    "\n",
    "2. If our design matrix $\\mathbb{X}$ has features that are **linear combinations** of other features:\n",
    "    * By definition, rank of $\\mathbb{X}$ is number of linearly independent columns in $\\mathbb{X}$.\n",
    "    * Example: If “Width”, “Height”, and “Perimeter” are all columns,\n",
    "      * Perimeter = 2 * Width + 2 * Height  $\\rightarrow$  $\\mathbb{X}$ is not full rank.\n",
    "    * Important with one-hot encoding (to discuss later).\n",
    "\n",
    "## Evaluating Model Performance\n",
    "\n",
    "Our geometric view of multiple linear regression has taken us far! We have identified the optimal set of parameter values to minimize MSE in a model of multiple features. Now, we want to understand how well our fitted model performs. \n",
    "\n",
    "### RMSE\n",
    "One measure of model performance is the **Root Mean Squared Error**, or RMSE. The RMSE is simply the square root of MSE. Taking the square root converts the value back into the original, non-squared units of $y_i$, which is useful for understanding the model's performance. A low RMSE indicates more \"accurate\" predictions – that there is a lower average loss across the dataset.\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "Note that RMSE is more interpretable as it is in the same units as the original data. On the other hand, MSE is differentiable everywhere making it useful for running optimization algorithms like gradient descent.\n",
    "\n",
    ":::\n",
    "\n",
    "### Residual Plots\n",
    "When working with SLR, we generated plots of the residuals against a single feature to understand the behavior of residuals. When working with several features in multiple linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple linear model performs well if its residual plot shows no patterns.\n",
    "\n",
    "<div align=\"middle\">\n",
    "  <table style=\"width:100%\">\n",
    "    <tr align=\"center\">\n",
    "      <td>\n",
    "        <img src=\"images/residual_plot.png\" alt='residual_plot' width='500'>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "### Multiple $R^2$\n",
    "For SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple linear model setting, we will need a performance metric that can account for multiple features at once. **Multiple $R^2$**, also called the **coefficient of determination**, is the **proportion of variance** of our **fitted values** (predictions) $\\hat{y}_i$ to our true values $y_i$. It ranges from 0 to 1 and is effectively the *proportion* of variance in the observations that the **model explains**. \n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of } \\hat{y}_i}{\\text{variance of } y_i} = \\frac{\\sigma^2_{\\hat{y}}}{\\sigma^2_y}$$\n",
    "\n",
    "Note that for OLS with an intercept term, for example $\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_px_p$, $R^2$ is equal to the square of the correlation between $y$ and $\\hat{y}$. On the other hand for SLR, $R^2$ is equal to $r^2$, the correlation between $x$ and $y$. The proof of these last two properties is out of scope for this course.\n",
    "\n",
    "Additionally, as we add more features, our fitted values tend to become closer and closer to our actual values. Thus, $R^2$ increases.\n",
    "\n",
    "Adding more features doesn't always mean our model is better though! We'll see why later in the course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Library/Frameworks/Python.framework/Versions/3.12/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
