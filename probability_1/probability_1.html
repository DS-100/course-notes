<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="probability_1_files/libs/clipboard/clipboard.min.js"></script>
<script src="probability_1_files/libs/quarto-html/quarto.js"></script>
<script src="probability_1_files/libs/quarto-html/popper.min.js"></script>
<script src="probability_1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="probability_1_files/libs/quarto-html/anchor.min.js"></script>
<link href="probability_1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="probability_1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="probability_1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="probability_1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="probability_1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Random Variables</h2>
   
  <ul>
  <li><a href="#random-variables-and-distributions" id="toc-random-variables-and-distributions" class="nav-link active" data-scroll-target="#random-variables-and-distributions">Random Variables and Distributions</a>
  <ul class="collapse">
  <li><a href="#distribution" id="toc-distribution" class="nav-link" data-scroll-target="#distribution">Distribution</a></li>
  <li><a href="#example-tossing-a-coin" id="toc-example-tossing-a-coin" class="nav-link" data-scroll-target="#example-tossing-a-coin">Example: Tossing a Coin</a></li>
  <li><a href="#simulation" id="toc-simulation" class="nav-link" data-scroll-target="#simulation">Simulation</a></li>
  </ul></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance">Expectation and Variance</a>
  <ul class="collapse">
  <li><a href="#expectation" id="toc-expectation" class="nav-link" data-scroll-target="#expectation">Expectation</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance">Variance</a></li>
  <li><a href="#example-dice" id="toc-example-dice" class="nav-link" data-scroll-target="#example-dice">Example: Dice</a></li>
  </ul></li>
  <li><a href="#sums-of-random-variables" id="toc-sums-of-random-variables" class="nav-link" data-scroll-target="#sums-of-random-variables">Sums of Random Variables</a>
  <ul class="collapse">
  <li><a href="#equal-vs.-identically-distributed-vs.-i.i.d" id="toc-equal-vs.-identically-distributed-vs.-i.i.d" class="nav-link" data-scroll-target="#equal-vs.-identically-distributed-vs.-i.i.d">Equal vs.&nbsp;Identically Distributed vs.&nbsp;i.i.d</a></li>
  <li><a href="#properties-of-expectation" id="toc-properties-of-expectation" class="nav-link" data-scroll-target="#properties-of-expectation">Properties of Expectation</a></li>
  <li><a href="#properties-of-variance" id="toc-properties-of-variance" class="nav-link" data-scroll-target="#properties-of-variance">Properties of Variance</a></li>
  <li><a href="#covariance-and-correlation" id="toc-covariance-and-correlation" class="nav-link" data-scroll-target="#covariance-and-correlation">Covariance and Correlation</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul></li>
  <li><a href="#common-random-variables" id="toc-common-random-variables" class="nav-link" data-scroll-target="#common-random-variables">Common Random Variables</a></li>
  <li><a href="#populations-and-samples" id="toc-populations-and-samples" class="nav-link" data-scroll-target="#populations-and-samples">Populations and Samples</a>
  <ul class="collapse">
  <li><a href="#sample-mean" id="toc-sample-mean" class="nav-link" data-scroll-target="#sample-mean">Sample Mean</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">Central Limit Theorem</a></li>
  <li><a href="#using-the-sample-mean-to-estimate-the-population-mean" id="toc-using-the-sample-mean-to-estimate-the-population-mean" class="nav-link" data-scroll-target="#using-the-sample-mean-to-estimate-the-population-mean">Using the Sample Mean to Estimate the Population Mean</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Random Variables</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li>Define a random variable in terms of its distribution</li>
<li>Compute the expectation and variance of a random variable</li>
<li>Gain familiarity with the Bernoulli and binomial random variables</li>
</ul>
</div>
</div>
</div>
<p>In the past few lectures, we’ve examined the role of complexity in influencing model performance. We’ve considered model complexity in the context of a tradeoff between two competing factors: model variance and training error.</p>
<p>Thus far, our analysis has been mostly qualitative. We’ve acknowledged that our choice of model complexity needs to strike a balance between model variance and training error, but we haven’t yet discussed <em>why</em> exactly this tradeoff exists.</p>
<p>To better understand the origin of this tradeoff, we will need to introduce the language of <strong>random variables</strong>. The next two lectures on probability will be a brief digression from our work on modeling so we can build up the concepts needed to understand this so-called <strong>bias-variance tradeoff</strong>. Our roadmap for the next few lectures will be:</p>
<ol type="1">
<li>Random Variables Estimators: introduce random variables, considering the concepts of expectation, variance, and covariance</li>
<li>Estimators, Bias, and Variance: re-express the ideas of model variance and training error in terms of random variables and use this new perspective to investigate our choice of model complexity</li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Data 8 Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Recall the following concepts from Data 8:</p>
<ol type="1">
<li><p>Sample mean: the mean of your random sample</p></li>
<li><p>Central Limit Theorem: If you draw a large random sample with replacement, then, regardless of the population distribution, the probability distribution of the sample mean</p>
<ol type="a">
<li><p>is roughly normal</p></li>
<li><p>is centered at the population mean</p></li>
<li><p>has an <span class="math inline">\(SD = \frac{\text{population SD}}{\sqrt{\text{sample size}}}\)</span></p></li>
</ol></li>
</ol>
</div>
</div>
</div>
<section id="random-variables-and-distributions" class="level2">
<h2 class="anchored" data-anchor-id="random-variables-and-distributions">Random Variables and Distributions</h2>
<p>Suppose we generate a set of random data, like a random sample from some population. A <strong>random variable</strong> is a <em>numerical function</em> of the randomness in the data. It is <em>random</em> since our sample was drawn at random; it is <em>variable</em> because its exact value depends on how this random sample came out. As such, the domain or input of our random variable is all possible (random) outcomes in a <em>sample space</em>, and its range or output is the number line. We typically denote random variables with uppercase letters, such as <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>.</p>
<section id="distribution" class="level3">
<h3 class="anchored" data-anchor-id="distribution">Distribution</h3>
<p>For any random variable <span class="math inline">\(X\)</span>, we need to be able to specify 2 things:</p>
<ol type="1">
<li>Possible values: the set of values the random variable can take on.</li>
<li>Probabilities: the set of probabilities describing how the total probability of 100% is split over the possible values.</li>
</ol>
<p>If <span class="math inline">\(X\)</span> is discrete (has a finite number of possible values), the probability that a random variable <span class="math inline">\(X\)</span> takes on the value <span class="math inline">\(x\)</span> is given by <span class="math inline">\(P(X=x)\)</span>, and probabilities must sum to 1: <span class="math inline">\(\sum_{\text{all} x} P(X=x) = 1\)</span>,</p>
<p>We can often display this using a <strong>probability distribution table</strong>, which you will see in the coin toss example below.</p>
<p>The <strong>distribution</strong> of a random variable <span class="math inline">\(X\)</span> is a description of how the total probability of 100% is split over all the possible values of <span class="math inline">\(X\)</span>, and it fully defines a random variable. The distribution of a discrete random variable can also be represented using a histogram. If a variable is <strong>continuous</strong> – it can take on infinitely many values – we can illustrate its distribution using a density curve.</p>
<p align="center">
<img src="images/discrete_continuous.png" alt="discrete_continuous" width="700">
</p>
<p>Probabilities are areas. For discrete random variables, the <em>area of the red bars</em> represent the probability that a discrete random variable <span class="math inline">\(X\)</span> falls within those values. For continuous random variables, the <em>area under the curve</em> represents the probability that a discrete random variable <span class="math inline">\(Y\)</span> falls within those values.</p>
<p align="center">
<img src="images/probability_areas.png" alt="discrete_continuous" width="600">
</p>
<p>If we sum up the total area of the bars/under the density curve, we should get 100%, or 1.</p>
</section>
<section id="example-tossing-a-coin" class="level3">
<h3 class="anchored" data-anchor-id="example-tossing-a-coin">Example: Tossing a Coin</h3>
<p>To give a concrete example, let’s formally define a fair coin toss. A fair coin can land on heads (<span class="math inline">\(H\)</span>) or tails (<span class="math inline">\(T\)</span>), each with a probability of 0.5. With these possible outcomes, we can define a random variable <span class="math inline">\(X\)</span> as: <span class="math display">\[X = \begin{cases}
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails}
   \end{cases}\]</span></p>
<p><span class="math inline">\(X\)</span> is a function with a domain, or input, of <span class="math inline">\(\{H, T\}\)</span> and a range, or output, of <span class="math inline">\(\{1, 0\}\)</span>. We can write this in function notation as <span class="math display">\[\begin{cases}  X(H) = 1 \\ X(T) = 0 \end{cases}\]</span> The probability distribution table of <span class="math inline">\(X\)</span> is given by.</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(P(X=x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\frac{1}{2}\)</span></td>
</tr>
</tbody>
</table>
<p>Suppose we draw a random sample <span class="math inline">\(s\)</span> of size 3 from all students enrolled in Data 100. We can define <span class="math inline">\(Y\)</span> as the number of data science students in our sample. Its domain is all possible samples of size 3, and its range is <span class="math inline">\(\{0, 1, 2, 3\}\)</span>.</p>
<p align="center">
<img src="images/rv.png" alt="rv" width="600" class="center">
</p>
<p>We can show the distribution of <span class="math inline">\(Y\)</span> in the following tables. The table on the left lists all possible samples of <span class="math inline">\(s\)</span> and the number of times they can appear (<span class="math inline">\(Y(s)\)</span>). We can use this to calculate the values for the table on the right, a <strong>probability distribution table</strong>.</p>
<p align="center">
<img src="images/distribution.png" alt="distribution" width="600">
</p>
</section>
<section id="simulation" class="level3">
<h3 class="anchored" data-anchor-id="simulation">Simulation</h3>
<p>Given a random variable <span class="math inline">\(X\)</span>’s distribution, how could we <strong>generate/simulate</strong> a population? To do so, we can randomly pick values of <span class="math inline">\(X\)</span> according to its distribution using <code>np.random.choice</code> or <code>df.sample</code>.</p>
</section>
</section>
<section id="expectation-and-variance" class="level2">
<h2 class="anchored" data-anchor-id="expectation-and-variance">Expectation and Variance</h2>
<p>There are several ways to describe a random variable. The methods shown above – a table of all samples <span class="math inline">\(s, X(s)\)</span>, distribution table <span class="math inline">\(P(X=x)\)</span>, and histograms – are all definitions that <em>fully describe</em> a random variable. Often, it is easier to describe a random variable using some <em>numerical summary</em> rather than fully defining its distribution. These numerical summaries are numbers that characterize some properties of the random variable. Because they give a “summary” of how the variable tends to behave, they are <em>not</em> random – think of them as a static number that describes a certain property of the random variable. In Data 100, we will focus our attention on the expectation and variance of a random variable.</p>
<section id="expectation" class="level3">
<h3 class="anchored" data-anchor-id="expectation">Expectation</h3>
<p>The <strong>expectation</strong> of a random variable <span class="math inline">\(X\)</span> is the weighted average of the values of <span class="math inline">\(X\)</span>, where the weights are the probabilities of each value occurring. There are two equivalent ways to compute the expectation:</p>
<ol type="1">
<li>Apply the weights one <em>sample</em> at a time: <span class="math display">\[\mathbb{E}[X] = \sum_{\text{all possible } s} X(s) P(s)\]</span>.</li>
<li>Apply the weights one possible <em>value</em> at a time: <span class="math display">\[\mathbb{E}[X] = \sum_{\text{all possible } x} x P(X=x)\]</span></li>
</ol>
<p>We want to emphasize that the expectation is a <em>number</em>, not a random variable. Expectation is a generalization of the average, and it has the same units as the random variable. It is also the center of gravity of the probability distribution histogram, meaning if we simulate the variable many times, it is the long-run average of the random variable.</p>
<section id="example-1-coin-toss" class="level4">
<h4 class="anchored" data-anchor-id="example-1-coin-toss">Example 1: Coin Toss</h4>
<p>Going back to our coin toss example, we define a random variable <span class="math inline">\(X\)</span> as: <span class="math display">\[X = \begin{cases}
      1, \text{if the coin lands heads} \\
      0, \text{if the coin lands tails}
   \end{cases}\]</span> We can calculate its expectation <span class="math inline">\(\mathbb{E}[X]\)</span> using the second method of applying the weights one possible value at a time: <span class="math display">\[\begin{align}
\mathbb{E}[X] &amp;= \sum_{x} x P(X=x) \\
&amp;= 1 * 0.5 + 0 * 0.5 \\
&amp;= 0.5
\end{align}\]</span> Note that <span class="math inline">\(\mathbb{E}[X] = 0.5\)</span> is not a possible value of <span class="math inline">\(X\)</span>; it’s an average. <strong>The expectation of X does not need to be a possible value of X</strong>.</p>
</section>
<section id="example-2" class="level4">
<h4 class="anchored" data-anchor-id="example-2">Example 2</h4>
<p>Consider the random variable <span class="math inline">\(X\)</span>:</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(P(X=x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>0.1</td>
</tr>
<tr class="even">
<td>4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>8</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>To calculate it’s expectation, <span class="math display">\[\begin{align}
\mathbb{E}[X] &amp;= \sum_{x} x P(X=x) \\
&amp;= 3 * 0.1 + 4 * 0.2 + 6 * 0.4 + 8 * 0.3 \\
&amp;= 0.3 + 0.8 + 2.4 + 2.4 \\
&amp;= 5.9
\end{align}\]</span> Again, note that <span class="math inline">\(\mathbb{E}[X] = 5.9\)</span> is not a possible value of <span class="math inline">\(X\)</span>; it’s an average. <strong>The expectation of X does not need to be a possible value of X</strong>.</p>
</section>
</section>
<section id="variance" class="level3">
<h3 class="anchored" data-anchor-id="variance">Variance</h3>
<p>The <strong>variance</strong> of a random variable is a measure of its chance error. It is defined as the expected squared deviation from the expectation of <span class="math inline">\(X\)</span>. Put more simply, variance asks: how far does <span class="math inline">\(X\)</span> typically vary from its average value, just by chance? What is the spread of <span class="math inline">\(X\)</span>’s distribution?</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</span></p>
<p>The units of variance are the square of the units of <span class="math inline">\(X\)</span>. To get it back to the right scale, use the standard deviation of <span class="math inline">\(X\)</span>: <span class="math display">\[\text{SD}(X) = \sqrt{\text{Var}(X)}\]</span></p>
<p>Like with expectation, <strong>variance is a number, not a random variable</strong>! Its main use is to quantify chance error.</p>
<p>By <a href="https://www.inferentialthinking.com/chapters/14/2/Variability.html#Chebychev's-Bounds">Chebyshev’s inequality</a>, which you saw in Data 8, no matter what the shape of the distribution of X is, the vast majority of the probability lies in the interval “expectation plus or minus a few SDs.”</p>
<p>If we expand the square and use properties of expectation, we can re-express variance as the <strong>computational formula for variance</strong>. This form is often more convenient to use when computing the variance of a variable by hand, and it is also useful in Mean Squared Error calculations, as <span class="math inline">\(\mathbb{E}[X^2] = \text{Var}(X)\)</span> if <span class="math inline">\(X\)</span> is centered and <span class="math inline">\(E(X)=0\)</span>.</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
   \text{Var}(X) &amp;= \mathbb{E}[(X-\mathbb{E}[X])^2] \\
   &amp;= \mathbb{E}(X^2 - 2X\mathbb{E}(X) + (\mathbb{E}(X))^2) \\
   &amp;= \mathbb{E}(X^2) - 2 \mathbb{E}(X)\mathbb{E}(X) +( \mathbb{E}(X))^2\\
   &amp;= \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{align}\]</span></p>
</div>
</div>
</div>
<p>How do we compute <span class="math inline">\(\mathbb{E}[X^2]\)</span>? Any function of a random variable is <em>also</em> a random variable – that means that by squaring <span class="math inline">\(X\)</span>, we’ve created a new random variable. To compute <span class="math inline">\(\mathbb{E}[X^2]\)</span>, we can simply apply our definition of expectation to the random variable <span class="math inline">\(X^2\)</span>.</p>
<p><span class="math display">\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x)\]</span></p>
</section>
<section id="example-dice" class="level3">
<h3 class="anchored" data-anchor-id="example-dice">Example: Dice</h3>
<p>Let <span class="math inline">\(X\)</span> be the outcome of a single fair dice roll. <span class="math inline">\(X\)</span> is a random variable defined as <span class="math display">\[X = \begin{cases}
      \frac{1}{6}, \text{if } x \in \{1,2,3,4,5,6\} \\
      0, \text{otherwise}
   \end{cases}\]</span></p>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What’s the expectation <span class="math inline">\(\mathbb{E}[X]?\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[ \begin{align}
         \mathbb{E}[X] &amp;= 1(\frac{1}{6}) + 2(\frac{1}{6}) + 3(\frac{1}{6}) + 4(\frac{1}{6}) + 5(\frac{1}{6}) + 6(\frac{1}{6}) \\
         &amp;= (\frac{1}{6}) ( 1 + 2 + 3 + 4 + 5 + 6) \\
         &amp;= \frac{7}{2}
      \end{align}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What’s the variance <span class="math inline">\(\text{Var}(X)?\)</span>
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Using approach 1: <span class="math display">\[\begin{align}
      \text{Var}(X) &amp;= (\frac{1}{6})((1 - \frac{7}{2})^2 + (2 - \frac{7}{2})^2 + (3 - \frac{7}{2})^2 + (4 - \frac{7}{2})^2 + (5 - \frac{7}{2})^2 + (6 - \frac{7}{2})^2) \\
      &amp;= \frac{35}{12}
   \end{align}\]</span></p>
<p>Using approach 2: <span class="math display">\[\mathbb{E}[X^2] = \sum_{x} x^2 P(X = x) = \frac{91}{6}\]</span> <span class="math display">\[\text{Var}(X) = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12}\]</span></p>
</div>
</div>
</div>
</section>
</section>
<section id="sums-of-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="sums-of-random-variables">Sums of Random Variables</h2>
<p>Often, we will work with multiple random variables at the same time. A function of a random variable is also a random variable; if you create multiple random variables based on your sample, then functions of those random variables are also random variables.</p>
<p>For example, if <span class="math inline">\(X_1, X_2, ..., X_n\)</span> are random variables, then so are all of these:</p>
<ul>
<li><span class="math inline">\(X_n^2\)</span></li>
<li><span class="math inline">\(\#\{i : X_i &gt; 10\}\)</span></li>
<li><span class="math inline">\(\text{max}(X_1, X_2, ..., X_n)\)</span></li>
<li><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (X_i - c)^2\)</span></li>
<li><span class="math inline">\(\frac{1}{n} \sum_{i=1}^n X_i\)</span></li>
</ul>
<section id="equal-vs.-identically-distributed-vs.-i.i.d" class="level3">
<h3 class="anchored" data-anchor-id="equal-vs.-identically-distributed-vs.-i.i.d">Equal vs.&nbsp;Identically Distributed vs.&nbsp;i.i.d</h3>
<p>Suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<ul>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>equal</strong> if <span class="math inline">\(X(s) = Y(s)\)</span> for every sample <span class="math inline">\(s\)</span>. Regardless of the exact sample drawn, <span class="math inline">\(X\)</span> is always equal to <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>identically distributed</strong> if the distribution of <span class="math inline">\(X\)</span> is equal to the distribution of <span class="math inline">\(Y\)</span>. We say “X and Y are equal in distribution.” That is, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> take on the same set of possible values, and each of these possible values is taken with the same probability. On any specific sample <span class="math inline">\(s\)</span>, identically distributed variables do <em>not</em> necessarily share the same value. If X = Y, then X and Y are identically distributed; however, the converse is not true (ex: Y = 7-X, X is a die)</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent and identically distributed (i.i.d)</strong> if
<ol type="1">
<li>The variables are identically distributed.</li>
<li>Knowing the outcome of one variable does not influence our belief of the outcome of the other.</li>
</ol></li>
</ul>
<p>For example, let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be numbers on rolls of two fair die. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are i.i.d, so <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the same distribution. However, the sums <span class="math inline">\(Y = X_1 + X_1 = 2X_1\)</span> and <span class="math inline">\(Z=X_1+X_2\)</span> have different distributions but the same expectation.</p>
<p align="center">
<img src="images/yz_distribution.png" alt="distribution" width="=500">
</p>
<p>However, <span class="math inline">\(Y = X_1\)</span> has a larger variance</p>
<p align="center">
<img src="images/yz.png" alt="distribution" width="200">
</p>
</section>
<section id="properties-of-expectation" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-expectation">Properties of Expectation</h3>
<p>Instead of simulating full distributions, we often just compute expectation and variance directly. Recall the definition of expectation: <span class="math display">\[\mathbb{E}[X] = \sum_{x} x P(X=x)\]</span> From it, we can derive some useful properties of expectation:</p>
<ol type="1">
<li><strong>Linearity of expectation</strong>. The expectation of the linear transformation <span class="math inline">\(aX+b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, is:</li>
</ol>
<p><span class="math display">\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
        \mathbb{E}[aX+b] &amp;= \sum_{x} (ax + b) P(X=x) \\
        &amp;= \sum_{x} (ax P(X=x) + bP(X=x)) \\
        &amp;= a\sum_{x}P(X=x) + b\sum_{x}P(X=x)\\
        &amp;= a\mathbb{E}(X) + b * 1
    \end{align}\]</span></p>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Expectation is also linear in <em>sums</em> of random variables.</li>
</ol>
<p><span class="math display">\[\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\]</span></p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
    \mathbb{E}[X+Y] &amp;= \sum_{s} (X+Y)(s) P(s) \\
    &amp;= \sum_{s} (X(s)P(s) + Y(s)P(s)) \\
    &amp;= \sum_{s} X(s)P(s) + \sum_{s} Y(s)P(s)\\
    &amp;= \mathbb{E}[X] + \mathbb{E}[Y]
\end{align}\]</span></p>
</div>
</div>
</div>
<ol start="3" type="1">
<li>If <span class="math inline">\(g\)</span> is a non-linear function, then in general, <span class="math display">\[\mathbb{E}[g(X)] \neq g(\mathbb{E}[X])\]</span> For example, if <span class="math inline">\(X\)</span> is -1 or 1 with equal probability, then <span class="math inline">\(\mathbb{E}[X] = 0\)</span>, but <span class="math inline">\(\mathbb{E}[X^2] = 1 \neq 0\)</span>.</li>
</ol>
</section>
<section id="properties-of-variance" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-variance">Properties of Variance</h3>
<p>Recall the definition of variance: <span class="math display">\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</span> Combining it with the properties of expectation, we can derive some useful properties of variance:</p>
<ol type="1">
<li>Unlike expectation, variance is <em>non-linear</em>. The variance of the linear transformation <span class="math inline">\(aX+b\)</span> is: <span class="math display">\[\text{Var}(aX+b) = a^2 \text{Var}(X)\]</span></li>
</ol>
<ul>
<li>Subsequently, <span class="math display">\[\text{SD}(aX+b) = |a| \text{SD}(X)\]</span></li>
<li>The full proof of this fact can be found using the definition of variance. As general intuition, consider that <span class="math inline">\(aX+b\)</span> scales the variable <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(a\)</span>, then shifts the distribution of <span class="math inline">\(X\)</span> by <span class="math inline">\(b\)</span> units.</li>
</ul>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>We know that <span class="math display">\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]</span></p>
<p>In order to compute <span class="math inline">\(\text{Var}(aX+b)\)</span>, consider that a shift by b units does not affect spread, so <span class="math inline">\(\text{Var}(aX+b) = \text{Var}(aX)\)</span>.</p>
<p>Then, <span class="math display">\[\begin{align}
    \text{Var}(aX+b) &amp;= \text{Var}(aX) \\
    &amp;= E((aX)^2) - (E(aX))^2 \\
    &amp;= E(a^2 X^2) - (aE(X))^2\\
    &amp;= a^2 (E(X^2) - (E(X))^2) \\
    &amp;= a^2 \text{Var}(X)
\end{align}\]</span></p>
</div>
</div>
</div>
<ul>
<li>Shifting the distribution by <span class="math inline">\(b\)</span> <em>does not</em> impact the <em>spread</em> of the distribution. Thus, <span class="math inline">\(\text{Var}(aX+b) = \text{Var}(aX)\)</span>.</li>
<li>Scaling the distribution by <span class="math inline">\(a\)</span> <em>does</em> impact the spread of the distribution.</li>
</ul>
<p align="center">
<img src="images/transformation.png" alt="transformation" width="600">
</p>
<ol start="2" type="1">
<li>Variance of sums of RVs is affected by the (in)dependence of the RVs. <span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) 2\text{cov}(X,Y)\]</span> <span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}\]</span></li>
</ol>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The variance of a sum is affected by the dependence between the two random variables that are being added. Let’s expand out the definition of <span class="math inline">\(\text{Var}(X + Y)\)</span> to see what’s going on.</p>
<p>To simplify the math, let <span class="math inline">\(\mu_x = \mathbb{E}[X]\)</span> and <span class="math inline">\(\mu_y = \mathbb{E}[Y]\)</span>.</p>
<p><span class="math display">\[ \begin{align}
\text{Var}(X + Y) &amp;= \mathbb{E}[(X+Y- \mathbb{E}(X+Y))^2] \\
&amp;= \mathbb{E}[((X - \mu_x) + (Y - \mu_y))^2] \\
&amp;= \mathbb{E}[(X - \mu_x)^2 + 2(X - \mu_x)(Y - \mu_y) + (Y - \mu_y)^2] \\
&amp;= \mathbb{E}[(X - \mu_x)^2] + \mathbb{E}[(Y - \mu_y)^2] + \mathbb{E}[(X - \mu_x)(Y - \mu_y)] \\
&amp;= \text{Var}(X) + \text{Var}(Y) + \mathbb{E}[(X - \mu_x)(Y - \mu_y)]
\end{align}\]</span></p>
</div>
</div>
</div>
</section>
<section id="covariance-and-correlation" class="level3">
<h3 class="anchored" data-anchor-id="covariance-and-correlation">Covariance and Correlation</h3>
<p>We define the <strong>covariance</strong> of two random variables as the expected product of deviations from expectation. Put more simply, covariance is a generalization of variance to <em>two</em> random variables: <span class="math inline">\(\text{Cov}(X, X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X)\)</span></p>
<p><span class="math display">\[\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\]</span></p>
<p>We can treat the covariance as a measure of association. Remember the definition of correlation given when we first established SLR?</p>
<p><span class="math display">\[r(X, Y) = \mathbb{E}\left[\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right] = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}\]</span></p>
<p>It turns out we’ve been quietly using covariance for some time now! If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{Cov}(X, Y) =0\)</span> and <span class="math inline">\(r(X, Y) = 0\)</span>. Note, however, that the converse is not always true: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> could have <span class="math inline">\(\text{Cov}(X, Y) = r(X, Y) = 0\)</span> but not be independent.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>Let <span class="math inline">\(X\)</span> be a random variable with distribution <span class="math inline">\(P(X=x)\)</span>.
<ul>
<li><span class="math inline">\(\mathbb{E}[X] = \sum_{x} x P(X=x)\)</span></li>
<li><span class="math inline">\(\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\)</span></li>
</ul></li>
<li>Let <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> be scalar values.
<ul>
<li><span class="math inline">\(\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\)</span></li>
<li><span class="math inline">\(\text{Var}(aX+b) = a^2 \text{Var}(X)\)</span></li>
</ul></li>
<li>Let <span class="math inline">\(Y\)</span> be another random variable.
<ul>
<li><span class="math inline">\(\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)</span></li>
<li><span class="math inline">\(\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) 2\text{cov}(X,Y)\)</span></li>
</ul></li>
</ul>
</section>
</section>
<section id="common-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="common-random-variables">Common Random Variables</h2>
<p>There are several cases of random variables that appear often and have useful properties. Below are the ones we will explore further in this course. The numbers in parentheses are the parameters of a random variable, which are constants. Parameters define a random variable’s shape (i.e., distribution) and its values.</p>
<ul>
<li>Bernoulli(p)
<ul>
<li>Takes on <u>value 1 with probability p</u>, and <u>0 with probability 1 - p</u>.</li>
<li>AKA the “indicator” random variable.</li>
<li>Let X be a Bernoulli(p) random variable
<ul>
<li><span class="math inline">\(\mathbb{E}[X] = 1 * p + 0 * (1-p) = p\)</span>
<ul>
<li><span class="math inline">\(\mathbb{E}[X^2] = 1^2 * p + 0 * (1-p) = p\)</span></li>
</ul></li>
<li><span class="math inline">\(\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2 = p - p^2 = p(1-p)\)</span></li>
</ul></li>
</ul></li>
<li>Binomial(n, p)
<ul>
<li>Number of 1s in <span class="math inline">\(n\)</span> independent Bernoulli(p) trials.</li>
<li>Let <span class="math inline">\(Y\)</span> be a Binomial(n, p) random variable.
<ul>
<li>The distribution of <span class="math inline">\(Y\)</span> is given by the binomial formula, and we can write <span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span> where:
<ul>
<li><span class="math inline">\(X_i\)</span> s the indicator of success on trial i. <span class="math inline">\(X_i = 1\)</span> if trial i is a success, else 0.</li>
<li>All <span class="math inline">\(X_i\)</span> are i.i.d. and Bernoulli(p).</li>
</ul></li>
<li><span class="math inline">\(\mathbb{E}[Y] = \sum_{i=1}^n \mathbb{E}[X_i] = np\)</span></li>
<li><span class="math inline">\(\text{Var}(X) = \sum_{i=1}^n \text{Var}(X_i) = np(1-p)\)</span>
<ul>
<li><span class="math inline">\(X_i\)</span>’s are independent, so <span class="math inline">\(\text{Cov}(X_i, X_j) = 0\)</span> for all i, j.</li>
</ul></li>
</ul></li>
</ul></li>
<li>Uniform on a finite set of values
<ul>
<li>Probability of each value is 1 / (number of possible values).</li>
<li>For example, a standard/fair die.</li>
</ul></li>
<li>Uniform on the unit interval (0, 1)
<ul>
<li>Density is flat at 1 on (0, 1) and 0 elsewhere.</li>
</ul></li>
<li>Normal(<span class="math inline">\(\mu, \sigma^2\)</span>)
<ul>
<li><span class="math inline">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right)\)</span></li>
</ul></li>
</ul>
</section>
<section id="populations-and-samples" class="level2">
<h2 class="anchored" data-anchor-id="populations-and-samples">Populations and Samples</h2>
<p>Today, we’ve talked extensively about populations; if we know the distribution of a random variable, we can reliably compute expectation, variance, functions of the random variable, etc.</p>
<p>In Data Science, however, we often do not have access to the whole population, so we don’t know its distribution. As such, we need to collect a sample and use its distribution to estimate or infer properties of the population.</p>
<p>When sampling, we make the (big) assumption that we sample uniformly at random with replacement from the population; each observation in our sample is a random variable drawn i.i.d from our population distribution.</p>
<section id="sample-mean" class="level3">
<h3 class="anchored" data-anchor-id="sample-mean">Sample Mean</h3>
<p>Consider an i.i.d. sample <span class="math inline">\(X_1, X_2, ..., X_n\)</span> drawn from a population with mean 𝜇 and SD 𝜎. We define the sample mean as <span class="math display">\[\bar{X_n} = \frac{1}{n} \sum_{i=1}^n X_i\]</span></p>
<p>The expectation of the sample mean is given by: <span class="math display">\[\begin{align}
    \mathbb{E}[\bar{X_n}] &amp;= \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] \\
    &amp;= \frac{1}{n} (n \mu) \\
    &amp;= \mu
\end{align}\]</span></p>
<p>The variance is given by: <span class="math display">\[\begin{align}
    \text{Var}(\bar{X_n}) &amp;= \frac{1}{n^2} \text{Var}( \sum_{i=1}^n X_i) \\
    &amp;=  \frac{1}{n^2} \left( \sum_{i=1}^n \text{Var}(X_i) \right) \\
    &amp;=  \frac{1}{n^2} (n \sigma^2) = \frac{\sigma^2}{n}
\end{align}\]</span></p>
<p><span class="math inline">\(\bar{X_n}\)</span> is normally distributed by the Central Limit Theorem (CLT).</p>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">Central Limit Theorem</h3>
<p>The CLT states that no matter what population you are drawing from, if an i.i.d. sample of size <span class="math inline">\(n\)</span> is large, the probability distribution of the sample mean is roughly normal with mean 𝜇 and SD <span class="math inline">\(\sigma/\sqrt{n}\)</span>.</p>
<p>Any theorem that provides the rough distribution of a statistic and doesn’t need the distribution of the population is valuable to data scientists because we rarely know a lot about the population!</p>
<p>For a more in-depth demo, check out <a href="https://onlinestatbook.com/stat_sim/sampling_dist/">onlinestatbook</a>.</p>
<p>The CLT applies if the sample size <span class="math inline">\(n\)</span> is large, but how large does n have to be for the normal approximation to be good? It depends on the shape of the distribution of the population.</p>
<ul>
<li>If the population is roughly symmetric and unimodal/uniform, could need as few as <span class="math inline">\(n = 20\)</span>.</li>
<li>If the population is very skewed, you will need a bigger n.</li>
<li>If in doubt, you can bootstrap the sample mean and see if the bootstrapped distribution is bell-shaped.</li>
</ul>
</section>
<section id="using-the-sample-mean-to-estimate-the-population-mean" class="level3">
<h3 class="anchored" data-anchor-id="using-the-sample-mean-to-estimate-the-population-mean">Using the Sample Mean to Estimate the Population Mean</h3>
<p>Our goal with sampling is often to estimate some characteristic of a population. When we collect a single sample, it has just one average. Since our sample was random, it <em>could</em> have come out differently. The CLT helps us understand this difference. We should consider the average value and spread of all possible sample means, and what this means for how big <span class="math inline">\(n\)</span> should be.</p>
<p>For every sample size, the expected value of the sample mean is the population mean. <span class="math inline">\(\mathbb{E}[\bar{X_n}] = \mu\)</span>. We call the sample mean an unbiased estimator of the population mean, and we’ll cover this more in the next lecture.</p>
<p>The square root law (<a href="https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html#the-square-root-law">Data 8</a>) states that if you increase the sample size by a factor, the SD decreases by the square root of the factor. <span class="math inline">\(\text{SD}(\bar{X_n}) = \frac{\sigma}{\sqrt{n}}\)</span>. The sample mean is more likely to be close to the population mean if we have a larger sample size.</p>
<p align="center">
<img src="images/SD_change.png" alt="transformation" width="400">
</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>