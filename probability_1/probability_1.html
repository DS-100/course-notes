<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Probability I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="probability_1_files/libs/clipboard/clipboard.min.js"></script>
<script src="probability_1_files/libs/quarto-html/quarto.js"></script>
<script src="probability_1_files/libs/quarto-html/popper.min.js"></script>
<script src="probability_1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="probability_1_files/libs/quarto-html/anchor.min.js"></script>
<link href="probability_1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="probability_1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="probability_1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="probability_1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="probability_1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Probability I</h2>
   
  <ul>
  <li><a href="#random-variables-and-distributions" id="toc-random-variables-and-distributions" class="nav-link active" data-scroll-target="#random-variables-and-distributions">Random Variables and Distributions</a>
  <ul class="collapse">
  <li><a href="#common-random-variables" id="toc-common-random-variables" class="nav-link" data-scroll-target="#common-random-variables">Common Random Variables</a></li>
  <li><a href="#simulation" id="toc-simulation" class="nav-link" data-scroll-target="#simulation">Simulation</a></li>
  </ul></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance">Expectation and Variance</a>
  <ul class="collapse">
  <li><a href="#descriptive-properties-of-random-variables" id="toc-descriptive-properties-of-random-variables" class="nav-link" data-scroll-target="#descriptive-properties-of-random-variables">Descriptive Properties of Random Variables</a></li>
  <li><a href="#review" id="toc-review" class="nav-link" data-scroll-target="#review">Review</a></li>
  <li><a href="#expectation" id="toc-expectation" class="nav-link" data-scroll-target="#expectation">Expectation</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance">Variance</a></li>
  <li><a href="#sums-of-random-variables" id="toc-sums-of-random-variables" class="nav-link" data-scroll-target="#sums-of-random-variables">Sums of Random Variables</a></li>
  <li><a href="#bernoulli-and-binomial-random-variables" id="toc-bernoulli-and-binomial-random-variables" class="nav-link" data-scroll-target="#bernoulli-and-binomial-random-variables">Bernoulli and Binomial Random Variables</a></li>
  <li><a href="#sample-statistics" id="toc-sample-statistics" class="nav-link" data-scroll-target="#sample-statistics">Sample Statistics</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Probability I</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Learning Outcomes
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Define a random variable in terms of its distribution</li>
<li>Compute the expectation and variance of a random variable</li>
<li>Gain familiarity with the Bernoulli and binomial random variables</li>
</ul>
</div>
</div>
</div>
<p>In the past few lectures, we’ve examined the role of complexity in influencing model performance. We’ve considered model complexity in the context of a tradeoff between two competing factors: model variance and training error.</p>
<p>Thus far, our analysis has been mostly qualitative. We’ve acknowledged that our choice of model complexity needs to strike a balance between model variance and training error, but we haven’t yet discussed <em>why</em> exactly this tradeoff exists.</p>
<p>To better understand the origin of this tradeoff, we will need to introduce the language of <strong>random variables</strong>. The next two lectures of probability will be a brief digression from our work on modeling so we can build up the concepts needed to understand this so-called <strong>bias-variance tradeoff</strong>. Our roadmap for the next few lectures will be:</p>
<ol type="1">
<li>Probability I: introduce random variables, considering the concepts of expectation, variance, and covariance</li>
<li>Probability II: re-express the ideas of model variance and training error in terms of random variables and use this new pespective to investigate our choice of model complexity</li>
</ol>
<p>Let’s get to it.</p>
<section id="random-variables-and-distributions" class="level2">
<h2 class="anchored" data-anchor-id="random-variables-and-distributions">Random Variables and Distributions</h2>
<p>Suppose we generate a set of random data, like a random sample from some population. A <strong>random variable</strong> is a numerical function of the randomness in the data. It is <em>random</em> from the randomness of the sample; it is <em>variable</em> because its exact value depends on how this random sample came out.</p>
<ul>
<li>We typically denote random variables with uppercase letters, such as <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>.</li>
<li>It’s value on any given draw is called a realization.</li>
<li>Domain (input): all random samples of size n / all possible outcomes of our random process.</li>
<li>Range (output): any subset of the real number line.</li>
</ul>
<p>To give a concrete example: say we draw a random sample <span class="math inline">\(s\)</span> of size 3 from all students enrolled in Data 100. We might then define the random variable <span class="math inline">\(X\)</span> to be the number of Data Science majors in this sample.</p>
<p><img src="images/rv.png" alt="rv" width="600"></p>
<p>The <strong>distribution</strong> of a random variable <span class="math inline">\(X\)</span> describes how the total probability of 100% is split over all possible values that <span class="math inline">\(X\)</span> could take.</p>
<p>If <span class="math inline">\(X\)</span> is a <strong>discrete random variable</strong> with a finite number of possible values, define its distribution by stating the probability of <span class="math inline">\(X\)</span> taking on some specific value, <span class="math inline">\(x\)</span>, for all possible values of <span class="math inline">\(x\)</span>.</p>
<p><img src="images/distribution.png" alt="distribution" width="400"></p>
<p>The distribution of a discrete variable can also be represented using a histogram. If a variable is <strong>continuous</strong> – it can take on infinitely many values – we can illustrate its distribution using a density curve.</p>
<p><img src="images/discrete_continuous_2.png" alt="discrete_continuous" width="600"></p>
<ul>
<li>A distribution fully defines a random variable.</li>
<li><span class="math inline">\(P(X = x)\)</span>: the probability that a random variable <span class="math inline">\(X\)</span> takes on the value <span class="math inline">\(x\)</span></li>
<li>$_{x } P(X = x) = 1 $: Probabilities must sum to 1 (for discrete random variables)</li>
</ul>
<section id="common-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="common-random-variables">Common Random Variables</h3>
<p>Random variables are defined by <em>parameters</em>. These are numbers that determine the distribution of our random variable. Distributions can have any number of parameters that describe them. Examples of parameters: <span class="math inline">\(\mu\)</span> describes the mean for normal distributions, <span class="math inline">\(n\)</span> describes the number of samples for a Binomial distribution, etc.</p>
<ol type="1">
<li>Bernoulli(p)
<ul>
<li>Takes on value 1 with probability p, and 0 with probability 1 - p.</li>
<li>AKA the “indicator” random variable.</li>
<li>Can be seen as a special case (n = 1) of the Binomial</li>
<li>EX: Single coin flip</li>
</ul></li>
<li>Binomial(n, p)
<ul>
<li>Number of 1s in ‘n’ independent Bernoulli(p) trials.</li>
<li>EX: flipping n coins all at once.</li>
</ul></li>
<li>Uniform on a finite set of values
<ul>
<li>Probability of each value is 1 / (size of set).</li>
<li>EX: standard/fair die.</li>
</ul></li>
<li>Uniform on the unit interval(0, 1)
<ul>
<li>Density is flat on (0, 1) and 0 elsewhere.</li>
</ul></li>
<li>Normal(<span class="math inline">\(\mu, \sigma^2\)</span>)
<ul>
<li>Density: <span class="math inline">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\)</span></li>
<li><img src="images/normal_curve.png" alt="discrete_continuous" width="300"></li>
</ul></li>
</ol>
</section>
<section id="simulation" class="level3">
<h3 class="anchored" data-anchor-id="simulation">Simulation</h3>
<p>In many cases, if we are given a random variable we want can use code to create <em>simulations</em> of the random variable. To do so, we randomly pick values of <span class="math inline">\(X\)</span> according to its <em>distribution</em>.</p>
<ul>
<li><code>numpy.random.choice</code></li>
<li><code>DataFrame.sample</code></li>
</ul>
<p>We also have preprogrammed functions for drawing from common distributions.</p>
<ul>
<li><code>Binomial: numpy.random.binomial</code></li>
<li><code>Uniform: numpy.random.uniform</code></li>
<li><code>Normal: numpy.random.normal</code></li>
</ul>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Our random variable X</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>dist_df <span class="op">=</span> pd.DataFrame({<span class="st">"x"</span>: [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>],</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"P(X = x)"</span>: [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.3</span>]})</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>dist_df</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x</th>
      <th>P(X = x)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8</td>
      <td>0.3</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_samples(df, xname<span class="op">=</span><span class="st">"x"</span>, pname<span class="op">=</span><span class="st">"P(X = x)"</span>, size<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.choice(</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                df[xname], <span class="co"># draw from these choiecs</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                size<span class="op">=</span>size, <span class="co"># this many times</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                p<span class="op">=</span>df[pname]) <span class="co"># according to this distribution</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">80000</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>all_samples <span class="op">=</span> simulate_samples(dist_df, size<span class="op">=</span>N)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>sim_df <span class="op">=</span> pd.DataFrame({<span class="st">"X(s)"</span>: all_samples})</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>sim_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>X(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
    </tr>
    <tr>
      <th>3</th>
      <td>6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="34">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate a sample from our full data. Note how it's in a DataFrame</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x_0 <span class="op">=</span> sim_df.sample() </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>x_0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="34">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>X(s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>47044</th>
      <td>8</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#alternative for when given the distribution</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x_1 <span class="op">=</span> np.random.choice(dist_df[<span class="st">'x'</span>],p<span class="op">=</span>dist_df[<span class="st">'P(X = x)'</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"single draw from dist_df: "</span>,x_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>single draw from dist_df:  6</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#generate draws from common distributions</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>x_2 <span class="op">=</span> np.random.binomial(n <span class="op">=</span> <span class="dv">3</span>,p <span class="op">=</span> <span class="fl">0.5</span>) </span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"single draw of number of heads in 3 coin flips:"</span>, x_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>single draw of number of heads in 3 coin flips: 1</code></pre>
</div>
</div>
</section>
</section>
<section id="expectation-and-variance" class="level2">
<h2 class="anchored" data-anchor-id="expectation-and-variance">Expectation and Variance</h2>
<p>Often, it is easier to describe a random variable using some numerical summary, rather than fully defining its distribution. These numerical summaries are numbers that characterize some properties of the random variable. Because they give a “summary” of how the variable tends to behave, they are <em>not</em> random – think of them as a static number that describes a certain property of the random variable. In Data 100, we will focus our attention on the expectation and variance of a random variable.</p>
<section id="descriptive-properties-of-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="descriptive-properties-of-random-variables">Descriptive Properties of Random Variables</h3>
<p><img src="images/exp_var.png" alt="description plots" width="600"></p>
</section>
<section id="review" class="level3">
<h3 class="anchored" data-anchor-id="review">Review</h3>
<p>The <em>mean</em> or <em>expectation</em> of a random variable can be visually seen as the center of gravity or balance point of the histogram.</p>
<p>The <em>variance</em> is a measure of spread. It is the expected squared deviation from the mean.</p>
<p>Visually, these can be sanity checked by printing the histogram as so:</p>
<p><img src="images/labeled_hist.png" alt="description plots" width="400"></p>
</section>
<section id="expectation" class="level3">
<h3 class="anchored" data-anchor-id="expectation">Expectation</h3>
<p>The <strong>expectation</strong> of a random variable <span class="math inline">\(X\)</span> is the weighted average of the values of <span class="math inline">\(X\)</span>, where the weights are the probabilities of each value occurring. To compute the expectation, we find each value <span class="math inline">\(x\)</span> that the variable could possibly take, weight by the probability of the variable taking on each specific value, and sum across all possible values of <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[\mathbb{E}[X] = \sum_{\text{all possible } x} x P(X=x)\]</span></p>
<section id="example" class="level4">
<h4 class="anchored" data-anchor-id="example">Example</h4>
<p>Consider the random variable <span class="math inline">\(X\)</span> we defined earlier. Note that the expectation can be a value that is not attainable in one draw of X!</p>
<p><span class="math display">\[E[X] = 3(0.1) + 4(0.2) + 6(0.4) + 8(0.3) = 5.9\]</span></p>
</section>
<section id="recap" class="level4">
<h4 class="anchored" data-anchor-id="recap">Recap</h4>
<p><strong>Expectation</strong> is a <strong>number</strong>, not a <strong>random variable</strong>! - It is analogous to the average (same units as the random variable). - It is the center of gravity of the probability histogram. - It is the long run average of the random variable, if you simulate the variable many times.</p>
</section>
</section>
<section id="variance" class="level3">
<h3 class="anchored" data-anchor-id="variance">Variance</h3>
<p>The <strong>variance</strong> of a random variable is a measure of its chance error. It is defined as the expected squared deviation from the expectation of <span class="math inline">\(X\)</span>. Put more simply, variance asks: how far does <span class="math inline">\(X\)</span> typically vary from its average value? What is the spread of <span class="math inline">\(X\)</span>’s distribution?</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[(X-\mathbb{E}[X])^2]\]</span></p>
<p>If we expand the square and use properties of expectation, we can re-express this statement as the <strong>computational formula for variance</strong>. This form is often more convenient to use when computing the variance of a variable by hand.</p>
<p><span class="math display">\[\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2\]</span></p>
<p>How do we compute the expectation of <span class="math inline">\(X^2\)</span>? Any function of a random variable is <em>also</em> a random variable – that means that by squaring <span class="math inline">\(X\)</span>, we’ve created a new random variable. To compute <span class="math inline">\(\mathbb{E}[X^2]\)</span>, we can simply apply our definition of expectation to the random variable <span class="math inline">\(X^2\)</span>.</p>
<p><span class="math display">\[\mathbb{E}[X^2] = \sum_{\text{all possible } x} x^2 P(X^2 = x^2)\]</span></p>
<section id="example-1" class="level4">
<h4 class="anchored" data-anchor-id="example-1">Example</h4>
<p>Let’s revisit a common example: the roll of a fair 6-sided die (<span class="math inline">\(Y\)</span>).</p>
<ol type="1">
<li>What is the expectation <span class="math inline">\(E(X)\)</span>?</li>
</ol>
<p><span class="math display">\[E[X] = 1(\frac{1}{6}) + 2(\frac{1}{6}) + 3(\frac{1}{6}) + 4(\frac{1}{6}) + 5(\frac{1}{6}) + 6(\frac{1}{6}) = \frac{7}{2}\]</span></p>
<ol start="2" type="1">
<li>What is the variance <span class="math inline">\(Var(X)\)</span>?</li>
</ol>
<p>Using Definition:</p>
<p><span class="math inline">\(Var(X) = (\frac{1}{6})(1 - \frac{7}{2}) + (\frac{1}{6})(2 - \frac{7}{2}) + (\frac{1}{6})(3 - \frac{7}{2}) + (\frac{1}{6})(4 - \frac{7}{2}) + (\frac{1}{6})(5 - \frac{7}{2}) +(\frac{1}{6})(6 - \frac{7}{2}) = \frac{35}{12}\)</span></p>
<p>Using Variance Decomposition:</p>
<p><span class="math inline">\(E(X^2) = 1(\frac{1}{6}) + 4(\frac{1}{6}) + 9(\frac{1}{6}) + 16(\frac{1}{6}) + 25(\frac{1}{6}) + 36(\frac{1}{6}) = \frac{91}{6}\)</span></p>
<p><span class="math inline">\(E(X)^2 = (\frac{7}{2})^2\)</span></p>
<p><span class="math inline">\(Var(X) = E(X^2) - E(X)^2 = \frac{91}{6} - (\frac{7}{2})^2 = \frac{35}{12}\)</span></p>
</section>
<section id="recap-1" class="level4">
<h4 class="anchored" data-anchor-id="recap-1">Recap</h4>
<p><strong>Variance</strong> is a <strong>number</strong>, not a <strong>random variable</strong>! - It is analogous to the average squared distance from average (measured in squared units of the random variable). - It is the long run squared difference of the random variable from its mean if you simulate the variable many times.</p>
</section>
</section>
<section id="sums-of-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="sums-of-random-variables">Sums of Random Variables</h3>
<section id="equality-vs-identically-distributed-vs.-iid" class="level4">
<h4 class="anchored" data-anchor-id="equality-vs-identically-distributed-vs.-iid">Equality vs Identically Distributed vs.&nbsp;IID</h4>
<p>Often, we will work with multiple random variables at the same time. In our example above, we could have defined the random variable <span class="math inline">\(X\)</span> as the number of Data Science majors in our sample of students, and the variable <span class="math inline">\(Y\)</span> as the number of Statistics majors in the sample. For any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<ul>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>equal</strong> if <span class="math inline">\(X(s) = Y(s)\)</span> for every sample <span class="math inline">\(s\)</span>. Regardless of the exact sample drawn, <span class="math inline">\(X\)</span> is always equal to <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>identically distributed</strong> if the distribution of <span class="math inline">\(X\)</span> is equal to the distribution of <span class="math inline">\(Y\)</span>. That is, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> take on the same set of possible values, and each of these possible values is taken with the same probability. On any specific sample <span class="math inline">\(s\)</span>, identically distributed variables do <em>not</em> necessarily share the same value.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent and identically distributed (IID)</strong> if 1) the variables are identically distributed and 2) knowing the outcome of one variable does not influence our belief of the outcome of the other.</li>
</ul>
</section>
<section id="properties-of-expectation-and-variance" class="level4">
<h4 class="anchored" data-anchor-id="properties-of-expectation-and-variance">Properties of Expectation and Variance</h4>
<p>An important property in probability is the <strong>linearity of expectation</strong>. The expectation of the linear transformation <span class="math inline">\(aX+b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, is:</p>
<p><span class="math display">\[\mathbb{E}[aX+b] = aE[\mathbb{X}] + b\]</span></p>
<p>Expectation is also linear in <em>sums</em> of random variables.</p>
<p><span class="math display">\[\mathbb{E}[X+Y] = \mathbb{E}[X] + \mathbb{E}[Y]\]</span></p>
<p>Unlike expectation, variance is <em>non-linear</em>. The variance of the linear transformation <span class="math inline">\(aX+b\)</span> is:</p>
<p><span class="math display">\[\text{Var}(aX+b) = a^2 \text{Var}(X)\]</span></p>
<p>The full proof of this fact can be found using the definition of variance. As general intuition, consider that <span class="math inline">\(aX+b\)</span> scales the variable <span class="math inline">\(X\)</span> by a factor of <span class="math inline">\(a\)</span>, then shifts the distribution of <span class="math inline">\(X\)</span> by <span class="math inline">\(b\)</span> units.</p>
<ul>
<li>Shifting the distribution by <span class="math inline">\(b\)</span> <em>does not</em> impact the <em>spread</em> of the distribution. Thus, <span class="math inline">\(\text{Var}(aX+b) = \text{Var}(aX)\)</span>.</li>
<li>Scaling the distribution by <span class="math inline">\(a\)</span> <em>does</em> impact the spread of the distribution.</li>
</ul>
<p><img src="images/transformation.png" alt="transformation" width="600"></p>
<p>If we wish to understand the spread in the distribution of the <em>summed</em> random variables <span class="math inline">\(X + Y\)</span>, we can manipulate the definition of variance to find:</p>
<p><span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]\]</span></p>
</section>
<section id="covariance" class="level4">
<h4 class="anchored" data-anchor-id="covariance">Covariance</h4>
<p>This last term is of special significance. We define the <strong>covariance</strong> of two random variables as the expected product of deviations from expectation. Put more simply, covariance is a generalization of variance to <em>two</em> random variables: <span class="math inline">\(\text{Cov}(X, X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \text{Var}(X)\)</span>.</p>
<p><span class="math display">\[\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]\]</span></p>
<p>We can treat the covariance as a measure of association. Remember the definition of correlation given when we first established SLR?</p>
<p><span class="math display">\[r(X, Y) = \mathbb{E}\left[\left(\frac{X-\mathbb{E}[X]}{\text{SD}(X)}\right)\left(\frac{Y-\mathbb{E}[Y]}{\text{SD}(Y)}\right)\right] = \frac{\text{Cov}(X, Y)}{\text{SD}(X)\text{SD}(Y)}\]</span></p>
<p>It turns out we’ve been quietly using covariance for some time now! If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(\text{Cov}(X, Y) =0\)</span> and <span class="math inline">\(r(X, Y) = 0\)</span>. Note, however, that the converse is not always true: <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> could have <span class="math inline">\(\text{Cov}(X, Y) = r(X, Y) = 0\)</span> but not be independent. This means that the variance of a sum of independent random variables is the sum of their variances: <span class="math display">\[\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \qquad \text{if } X, Y \text{ independent}\]</span></p>
</section>
</section>
<section id="bernoulli-and-binomial-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-and-binomial-random-variables">Bernoulli and Binomial Random Variables</h3>
<p>Recall that Bernoulli random variables have one parameter(p) that represents the probability of observing the value 1. Bernoulli variables can only take on the values 1 and 0. The distribution can be summarized as follows <a href="http://prob140.org/textbook/content/Chapter_03/02_Distributions.html?highlight=boolean#named-distributions">source</a>:</p>
<table class="table">
<caption>Bernoulli Distribution</caption>
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>x</th>
<th>0</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>P(X=x)</td>
<td>1-p</td>
<td>p</td>
</tr>
</tbody>
</table>
<p>Bernoulli:</p>
<ul>
<li>Expectation: <span class="math inline">\(E(X) = p\)</span></li>
<li>Variance: <span class="math inline">\(Var(X) = p(1-p)\)</span></li>
</ul>
<p>Furthermore, the sum of Bernoulli variables is so commonly used that it has its own name: the Binomial. Binomial random variables have two parameters(n,p) that represent the number of independent Bernoulli variables and the probability of observing a 1 from each Bernoulli. We actually saw this earlier in the course via the Binomial formula.</p>
<p><span class="math inline">\(P(Y = y) = \binom{n}{y} p^y (1-p)^{n-y}\)</span></p>
<p>We can write a Binomial(n,p), <span class="math inline">\(Y\)</span> , equivalently as the sum of <span class="math inline">\(n\)</span> Binomial random variables, <span class="math inline">\(X_i\)</span>, like so:</p>
<p><span class="math inline">\(Y = \sum_{i=1}^n X_i\)</span></p>
<p>Using the rules of expectation and variance, we arrive at the following:</p>
<ul>
<li>Expectation: <span class="math inline">\(E(Y) = \sum_{i = 1}^n E(X_i) = np\)</span></li>
<li>Variance: <span class="math inline">\(Var(Y) = \sum_{i = 1}^n Var(X_i) = np(1-p)\)</span></li>
</ul>
</section>
<section id="sample-statistics" class="level3">
<h3 class="anchored" data-anchor-id="sample-statistics">Sample Statistics</h3>
<p>Previously, we’ve talked extensively about <strong>populations</strong>:</p>
<ul>
<li>If we know the <strong>distribution of a random variable</strong>, we can reliably compute expectation, variance, functions of the random variable, etc.</li>
</ul>
<p>However, in Data Science, we often collect <strong>samples</strong>.</p>
<ul>
<li>We don’t know the distribution of our population.</li>
<li>We’d like to use the distribution of your sample to estimate/infer properties of the population.</li>
</ul>
<p>A big assumption we make in modeling/inference is that out sampling is unbiased. This is a heavy assumption, and is fundamentally untestable. This is why we have talked so much about sampling and the kinds of biases that could arise up until now.</p>
<p><img src="images\sampling.png" width="400"></p>
<section id="sample-as-a-set-of-iid-random-variables" class="level4">
<h4 class="anchored" data-anchor-id="sample-as-a-set-of-iid-random-variables">Sample as a Set of IID Random Variables</h4>
<p>If done correctly, each observation in our sample is a <strong>Random Variable</strong> drawn <strong>IID</strong> from our population <strong>distribution</strong>.</p>
</section>
<section id="sample-mean" class="level4">
<h4 class="anchored" data-anchor-id="sample-mean">Sample Mean</h4>
<p>Consider an IID sample <span class="math inline">\(X_1 , X_2 , ..., X_n\)</span> drawn from a population with <em>each</em> a mean (<span class="math inline">\(\mu\)</span>) and standard deviation(<span class="math inline">\(\sigma\)</span>).</p>
<p>Sample mean: <span class="math inline">\(\bar{X}_n = \frac{1}{n} \sum_{i = 1}^{n} X_i\)</span></p>
<p>The <strong>sample mean</strong> is a <strong>random variable</strong> because it is the sum of other <strong>random variables</strong>.</p>
<p><span class="math inline">\(E(\bar{X}_n) = \frac{1}{n} \sum_{i = 1}^{n} E(X_i) = \frac{1}{n} \cdot n \cdot \mu\)</span></p>
<p><span class="math inline">\(Var(\bar{X}_n) = Var(\frac{1}{n} \sum_{i = 1}^{n} X_i) = \frac{1}{n^2} Var(\sum_{i = 1}^{n} X_i) = \frac{1}{n^2} \sum_{i = 1}^{n} Var(X_i) = \frac{n \sigma^2}{n^2} = \frac{\sigma^2}{n}\)</span></p>
</section>
<section id="central-limit-theorem" class="level4">
<h4 class="anchored" data-anchor-id="central-limit-theorem">Central Limit Theorem</h4>
<p><strong>No matter what population you are drawing from</strong>: If an IID sample of size <strong>n is large</strong>, the probability distribution of the sample mean is <strong>roughly normal</strong> with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the population mean and standard deviation respectively. For more details, see the <a href="http://prob140.org/textbook/content/Chapter_14/00_The_Central_Limit_Theorem.html?highlight=central%20limit">Stat 140 Textbook</a>.</p>
<p><strong>How large</strong> does n have to be for the normal approximation to be good? <strong>It depends</strong> on the shape of the distribution of the population.</p>
<ul>
<li>If population is <em>roughly symmetric</em> and unimodal/uniform, could need as few as n = 20.</li>
<li>If population is very <em>skewed</em>, you will need bigger n.</li>
<li>If in doubt, you can <strong>bootstrap</strong> the sample mean and see if the bootstrapped distribution is bell-shaped.</li>
</ul>
</section>
<section id="accuracy-and-spread-of-the-sample-mean" class="level4">
<h4 class="anchored" data-anchor-id="accuracy-and-spread-of-the-sample-mean">Accuracy and Spread of the Sample Mean</h4>
<p>Say our goal is often to estimate some characteristic of a population.</p>
<ul>
<li>Example: average height of Cal undergraduates.</li>
<li>We typically can collect a single sample. It has just one average.</li>
<li>Since that sample was random, it could have come out differently.</li>
</ul>
<p>We should consider the average value and spread of all possible sample means, and what this means for how big n should be.</p>
<p><span class="math inline">\(E[\bar{X}_n] = \mu, SD(\bar{X}_n = \frac{\sigma}{\sqrt{n}}\)</span></p>
<p>For every sample size, the expected value of the sample mean is the population mean.</p>
<p><img src="images\sample_mean.png" width="400"></p>
<p>We call the sample mean an <strong>unbiased estimator</strong> of the population mean. (more in next lecture)</p>
<p><strong>Square root law</strong> (<a href="https://inferentialthinking.com/chapters/14/5/Variability_of_the_Sample_Mean.html#the-square-root-law">Data 8</a>): If you increase the sample size by a factor, the SD decreases by the square root of the factor.</p>
<p>The sample mean is more likely to be close to the population mean if we have a larger sample size.</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>