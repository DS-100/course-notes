{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Cross Validation and Regularization\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 5\n",
    "    toc-location: right\n",
    "    code-fold: false\n",
    "    theme:\n",
    "      - cosmo\n",
    "      - cerulean\n",
    "    callout-icon: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended last lecture with a question: how do we control the complexity of our model? To answer this, we must know precisely when our model begins to overfit. The key to this lies in evaluating the model on unseen data using a process called Cross-Validation. A second point this note will address is how to combat overfitting -- namely, through a technique known as regularization.\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "From the last lecture, we learned that *increasing* model complexity *decreased* our model's training error but increased variance. This makes intuitive sense; adding more features causes our model to better fit the given data, but generalize worse to new data. For this reason, a low training error is not representative of our model's underlying performance -- this may be a side effect of overfitting.\n",
    "\n",
    "Truly, the only way to know when our model overfits is by evaluating it on unseen data. Unfortunately, that means we need to wait for more data. This may be very expensive and time consuming.\n",
    "\n",
    "How should we proceed? In this section, we will build up a viable solution to this problem.\n",
    "\n",
    "### The Holdout Method\n",
    "\n",
    "The simplest approach to avoid overfitting is to keep some of our data secret from ourselves. This is known as the **holdout method**. We will train our models on *most* of the available data points -- known as the **training data** . We'll then evaluate the models’ performance on the unseen data points (called the **validation set**) to measure overfitting.\n",
    "\n",
    "Imagine the following example where we wish to train a model on 35 data points. We choose the training set to be a random sample of 25 of these points, and the validation set to be the remaining 10 points. Using this data, we train 7 models, each with a higher polynomial degree than the last. \n",
    "\n",
    "We get the following mean squared error (MSE) on the training data.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "<img src=\"images/mse_graph.png\" alt='mse_graph' width='400'>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"20%\"}\n",
    "<!-- empty column to create gap -->\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "<img src=\"images/mse_errors.png\" alt='mse_errors' width='120'>\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "Using these same models, we compute the MSE on our 10 validation points and observe the following.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "<img src=\"images/mse_validation_graph.png\" alt='mse_validation_graph' width='400'>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"20%\"}\n",
    "<!-- empty column to create gap -->\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "<img src=\"images/mse_validation_errors.png\" alt='mse_validation_errors' width='250'>\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "Notice how the training error monotonically *decreases* as polynomial degree *increases*. This is consistent with our knowledge. However, validation error first *decreases*, then *increases* (from k = 3). These higher degree models are performing worse on unseen data -- this indicates they are overfitting. As such, the best choice of polynomial degree in this example is **k = 2**.\n",
    "\n",
    "More generally, we can represent this relationship with the following diagram.\n",
    "\n",
    "<img src=\"images/training_validation_curve.png\" alt='training_validation_curve' width='500'>\n",
    "\n",
    "Our goal is to train a model with complexity near the red line. Note that this relationship is a simplification of the real-world. But for purposes of Data 100, this is good enough.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "In machine learning, a **hyperparameter** is a value that controls the learning process. In our example, we built seven models, each of which had a hyperparameter `k` that controlled the polynomial degree of the model.\n",
    "\n",
    "To choose between hyperparameters, we use the validation set. This was evident in our example above; we found that `k` = 2 had the lowest validation error. However, this holdout method is a bit naive. Imagine our random sample of 10 validation points coincidentally favored a higher degree polynomial. This would have led us to incorrectly favor a more complex model. In other words, our small amount of validation data may be different from real-world data.\n",
    "\n",
    "To minimize this possiblity, we need to evaluate our model on more data. Decreasing the size of the training set is not an option -- doing so will worsen our model. How should we proceed?\n",
    "\n",
    "### K-Fold Cross Validation\n",
    "\n",
    "In the holdout method, we train a model on *only* the training set, and assess the quality *only* on the validation set. On the other hand, **K-Fold Cross Validation** is a technique that determines the quality of a hyperparameter by evaluating a model-hyperparameter combination on `k` independent \"folds\" of data, which together make up the *entire* dataset. This is a more robust alternative to the holdout method. Let's break down each step.\n",
    "\n",
    "**Note**: The `k` in k-fold cross validation is different from the `k` polynomial degree discussed in the earlier example.\n",
    "\n",
    "In the k-fold cross-validation approach, we split our data into `k` equally sized groups (often called folds). In the example where `k` = 5:\n",
    "\n",
    "![](images/cvfolds.png)\n",
    "\n",
    "To determine the \"quality\" of a particular hyperparameter:\n",
    "\n",
    "- Pick a fold, which we’ll call the validation fold. Train a model on all other `k-1` folds. Compute an error on the validation fold.\n",
    "- Repeat the step above for all `k` possible choices of validation fold, each time training a new model.\n",
    "- Average the `k` validation fold errors. This will give a single error for the hyperparameter.\n",
    "\n",
    "For `k` = 5, we have the following partitions. At each iteration, we train a model on the blue data and validate on the orange data. This gives us a total of 5 errors which we average to obtain a single representative error for the hyperparameter.\n",
    "\n",
    "![](images/kfold.png)\n",
    "\n",
    "Note that the value of the hyperparameter is fixed during this process. By doing so, we can be confident in the hyperparameter's performance on the *entire* dataset. To compare multiple choices of a hyperparameter, say `m`, we run k-fold cross validation `m` times. The smallest of the `m` resulting errors corresponds to the best hyperparameter value.\n",
    "\n",
    "#### Picking K\n",
    "\n",
    "Typical choices of `k` are 5, 10, and N, where N is the number of data points. \n",
    "\n",
    "`k` = N is known as \"leave one out cross validation\", and will typically give you the best results.\n",
    "\n",
    "- In this approach, each validation set is only one point.\n",
    "- Every point gets a chance to get used as the validation set.\n",
    "\n",
    "However, `k` = N is very expensive and requires you to fit a large number of models.\n",
    "\n",
    "### Test Sets\n",
    "\n",
    "Suppose we’re researchers building a state of the art regression model. We choose a model with the lowest validation error and want to report this out to the world. Unfortunately, our validation error may be biased; that is, not representative of error on real-world data. This is because during our hyperparameter search, we were implicitly \"learning\" from our validation data by tuning our model to achieve better results. Before reporting our results, we should run our model on a special **test set** that we’ve never seen or used for any purpose whatsoever. \n",
    "\n",
    "A test set can be something that we generate ourselves, or it can be a common dataset whose true values are unknown. In the case of the former, we can split our our available data into 3 partitions: the training set, testing set, and validation set. The exact amount of data allocated to each partition varies, but a common split is 80% train, 10% test, 10% validation.\n",
    "\n",
    "Below is an implementation of extracting a training, testing and validation set using `sklearn.train_test_split` on a data matrix `X` and an array of observations `y`.\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)`\n",
    "\n",
    "`X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.11)`\n",
    "\n",
    "As a recap:\n",
    "\n",
    "- Training set used to pick parameters.\n",
    "- Validation set used to pick hyperparameters (or pick between different models).\n",
    "- Test set used to provide an unbiased error at the end.\n",
    "\n",
    "Here is an idealized relationship between training error, test error, and validation error.\n",
    "\n",
    "<img src=\"images/training_test_valid_curve.png\" alt='training_test_valid_curve' width='500'>\n",
    "\n",
    "Notice how the test error behaves similarily to the validation error. Both come from data that is unseen during the model training process, so both are fairly good estimates of real-world data. Of the two, the test error is more unbiased for the reasons mentioned above. \n",
    "\n",
    "As before, the optimal complexity level exists where validation error is minimized. Logically, we can't design a model that minimizes test error because we don't use the test set until final evaluation.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Earlier, we found an optimal model complexity by choosing the hyperparameter that minimized validation error. This was the polynomial degree `k`= 2. Tweaking the \"complexity\" was simple; it was only a matter of adjusting the polynomial degree.\n",
    "\n",
    "However, in most machine learning problems, complexity is defined differently. Today, we'll explore two different definitions of complexity: the *squared* and *absolute* magnitude of $\\theta_i$ coefficients.\n",
    "\n",
    "#### Constraining Gradient Descent\n",
    "\n",
    "Before we discuss these definitions, let's first familiarize ourselves with the concept of **constrained gradient descent**. Imagine we have a two feature model with coeffiecient weights of $\\theta_1$ and $\\theta_2$. Below we've plotted a two dimensional contour plot of the OLS loss surface -- darker areas indicate regions of lower loss. Gradient descent will find the optimal paramaters during training $(\\theta_1, \\theta_2) = \\hat\\theta_{No Reg.}$. \n",
    "\n",
    "<img src=\"images/constrained_gd.png\" alt='constrained_gd' width='700'>\n",
    "\n",
    "Suppose we arbitrarily decide that gradient descent can never land outside of the green ball.\n",
    "\n",
    "<img src=\"images/green_constrained_gd_sol.png\" alt='green_constrained_gd' width='700'> \n",
    "\n",
    "Gradient descent finds a new solution at $\\hat\\theta_{Reg.}$. This is far from the global optimal solution  $\\hat\\theta_{No Reg.}$ -- however, it is the closest point that lives in the green ball. In other words, $\\hat\\theta_{Reg.}$ is the **constrained optimal solution**.\n",
    "\n",
    "The size of this ball is completely arbitrary. Increasing its size allows for a constrained solution closer to the global optimum, and vice versa.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "<img src=\"images/small_constrained_gd.png\" alt='small_constrained_gd' width='450'>\n",
    ":::\n",
    "\n",
    "::: {.column width=\"20%\"}\n",
    "<!-- empty column to create gap -->\n",
    ":::\n",
    "\n",
    "::: {.column width=\"30%\"}\n",
    "<img src=\"images/large_constrained_gd.png\" alt='large_constrained_gd' width='425'>\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "In fact, the size of this ball is inherently linked to model complexity. A smaller ball constrains ($\\theta_1$, $\\theta_2$) more than a larger ball. This is synonymous with the behavior of a *less* complex model, which finds a solution farther from the optimum. A *larger* ball, on the other hand, is synonymous to a *more* complex model that can achieve a near optimal solution.\n",
    "\n",
    "Consider the extreme case where the radius is infinitely small. The solution to every constrained modeling problem would lie on the origin, at $(\\theta_1, \\theta_2) = (0, 0)$. This is equivalent to the constant model we studied -- the least complex of all models. In the case where the radius is infinitely large, the optimal constrained solution exists at  $\\hat\\theta_{No Reg.}$ itself! This is the solution obtained from OLS with no limitations on complexity.\n",
    "\n",
    "<img src=\"images/train_test_validation_curve_with_complexity.png\" alt='train_test_validation_curve_with_complexity' width='550'>\n",
    "\n",
    "The intercept coefficient is typically *not* constrained; $\\theta_0$ can be any value. This way, if all $\\theta_i = 0$ except $\\theta_0$, the resulting model is a constant model (and $\\theta_0$ is the mean of all observations).\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "#### The Constrained Form\n",
    "\n",
    "**Regularization** is the formal term that describes the process of limiting a model's complexity. This is done by constraining the solution of a cost function, much like how we constrained the set of permissible ($\\theta_1$, $\\theta_2$) above. **L2 Regularization**, commonly referred to as **Ridge Regression**, is the technique of constraining our model's parameters to lie within a *ball* around the origin. Formally, it is defined as \n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||$$  \n",
    "\n",
    "<center>such that $\\sum_{j=1}^{d} \\theta_j^{2} \\le Q$</center> <br />\n",
    "\n",
    "The mathematical definition of complexity in Ridge Regression is $\\sum_{j=1}^{d} \\theta_j^{2} \\le Q$. This formulation of complexity limits the total squared magnitude of the coefficients to some constant $Q$. In two dimensional space, this is $\\theta_{1}^{2} + \\theta_{2}^{2} \\le Q$. You'll recognize this as the equation of a circle with axes $\\theta_{1}, \\theta_{2}$ and radius $Q$. In higher dimensions, this circle becomes a hypersphere and is conventionally referred to as the **L2 norm ball**. Decreasing $Q$ shrinks the norm ball, and limits the complexity of the model (discussed in the previous section). Likewise, expanding the norm ball increases the allowable model complexity.\n",
    "\n",
    "Without the constraint $\\sum_{j=1}^{d} \\theta_j^{2} \\le Q$, the optimal solution is $\\hat{\\theta} = \\hat\\theta_{No Reg.}$. With an appropriate value of $Q$ applied to the constraint, the solution $\\hat{\\theta} = \\hat\\theta_{Reg.}$ is sub-optimal on the training data but generalizes better to new data.\n",
    "\n",
    "#### The Functional Form\n",
    "\n",
    "Unfortunately, the function above requires some work. It's not easy to mathematically optimize over a constraint. Instead, in most machine learning text, you'll see a different formulation of Ridge Regression.\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} \\theta_j^{2}$$  \n",
    "\n",
    "These two equations are equivalent by Lagrangian Duality (not in scope).\n",
    "\n",
    "Notice that we've replaced the constraint with a second term in our cost function. We're now minimizing a function with a regularization term that penalizes large coefficients. The $\\alpha$ factor controls the degree of regularization. In fact, $\\alpha \\approx \\frac{1}{Q}$. <br /> To understand why, let's consider these 2 extreme examples:\n",
    "\n",
    "- Assume $\\alpha \\rightarrow \\infty$. Then, $\\alpha \\sum_{j=1}^{d} \\theta_j^{2}$ dominates the cost function. To minimize this term, we set $\\theta_j = 0$ for all $j \\ge 1$. This is a very constrained model that is mathematically equivalent to the constant model. Earlier, we explained the constant model also arises when the L2 norm ball radius $Q \\rightarrow 0$.\n",
    "\n",
    "- Assume $\\alpha \\rightarrow 0$. Then, $\\alpha \\sum_{j=1}^{d} \\theta_j^{2}$ is infinitely small. Minimizing the cost function is equivalent to $\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||$. This is just OLS, and the optimal solution is the global minimum $\\hat{\\theta} = \\hat\\theta_{No Reg.}$. We showed that the global optimum is achieved when the L2 norm ball radius $Q \\rightarrow \\infty$.\n",
    "\n",
    "#### Closed Form Solution\n",
    "\n",
    "An additional benefit to Ridge Regression is that it has a closed form solution. \n",
    "\n",
    "$$\\hat\\theta_{ridge} = (X^TX + n\\alpha I)^{-1}X^TY$$\n",
    "\n",
    "This solution exists even if there is linear dependence in the columns of the data matrix. We will not derive this result in Data 100, as it involves a fair bit of matrix calculus.\n",
    "\n",
    "#### Implementation of Ridge Regression\n",
    "\n",
    "Of course, `sklearn` has a built-in implementation of Ridge Regression. Simply import the `Ridge` class of the `sklearn.linear_model` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will various Ridge Regression models on the familiar `vehicles` DataFrame from last lecture. This will help solidfy some of the theoretical concepts discussed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import pandas as pd\n",
    "vehicles = pd.read_csv(\"data/vehicle_data.csv\", index_col=0)\n",
    "vehicles_mpg = pd.read_csv(\"data/vehicle_mpg.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>cylinders^2</th>\n",
       "      <th>displacement^2</th>\n",
       "      <th>horsepower^2</th>\n",
       "      <th>weight^2</th>\n",
       "      <th>acceleration^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>64</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>16900.0</td>\n",
       "      <td>12278016</td>\n",
       "      <td>144.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "      <td>122500.0</td>\n",
       "      <td>27225.0</td>\n",
       "      <td>13638249</td>\n",
       "      <td>132.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>64</td>\n",
       "      <td>101124.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>11806096</td>\n",
       "      <td>121.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>64</td>\n",
       "      <td>92416.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>11785489</td>\n",
       "      <td>144.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>64</td>\n",
       "      <td>91204.0</td>\n",
       "      <td>19600.0</td>\n",
       "      <td>11895601</td>\n",
       "      <td>110.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cylinders  displacement  horsepower  weight  acceleration  cylinders^2  \\\n",
       "0          8         307.0       130.0    3504          12.0           64   \n",
       "1          8         350.0       165.0    3693          11.5           64   \n",
       "2          8         318.0       150.0    3436          11.0           64   \n",
       "3          8         304.0       150.0    3433          12.0           64   \n",
       "4          8         302.0       140.0    3449          10.5           64   \n",
       "\n",
       "   displacement^2  horsepower^2  weight^2  acceleration^2  \n",
       "0         94249.0       16900.0  12278016          144.00  \n",
       "1        122500.0       27225.0  13638249          132.25  \n",
       "2        101124.0       22500.0  11806096          121.00  \n",
       "3         92416.0       22500.0  11785489          144.00  \n",
       "4         91204.0       19600.0  11895601          110.25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we fit an extremeley regularized model without an intercept. Note the small coefficient values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.56292915e-04, -5.92399474e-02, -9.81013894e-02,\n",
       "        -9.66985253e-03, -5.08353226e-03,  1.49576895e-02,\n",
       "         1.04959034e-04,  1.14786826e-04,  9.07086742e-07,\n",
       "        -4.60397349e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model_large_reg = Ridge(alpha = 10000)\n",
    "ridge_model_large_reg.fit(vehicles, vehicles_mpg)\n",
    "ridge_model_large_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how Ridge Regression effectively **spreads a small weight across many features**. \n",
    "\n",
    "When we apply very little regularization, our coefficients increase in size. Notice how they are identical to the coefficients retrieved from the `LinearRegression` model. This indicates the radius $Q$ of the L2 norm ball is massive and encompasses the unregularized optimal solution. Once again, we see that $\\alpha$ and $Q$ are inversely related.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.06754383e-01, -6.32025048e-02, -2.92851012e-01,\n",
       "        -3.41032156e-03, -1.43877512e+00,  1.25829303e-01,\n",
       "         7.72841216e-05,  6.99398090e-04,  3.11031744e-07,\n",
       "         3.16084838e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model_small_reg = Ridge(alpha = 10**-5)\n",
    "ridge_model_small_reg.fit(vehicles, vehicles_mpg)\n",
    "ridge_model_small_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.06756280e-01, -6.32024872e-02, -2.92851021e-01,\n",
       "        -3.41032211e-03, -1.43877559e+00,  1.25829450e-01,\n",
       "         7.72840884e-05,  6.99398114e-04,  3.11031832e-07,\n",
       "         3.16084973e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(vehicles, vehicles_mpg)\n",
    "linear_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data for Regularization\n",
    "\n",
    "One issue with our approach is that our features are on vastly different scales. For example, `weight^2` is in the millions, while the number of `cylinders` are under 10. Intuitively, the coefficient value for `weight^2` must be very small to offset the large magnitude of the feature. On the other hand, the coefficient of the `cylinders` feature is likely quite large in comparison. We see these claims are true in the `LinearRegression` model above.\n",
    "\n",
    "However, a problem arises in Ridge Regression. If we constrain our coefficients to a small region around the origin, we are unfairly restricting larger coefficients -- like that of the `cylinders` feature. A smaller coefficient -- that of the `weight^2` feature -- likely lies within this region, so the value changes very little. Compare the coefficients of the regularized and unregularized `Ridge` models above, and you'll see this is true.\n",
    "\n",
    "Therefore, it's imperative to standardize your data. We can do so using z-scores. \n",
    "\n",
    "$$z_k = \\frac{x_k - u_k}{\\sigma_k}$$\n",
    "\n",
    "You’ll do this on lab 8 using the “StandardScaler” transformer. The resulting model coefficients will be all on the same scale.\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "**L1 Regularization**, commonly referred to as **Lasso Regression**, is an alternate regularization technique that limits the the *absolute* sum of $\\theta_i$ coefficients. \n",
    "\n",
    "#### The Constrained Form\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n} || Y - X\\theta ||$$  \n",
    "\n",
    "<center>such that $\\sum_{j=1}^{d} |\\theta_j| \\le Q$</center> <br />\n",
    "\n",
    "In two dimensions, our constraint equation is $|\\theta_1| + |\\theta_2| \\le Q$. This is the graph of a diamond centered on the origin with endpoints $Q$ units away on each axis.\n",
    "\n",
    "<img src=\"images/lasso_constrained.png\" alt='lasso_constrained' width='700'>\n",
    "\n",
    "#### The Functional Form\n",
    "\n",
    "A more convenient way to express Lasso Regression is as follows:\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n} || Y - X\\theta || + \\alpha \\sum_{j=1}^{d} |\\theta_j|$$  \n",
    "\n",
    "As with Ridge Regression, the hyperparameter $\\alpha$ has the same effect on Lasso Regression. That is, *increasing* &nbsp;$\\alpha$ (equivalently, *decreasing* &nbsp;$Q$) *increases* the amount of regularization, and vice versa. \n",
    "\n",
    "Unfortunately, Lasso Regression does not have a closed form solution -- the cost function is not differentiable everywhere. Specifically, the sum $\\sum_{j=1}^{d} |\\theta_j|$ is problematic because it is composed of absolute value functions, each of which are non-differentiable at the origin. \n",
    "\n",
    "So why use Lasso Regression? As we'll see shortly, it is great at implicit **feature selection**.\n",
    "\n",
    "#### Implementation of Lasso Regression\n",
    "\n",
    "Lasso Regression is great at reducing complexity by eliminating the least important features in a model. It does so by setting their respective feature weights to $0$. See the following example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14009981, -0.28452369, -1.14351999, -4.11329618,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        ,  0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_model = Lasso(alpha = 1)\n",
    "\n",
    "standardized_vehicles=(vehicles-vehicles.mean())/vehicles.std()\n",
    "lasso_model.fit(standardized_vehicles, vehicles_mpg)\n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we standardized our data first. Lasso Regression then set the coefficients of our squared features to $0$ -- presumably, these are the least important predictors of `mpg`.\n",
    "\n",
    "### Summary of Regularization Methods\n",
    "\n",
    "Understanding the distinction between Ridge Regression and Lasso Regression is important. We've provided a helpful visual that summarizes the key differences.\n",
    "\n",
    "![](https://people.eecs.berkeley.edu/~jrs/189/ridgelassoItayEvron.gif)\n",
    "\n",
    "This diagram displays the L1 and L2 constrained solution for various orientations of the OLS loss surface. Notice how the L1 (Lasso) solution almost always lies on some axis, or edge of the diamond. Graphically, this makes sense; the edges of the diamond are the farthest from the origin, and usually closest to the global optimum. When this happens, only one feature has a non-zero coefficient; this argument extends quite nicely to multiple features in higher dimensional space.\n",
    "\n",
    "The L2 (Ridge) solution, however, typically has an optimal solution in some quadrant of the graph. Every point on the circumference of the L2 norm ball is equidistant from the origin, and thus similar in distance to the global optimum. As such, this technique of regularization is great at distributing a small weight across both features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
