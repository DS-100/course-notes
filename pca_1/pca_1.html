<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>PCA I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="pca_1_files/libs/clipboard/clipboard.min.js"></script>
<script src="pca_1_files/libs/quarto-html/quarto.js"></script>
<script src="pca_1_files/libs/quarto-html/popper.min.js"></script>
<script src="pca_1_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="pca_1_files/libs/quarto-html/anchor.min.js"></script>
<link href="pca_1_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="pca_1_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="pca_1_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="pca_1_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="pca_1_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#dimensionality" id="toc-dimensionality" class="nav-link active" data-scroll-target="#dimensionality">Dimensionality</a></li>
  <li><a href="#matrix-decomposition-factorization" id="toc-matrix-decomposition-factorization" class="nav-link" data-scroll-target="#matrix-decomposition-factorization">Matrix Decomposition (Factorization)</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a>
  <ul>
  <li><a href="#pca-procedure-overview" id="toc-pca-procedure-overview" class="nav-link" data-scroll-target="#pca-procedure-overview">PCA Procedure (Overview)</a></li>
  <li><a href="#deriving-pca-as-error-minimization" id="toc-deriving-pca-as-error-minimization" class="nav-link" data-scroll-target="#deriving-pca-as-error-minimization">Deriving PCA as Error Minimization</a></li>
  </ul></li>
  <li><a href="#singular-value-decomposition-svd" id="toc-singular-value-decomposition-svd" class="nav-link" data-scroll-target="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a>
  <ul>
  <li><a href="#u" id="toc-u" class="nav-link" data-scroll-target="#u"><span class="math inline">\(U\)</span></a></li>
  <li><a href="#sigma" id="toc-sigma" class="nav-link" data-scroll-target="#sigma"><span class="math inline">\(\Sigma\)</span></a></li>
  <li><a href="#vt" id="toc-vt" class="nav-link" data-scroll-target="#vt"><span class="math inline">\(V^T\)</span></a></li>
  <li><a href="#svd-geometric-perspective" id="toc-svd-geometric-perspective" class="nav-link" data-scroll-target="#svd-geometric-perspective">SVD: Geometric Perspective</a></li>
  <li><a href="#svd-in-numpy" id="toc-svd-in-numpy" class="nav-link" data-scroll-target="#svd-in-numpy">SVD in <code>NumPy</code></a></li>
  </ul></li>
  <li><a href="#pca-with-svd" id="toc-pca-with-svd" class="nav-link" data-scroll-target="#pca-with-svd">PCA with SVD</a></li>
  <li><a href="#bonus-pca-vs.-regression" id="toc-bonus-pca-vs.-regression" class="nav-link" data-scroll-target="#bonus-pca-vs.-regression">(Bonus) PCA vs.&nbsp;Regression</a>
  <ul>
  <li><a href="#regression-minimizing-horizontalverticle-error" id="toc-regression-minimizing-horizontalverticle-error" class="nav-link" data-scroll-target="#regression-minimizing-horizontalverticle-error">Regression: Minimizing Horizontal/Verticle Error</a></li>
  <li><a href="#svd-minimizing-perpendicular-error" id="toc-svd-minimizing-perpendicular-error" class="nav-link" data-scroll-target="#svd-minimizing-perpendicular-error">SVD: Minimizing Perpendicular Error</a></li>
  <li><a href="#beyond-1d-and-2d" id="toc-beyond-1d-and-2d" class="nav-link" data-scroll-target="#beyond-1d-and-2d">Beyond 1D and 2D</a></li>
  </ul></li>
  <li><a href="#bonus-automatic-factorization" id="toc-bonus-automatic-factorization" class="nav-link" data-scroll-target="#bonus-automatic-factorization">(Bonus) Automatic Factorization</a></li>
  <li><a href="#bonus-proof-of-component-score" id="toc-bonus-proof-of-component-score" class="nav-link" data-scroll-target="#bonus-proof-of-component-score">(Bonus) Proof of Component Score</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">PCA I</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Outcomes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Discuss the dimensionality of a dataset and strategies for dimensionality reduction</li>
<li>Define and carry out the procedure of PCA</li>
<li>Understand the connection between PCA and SVD</li>
</ul>
</div>
</div>
<section id="dimensionality" class="level2">
<h2 class="anchored" data-anchor-id="dimensionality">Dimensionality</h2>
<p>Previously, we have been working with data tables with rows and columns. These rows and columns correspond to observations and attributes about said observations. Now, we have to be a bit more clear with our wording to follow the language of linear algebra.</p>
<p>Suppose we have a dataset of:</p>
<ul>
<li>N observations (data points/rows)</li>
<li>d attributes (features/columns)</li>
</ul>
<p>In Linear Algebra, we think of data being a collection of vectors. Vectors have a <em>dimension</em>, meaning they have some number of unique elements. Row and column now denote the direction a vector is written (horizontally, like a row, or vertically, like a column):</p>
<p>Linear Algebra views our data as a matrix:</p>
<ul>
<li>N row vectors in a d-Dimensional space, OR</li>
<li>d column vectors in an N-Dimensions space</li>
</ul>
<p><strong>Dimensionality</strong> of data is a complex topic. Sometimes, it is clear from looking at the number of rows/columns, but other times it is not.</p>
<center>
<img src="images/dataset_dims.png" width="600vw">
</center>
<p><br> So far, we’ve used visualizations like rugplots or scatterplots to help us identify clusters in our dataset. Since we humans are 3D beings, we can’t visualize beyond three dimensions, but many datasets come with more than three features. In order to visualize multidimensional data, we can reduce the dataset to lower dimensions through <strong>dimensionality reduction</strong>.</p>
<p>For example, the dataset below has 4 columns, but the <code>Weight (lbs)</code> column is actually just a linear transformation of the <code>Weight (kg)</code> column. Thus, no new information is captured, and the matrix of our dataset has a (column) rank of 3! Therefore, despite having 4 columns, we still say that this data is 3-dimensional.</p>
<center>
<img src="images/dataset4.png" width="400vw">
</center>
<p>Plotting the weight columns together reveals the key visual intuition. While the two columns visually span a 2D space as a line, the data does not deviate at all from that singular line. This means that one of the weight columns is redundant! Even given the option to cover the whole 2D space, the data below does not. It might as well not have this dimension, which is why we still do not consider the data below to span more than 1 dimension.</p>
<center>
<img src="images/dataset3.png" width="400vw">
</center>
<p>What happens when there are outliers? Below, we’ve added one outlier point to the dataset above, and just that is enough to change the rank of the matrix from 1 to 2 dimensions. However, the data is still very much 1-dimensional. Here, we can say that the data is intinsically 1-dimensional.</p>
<p>The <strong>Intrinsic Dimension</strong> of a dataset is the minimal set of dimensions needed to approximately represent the data.</p>
<center>
<img src="images/dataset3_outlier.png" width="400vw">
</center>
<p>As such, dimensionality reduction is generally an <em>approximation</em> of the original data that’s achieved by projecting the data onto a desired dimension. However, there are many ways to project a dataset onto a lower dimension. How do we choose the best one?</p>
<center>
<img src="images/diff_reductions.png" width="800vw">
</center>
<p><br> In general, we want the projection that is the best approximation for the original data (the graph on the right). In other words, we want the projection that captures the most variance of the original data. In the next section, we’ll see how this is calculated.</p>
</section>
<section id="matrix-decomposition-factorization" class="level2">
<h2 class="anchored" data-anchor-id="matrix-decomposition-factorization">Matrix Decomposition (Factorization)</h2>
<p>One linear technique to dimensionality reduction is via matrix decomposition, which is closely tied with matrix multiplication. In this section, we will decompose our data matrix X into a lower-dimensional matrix Z that when multiplied by W approximately recovers the original data.</p>
<center>
<img src="images/factorization.png" width="800vw">
</center>
<p>First, consider the matrix multiplication example below:</p>
<center>
<img src="images/matmul.png" width="700vw">
</center>
<ul>
<li>For table 1, each row of the fruits matrix represents one bowl of fruit; for example, the first bowl/row has 2 apples, 2 lemons, and 2 melons.</li>
<li>For table 2, each column of the dollars matrix represents the cost of fruit at a store; for example, the first store/column charges 2 dollars for an apple, 1 dollar for a lemon, and 4 dollars for a melon.</li>
<li>The output is the cost of each bowl at each store.</li>
</ul>
<p>In general, there are two ways to interpret matrix multiplication:</p>
<ol type="1">
<li>Row dot column to get each datapoint. In this view, we perform multiple linear operations on the data
<center>
<img src="images/matmul2.png" width="550vw">
</center></li>
<li>Linear transformation of columns
<center>
<img src="images/matmul3.png" width="550vw">
</center></li>
</ol>
<p>We will be using the second interpretation to link matrix multiplication with matrix decomposition, where we receive a lower dimensional representation of data along with a transformation matrix.</p>
<p>Matrix factorization is the opposite of matrix multiplication. Just like real numbers, there exists infinite ways to decompose a matrix into a product of two matrices. That said, the rank of the matrix puts certain constraints on the factorization.</p>
<center>
<img src="images/factorization_constraints.png" width="550vw">
</center>
<p>In the above image, you can see how at the higher end, when we approximate our data to have higher dimensions than the rank of the data, we will get a column of 0s. This decomposition works mathematically but goes against the idea of dimensionality reduction. At the lower end, in a 1D representation, we fail to generate an accurate decomposition as now we have lower number of dimensions than the rank of the data.</p>
<p>In practice we will be working with a lot of features in our dataset, so we want to usually construct decompositions where the dimensionality is below the rank of the original matrix. While we will not be able to recover our data exactly, we will still be able to provide approximate reconstructions of the matrix.</p>
<p>In the next section, we will discuss a method to automatically and approximately factorize given data. Since some approximations are better than the others, we will also discuss how the method helps us capture a lot of information in low number of dimensions.</p>
</section>
<section id="principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>In PCA, our goal is to transform observations from high-dimensional data down to low dimensions (often 2, as most visualizations are 2D) through linear transformations. In other words, we want to find a linear transformation that creates a low-dimension representation that captures as much of the original data’s <em>total variance</em> as possible.</p>
<center>
<img src="images/PCA_1.png" width="600vw">
</center>
<p>We often perform PCA during the Exploratory Data Analysis (EDA) stage of our data science lifecycle when we don’t know what model to use. It helps us with:</p>
<ul>
<li>Visually identifying clusters of similar observations in high dimensions.</li>
<li>Removing irrelevant dimensions if we suspect that the dataset is inherently low rank. For example, if the columns are collinear, there are many attributes but only a few mostly determine the rest through linear associations.</li>
<li>Creating a transformed dataset of decorrelated features.</li>
</ul>
<center>
<img src="images/pca_example.png" width="550vw">
</center>
<p>There are two equivalent ways of framing PCA:</p>
<ul>
<li>Finding directions of <strong>maximum variability</strong> in the data</li>
<li>Finding the low dimensional (rank) matrix factorization that <strong>best approximates the data</strong></li>
</ul>
<p>In the first approach, we can find the variances of each attribute and then keep the two attributes with the highest variance. However, in this appraoch, we are limited to just working with attributes individually not as a combination. Following the PCA procedure (as described in the next section), can help us capture the most variance in the data (even higher than first approach) by using <strong>linear combinations of features</strong></p>
<section id="pca-procedure-overview" class="level3">
<h3 class="anchored" data-anchor-id="pca-procedure-overview">PCA Procedure (Overview)</h3>
<p>To perform PCA on a matrix:</p>
<ol type="1">
<li>Center the data matrix by subtracting the mean of each attribute column.</li>
<li>To find the <span class="math inline">\(i\)</span>-th principal component <span class="math inline">\(v_i\)</span>:
<ol type="1">
<li><span class="math inline">\(v\)</span> is a unit vector that linearly combines the attributes.</li>
<li><span class="math inline">\(v\)</span> gives a one-dimensional projection of the data.</li>
<li><span class="math inline">\(v\)</span> is chosen to minimize the sum of squared distances between each point and its projection onto <span class="math inline">\(v\)</span>.</li>
<li>Choose <span class="math inline">\(v\)</span> such that it is orthogonal to all previous principal components.</li>
</ol></li>
</ol>
<p>The <span class="math inline">\(k\)</span> principal components capture the most variance of any <span class="math inline">\(k\)</span>-dimensional reduction of the data matrix.</p>
<p>In practice, however, we don’t carry out the procedures in step 2 because they take too long to compute. Instead, we use singular value decomposition (SVD) to efficiently find all principal components.</p>
</section>
<section id="deriving-pca-as-error-minimization" class="level3">
<h3 class="anchored" data-anchor-id="deriving-pca-as-error-minimization">Deriving PCA as Error Minimization</h3>
<p>In this section, we will derive PCA keeping the following goal in mind: Minimize the reconstruction loss for our matrix factorization model. The reconstruction loss is defined below:</p>
<center>
<img src="images/loss.png" width="550vw">
</center>
<p><span class="math display">\[\begin{aligned}
L(Z,W) &amp;= \frac{1}{n}\sum_{i=1}^{n}||X_i - Z_{i}W||^2 \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(X_i - Z_{i}W)(X_i - Z_{i}W)^T \\
\end{aligned}\]</span></p>
<p>There exists many solutions to the above but we will constrain our model such that W is a row-orthonormal matrix (i.e.&nbsp;<span class="math inline">\(WW^T=I\)</span>) where the rows of W are our Principal Components.</p>
<p>In our derivation, lets first work with the case where <span class="math inline">\(k=1\)</span>. Here Z will be an <span class="math inline">\(nx1\)</span> vector and W will be a <span class="math inline">\(1xd\)</span> vector.</p>
<p><span class="math display">\[\begin{aligned}
L(z,w) &amp;= \frac{1}{n}\sum_{i=1}^{n}(X_i - z_{i}w)(X_i - z_{i}w)^T \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(X_{i}X_{i}^T - 2z_{i}X_{i}w^T + z_{i}^{2}ww^T)\\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(-2z_{i}X_{i}w^T + z_{i}^{2}) &amp; \text{(First term is constant and }ww^T=1\text{)} \\
\frac{\partial{L(Z,W)}}{\partial{z_i}} &amp;= \frac{1}{n}(-2z_{i}X_{i}w^T + 2z_{i}) \\
z_i &amp;= X_iw^T &amp; \text{(Setting derivative equal to 0 and solving for }z_i\text{)}
\end{aligned}\]</span></p>
<p>We can now substitute our solution for <span class="math inline">\(z_i\)</span> in our loss function:</p>
<p><span class="math display">\[\begin{aligned}
L(z,w) &amp;= \frac{1}{n}\sum_{i=1}^{n}(-2z_{i}X_{i}w^T + z_{i}^{2}) \\
L(z=X_iw^T,w) &amp;= \frac{1}{n}\sum_{i=1}^{n}(-2X_iw^TX_{i}w^T + (X_iw^T)^{2}) \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(-X_iw^TX_{i}w^T) \\
&amp;= \frac{1}{n}\sum_{i=1}^{n}(-wX_{i}^TX_{i}w^T) \\
&amp;= -w\frac{1}{n}\sum_{i=1}^{n}(X_i^TX_{i})w^T \\
&amp;= -w\Sigma w^T
\end{aligned}\]</span></p>
<p>Now, we need to minimize our loss with respect to w. Since we have a negative sign, one way we can do this is by making w really big but we also have the orthonormality constraint <span class="math inline">\(ww^T=1\)</span>. We can add this constraint by using <span class="math inline">\(\lambda\)</span>, our Lagrange multiplier.</p>
<p><span class="math display">\[\begin{aligned}
L(w,\lambda) &amp;= -w\Sigma w^T + \lambda(ww^T-1) \\
\frac{\partial{L(w,\lambda)}}{w} &amp;= -2\Sigma w^T + 2\lambda w^T \\
2\Sigma w^T - 2\lambda w^T &amp;= 0 &amp; \text{(Setting derivative equal to 0)} \\
\Sigma w^T &amp;= \lambda w^T \\
\end{aligned}\]</span></p>
<p>The result implies that:</p>
<ul>
<li>w is a <strong>unitary eigenvector</strong> of the covariance matrix and</li>
<li>the error is minimized when w is the eigenvector with the largest eigenvalue <span class="math inline">\(\lambda\)</span></li>
</ul>
<p>This derivation can inductively be used for the next principal component (not shown).</p>
<p>The final takeaway from this derivation is that the <strong>principal components</strong> are the <strong>eigenvectors</strong> with the <strong>largest eigenvalues</strong> of the <strong>covariance matrix</strong>. These are the <strong>directions</strong> of the <strong>maximum variance</strong> of the data.</p>
</section>
</section>
<section id="singular-value-decomposition-svd" class="level2">
<h2 class="anchored" data-anchor-id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h2>
<p>Singular value decomposition (SVD) is an important concept in linear algebra. Since this class requires a linear algebra course (MATH 54, 56 or EECS 16A) as a pre/co-requisite, we assume you have taken or are taking a linear algebra course, so we won’t explain SVD in its entirety. In particular, we will go over:</p>
<ul>
<li>Why SVD is a valid decomposition of rectangular matrices</li>
<li>Why PCA is an application of SVD.</li>
</ul>
<p>We will not go much into the theory and details of SVD. Instead, we will only cover what is needed for a data science interpretation. If you’d like more information, check out <a href="https://eecs16b.org/notes/sp23/note14.pdf">EECS 16B Note 14</a> or <a href="https://eecs16b.org/notes/sp23/note15.pdf">EECS 16B Note 15</a>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
[Linear Algebra] Orthonormality
</div>
</div>
<div class="callout-body-container callout-body">
<p>Orthonormal is a combination of two words: orthogonal and normal.</p>
When we say the columns of a matrix are orthonormal, we say that 1. The columns are all orthogonal to each other (all pairs of columns have a dot product of zero) 2. All columns are unit vectors (the length of each column vector is 1)
<center>
<img src="images/orthonormal.png" width="400vw">
</center>
<p>Orthonormal matrices have a few important properties</p>
<ul>
<li><strong>Orthonormal inverse</strong>: If an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(Q\)</span> has orthonormal columns, <span class="math inline">\(QQ^T= Iₘ\)</span> and <span class="math inline">\(Q^TQ=Iₙ\)</span>.</li>
<li><strong>Rotation of coordinates</strong>: the linear transformation represented by an orthonormal matrix is often a rotation (and less often a reflection). We can imagine columns of the matrix as where the unit vectors of the original space will land.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
[Linear Algebra] Diagnomal Matrices
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Diagonal matrices</strong> are square matrices with non-zero values on the diagonal axis and zero everywhere else.</p>
<p>Right-multiplied diagonal matrices scale each column up or down by a constant factor. Geometrically, this transformation can be viewed as scaling the coordinate system.</p>
<center>
<img src="images/diag_matrix.png" width="600vw">
</center>
</div>
</div>
<p>Singular value decomposition (SVD) describes a matrix <span class="math inline">\(X\)</span>’s decomposition into three matrices: <span class="math display">\[ X = U \Sigma V^T \]</span></p>
<p>Let’s break down each of these terms one by one.</p>
<section id="u" class="level3">
<h3 class="anchored" data-anchor-id="u"><span class="math inline">\(U\)</span></h3>
<ul>
<li><span class="math inline">\(U\)</span> is an <span class="math inline">\(n \times d\)</span> matrix: <span class="math inline">\(U \in \mathbb{R}^{n \times d}\)</span>.</li>
<li>Its columns are <strong>orthonormal</strong>
<ul>
<li><span class="math inline">\(\bar{u_i}^T\bar{u_j} = 0\)</span> for all pairs <span class="math inline">\(i, j\)</span></li>
<li>all vectors <span class="math inline">\(\bar{u_i}\)</span> are unit vectors with length = 1.</li>
</ul></li>
<li>Columns of U are called the <strong>left singular vectors</strong>.</li>
<li><span class="math inline">\(UU^T = I_n\)</span> and <span class="math inline">\(U^TU = I_d\)</span>.</li>
<li>We can think of <span class="math inline">\(U\)</span> as a rotation.</li>
</ul>
<center>
<img src="images/u.png" width="600vw">
</center>
</section>
<section id="sigma" class="level3">
<h3 class="anchored" data-anchor-id="sigma"><span class="math inline">\(\Sigma\)</span></h3>
<ul>
<li><span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(d \times d\)</span> matrix: <span class="math inline">\(\Sigma \in \mathbb{R}^{d \times d}\)</span>.</li>
<li>The majority of the matrix is zero</li>
<li>It has <span class="math inline">\(r\)</span> <strong>non-zero</strong> <strong>singular values</strong>, and <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(X\)</span></li>
<li>Diagonal values (<strong>singular values</strong> <span class="math inline">\(\sigma_1, \sigma_2, ... \sigma_r\)</span>), are ordered from greatest to least <span class="math inline">\(\sigma_1 &gt; \sigma_2 &gt; ... &gt; \sigma_r\)</span></li>
<li>We can think of <span class="math inline">\(\Sigma\)</span> as scaling.</li>
</ul>
<center>
<img src="images/sigma.png" width="400vw">
</center>
</section>
<section id="vt" class="level3">
<h3 class="anchored" data-anchor-id="vt"><span class="math inline">\(V^T\)</span></h3>
<ul>
<li><span class="math inline">\(V^T\)</span> is an <span class="math inline">\(d \times d\)</span> matrix: <span class="math inline">\(V \in \mathbb{R}^{d \times d}\)</span>.</li>
<li>Columns of <span class="math inline">\(V\)</span> are orthonormal, so the rows of <span class="math inline">\(V^T\)</span> are orthonormal</li>
<li>Columns of <span class="math inline">\(V\)</span> are called the <strong>right singular vectors</strong>.</li>
<li><span class="math inline">\(VV^T = V^TV = I_d\)</span></li>
<li>We can think of <span class="math inline">\(V\)</span> as a rotation.</li>
</ul>
<center>
<img src="images/v.png" width="300vw">
</center>
</section>
<section id="svd-geometric-perspective" class="level3">
<h3 class="anchored" data-anchor-id="svd-geometric-perspective">SVD: Geometric Perspective</h3>
<center>
<img src="images/svd.png" width="500vw">
</center>
<br> We’ve seen that <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> represent rotations, and <span class="math inline">\(\Sigma\)</span> represents scaling. Therefore, SVD says that any matrix can be decomposed into a rotation, then a scaling, and another rotation.
<center>
<img src="images/svd_geo.png" width="600vw">
</center>
</section>
<section id="svd-in-numpy" class="level3">
<h3 class="anchored" data-anchor-id="svd-in-numpy">SVD in <code>NumPy</code></h3>
<p>For this demo, we’ll continue working with our rectangular dataset from before with <span class="math inline">\(n=100\)</span> rows and <span class="math inline">\(d=4\)</span> columns.</p>
<div id="cell-2" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">23</span>) <span class="co">#kallisti</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.dpi'</span>] <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>rectangle <span class="op">=</span> pd.read_csv(<span class="st">"data/rectangle_data.csv"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>rectangle.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In <code>NumPy</code>, the SVD algorithm is already written and can be called with <code>np.linalg.svd</code> (<a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html">documentation</a>). There are multiple versions of SVD; to get the version that we will follow, we need to set the <code>full_matrices</code> parameter to <code>False</code>.</p>
<div id="cell-4" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(rectangle, full_matrices <span class="op">=</span> <span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First, let’s examine <code>U</code>. As we can see, it’s dimensions are <span class="math inline">\(n \times d\)</span>.</p>
<div id="cell-6" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>U.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The first 5 rows of <code>U</code> are shown below.</p>
<div id="cell-8" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(U).head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math inline">\(\Sigma\)</span> is a little different in <code>NumPy</code>. Since the only useful values in the diagonal matrix <span class="math inline">\(\Sigma\)</span> are the singular values on the diagonal axis, only those values are returned and they are stored in an array.</p>
<p>Our <code>rectangle_data</code> has a rank of <span class="math inline">\(3\)</span>, so we should have 3 non-zero singular values, <strong>sorted from largest to smallest</strong>.</p>
<div id="cell-10" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>S</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It seems like we have 4 non-zero values instead of 3, but notice that the last value is so small (<span class="math inline">\(10^{-15}\)</span>) that it’s practically <span class="math inline">\(0\)</span>. Hence, we can round the values to get 3 singular values.</p>
<div id="cell-12" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">round</span>(S)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get <code>S</code> in matrix format, we use <code>np.diag</code>.</p>
<div id="cell-14" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>Sm <span class="op">=</span> np.diag(S)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>Sm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we can see that <code>Vt</code> is indeed a <span class="math inline">\(d \times d\)</span> matrix.</p>
<div id="cell-16" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Vt.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-17" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(Vt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To check that this SVD is a valid decomposition, we can reverse it and see if it matches our original table (it does, yay!).</p>
<div id="cell-19" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(U <span class="op">@</span> Sm <span class="op">@</span> Vt).head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="pca-with-svd" class="level2">
<h2 class="anchored" data-anchor-id="pca-with-svd">PCA with SVD</h2>
<p>Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) can be easily mixed up, especially when you have to keep track of so many acronyms. Here is a quick summary:</p>
<ul>
<li>SVD: a linear algebra algorithm that splits a matrix into 3 component parts.</li>
<li>PCA: a data science procedure used for dimensionality reduction that <em>uses</em> SVD as one of the steps.</li>
</ul>
<p>In order to get the first <span class="math inline">\(k\)</span> principal components from an <span class="math inline">\(n \times d\)</span> matrix <span class="math inline">\(X\)</span>, we:</p>
<ol type="1">
<li>Center <span class="math inline">\(X\)</span> by subtracting the mean from each column. Notice how we specify <code>axis=0</code> so that the mean is computed per column.</li>
</ol>
<div id="cell-21" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>centered_df <span class="op">=</span> rectangle <span class="op">-</span> np.mean(rectangle, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>centered_df.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Get the Singular Value Decomposition of centered <span class="math inline">\(X\)</span>: <span class="math inline">\(U\)</span>, <span class="math inline">\(Σ\)</span> and <span class="math inline">\(V^T\)</span></li>
</ol>
<div id="cell-23" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>U, S, Vt <span class="op">=</span> np.linalg.svd(centered_df, full_matrices <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>Sm <span class="op">=</span> pd.DataFrame(np.diag(np.<span class="bu">round</span>(S, <span class="dv">1</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Multiply either <span class="math inline">\(UΣ\)</span> or <span class="math inline">\(XV\)</span>. Mathematically, these give the same result, but computationally, floating point approximation results in slightly different numbers for very small values (check out the right-most column in the cells below).</li>
</ol>
<div id="cell-25" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># UΣ </span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(U <span class="op">@</span> np.diag(S)).head(<span class="dv">5</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># XV</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(centered_df <span class="op">@</span> Vt.T).head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Take the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(UΣ\)</span> (or <span class="math inline">\(XV\)</span>). These are the first <span class="math inline">\(k\)</span> principal components of <span class="math inline">\(X\)</span>.</li>
</ol>
<div id="cell-28" class="cell" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>two_PCs <span class="op">=</span> (U <span class="op">@</span> np.diag(S))[:, :<span class="dv">2</span>] <span class="co"># using UΣ </span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>two_PCs <span class="op">=</span> (centered_df <span class="op">@</span> Vt.T).iloc[:, :<span class="dv">2</span>] <span class="co"># using XV</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(two_PCs).head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="bonus-pca-vs.-regression" class="level2">
<h2 class="anchored" data-anchor-id="bonus-pca-vs.-regression">(Bonus) PCA vs.&nbsp;Regression</h2>
<section id="regression-minimizing-horizontalverticle-error" class="level3">
<h3 class="anchored" data-anchor-id="regression-minimizing-horizontalverticle-error">Regression: Minimizing Horizontal/Verticle Error</h3>
<p>Suppose we know the child mortality rate of a given country. Linear regression tries to predict the fertility rate from the mortality rate; for example, if the mortality is 6, we might guess the fertility is near 4. The regression line tells us the “best” prediction of fertility given all possible mortality values by minimizing the root mean squared error [see vertical red lines, only some shown].</p>
<center>
<img src="images/lin_reg.png" width="400vw">
</center>
<p><br> We can also perform a regression in the reverse direction, that is, given the fertility, we try to predict the mortality. In this case, we get a different regression line which minimizes the root mean squared length of the horizontal lines.</p>
<center>
<img src="images/lin_reg_reverse.png" width="400vw">
</center>
</section>
<section id="svd-minimizing-perpendicular-error" class="level3">
<h3 class="anchored" data-anchor-id="svd-minimizing-perpendicular-error">SVD: Minimizing Perpendicular Error</h3>
<p>The rank 1 approximation is close but not the same as the mortality regression line. Instead of minimizing <em>horizontal</em> or <em>vertical</em> error, our rank 1 approximation minimizes the error <em>perpendicular</em> to the subspace onto which we’re projecting. That is, SVD finds the line such that if we project our data onto that line, the error between the projection and our original data is minimized. The similarity of the rank 1 approximation and the fertility was just a coincidence. Looking at adiposity and bicep size from our body measurements dataset, we see the 1D subspace onto which we are projecting is between the two regression lines.</p>
<center>
<img src="images/rank1.png" width="400vw">
</center>
</section>
<section id="beyond-1d-and-2d" class="level3">
<h3 class="anchored" data-anchor-id="beyond-1d-and-2d">Beyond 1D and 2D</h3>
<p>In higher dimensions, the idea behind principal components is just the same! Suppose we have 30-dimensional data and decide to use the first 5 principal components. Our procedure minimizes the error between the original 30-dimensional data and the projection of that 30-dimensional data on to the “best” 5-dimensional subspace. See <a href="https://www.eecs189.org/static/notes/n10.pdf">CS189</a> for more details.</p>
</section>
</section>
<section id="bonus-automatic-factorization" class="level2">
<h2 class="anchored" data-anchor-id="bonus-automatic-factorization">(Bonus) Automatic Factorization</h2>
<p>One key fact to remember is that the decomposition are not arbitrary. The <em>rank</em> of a matrix limits how small our inner dimensions can be if we want to perfectly recreate our matrix. The proof for this is out of scope.</p>
<p>Even if we know we have to factorize our matrix using an inner dimension of R, that still leaves a large space of solutions to traverse. What if we have a procedure to automatically factorize a rank R matrix into an R dimensional representation with some transformation matrix?</p>
<ul>
<li>Lower dimensional representation avoids redundant features.</li>
<li>Imagine a 1000-dimensional dataset: If the rank is only 5, it’s much easier to do EDA after this mystery procedure.</li>
</ul>
<p>What if we wanted a 2D representation? It’s valuable to compress all of the data that is relevant onto as few dimensions as possible in order to plot it efficiently. Some 2D matrices yield better approximations than others. How well can we do?</p>
</section>
<section id="bonus-proof-of-component-score" class="level2">
<h2 class="anchored" data-anchor-id="bonus-proof-of-component-score">(Bonus) Proof of Component Score</h2>
<p>The proof defining component score is out of scope for this class, but it is included below for your convenience.</p>
<p><em>Setup</em>: Consider the design matrix <span class="math inline">\(X \in \mathbb{R}^{n \times d}\)</span>, where the <span class="math inline">\(j\)</span>-th column (corresponding to the <span class="math inline">\(j\)</span>-th feature) is <span class="math inline">\(x_j \in \mathbb{R}^n\)</span> and the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> is <span class="math inline">\(x_{ij}\)</span>. Further, define <span class="math inline">\(\tilde{X}\)</span> as the <strong>centered</strong> design matrix. The <span class="math inline">\(j\)</span>-th column is <span class="math inline">\(\tilde{x}_j \in \mathbb{R}^n\)</span> and the element in row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span> is <span class="math inline">\(\tilde{x}_{ij} = x_{ij} - \bar{x_j}\)</span>, where <span class="math inline">\(\bar{x_j}\)</span> is the mean of the <span class="math inline">\(x_j\)</span> column vector from the original <span class="math inline">\(X\)</span>.</p>
<p><em>Variance</em>: Construct the <strong>covariance matrix</strong>: <span class="math inline">\(\frac{1}{n} \tilde{X}^T \tilde{X} \in \mathbb{R}^{d \times d}\)</span>. The <span class="math inline">\(j\)</span>-th element along the diagonal is the <strong>variance</strong> of the <span class="math inline">\(j\)</span>-th column of the original design matrix <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} = \frac{1}{n} \tilde{x}_j ^T \tilde{x}_j = \frac{1}{n} \sum_{i=i}^n (\tilde{x}_{ij} )^2 = \frac{1}{n} \sum_{i=i}^n (x_{ij} - \bar{x_j})^2\]</span></p>
<p><em>SVD</em>: Suppose singular value decomposition of the <em>centered</em> design matrix <span class="math inline">\(\tilde{X}\)</span> yields <span class="math inline">\(\tilde{X} = U \Sigma V^T\)</span>, where <span class="math inline">\(U \in \mathbb{R}^{n \times d}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{d \times d}\)</span> are matrices with orthonormal columns, and <span class="math inline">\(\Sigma \in \mathbb{R}^{d \times d}\)</span> is a diagonal matrix with singular values of <span class="math inline">\(\tilde{X}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
\tilde{X}^T \tilde{X} &amp;= (U \Sigma V^T )^T (U \Sigma V^T) \\
&amp;= V \Sigma U^T U \Sigma V^T  &amp; (\Sigma^T = \Sigma) \\
&amp;= V \Sigma^2 V^T &amp; (U^T U = I) \\
\frac{1}{n} \tilde{X}^T \tilde{X} &amp;= \frac{1}{n} V \Sigma V^T =V \left( \frac{1}{n} \Sigma \right) V^T \\
\frac{1}{n} \tilde{X}^T \tilde{X} V &amp;= V \left( \frac{1}{n} \Sigma \right) V^T V = V \left( \frac{1}{n} \Sigma \right) &amp; \text{(right multiply by }V \rightarrow V^T V = I \text{)} \\
V^T \frac{1}{n} \tilde{X}^T \tilde{X} V &amp;= V^T V \left( \frac{1}{n} \Sigma \right) = \frac{1}{n} \Sigma &amp; \text{(left multiply by }V^T \rightarrow V^T V = I \text{)} \\
\left( \frac{1}{n} \tilde{X}^T \tilde{X} \right)_{jj} &amp;= \frac{1}{n}\sigma_j^2  &amp; \text{(Define }\sigma_j\text{ as the} j\text{-th singular value)} \\
\frac{1}{n} \sigma_j^2 &amp;= \frac{1}{n} \sum_{i=i}^n (x_{ij} - \bar{x_j})^2
\end{aligned}\]</span></p>
<p>The last line defines the <span class="math inline">\(j\)</span>-th component score.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>