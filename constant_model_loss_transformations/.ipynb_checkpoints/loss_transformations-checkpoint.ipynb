{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Constant Model, Loss, and Transformations'\n",
    "execute:\n",
    "  echo: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    toc: true\n",
    "    toc-title: 'Constant Model, Loss, and Transformations'\n",
    "    page-layout: full\n",
    "    theme:\n",
    "      - cosmo\n",
    "      - cerulean\n",
    "    callout-icon: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note collapse=\"true\"}\n",
    "## Learning Outcomes\n",
    "* Derive the optimal model parameters for the constant model under MSE and MAE cost functions\n",
    "* Evaluate the differences between MSE and MAE risk\n",
    "* Understand the need for linearization of variables and apply the Tukey-Mosteller bulge diagram for transformations\n",
    ":::\n",
    "\n",
    "Last time, we introduced the modeling process. We set up a framework to predict target variables as functions of our features, following a set workflow:\n",
    "\n",
    "1. Choose a model\n",
    "2. Choose a loss function\n",
    "3. Fit the model\n",
    "4. Evaluate model performance\n",
    "\n",
    "To illustrate this process, we derived the optimal model parameters under simple linear regression with mean squared error as the empirical risk function. In this lecture, we'll continue familiarizing ourselves with the modeling process by finding the best model parameters under a new model. We'll also test out two different loss functions to understand how our choice of loss influences model design. Later on, we'll consider what happens when a linear model isn't the best choice to capture trends in our data – and what solutions there are to create better models.\n",
    "\n",
    "## Constant Model + MSE\n",
    "\n",
    "In today's lecture, our focus will be on the **constant model**. The constant model is slightly different from the simple linear regression model we've explored previously. Rather than generate predictions from an inputted feature variable, the constant model *predicts the same constant number every time.* We call this constant $\\theta$.\n",
    "\n",
    "$$\\hat{y} = \\theta$$\n",
    "\n",
    "$\\theta$ is the parameter of the constant model, just as $a$ and $b$ were the parameters in SLR. Our task now is to determine what value of $\\theta$ represents the optimal model – in other words, what number should we guess each time to have the lowest possible average loss on our data?\n",
    "\n",
    "Consider the case where L2 (squared) loss is used as the loss function and mean squared error is used as the empirical risk. At this stage, we're well into the modeling process:\n",
    "\n",
    "1. Choose a model: constant model\n",
    "2. Choose a loss function: L2 loss\n",
    "3. Fit the model\n",
    "4. Evaluate model performance\n",
    "\n",
    "In Homework 5, you will fit the constant model under MSE risk to find that the best choice of $\\theta$ is the **mean of the observed $y$ values**. In other words, $\\hat{\\theta} = \\bar{y}$. \n",
    "\n",
    "Let's take a moment to interpret this result. Our optimal model parameter is the value of the parameter that minimizes the empirical risk. This minimum value of risk can be expressed:\n",
    "\n",
    "$$R(\\hat{\\theta}) = \\min_{\\theta} R(\\theta)$$\n",
    "\n",
    "To restate the above in plain English: we are looking at the value of the risk function when it takes the best parameter as input. This optimal model parameter, $\\hat{\\theta}$, is the value of $\\theta$ that minimizes the risk $R$.\n",
    "\n",
    "For modeling purposes, we care less about the minimum value of risk, $R(\\hat{\\theta})$, and more about the *value of $\\theta$* that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)$$\n",
    "\n",
    "That is, we want to find the **arg**ument $\\theta$ that **min**imizes the risk function.\n",
    "\n",
    "## Constant Model + MAE \n",
    "\n",
    "We see now that changing the model used for prediction leads to a wildly different result for the optimal model parameter. What happens if we instead change the loss function used in model evaluation?\n",
    "\n",
    "This time, we will consider the constant model with L1 (absolute loss) as the loss function. This means that the average loss will be expressed as the mean absolute error. \n",
    "\n",
    "1. Choose a model: constant model\n",
    "2. Choose a loss function: L1 loss\n",
    "3. Fit the model\n",
    "4. Evaluate model performance\n",
    "\n",
    "To fit the model and find the optimal parameter value $\\hat{\\theta}$, follow the usual process of differentiating the risk function with respect to $\\theta$, setting the derivative equal to zero, and solving for $\\theta$. Writing this out in longhand:\n",
    "\n",
    "$$\\begin{align}\n",
    "R(\\theta) &= \\frac{1}{n}\\sum^{n}_{i=1} |y_i - \\theta| \\\\\n",
    "\\frac{d}{d\\theta} R(\\theta) &= \\frac{d}{d\\theta} \\left(\\frac{1}{n} \\sum^{n}_{i=1} |y_i - \\theta| \\right) \\\\\n",
    "\\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta|\n",
    "\\end{align}$$\n",
    "\n",
    "Here, we seem to have run into a problem: the derivative of an absolute value is undefined when the argument is 0 (i.e. when $y_i = \\theta$). For now, we'll ignore this issue. It turns out that ignoring this case doesn't influence our final result.\n",
    "\n",
    "To perform the derivative, consider two cases. When $\\theta$ is *less than* $y_i$, the term $y_i - \\theta$ will be positive and the absolute value has no impact. When $\\theta$ is *greater than* $y_i$, the term $y_i - \\theta$ will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying $-(y_i - \\theta) = \\theta - y_i$. \n",
    "\n",
    "$$|y_i - \\theta| = \\begin{cases} y_i - \\theta && \\text{if}\\:\\theta \\leq y_i \\\\ \\theta - y_i && \\text{if}\\:\\theta \\geq y_i \\end{cases}$$\n",
    "\n",
    "Taking derivatives:\n",
    "\n",
    "$$\\frac{d}{d\\theta} |y_i - \\theta| = \\begin{cases} -1 && \\text{if}\\:\\theta \\leq y_i \\\\ 1 && \\text{if}\\:\\theta \\geq y_i \\end{cases}$$\n",
    "\n",
    "This means that we obtain a different value for the derivative for datapoints where $\\theta < y_i$ and where $\\theta > y_i$. We can summarize this by saying:\n",
    "\n",
    "$$\\begin{align} \\frac{d}{d\\theta} R(\\theta) &= \\frac{1}{n} \\sum^{n}_{i=1} \\frac{d}{d\\theta} |y_i - \\theta| \\\\\n",
    "&= \\frac{1}{n} \\left[\\sum_{\\theta < y_i} (-1) + \\sum_{\\theta > y_i} (+1) \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "To finish finding the best value of $\\theta$, set this derivative equal to zero and solve for $\\theta$. You'll do this in Homework 5 to show that $\\hat{\\theta} = \\text{median}(y)$.\n",
    "\n",
    "## Comparing Loss Functions\n",
    "\n",
    "Now, we've tried our hand at fitting a model under both MSE and MAE risk. How do the two results compare?\n",
    "\n",
    "Let's consider a dataset where each entry represents the number of drinks sold at a bubble tea store each day. We'll fit a constant model to predict the number of drinks that will be sold tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 21, 22, 29, 33])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "import numpy as np\n",
    "drinks = np.array([20, 21, 22, 29, 33])\n",
    "drinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our derivations above, we know that the optimal model parameter under MSE risk is the mean of the dataset. Under MAE risk, the optimal parameter is the median of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.0, 22.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "np.mean(drinks), np.median(drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot each empirical risk function across several possible values of $\\theta$, we find that each $\\hat{\\theta}$ does indeed correspond to the lowest value of error:\n",
    "\n",
    "![Constant models fit using the MSE and MAE](images/error.png)\n",
    "\n",
    "Notice that the MSE risk above is a **smooth** function – it is differentiable at all points, making it easy to minimize using numerical methods. The MAE risk, in contrast, is not differentiable at each of its \"kinks.\" We'll explore how the smoothness of the empirical risk function can impact our ability to apply numerical optimization in a few weeks. \n",
    "\n",
    "How do outliers affect each loss? Imagine we replace the largest value in the dataset with 1000. The mean of the data increases substantially, while the median is nearly unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  20,   21,   22,   29,   33, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(187.5, 25.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "drinks_with_outlier = np.append(drinks, 1000)\n",
    "display(drinks_with_outlier)\n",
    "np.mean(drinks_with_outlier), np.median(drinks_with_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that under MSE risk, the optimal model parameter $\\hat{\\theta}$ is strongly affected by the presence of outliers. Under MAE risk, the optimal parameter is not as influenced by outlying data. We can generalize this by saying that the MSE is **sensitive** to outliers, while the MAE is **robust** to outliers.\n",
    "\n",
    "Let's try another experiment. This time, we'll add an additional, non-outlying datapoint to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 21, 22, 29, 33, 35])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "drinks_with_additional_observation = np.append(drinks, 35)\n",
    "drinks_with_additional_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we again visualize the empirical risk functions, we find that the MAE now plots a horizontal line between 22 and 29. This means that there are *infinitely* many optimal values for the model parameter: any value $\\hat{\\theta} \\in [22, 29]$ will minimize the MAE. In contrast, the MSE still has a single best value for $\\hat{\\theta}$. In other words, the MSE has a **unique** solution for $\\hat{\\theta}$; the MAE is not guaranteed to have a single unique solution.\n",
    "\n",
    "![Comparing the uniqueness properties of MSE and MAE](images/compare_loss.png)\n",
    "\n",
    "## Linear Transformations\n",
    "\n",
    "TODO: write section after lecture structure reworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BTW Bella, I wrote a section up on this in the Viz 2 (Transformations) Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
